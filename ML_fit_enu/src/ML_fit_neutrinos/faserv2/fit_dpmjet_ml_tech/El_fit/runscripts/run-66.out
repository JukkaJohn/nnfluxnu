data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.7244063627196207 7.9813710529351445 48.54282098357814
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 79.09626770019531 500
training loss = 74.73138427734375 1000
training loss = 74.38258361816406 1500
training loss = 73.76950073242188 2000
training loss = 72.18437957763672 2500
training loss = 54.91338348388672 3000
training loss = 6.497992515563965 3500
training loss = 4.394927978515625 4000
training loss = 3.390822172164917 4500
training loss = 2.9813826084136963 5000
training loss = 2.8572914600372314 5500
training loss = 2.8286004066467285 6000
training loss = 2.8132383823394775 6500
training loss = 2.8097083568573 7000
training loss = 2.8114142417907715 7500
training loss = 3.090074300765991 8000
training loss = 3.001079797744751 8500
training loss = 4.176860809326172 9000
training loss = 2.799464702606201 9500
training loss = 2.799037456512451 10000
training loss = 3.245466470718384 10500
training loss = 2.8162572383880615 11000
training loss = 2.8034989833831787 11500
training loss = 2.825110912322998 12000
training loss = 2.790153980255127 12500
training loss = 2.839491605758667 13000
training loss = 2.787588119506836 13500
training loss = 2.5608139038085938 14000
training loss = 2.4318647384643555 14500
training loss = 2.349924087524414 15000
training loss = 2.2816784381866455 15500
training loss = 2.21016526222229 16000
training loss = 2.138364791870117 16500
training loss = 2.0697829723358154 17000
training loss = 2.0069081783294678 17500
training loss = 1.9512240886688232 18000
training loss = 1.9035978317260742 18500
training loss = 1.8642668724060059 19000
training loss = 1.832205891609192 19500
training loss = 1.80649733543396 20000
training loss = 1.7857236862182617 20500
training loss = 1.7693425416946411 21000
training loss = 1.7563139200210571 21500
training loss = 1.746000051498413 22000
training loss = 1.7378058433532715 22500
training loss = 1.7313393354415894 23000
training loss = 1.7261974811553955 23500
training loss = 1.7220884561538696 24000
training loss = 1.718769907951355 24500
training loss = 1.7160825729370117 25000
training loss = 1.7138835191726685 25500
training loss = 1.7120610475540161 26000
training loss = 1.7105540037155151 26500
training loss = 1.7092534303665161 27000
training loss = 1.7082879543304443 27500
training loss = 1.7072211503982544 28000
training loss = 1.7063980102539062 28500
training loss = 1.7056728601455688 29000
training loss = 1.705025315284729 29500
training loss = 1.7044414281845093 30000
training loss = 1.704129695892334 30500
training loss = 1.70353102684021 31000
training loss = 1.7029846906661987 31500
training loss = 1.7025583982467651 32000
training loss = 1.7021903991699219 32500
training loss = 1.701812744140625 33000
training loss = 1.7014553546905518 33500
training loss = 1.701119065284729 34000
training loss = 1.7007927894592285 34500
training loss = 1.7004904747009277 35000
reduced chi^2 level 2 = 1.700492024421692
Constrained alpha: 2.321206569671631
Constrained beta: 4.030847549438477
Constrained gamma: 35.9001350402832
(1, 38)
(1, 0)
