data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.9967831104620024 2.619212209102766 16.925250222564337
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 111.12993621826172 500
training loss = 73.01654815673828 1000
training loss = 72.91069030761719 1500
training loss = 72.89702606201172 2000
training loss = 72.8789291381836 2500
training loss = 72.85489654541016 3000
training loss = 72.8222885131836 3500
training loss = 72.77574920654297 4000
training loss = 72.70181274414062 4500
training loss = 72.55453491210938 5000
training loss = 72.01525115966797 5500
training loss = 5.868627548217773 6000
training loss = 5.509299278259277 6500
training loss = 5.419920444488525 7000
training loss = 5.08671236038208 7500
training loss = 5.090193271636963 8000
training loss = 6.347318649291992 8500
training loss = 5.05454158782959 9000
training loss = 5.054208755493164 9500
training loss = 5.055875301361084 10000
training loss = 5.054935932159424 10500
training loss = 5.053524017333984 11000
training loss = 5.053450584411621 11500
training loss = 5.053104400634766 12000
training loss = 5.052934169769287 12500
training loss = 5.052781581878662 13000
training loss = 5.0526123046875 13500
training loss = 5.052460193634033 14000
training loss = 5.052319049835205 14500
training loss = 5.052191734313965 15000
training loss = 5.052074432373047 15500
training loss = 5.051950454711914 16000
training loss = 5.0518388748168945 16500
training loss = 5.051745891571045 17000
training loss = 5.051645755767822 17500
training loss = 5.05155086517334 18000
training loss = 5.817105770111084 18500
training loss = 5.052972793579102 19000
training loss = 5.052311897277832 19500
training loss = 5.051560878753662 20000
training loss = 5.05116605758667 20500
training loss = 5.051079273223877 21000
training loss = 5.051026821136475 21500
training loss = 5.051483154296875 22000
training loss = 5.050909996032715 22500
training loss = 5.0508270263671875 23000
training loss = 5.050755500793457 23500
training loss = 5.050704002380371 24000
training loss = 5.0506720542907715 24500
training loss = 5.050609588623047 25000
training loss = 5.050728797912598 25500
training loss = 5.238444805145264 26000
training loss = 5.050749778747559 26500
training loss = 5.051995754241943 27000
training loss = 6.023205757141113 27500
training loss = 5.050395965576172 28000
training loss = 5.05031156539917 28500
training loss = 6.910336017608643 29000
training loss = 6.813421249389648 29500
training loss = 5.229393482208252 30000
training loss = 5.6855082511901855 30500
training loss = 7.313699245452881 31000
training loss = 5.050225734710693 31500
training loss = 6.9984941482543945 32000
training loss = 5.064803123474121 32500
training loss = 5.049954891204834 33000
training loss = 5.049922943115234 33500
training loss = 5.049901962280273 34000
training loss = 5.0498785972595215 34500
training loss = 5.9526262283325195 35000
(1, 38)
(1, 0)
