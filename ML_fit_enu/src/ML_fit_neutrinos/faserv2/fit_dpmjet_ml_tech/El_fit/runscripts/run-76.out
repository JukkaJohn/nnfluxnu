data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.3506974410600555 16.305524657105995 55.008450494979236
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 202.41162109375 500
training loss = 73.47501373291016 1000
training loss = 71.68406677246094 1500
training loss = 68.84020233154297 2000
training loss = 57.05944061279297 2500
training loss = 6.243706226348877 3000
training loss = 4.405909538269043 3500
training loss = 3.719907760620117 4000
training loss = 3.524773120880127 4500
training loss = 3.4899232387542725 5000
training loss = 3.4586522579193115 5500
training loss = 3.4454867839813232 6000
training loss = 3.434796094894409 6500
training loss = 3.425863265991211 7000
training loss = 3.4181768894195557 7500
training loss = 3.411115884780884 8000
training loss = 3.4033126831054688 8500
training loss = 3.3908631801605225 9000
training loss = 3.373250961303711 9500
training loss = 3.3613812923431396 10000
training loss = 3.3511624336242676 10500
training loss = 3.34169340133667 11000
training loss = 3.3323168754577637 11500
training loss = 3.32326078414917 12000
training loss = 3.31404447555542 12500
training loss = 4.185072898864746 13000
training loss = 3.5656261444091797 13500
training loss = 3.7926998138427734 14000
training loss = 3.305891275405884 14500
training loss = 3.2844393253326416 15000
training loss = 3.253679037094116 15500
training loss = 3.2648978233337402 16000
training loss = 3.2300779819488525 16500
training loss = 3.237304210662842 17000
training loss = 3.1991868019104004 17500
training loss = 3.1829512119293213 18000
training loss = 3.1654956340789795 18500
training loss = 3.14642333984375 19000
training loss = 3.125795841217041 19500
training loss = 3.1039657592773438 20000
training loss = 3.080806255340576 20500
training loss = 7.625441551208496 21000
training loss = 3.0608131885528564 21500
training loss = 3.2196385860443115 22000
training loss = 3.7740583419799805 22500
training loss = 2.9986414909362793 23000
training loss = 5.873599529266357 23500
training loss = 2.9121246337890625 24000
training loss = 3.1056251525878906 24500
training loss = 2.8907852172851562 25000
training loss = 2.8362295627593994 25500
training loss = 2.7985928058624268 26000
training loss = 2.778496503829956 26500
training loss = 2.7601189613342285 27000
training loss = 3.1124954223632812 27500
training loss = 2.929530143737793 28000
training loss = 3.0346133708953857 28500
training loss = 2.7033445835113525 29000
training loss = 2.6929497718811035 29500
training loss = 2.6837430000305176 30000
training loss = 2.6755030155181885 30500
training loss = 2.668445348739624 31000
training loss = 2.6621789932250977 31500
training loss = 2.656137466430664 32000
training loss = 2.6508312225341797 32500
training loss = 2.646160840988159 33000
training loss = 2.6417715549468994 33500
training loss = 2.6451117992401123 34000
training loss = 4.542941093444824 34500
training loss = 2.7949655055999756 35000
(1, 38)
(1, 0)
