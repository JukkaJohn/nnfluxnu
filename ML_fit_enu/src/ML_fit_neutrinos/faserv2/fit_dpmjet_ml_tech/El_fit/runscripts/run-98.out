data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.725105231326795 9.791851172010322 19.462642807226814
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 73.96623229980469 500
training loss = 70.3116455078125 1000
training loss = 70.22345733642578 1500
training loss = 70.1200942993164 2000
training loss = 69.99230194091797 2500
training loss = 69.83576202392578 3000
training loss = 69.63829803466797 3500
training loss = 69.36209869384766 4000
training loss = 68.84636688232422 4500
training loss = 66.43995666503906 5000
training loss = 5.210969924926758 5500
training loss = 4.20800256729126 6000
training loss = 4.088893413543701 6500
training loss = 4.076300144195557 7000
training loss = 4.070687294006348 7500
training loss = 4.065476894378662 8000
training loss = 4.060329914093018 8500
training loss = 4.0553717613220215 9000
training loss = 4.050603866577148 9500
training loss = 4.0460381507873535 10000
training loss = 4.041683673858643 10500
training loss = 4.037644863128662 11000
training loss = 4.1202073097229 11500
training loss = 4.030671119689941 12000
training loss = 4.02651309967041 12500
training loss = 4.023692607879639 13000
training loss = 4.020318508148193 13500
training loss = 4.0169997215271 14000
training loss = 4.014182090759277 14500
training loss = 4.011429786682129 15000
training loss = 4.008831024169922 15500
training loss = 4.006361484527588 16000
training loss = 4.003998279571533 16500
training loss = 4.001748561859131 17000
training loss = 3.9996066093444824 17500
training loss = 3.997483015060425 18000
training loss = 3.9954752922058105 18500
training loss = 3.9935462474823 19000
training loss = 3.9916698932647705 19500
training loss = 3.989853620529175 20000
training loss = 3.988184928894043 20500
training loss = 3.990222692489624 21000
training loss = 3.9847445487976074 21500
training loss = 3.983088493347168 22000
training loss = 4.529936790466309 22500
training loss = 3.9983696937561035 23000
training loss = 3.978933334350586 23500
training loss = 3.9770121574401855 24000
training loss = 4.004815578460693 24500
training loss = 4.09708309173584 25000
training loss = 3.972543954849243 25500
training loss = 3.9711062908172607 26000
training loss = 3.9696967601776123 26500
training loss = 3.9682817459106445 27000
training loss = 3.966862678527832 27500
training loss = 3.9658448696136475 28000
training loss = 3.964069128036499 28500
training loss = 3.962646484375 29000
training loss = 3.961233139038086 29500
training loss = 3.9598536491394043 30000
training loss = 5.673257350921631 30500
training loss = 3.993070125579834 31000
training loss = 5.987344264984131 31500
training loss = 3.954005718231201 32000
training loss = 3.9525163173675537 32500
training loss = 3.9509880542755127 33000
training loss = 3.9494564533233643 33500
training loss = 3.947885751724243 34000
training loss = 3.9463038444519043 34500
training loss = 3.944666862487793 35000
(1, 38)
(1, 0)
