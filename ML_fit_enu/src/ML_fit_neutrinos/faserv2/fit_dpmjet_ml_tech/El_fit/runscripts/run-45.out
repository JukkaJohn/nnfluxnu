data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.981822862108821 7.976739088766922 66.53794645869515
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 82.85852813720703 500
training loss = 73.75831604003906 1000
training loss = 73.70680236816406 1500
training loss = 73.65326690673828 2000
training loss = 73.58454132080078 2500
training loss = 73.4977035522461 3000
training loss = 73.38824462890625 3500
training loss = 73.2472152709961 4000
training loss = 73.0536880493164 4500
training loss = 72.74182891845703 5000
training loss = 72.26275634765625 5500
training loss = 4.728935718536377 6000
training loss = 3.9764716625213623 6500
training loss = 3.607941150665283 7000
training loss = 3.418306827545166 7500
training loss = 3.2962706089019775 8000
training loss = 3.173485279083252 8500
training loss = 2.983045816421509 9000
training loss = 2.7169859409332275 9500
training loss = 2.4781322479248047 10000
training loss = 2.30171275138855 10500
training loss = 6.0529656410217285 11000
training loss = 2.5486690998077393 11500
training loss = 2.075023889541626 12000
training loss = 2.024036169052124 12500
training loss = 2.02614164352417 13000
training loss = 1.9870100021362305 13500
training loss = 1.976385474205017 14000
training loss = 1.9687650203704834 14500
training loss = 1.9663077592849731 15000
training loss = 1.9588329792022705 15500
training loss = 1.9627726078033447 16000
training loss = 1.9681777954101562 16500
training loss = 1.95172119140625 17000
training loss = 2.247267007827759 17500
training loss = 1.946933388710022 18000
training loss = 1.9577715396881104 18500
training loss = 1.9488096237182617 19000
training loss = 1.9432276487350464 19500
training loss = 1.9421230554580688 20000
training loss = 1.9411776065826416 20500
training loss = 1.94036865234375 21000
training loss = 1.9394161701202393 21500
training loss = 1.9456875324249268 22000
training loss = 1.9405840635299683 22500
training loss = 1.941149115562439 23000
training loss = 3.4734649658203125 23500
training loss = 1.9698407649993896 24000
training loss = 1.9676874876022339 24500
training loss = 1.9342968463897705 25000
training loss = 1.933699131011963 25500
training loss = 1.9954516887664795 26000
training loss = 1.932448148727417 26500
training loss = 1.932015299797058 27000
training loss = 2.6206512451171875 27500
training loss = 1.9470375776290894 28000
training loss = 1.9303388595581055 28500
training loss = 1.9334713220596313 29000
training loss = 4.382495403289795 29500
training loss = 1.9369603395462036 30000
training loss = 2.0045182704925537 30500
training loss = 1.9278006553649902 31000
training loss = 1.9473909139633179 31500
training loss = 3.0266273021698 32000
training loss = 2.1913764476776123 32500
training loss = 2.0408496856689453 33000
training loss = 2.1070854663848877 33500
training loss = 2.029362678527832 34000
training loss = 1.9250560998916626 34500
training loss = 1.9242433309555054 35000
reduced chi^2 level 2 = 1.9252865314483643
Constrained alpha: 2.298210620880127
Constrained beta: 4.054508209228516
Constrained gamma: 42.42490005493164
(1, 38)
(1, 0)
