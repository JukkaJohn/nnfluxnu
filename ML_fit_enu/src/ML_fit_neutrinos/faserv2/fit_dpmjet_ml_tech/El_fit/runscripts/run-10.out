data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.372612399554636 7.734846175300001 11.212140283572491
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 79.85520935058594 500
training loss = 75.02324676513672 1000
training loss = 74.90998840332031 1500
training loss = 74.79183959960938 2000
training loss = 74.65092468261719 2500
training loss = 74.48605346679688 3000
training loss = 74.28923797607422 3500
training loss = 74.02476501464844 4000
training loss = 73.52202606201172 4500
training loss = 71.12096405029297 5000
training loss = 5.05216646194458 5500
training loss = 3.2865214347839355 6000
training loss = 3.133563756942749 6500
training loss = 3.1044623851776123 7000
training loss = 3.087190866470337 7500
training loss = 3.074361801147461 8000
training loss = 3.064943790435791 8500
training loss = 3.057354688644409 9000
training loss = 3.051093339920044 9500
training loss = 3.045511484146118 10000
training loss = 3.0404207706451416 10500
training loss = 4.454892158508301 11000
training loss = 3.031219005584717 11500
training loss = 3.0269901752471924 12000
training loss = 3.0229830741882324 12500
training loss = 3.0191562175750732 13000
training loss = 3.0155510902404785 13500
training loss = 3.0122077465057373 14000
training loss = 3.0088717937469482 14500
training loss = 3.0058281421661377 15000
training loss = 3.0029571056365967 15500
training loss = 3.0002524852752686 16000
training loss = 2.997722864151001 16500
training loss = 2.995361566543579 17000
training loss = 2.9931299686431885 17500
training loss = 2.9910242557525635 18000
training loss = 2.989060163497925 18500
training loss = 2.9872071743011475 19000
training loss = 2.9854791164398193 19500
training loss = 2.9839065074920654 20000
training loss = 3.29202938079834 20500
training loss = 3.171180009841919 21000
training loss = 2.986321210861206 21500
training loss = 3.33203125 22000
training loss = 2.9772403240203857 22500
training loss = 2.977202892303467 23000
training loss = 2.9749252796173096 23500
training loss = 2.9741125106811523 24000
training loss = 2.9730007648468018 24500
training loss = 2.972119092941284 25000
training loss = 2.971289873123169 25500
training loss = 2.9705188274383545 26000
training loss = 2.969778537750244 26500
training loss = 2.969085693359375 27000
training loss = 2.9684793949127197 27500
training loss = 2.967846632003784 28000
training loss = 3.0662693977355957 28500
training loss = 2.9666824340820312 29000
training loss = 2.966165781021118 29500
training loss = 2.9656782150268555 30000
training loss = 2.9698827266693115 30500
training loss = 2.965111494064331 31000
training loss = 2.9644181728363037 31500
training loss = 2.963963508605957 32000
training loss = 2.9635777473449707 32500
training loss = 2.9632303714752197 33000
training loss = 5.88871431350708 33500
training loss = 3.160250663757324 34000
training loss = 3.1566154956817627 34500
training loss = 3.134289503097534 35000
(1, 38)
(1, 0)
