data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.906036461511 14.994549156738886 38.44657601417354
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 84.0791015625 500
training loss = 74.82318878173828 1000
training loss = 74.81168365478516 1500
training loss = 74.7976303100586 2000
training loss = 74.78089141845703 2500
training loss = 74.76040649414062 3000
training loss = 74.73028564453125 3500
training loss = 74.6576156616211 4000
training loss = 73.81646728515625 4500
training loss = 3.3054158687591553 5000
training loss = 2.6914196014404297 5500
training loss = 4.710754871368408 6000
training loss = 2.620633125305176 6500
training loss = 2.6593270301818848 7000
training loss = 2.5802369117736816 7500
training loss = 2.622225522994995 8000
training loss = 2.5746994018554688 8500
training loss = 2.572303295135498 9000
training loss = 2.570087194442749 9500
training loss = 2.567993640899658 10000
training loss = 2.5660271644592285 10500
training loss = 2.564208984375 11000
training loss = 2.562345504760742 11500
training loss = 2.560659408569336 12000
training loss = 2.5590267181396484 12500
training loss = 3.3000612258911133 13000
training loss = 3.253612756729126 13500
training loss = 2.3375372886657715 14000
training loss = 1.6461008787155151 14500
training loss = 1.5439403057098389 15000
training loss = 1.511460542678833 15500
training loss = 1.42838454246521 16000
training loss = 1.4090667963027954 16500
training loss = 1.3704466819763184 17000
training loss = 1.3514480590820312 17500
training loss = 1.336463212966919 18000
training loss = 1.3243324756622314 18500
training loss = 2.0611279010772705 19000
training loss = 1.3231052160263062 19500
training loss = 1.5917112827301025 20000
training loss = 1.325271725654602 20500
training loss = 4.14962100982666 21000
training loss = 1.607519507408142 21500
training loss = 1.2840772867202759 22000
training loss = 1.2748912572860718 22500
training loss = 1.2749983072280884 23000
training loss = 1.2688554525375366 23500
training loss = 1.2663884162902832 24000
training loss = 1.6475597620010376 24500
training loss = 1.2619155645370483 25000
training loss = 1.260005235671997 25500
training loss = 1.2582286596298218 26000
training loss = 1.2566245794296265 26500
training loss = 2.0456056594848633 27000
training loss = 1.2539567947387695 27500
training loss = 1.253103256225586 28000
training loss = 1.5151000022888184 28500
training loss = 1.2766352891921997 29000
training loss = 1.2509868144989014 29500
training loss = 1.248490571975708 30000
training loss = 1.281009554862976 30500
training loss = 2.039203405380249 31000
training loss = 1.3387398719787598 31500
training loss = 2.1865928173065186 32000
training loss = 1.6429307460784912 32500
training loss = 1.2564420700073242 33000
training loss = 1.2474967241287231 33500
training loss = 1.243025779724121 34000
training loss = 1.2427626848220825 34500
training loss = 1.2420228719711304 35000
reduced chi^2 level 2 = 1.2420222759246826
Constrained alpha: 2.2898073196411133
Constrained beta: 4.222975730895996
Constrained gamma: 30.187902450561523
(1, 38)
(1, 0)
