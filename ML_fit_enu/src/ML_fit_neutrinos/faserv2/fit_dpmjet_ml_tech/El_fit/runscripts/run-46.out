data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.2568543904942584 0.9835031511080117 31.506001182452415
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 75.25020599365234 500
training loss = 31.915727615356445 1000
training loss = 27.834491729736328 1500
training loss = 24.675167083740234 2000
training loss = 16.2531681060791 2500
training loss = 7.25632381439209 3000
training loss = 3.327087879180908 3500
training loss = 2.7047126293182373 4000
training loss = 2.474468469619751 4500
training loss = 2.2745020389556885 5000
training loss = 2.1417388916015625 5500
training loss = 2.032837152481079 6000
training loss = 1.9334311485290527 6500
training loss = 1.841707706451416 7000
training loss = 1.7580373287200928 7500
training loss = 1.6837408542633057 8000
training loss = 1.6179327964782715 8500
training loss = 1.5608599185943604 9000
training loss = 1.5125267505645752 9500
training loss = 1.475059151649475 10000
training loss = 1.621479868888855 10500
training loss = 1.7063703536987305 11000
training loss = 1.50718092918396 11500
training loss = 1.361321210861206 12000
training loss = 1.3435404300689697 12500
training loss = 1.3281258344650269 13000
training loss = 1.3148980140686035 13500
training loss = 1.3033562898635864 14000
training loss = 1.2936893701553345 14500
training loss = 1.284249186515808 15000
training loss = 1.2762606143951416 15500
training loss = 1.2693345546722412 16000
training loss = 1.3047173023223877 16500
training loss = 1.2630988359451294 17000
training loss = 1.2513887882232666 17500
training loss = 1.2454849481582642 18000
training loss = 1.2448803186416626 18500
training loss = 1.2356736660003662 19000
training loss = 1.231970191001892 19500
training loss = 1.2267879247665405 20000
training loss = 1.2294673919677734 20500
training loss = 1.2190030813217163 21000
training loss = 1.2116552591323853 21500
training loss = 3.6209139823913574 22000
training loss = 1.4275058507919312 22500
training loss = 1.2017974853515625 23000
training loss = 1.194022297859192 23500
training loss = 1.1964821815490723 24000
training loss = 1.186645746231079 24500
training loss = 1.1823136806488037 25000
training loss = 1.1777207851409912 25500
training loss = 1.1742362976074219 26000
training loss = 1.1645337343215942 26500
training loss = 4.524000644683838 27000
training loss = 1.2729350328445435 27500
training loss = 1.1592035293579102 28000
training loss = 1.134535789489746 28500
training loss = 1.1238288879394531 29000
training loss = 1.1171156167984009 29500
training loss = 1.2221698760986328 30000
training loss = 2.5007457733154297 30500
training loss = 1.470422625541687 31000
training loss = 1.2205767631530762 31500
training loss = 1.0748887062072754 32000
training loss = 1.0677968263626099 32500
training loss = 1.0612514019012451 33000
training loss = 1.055356502532959 33500
training loss = 1.0499807596206665 34000
training loss = 1.0453449487686157 34500
training loss = 1.0410871505737305 35000
reduced chi^2 level 2 = 1.0410772562026978
Constrained alpha: 2.325172185897827
Constrained beta: -0.3322196304798126
Constrained gamma: 29.668563842773438
(1, 38)
(1, 0)
