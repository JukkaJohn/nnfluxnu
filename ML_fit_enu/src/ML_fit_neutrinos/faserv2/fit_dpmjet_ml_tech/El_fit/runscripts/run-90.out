data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.2401413413588962 5.629687648470377 21.86340447669064
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 91.56720733642578 500
training loss = 74.29444122314453 1000
training loss = 74.11653137207031 1500
training loss = 73.98330688476562 2000
training loss = 73.80826568603516 2500
training loss = 73.57452392578125 3000
training loss = 73.2341537475586 3500
training loss = 72.57384490966797 4000
training loss = 68.62773132324219 4500
training loss = 4.914806365966797 5000
training loss = 4.323340892791748 5500
training loss = 4.205844402313232 6000
training loss = 4.180045127868652 6500
training loss = 4.163548946380615 7000
training loss = 4.145042419433594 7500
training loss = 4.123852729797363 8000
training loss = 4.100100517272949 8500
training loss = 7.3936543464660645 9000
training loss = 4.787179470062256 9500
training loss = 4.157731533050537 10000
training loss = 4.024970054626465 10500
training loss = 4.014679908752441 11000
training loss = 4.0067572593688965 11500
training loss = 4.168302536010742 12000
training loss = 3.985011577606201 12500
training loss = 3.9671742916107178 13000
training loss = 3.9543585777282715 13500
training loss = 3.947354793548584 14000
training loss = 3.9406771659851074 14500
training loss = 3.9344656467437744 15000
training loss = 3.928624153137207 15500
training loss = 3.9230258464813232 16000
training loss = 3.9176628589630127 16500
training loss = 3.912480354309082 17000
training loss = 3.907501220703125 17500
training loss = 6.268317222595215 18000
training loss = 3.9414424896240234 18500
training loss = 3.9126012325286865 19000
training loss = 3.8885252475738525 19500
training loss = 3.8838863372802734 20000
training loss = 3.879232406616211 20500
training loss = 3.874552011489868 21000
training loss = 3.976076602935791 21500
training loss = 3.864978313446045 22000
training loss = 3.860158920288086 22500
training loss = 3.9839277267456055 23000
training loss = 3.8493688106536865 23500
training loss = 3.8442864418029785 24000
training loss = 3.838348388671875 24500
training loss = 3.9376256465911865 25000
training loss = 3.824777364730835 25500
training loss = 3.8178603649139404 26000
training loss = 3.8089816570281982 26500
training loss = 3.800791025161743 27000
training loss = 3.7899186611175537 27500
training loss = 3.778485059738159 28000
training loss = 3.7653510570526123 28500
training loss = 3.749948740005493 29000
training loss = 3.731842041015625 29500
training loss = 3.7096333503723145 30000
training loss = 3.6804192066192627 30500
training loss = 3.642737865447998 31000
training loss = 3.589287281036377 31500
training loss = 3.503998279571533 32000
training loss = 3.0007946491241455 32500
training loss = 2.8024063110351562 33000
training loss = 2.6278879642486572 33500
training loss = 2.4908816814422607 34000
training loss = 2.409379720687866 34500
training loss = 2.3208065032958984 35000
(1, 38)
(1, 0)
