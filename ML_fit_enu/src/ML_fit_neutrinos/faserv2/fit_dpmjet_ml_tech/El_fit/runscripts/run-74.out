data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.607174414953045 18.917500214905616 9.252811358346236
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 182.96026611328125 500
training loss = 70.9974594116211 1000
training loss = 70.69108581542969 1500
training loss = 70.45826721191406 2000
training loss = 70.1462173461914 2500
training loss = 69.61749267578125 3000
training loss = 68.03707122802734 3500
training loss = 26.515287399291992 4000
training loss = 4.975851058959961 4500
training loss = 3.7765872478485107 5000
training loss = 3.5654313564300537 5500
training loss = 3.5344929695129395 6000
training loss = 3.5225605964660645 6500
training loss = 3.5133635997772217 7000
training loss = 3.5053462982177734 7500
training loss = 3.4982264041900635 8000
training loss = 3.4917547702789307 8500
training loss = 3.4858040809631348 9000
training loss = 3.481682300567627 9500
training loss = 3.482541561126709 10000
training loss = 3.4817614555358887 10500
training loss = 3.465644121170044 11000
training loss = 3.4613165855407715 11500
training loss = 3.4576311111450195 12000
training loss = 3.453263998031616 12500
training loss = 3.449597120285034 13000
training loss = 3.446108818054199 13500
training loss = 3.4428188800811768 14000
training loss = 3.4396965503692627 14500
training loss = 3.436753034591675 15000
training loss = 3.4339656829833984 15500
training loss = 3.4405059814453125 16000
training loss = 3.490821361541748 16500
training loss = 3.474510908126831 17000
training loss = 3.4241349697113037 17500
training loss = 3.620048999786377 18000
training loss = 4.1823248863220215 18500
training loss = 3.816072702407837 19000
training loss = 3.416012763977051 19500
training loss = 3.414208173751831 20000
training loss = 3.425703287124634 20500
training loss = 3.4144375324249268 21000
training loss = 3.4095892906188965 21500
training loss = 3.414372444152832 22000
training loss = 3.407749891281128 22500
training loss = 3.415325880050659 23000
training loss = 3.403099536895752 23500
training loss = 3.4016897678375244 24000
training loss = 3.400292158126831 24500
training loss = 3.3989293575286865 25000
training loss = 3.3976027965545654 25500
training loss = 3.396263837814331 26000
training loss = 3.3949625492095947 26500
training loss = 3.393673896789551 27000
training loss = 3.392392635345459 27500
training loss = 3.400095224380493 28000
training loss = 3.4232571125030518 28500
training loss = 3.393714427947998 29000
training loss = 3.387341260910034 29500
training loss = 3.3861083984375 30000
training loss = 3.38481068611145 30500
training loss = 3.3841753005981445 31000
training loss = 3.3822267055511475 31500
training loss = 3.381072998046875 32000
training loss = 3.3796021938323975 32500
training loss = 3.3782742023468018 33000
training loss = 3.3769233226776123 33500
training loss = 3.3755528926849365 34000
training loss = 3.3741636276245117 34500
training loss = 5.682743549346924 35000
(1, 38)
(1, 0)
