data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.8480317807652266 2.3242079778590674 30.08194729893704
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 105.06537628173828 500
training loss = 76.64282989501953 1000
training loss = 76.52473449707031 1500
training loss = 76.49776458740234 2000
training loss = 76.46199035644531 2500
training loss = 76.4089584350586 3000
training loss = 76.29298400878906 3500
training loss = 75.20137023925781 4000
training loss = 4.7710089683532715 4500
training loss = 3.8711390495300293 5000
training loss = 3.5247879028320312 5500
training loss = 3.4608559608459473 6000
training loss = 3.4481594562530518 6500
training loss = 3.440443277359009 7000
training loss = 3.4340837001800537 7500
training loss = 3.428802490234375 8000
training loss = 3.4235615730285645 8500
training loss = 3.4179606437683105 9000
training loss = 3.4124627113342285 9500
training loss = 3.4069390296936035 10000
training loss = 3.4015114307403564 10500
training loss = 3.396230697631836 11000
training loss = 3.3910257816314697 11500
training loss = 3.3855953216552734 12000
training loss = 3.3761062622070312 12500
training loss = 3.228736162185669 13000
training loss = 3.1750175952911377 13500
training loss = 3.1426994800567627 14000
training loss = 3.1143016815185547 14500
training loss = 3.087923288345337 15000
training loss = 3.0615131855010986 15500
training loss = 3.03482723236084 16000
training loss = 3.0079128742218018 16500
training loss = 2.979940891265869 17000
training loss = 2.9515819549560547 17500
training loss = 2.9224677085876465 18000
training loss = 2.893528938293457 18500
training loss = 2.8647255897521973 19000
training loss = 2.835315704345703 19500
training loss = 2.8070614337921143 20000
training loss = 2.778775215148926 20500
training loss = 2.750998020172119 21000
training loss = 2.7243239879608154 21500
training loss = 2.698561191558838 22000
training loss = 2.6743197441101074 22500
training loss = 2.6505191326141357 23000
training loss = 6.298486709594727 23500
training loss = 3.0150349140167236 24000
training loss = 2.5895256996154785 24500
training loss = 2.567579507827759 25000
training loss = 2.549788475036621 25500
training loss = 2.533431053161621 26000
training loss = 2.516493558883667 26500
training loss = 2.501457452774048 27000
training loss = 2.489316940307617 27500
training loss = 2.473489284515381 28000
training loss = 2.4608166217803955 28500
training loss = 2.44892954826355 29000
training loss = 2.437788248062134 29500
training loss = 2.4271786212921143 30000
training loss = 2.4170753955841064 30500
training loss = 2.4078304767608643 31000
training loss = 2.399139881134033 31500
training loss = 2.3912651538848877 32000
training loss = 2.3836829662323 32500
training loss = 2.3765761852264404 33000
training loss = 2.3698718547821045 33500
training loss = 2.3635332584381104 34000
training loss = 2.357652425765991 34500
training loss = 2.352250814437866 35000
(1, 38)
(1, 0)
