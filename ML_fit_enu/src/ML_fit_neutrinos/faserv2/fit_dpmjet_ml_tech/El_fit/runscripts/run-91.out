data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.98739207242749 8.943751963932867 78.3039083827872
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 78.87864685058594 500
training loss = 78.24021911621094 1000
training loss = 77.56978607177734 1500
training loss = 76.42840576171875 2000
training loss = 72.47833251953125 2500
training loss = 4.166948318481445 3000
training loss = 3.3120129108428955 3500
training loss = 3.0932676792144775 4000
training loss = 2.9979803562164307 4500
training loss = 2.9451076984405518 5000
training loss = 2.9178202152252197 5500
training loss = 2.9019429683685303 6000
training loss = 2.8916900157928467 6500
training loss = 2.8859198093414307 7000
training loss = 2.8911337852478027 7500
training loss = 2.8769118785858154 8000
training loss = 2.87422513961792 8500
training loss = 2.8723602294921875 9000
training loss = 2.870601177215576 9500
training loss = 2.955033540725708 10000
training loss = 2.9141292572021484 10500
training loss = 2.871005058288574 11000
training loss = 2.8669440746307373 11500
training loss = 2.8660407066345215 12000
training loss = 2.8655543327331543 12500
training loss = 2.8651516437530518 13000
training loss = 2.864795684814453 13500
training loss = 2.8644654750823975 14000
training loss = 2.8642115592956543 14500
training loss = 2.8639538288116455 15000
training loss = 2.8637259006500244 15500
training loss = 2.86352276802063 16000
training loss = 2.8633363246917725 16500
training loss = 2.97920560836792 17000
training loss = 2.8634228706359863 17500
training loss = 8.523909568786621 18000
training loss = 2.862889289855957 18500
training loss = 4.339694499969482 19000
training loss = 2.8695027828216553 19500
training loss = 2.8648574352264404 20000
training loss = 2.8659026622772217 20500
training loss = 2.907209634780884 21000
training loss = 7.516689777374268 21500
training loss = 2.8622212409973145 22000
training loss = 2.8621585369110107 22500
training loss = 2.86210560798645 23000
training loss = 2.8620402812957764 23500
training loss = 2.861994743347168 24000
training loss = 2.8619534969329834 24500
training loss = 2.8622329235076904 25000
training loss = 6.125125408172607 25500
training loss = 2.9186413288116455 26000
training loss = 2.863298177719116 26500
training loss = 2.8622124195098877 27000
training loss = 2.861922264099121 27500
training loss = 2.8617072105407715 28000
training loss = 2.8616695404052734 28500
training loss = 2.861661911010742 29000
training loss = 2.8616137504577637 29500
training loss = 2.8615896701812744 30000
training loss = 2.861581563949585 30500
training loss = 2.8617141246795654 31000
training loss = 2.8804330825805664 31500
training loss = 3.124877452850342 32000
training loss = 2.8615095615386963 32500
training loss = 2.8621442317962646 33000
training loss = 2.8617801666259766 33500
training loss = 2.861492395401001 34000
training loss = 2.8744072914123535 34500
training loss = 2.8618621826171875 35000
(1, 38)
(1, 0)
