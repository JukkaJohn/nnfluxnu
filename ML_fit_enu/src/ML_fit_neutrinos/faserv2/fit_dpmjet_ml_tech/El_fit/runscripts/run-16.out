data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.07700277524896271 6.939137926617982 39.98158201854002
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 76.61778259277344 500
training loss = 76.1410140991211 1000
training loss = 75.74954223632812 1500
training loss = 75.21084594726562 2000
training loss = 74.43180847167969 2500
training loss = 73.10307312011719 3000
training loss = 69.61524200439453 3500
training loss = 28.569040298461914 4000
training loss = 7.042239665985107 4500
training loss = 5.030946254730225 5000
training loss = 4.690640926361084 5500
training loss = 3.9284064769744873 6000
training loss = 3.829282760620117 6500
training loss = 3.7970340251922607 7000
training loss = 3.7812540531158447 7500
training loss = 4.604754447937012 8000
training loss = 3.7720136642456055 8500
training loss = 3.7701833248138428 9000
training loss = 3.76947021484375 9500
training loss = 3.7700254917144775 10000
training loss = 3.7686731815338135 10500
training loss = 3.771876811981201 11000
training loss = 3.7679331302642822 11500
training loss = 3.7691614627838135 12000
training loss = 3.7670371532440186 12500
training loss = 3.7666971683502197 13000
training loss = 3.7664554119110107 13500
training loss = 3.7661900520324707 14000
training loss = 3.765981674194336 14500
training loss = 3.76572585105896 15000
training loss = 3.765505313873291 15500
training loss = 3.7652480602264404 16000
training loss = 3.7649953365325928 16500
training loss = 3.76692271232605 17000
training loss = 5.287166595458984 17500
training loss = 4.0541229248046875 18000
training loss = 4.233463287353516 18500
training loss = 3.7638702392578125 19000
training loss = 3.7636373043060303 19500
training loss = 3.763410806655884 20000
training loss = 3.7632040977478027 20500
training loss = 3.7629899978637695 21000
training loss = 3.801628589630127 21500
training loss = 3.7650022506713867 22000
training loss = 3.762371778488159 22500
training loss = 3.7621958255767822 23000
training loss = 4.266445159912109 23500
training loss = 3.964977979660034 24000
training loss = 3.7677862644195557 24500
training loss = 3.7616851329803467 25000
training loss = 3.7612686157226562 25500
training loss = 3.7610960006713867 26000
training loss = 3.7609286308288574 26500
training loss = 3.7607767581939697 27000
training loss = 3.7606542110443115 27500
training loss = 3.7775261402130127 28000
training loss = 3.7984893321990967 28500
training loss = 3.87109375 29000
training loss = 3.883234977722168 29500
training loss = 3.769821882247925 30000
training loss = 3.7610509395599365 30500
training loss = 4.479793548583984 31000
training loss = 3.8286421298980713 31500
training loss = 3.8651275634765625 32000
training loss = 4.636757850646973 32500
training loss = 3.759291410446167 33000
training loss = 7.772566795349121 33500
training loss = 4.11239767074585 34000
training loss = 3.9616551399230957 34500
training loss = 3.7603607177734375 35000
(1, 38)
(1, 0)
