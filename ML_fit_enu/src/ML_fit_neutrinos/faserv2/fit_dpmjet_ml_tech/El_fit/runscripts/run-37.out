data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.29088017925677 16.701580034419997 8.878205131448302
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 84.39757537841797 500
training loss = 73.63237762451172 1000
training loss = 73.51996612548828 1500
training loss = 73.39362335205078 2000
training loss = 73.25940704345703 2500
training loss = 73.1242904663086 3000
training loss = 72.99376678466797 3500
training loss = 72.87171173095703 4000
training loss = 72.76228332519531 4500
training loss = 72.67066955566406 5000
training loss = 72.58281707763672 5500
training loss = 66.99896240234375 6000
training loss = 3.786996603012085 6500
training loss = 3.5229058265686035 7000
training loss = 3.3573248386383057 7500
training loss = 3.329602003097534 8000
training loss = 3.3164477348327637 8500
training loss = 5.328273773193359 9000
training loss = 3.408167839050293 9500
training loss = 3.3006129264831543 10000
training loss = 3.285430431365967 10500
training loss = 3.2709641456604004 11000
training loss = 3.260962963104248 11500
training loss = 3.2510716915130615 12000
training loss = 3.2408125400543213 12500
training loss = 3.2302050590515137 13000
training loss = 3.21917724609375 13500
training loss = 3.2077488899230957 14000
training loss = 3.200761318206787 14500
training loss = 3.1878912448883057 15000
training loss = 3.516153573989868 15500
training loss = 3.2002532482147217 16000
training loss = 3.146146297454834 16500
training loss = 3.1306188106536865 17000
training loss = 3.1158955097198486 17500
training loss = 3.10062837600708 18000
training loss = 3.084918737411499 18500
training loss = 3.0701091289520264 19000
training loss = 3.052325487136841 19500
training loss = 3.8737592697143555 20000
training loss = 3.0274903774261475 20500
training loss = 3.000952959060669 21000
training loss = 2.9831278324127197 21500
training loss = 3.066791296005249 22000
training loss = 3.353307008743286 22500
training loss = 2.929321765899658 23000
training loss = 2.9144575595855713 23500
training loss = 2.903679609298706 24000
training loss = 2.8811464309692383 24500
training loss = 2.9415857791900635 25000
training loss = 2.853135108947754 25500
training loss = 3.82511305809021 26000
training loss = 2.830362558364868 26500
training loss = 3.894242525100708 27000
training loss = 2.690662384033203 27500
training loss = 2.7130706310272217 28000
training loss = 2.7524871826171875 28500
training loss = 2.6085753440856934 29000
training loss = 2.914163589477539 29500
training loss = 2.604649782180786 30000
training loss = 2.561692476272583 30500
training loss = 2.5479207038879395 31000
training loss = 2.5326616764068604 31500
training loss = 2.5175256729125977 32000
training loss = 2.495537042617798 32500
training loss = 2.473195791244507 33000
training loss = 2.448371410369873 33500
training loss = 2.4236674308776855 34000
training loss = 2.3976149559020996 34500
training loss = 2.373659610748291 35000
(1, 38)
(1, 0)
