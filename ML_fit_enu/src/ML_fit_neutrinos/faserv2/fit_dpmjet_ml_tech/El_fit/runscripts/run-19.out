data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.7337199158613983 13.588441192945345 65.4036818608566
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 110.92053985595703 500
training loss = 71.36905670166016 1000
training loss = 69.38013458251953 1500
training loss = 65.92322540283203 2000
training loss = 51.40792465209961 2500
training loss = 8.119779586791992 3000
training loss = 5.294676780700684 3500
training loss = 3.8238823413848877 4000
training loss = 3.1740550994873047 4500
training loss = 2.95302152633667 5000
training loss = 2.899878740310669 5500
training loss = 2.8845174312591553 6000
training loss = 2.8777167797088623 6500
training loss = 2.9051826000213623 7000
training loss = 2.8731679916381836 7500
training loss = 2.867281913757324 8000
training loss = 2.862485408782959 8500
training loss = 2.8670706748962402 9000
training loss = 3.7410166263580322 9500
training loss = 3.068418502807617 10000
training loss = 2.8722188472747803 10500
training loss = 2.8833062648773193 11000
training loss = 3.281137704849243 11500
training loss = 7.431812763214111 12000
training loss = 3.353492259979248 12500
training loss = 2.872713565826416 13000
training loss = 2.8540778160095215 13500
training loss = 2.9589550495147705 14000
training loss = 2.925011396408081 14500
training loss = 2.922074556350708 15000
training loss = 2.8379034996032715 15500
training loss = 2.846412181854248 16000
training loss = 2.8387725353240967 16500
training loss = 3.0481951236724854 17000
training loss = 2.8937578201293945 17500
training loss = 2.833817720413208 18000
training loss = 2.834538698196411 18500
training loss = 2.832598924636841 19000
training loss = 2.8320910930633545 19500
training loss = 2.83164381980896 20000
training loss = 2.831087350845337 20500
training loss = 2.830631971359253 21000
training loss = 2.8302249908447266 21500
training loss = 2.8867037296295166 22000
training loss = 3.3218863010406494 22500
training loss = 2.8717050552368164 23000
training loss = 2.880585193634033 23500
training loss = 2.8537819385528564 24000
training loss = 6.2190022468566895 24500
training loss = 2.8373794555664062 25000
training loss = 2.8309524059295654 25500
training loss = 2.8450324535369873 26000
training loss = 2.828756093978882 26500
training loss = 2.8300280570983887 27000
training loss = 2.829789638519287 27500
training loss = 2.914564847946167 28000
training loss = 2.826765298843384 28500
training loss = 2.826641321182251 29000
training loss = 2.8265299797058105 29500
training loss = 2.826427698135376 30000
training loss = 2.8263583183288574 30500
training loss = 2.8262405395507812 31000
training loss = 2.826164960861206 31500
training loss = 3.286526679992676 32000
training loss = 2.8283591270446777 32500
training loss = 2.872788190841675 33000
training loss = 2.836362838745117 33500
training loss = 2.8264081478118896 34000
training loss = 3.4839446544647217 34500
training loss = 5.104966640472412 35000
(1, 38)
(1, 0)
