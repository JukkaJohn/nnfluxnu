data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.4710136296451117 17.733737460976798 12.743977702235675
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 104.56513214111328 500
training loss = 78.76353454589844 1000
training loss = 78.63440704345703 1500
training loss = 78.49755859375 2000
training loss = 78.35745239257812 2500
training loss = 78.21818542480469 3000
training loss = 78.0779037475586 3500
training loss = 77.9174575805664 4000
training loss = 77.61820220947266 4500
training loss = 75.2746810913086 5000
training loss = 4.5265398025512695 5500
training loss = 5.4360527992248535 6000
training loss = 3.5570101737976074 6500
training loss = 3.488537549972534 7000
training loss = 3.469008684158325 7500
training loss = 3.4619529247283936 8000
training loss = 3.4592626094818115 8500
training loss = 3.4577934741973877 9000
training loss = 3.456756114959717 9500
training loss = 3.4559154510498047 10000
training loss = 3.4551596641540527 10500
training loss = 3.4447014331817627 11000
training loss = 3.211669445037842 11500
training loss = 3.1477575302124023 12000
training loss = 3.1128857135772705 12500
training loss = 3.089966297149658 13000
training loss = 3.0729482173919678 13500
training loss = 3.0591604709625244 14000
training loss = 3.0473215579986572 14500
training loss = 3.0365114212036133 15000
training loss = 3.0263302326202393 15500
training loss = 3.016493320465088 16000
training loss = 3.0070719718933105 16500
training loss = 3.120593547821045 17000
training loss = 2.988135814666748 17500
training loss = 2.9781785011291504 18000
training loss = 2.969437837600708 18500
training loss = 2.9599406719207764 19000
training loss = 2.951289653778076 19500
training loss = 2.94325852394104 20000
training loss = 2.936113119125366 20500
training loss = 3.037172555923462 21000
training loss = 2.9263861179351807 21500
training loss = 2.920781135559082 22000
training loss = 2.9315497875213623 22500
training loss = 2.920053243637085 23000
training loss = 2.939898729324341 23500
training loss = 2.904517412185669 24000
training loss = 2.9012396335601807 24500
training loss = 2.8985178470611572 25000
training loss = 2.895907402038574 25500
training loss = 2.8934648036956787 26000
training loss = 2.891122341156006 26500
training loss = 2.8888754844665527 27000
training loss = 2.8866970539093018 27500
training loss = 2.8846218585968018 28000
training loss = 6.465497970581055 28500
training loss = 2.8908724784851074 29000
training loss = 2.878760576248169 29500
training loss = 2.8766191005706787 30000
training loss = 2.874671220779419 30500
training loss = 2.8728010654449463 31000
training loss = 2.870905876159668 31500
training loss = 2.8690168857574463 32000
training loss = 2.867122173309326 32500
training loss = 2.865469217300415 33000
training loss = 2.863926649093628 33500
training loss = 2.86854887008667 34000
training loss = 4.8574934005737305 34500
training loss = 2.906118392944336 35000
(1, 38)
(1, 0)
