data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.0210072769184255 15.335881020675203 61.11461653664897
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 106.86014556884766 500
training loss = 74.92041015625 1000
training loss = 74.33800506591797 1500
training loss = 73.70323944091797 2000
training loss = 72.82920837402344 2500
training loss = 69.02400970458984 3000
training loss = 7.075380802154541 3500
training loss = 4.978946685791016 4000
training loss = 4.774843692779541 4500
training loss = 4.693826198577881 5000
training loss = 4.646610260009766 5500
training loss = 4.617170810699463 6000
training loss = 4.597609043121338 6500
training loss = 4.597497463226318 7000
training loss = 4.573147296905518 7500
training loss = 4.563783168792725 8000
training loss = 4.55668830871582 8500
training loss = 4.549116611480713 9000
training loss = 4.542171478271484 9500
training loss = 4.535961627960205 10000
training loss = 4.530098915100098 10500
training loss = 4.524522304534912 11000
training loss = 4.519191265106201 11500
training loss = 4.514171600341797 12000
training loss = 4.509377956390381 12500
training loss = 4.5048980712890625 13000
training loss = 4.50065803527832 13500
training loss = 4.496637344360352 14000
training loss = 4.493344306945801 14500
training loss = 4.4895339012146 15000
training loss = 4.485998153686523 15500
training loss = 4.482998371124268 16000
training loss = 4.4800004959106445 16500
training loss = 4.47714900970459 17000
training loss = 4.4745612144470215 17500
training loss = 4.472120761871338 18000
training loss = 4.4698381423950195 18500
training loss = 4.468253135681152 19000
training loss = 4.48789119720459 19500
training loss = 4.463789463043213 20000
training loss = 4.4620161056518555 20500
training loss = 4.460349082946777 21000
training loss = 4.4587812423706055 21500
training loss = 4.457308769226074 22000
training loss = 4.527042388916016 22500
training loss = 5.339024066925049 23000
training loss = 4.4608941078186035 23500
training loss = 4.452230453491211 24000
training loss = 4.457106590270996 24500
training loss = 4.450342178344727 25000
training loss = 4.449585914611816 25500
training loss = 4.448394775390625 26000
training loss = 4.447360038757324 26500
training loss = 4.446527004241943 27000
training loss = 4.446369647979736 27500
training loss = 4.481441020965576 28000
training loss = 4.445204734802246 28500
training loss = 4.445254325866699 29000
training loss = 4.443415641784668 29500
training loss = 4.442473411560059 30000
training loss = 4.441908359527588 30500
training loss = 4.441371917724609 31000
training loss = 4.440881729125977 31500
training loss = 4.4403977394104 32000
training loss = 4.4399518966674805 32500
training loss = 4.439556121826172 33000
training loss = 4.439120769500732 33500
training loss = 4.440980434417725 34000
training loss = 4.44020414352417 34500
training loss = 4.58441162109375 35000
(1, 38)
(1, 0)
