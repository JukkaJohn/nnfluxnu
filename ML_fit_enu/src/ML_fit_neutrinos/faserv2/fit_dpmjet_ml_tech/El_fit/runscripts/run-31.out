data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.11530364415782424 12.672009593718956 16.19822979669685
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 91.79795837402344 500
training loss = 80.78999328613281 1000
training loss = 80.43394470214844 1500
training loss = 80.01371002197266 2000
training loss = 79.46341705322266 2500
training loss = 78.51082611083984 3000
training loss = 75.05274963378906 3500
training loss = 8.810340881347656 4000
training loss = 5.081645488739014 4500
training loss = 3.792509078979492 5000
training loss = 3.5800905227661133 5500
training loss = 3.4238200187683105 6000
training loss = 3.4917855262756348 6500
training loss = 3.621843099594116 7000
training loss = 3.4667744636535645 7500
training loss = 3.383486270904541 8000
training loss = 3.393996000289917 8500
training loss = 3.488490581512451 9000
training loss = 3.3755228519439697 9500
training loss = 3.373033285140991 10000
training loss = 3.3718771934509277 10500
training loss = 3.385328531265259 11000
training loss = 3.3686110973358154 11500
training loss = 3.367489814758301 12000
training loss = 3.366659164428711 12500
training loss = 3.365302801132202 13000
training loss = 3.364424467086792 13500
training loss = 3.3635878562927246 14000
training loss = 3.3628108501434326 14500
training loss = 3.3621630668640137 15000
training loss = 3.3614394664764404 15500
training loss = 3.360812187194824 16000
training loss = 3.3866546154022217 16500
training loss = 3.3596913814544678 17000
training loss = 3.359163999557495 17500
training loss = 3.3586783409118652 18000
training loss = 3.3582072257995605 18500
training loss = 3.3577725887298584 19000
training loss = 3.357353925704956 19500
training loss = 3.356935977935791 20000
training loss = 3.35654616355896 20500
training loss = 3.356179714202881 21000
training loss = 3.3558144569396973 21500
training loss = 3.3554575443267822 22000
training loss = 3.3551268577575684 22500
training loss = 3.3547918796539307 23000
training loss = 3.3544859886169434 23500
training loss = 3.354189157485962 24000
training loss = 3.9077036380767822 24500
training loss = 5.253559112548828 25000
training loss = 3.353320598602295 25500
training loss = 3.3530523777008057 26000
training loss = 3.3528006076812744 26500
training loss = 3.3525426387786865 27000
training loss = 3.3523221015930176 27500
training loss = 3.3520541191101074 28000
training loss = 3.3518307209014893 28500
training loss = 3.351602792739868 29000
training loss = 3.351379871368408 29500
training loss = 3.351165771484375 30000
training loss = 3.3509573936462402 30500
training loss = 3.3507585525512695 31000
training loss = 3.3505518436431885 31500
training loss = 3.3503663539886475 32000
training loss = 3.3501827716827393 32500
training loss = 3.3499953746795654 33000
training loss = 3.350015878677368 33500
training loss = 3.3496477603912354 34000
training loss = 3.3494880199432373 34500
training loss = 3.3493270874023438 35000
(1, 38)
(1, 0)
