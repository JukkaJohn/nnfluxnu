data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.9257134331472234 7.196960538633752 74.88418526992614
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 84.7602767944336 500
training loss = 78.13196563720703 1000
training loss = 77.96817016601562 1500
training loss = 77.74059295654297 2000
training loss = 77.38510131835938 2500
training loss = 76.62217712402344 3000
training loss = 70.30107879638672 3500
training loss = 4.529207706451416 4000
training loss = 4.239536285400391 4500
training loss = 4.16488790512085 5000
training loss = 4.151638031005859 5500
training loss = 4.148444652557373 6000
training loss = 4.146411418914795 6500
training loss = 4.144543170928955 7000
training loss = 4.142696857452393 7500
training loss = 4.141245365142822 8000
training loss = 4.180805206298828 8500
training loss = 7.099635124206543 9000
training loss = 4.146608352661133 9500
training loss = 4.155357837677002 10000
training loss = 4.132875442504883 10500
training loss = 4.136195182800293 11000
training loss = 4.128976821899414 11500
training loss = 4.316962242126465 12000
training loss = 4.125537395477295 12500
training loss = 4.267060279846191 13000
training loss = 4.122473239898682 13500
training loss = 4.1211838722229 14000
training loss = 4.119431018829346 14500
training loss = 4.147264003753662 15000
training loss = 4.126031875610352 15500
training loss = 4.22755241394043 16000
training loss = 4.382732391357422 16500
training loss = 4.11258602142334 17000
training loss = 4.111329555511475 17500
training loss = 8.246904373168945 18000
training loss = 4.778726100921631 18500
training loss = 4.112528324127197 19000
training loss = 4.1071553230285645 19500
training loss = 4.106761932373047 20000
training loss = 4.138298988342285 20500
training loss = 4.1032938957214355 21000
training loss = 4.102025032043457 21500
training loss = 4.1009721755981445 22000
training loss = 4.099959373474121 22500
training loss = 4.100067138671875 23000
training loss = 4.097964286804199 23500
training loss = 4.097007751464844 24000
training loss = 4.096066474914551 24500
training loss = 4.095136642456055 25000
training loss = 4.09421968460083 25500
training loss = 4.093324184417725 26000
training loss = 4.092434883117676 26500
training loss = 4.091561794281006 27000
training loss = 4.090707778930664 27500
training loss = 4.089937686920166 28000
training loss = 5.28745174407959 28500
training loss = 4.422972202301025 29000
training loss = 4.087395191192627 29500
training loss = 4.08687162399292 30000
training loss = 4.085720539093018 30500
training loss = 4.084916591644287 31000
training loss = 4.084115505218506 31500
training loss = 4.287724494934082 32000
training loss = 4.171662330627441 32500
training loss = 4.178650856018066 33000
training loss = 4.080898284912109 33500
training loss = 4.0800981521606445 34000
training loss = 4.0916428565979 34500
training loss = 4.078464508056641 35000
(1, 38)
(1, 0)
