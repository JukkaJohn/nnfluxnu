data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.482374037236774 18.80537203027592 8.589239153535678
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 156.93792724609375 500
training loss = 79.74897003173828 1000
training loss = 79.55694580078125 1500
training loss = 79.38084411621094 2000
training loss = 79.18614196777344 2500
training loss = 78.96819305419922 3000
training loss = 78.69387817382812 3500
training loss = 78.1620101928711 4000
training loss = 70.20428466796875 4500
training loss = 5.979392051696777 5000
training loss = 4.282676696777344 5500
training loss = 3.732640027999878 6000
training loss = 3.5227649211883545 6500
training loss = 3.438138008117676 7000
training loss = 3.4032323360443115 7500
training loss = 3.3889639377593994 8000
training loss = 3.3819127082824707 8500
training loss = 3.378814697265625 9000
training loss = 3.3772037029266357 9500
training loss = 3.3761770725250244 10000
training loss = 3.375462293624878 10500
training loss = 3.464877128601074 11000
training loss = 3.374799966812134 11500
training loss = 3.539180278778076 12000
training loss = 3.601008415222168 12500
training loss = 3.404973030090332 13000
training loss = 3.3726179599761963 13500
training loss = 3.3718883991241455 14000
training loss = 3.3758625984191895 14500
training loss = 3.3713250160217285 15000
training loss = 3.382087230682373 15500
training loss = 3.3726813793182373 16000
training loss = 3.370185375213623 16500
training loss = 3.37042236328125 17000
training loss = 3.3704192638397217 17500
training loss = 3.3685553073883057 18000
training loss = 3.3681867122650146 18500
training loss = 3.36782169342041 19000
training loss = 3.3674890995025635 19500
training loss = 3.3671627044677734 20000
training loss = 3.366853713989258 20500
training loss = 3.392211437225342 21000
training loss = 3.3929154872894287 21500
training loss = 6.905140399932861 22000
training loss = 3.392913818359375 22500
training loss = 3.5594727993011475 23000
training loss = 3.3652706146240234 23500
training loss = 3.3652310371398926 24000
training loss = 3.435692310333252 24500
training loss = 3.365247964859009 25000
training loss = 3.364086151123047 25500
training loss = 3.363893747329712 26000
training loss = 3.363633155822754 26500
training loss = 3.3633854389190674 27000
training loss = 3.3630588054656982 27500
training loss = 3.368359327316284 28000
training loss = 3.3634650707244873 28500
training loss = 3.3648617267608643 29000
training loss = 3.3642537593841553 29500
training loss = 3.3618814945220947 30000
training loss = 3.361624240875244 30500
training loss = 3.3613924980163574 31000
training loss = 3.3611133098602295 31500
training loss = 3.3608829975128174 32000
training loss = 3.3606295585632324 32500
training loss = 3.3643240928649902 33000
training loss = 3.9184908866882324 33500
training loss = 3.4195051193237305 34000
training loss = 3.369905948638916 34500
training loss = 3.5172064304351807 35000
(1, 38)
(1, 0)
