data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.800590645638886 1.2216237623877735 29.257630625960417
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 32.08308410644531 500
training loss = 5.459458351135254 1000
training loss = 4.197284698486328 1500
training loss = 2.7094264030456543 2000
training loss = 2.439016342163086 2500
training loss = 2.3412983417510986 3000
training loss = 3.7902848720550537 3500
training loss = 2.282835006713867 4000
training loss = 2.2365498542785645 4500
training loss = 2.18701171875 5000
training loss = 2.1335701942443848 5500
training loss = 2.0762245655059814 6000
training loss = 2.020787000656128 6500
training loss = 1.9677226543426514 7000
training loss = 1.9285250902175903 7500
training loss = 1.8991845846176147 8000
training loss = 1.876370906829834 8500
training loss = 1.8591985702514648 9000
training loss = 1.846274495124817 9500
training loss = 1.8355644941329956 10000
training loss = 1.8262369632720947 10500
training loss = 1.818356990814209 11000
training loss = 13.127553939819336 11500
training loss = 17.19220542907715 12000
training loss = 1.7967115640640259 12500
training loss = 1.7895889282226562 13000
training loss = 1.78354811668396 13500
training loss = 1.7788724899291992 14000
training loss = 1.8048264980316162 14500
training loss = 1.772462010383606 15000
training loss = 1.7660590410232544 15500
training loss = 1.7613760232925415 16000
training loss = 1.7572230100631714 16500
training loss = 1.7515569925308228 17000
training loss = 1.7498221397399902 17500
training loss = 1.7443773746490479 18000
training loss = 1.7398083209991455 18500
training loss = 1.7361171245574951 19000
training loss = 1.7313581705093384 19500
training loss = 19.832374572753906 20000
training loss = 1.9245573282241821 20500
training loss = 1.7646358013153076 21000
training loss = 1.7267717123031616 21500
training loss = 1.7160090208053589 22000
training loss = 1.7110377550125122 22500
training loss = 1.7058184146881104 23000
training loss = 1.7012193202972412 23500
training loss = 1.696988582611084 24000
training loss = 1.6931219100952148 24500
training loss = 1.6902421712875366 25000
training loss = 1.6856681108474731 25500
training loss = 1.6805850267410278 26000
training loss = 1.6753287315368652 26500
training loss = 1.6686136722564697 27000
training loss = 1.66190505027771 27500
training loss = 1.6528722047805786 28000
training loss = 1.642382025718689 28500
training loss = 1.6297074556350708 29000
training loss = 1.6129034757614136 29500
training loss = 1.5925272703170776 30000
training loss = 1.5689607858657837 30500
training loss = 1.541704773902893 31000
training loss = 1.54058039188385 31500
training loss = 1.5834734439849854 32000
training loss = 1.4571462869644165 32500
training loss = 1.5803399085998535 33000
training loss = 1.402193546295166 33500
training loss = 1.3859084844589233 34000
training loss = 1.354369878768921 34500
training loss = 1.3329414129257202 35000
reduced chi^2 level 2 = 1.3328922986984253
Constrained alpha: 3.7686538696289062
Constrained beta: -0.013236763887107372
Constrained gamma: 27.38226318359375
(1, 38)
(1, 0)
