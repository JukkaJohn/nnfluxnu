data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.47024231448567433 2.7544665308319827 30.11561020500514
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 99.57972717285156 500
training loss = 77.6429672241211 1000
training loss = 77.56061553955078 1500
training loss = 77.54168701171875 2000
training loss = 77.51464080810547 2500
training loss = 77.47576141357422 3000
training loss = 77.41888427734375 3500
training loss = 77.3336181640625 4000
training loss = 77.20079803466797 4500
training loss = 76.98750305175781 5000
training loss = 76.91670989990234 5500
training loss = 76.02091979980469 6000
training loss = 71.64317321777344 6500
training loss = 3.5533881187438965 7000
training loss = 3.330075263977051 7500
training loss = 3.226592540740967 8000
training loss = 3.2851815223693848 8500
training loss = 3.3421571254730225 9000
training loss = 3.2120180130004883 9500
training loss = 3.1964616775512695 10000
training loss = 3.1890366077423096 10500
training loss = 3.187988758087158 11000
training loss = 3.1845786571502686 11500
training loss = 3.19691801071167 12000
training loss = 3.179898500442505 12500
training loss = 3.177661180496216 13000
training loss = 3.1792755126953125 13500
training loss = 3.1825716495513916 14000
training loss = 3.173353672027588 14500
training loss = 3.1721644401550293 15000
training loss = 3.2168197631835938 15500
training loss = 3.1863207817077637 16000
training loss = 3.169461727142334 16500
training loss = 3.1684844493865967 17000
training loss = 3.168280839920044 17500
training loss = 3.2954020500183105 18000
training loss = 3.167257308959961 18500
training loss = 3.167191743850708 19000
training loss = 3.1876380443573 19500
training loss = 3.164811372756958 20000
training loss = 3.1640963554382324 20500
training loss = 3.1636438369750977 21000
training loss = 4.146236896514893 21500
training loss = 3.1628592014312744 22000
training loss = 4.299259185791016 22500
training loss = 3.2352635860443115 23000
training loss = 3.1625475883483887 23500
training loss = 3.1974740028381348 24000
training loss = 3.1658260822296143 24500
training loss = 3.208249568939209 25000
training loss = 3.2690908908843994 25500
training loss = 3.180422067642212 26000
training loss = 4.595335960388184 26500
training loss = 3.1614861488342285 27000
training loss = 3.1602602005004883 27500
training loss = 3.1600348949432373 28000
training loss = 3.160992383956909 28500
training loss = 3.1617257595062256 29000
training loss = 3.159855604171753 29500
training loss = 3.4682183265686035 30000
training loss = 3.359361410140991 30500
training loss = 3.180832624435425 31000
training loss = 3.159247636795044 31500
training loss = 3.1591765880584717 32000
training loss = 3.159122943878174 32500
training loss = 3.16017746925354 33000
training loss = 3.159031629562378 33500
training loss = 3.1592390537261963 34000
training loss = 3.1589393615722656 34500
training loss = 3.158898115158081 35000
(1, 38)
(1, 0)
