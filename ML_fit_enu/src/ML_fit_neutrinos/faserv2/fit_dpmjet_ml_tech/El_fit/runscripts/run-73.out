data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.287524557729048 17.05566626645768 45.285898484603514
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 148.89358520507812 500
training loss = 73.42443084716797 1000
training loss = 72.82691192626953 1500
training loss = 71.8778305053711 2000
training loss = 67.94816589355469 2500
training loss = 4.0470290184021 3000
training loss = 3.4031147956848145 3500
training loss = 3.2317779064178467 4000
training loss = 3.1710996627807617 4500
training loss = 3.1410481929779053 5000
training loss = 3.1247847080230713 5500
training loss = 3.114900827407837 6000
training loss = 3.1087076663970947 6500
training loss = 3.104562282562256 7000
training loss = 3.1014182567596436 7500
training loss = 3.0991122722625732 8000
training loss = 3.0972869396209717 8500
training loss = 3.095869541168213 9000
training loss = 4.6857829093933105 9500
training loss = 8.45367431640625 10000
training loss = 3.0938355922698975 10500
training loss = 3.0921828746795654 11000
training loss = 3.0936362743377686 11500
training loss = 4.880282402038574 12000
training loss = 3.0899925231933594 12500
training loss = 3.0894315242767334 13000
training loss = 3.088930606842041 13500
training loss = 3.0885002613067627 14000
training loss = 3.088014841079712 14500
training loss = 3.0876216888427734 15000
training loss = 3.0872042179107666 15500
training loss = 3.0908703804016113 16000
training loss = 4.89412784576416 16500
training loss = 3.120332956314087 17000
training loss = 3.5428624153137207 17500
training loss = 3.0856876373291016 18000
training loss = 3.0852885246276855 18500
training loss = 3.0850019454956055 19000
training loss = 3.084759473800659 19500
training loss = 3.084524154663086 20000
training loss = 3.084674596786499 20500
training loss = 3.0841224193573 21000
training loss = 3.0838990211486816 21500
training loss = 3.083711862564087 22000
training loss = 3.0835230350494385 22500
training loss = 3.0833611488342285 23000
training loss = 3.0831878185272217 23500
training loss = 3.084064245223999 24000
training loss = 3.355750322341919 24500
training loss = 3.082760810852051 25000
training loss = 3.0826518535614014 25500
training loss = 3.0825326442718506 26000
training loss = 3.08243989944458 26500
training loss = 3.1281440258026123 27000
training loss = 3.1600794792175293 27500
training loss = 3.0837104320526123 28000
training loss = 3.082125186920166 28500
training loss = 3.08516001701355 29000
training loss = 3.086493968963623 29500
training loss = 3.0819077491760254 30000
training loss = 3.081793785095215 30500
training loss = 3.0817878246307373 31000
training loss = 3.081911325454712 31500
training loss = 3.081644296646118 32000
training loss = 3.086594581604004 32500
training loss = 3.082373857498169 33000
training loss = 3.0817415714263916 33500
training loss = 3.081536054611206 34000
training loss = 3.0814082622528076 34500
training loss = 3.0813722610473633 35000
(1, 38)
(1, 0)
