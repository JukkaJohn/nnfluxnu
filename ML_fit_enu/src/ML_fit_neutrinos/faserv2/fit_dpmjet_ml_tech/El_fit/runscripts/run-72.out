data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.13850392186493088 3.131333144138313 35.03895125598512
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 93.22014617919922 500
training loss = 75.248779296875 1000
training loss = 75.16648864746094 1500
training loss = 75.13849639892578 2000
training loss = 75.1057357788086 2500
training loss = 75.06866455078125 3000
training loss = 75.02652740478516 3500
training loss = 74.97413635253906 4000
training loss = 74.88306427001953 4500
training loss = 74.397216796875 5000
training loss = 4.237196445465088 5500
training loss = 3.44061279296875 6000
training loss = 3.19773268699646 6500
training loss = 3.1438488960266113 7000
training loss = 3.1332712173461914 7500
training loss = 3.1300907135009766 8000
training loss = 3.1280593872070312 8500
training loss = 3.126333475112915 9000
training loss = 3.1248998641967773 9500
training loss = 3.123814821243286 10000
training loss = 3.1230154037475586 10500
training loss = 3.122437000274658 11000
training loss = 3.122013568878174 11500
training loss = 3.121669292449951 12000
training loss = 3.121405601501465 12500
training loss = 5.852428913116455 13000
training loss = 4.7404327392578125 13500
training loss = 3.163259744644165 14000
training loss = 3.126283884048462 14500
training loss = 3.120373487472534 15000
training loss = 3.120229721069336 15500
training loss = 3.120006799697876 16000
training loss = 3.1198461055755615 16500
training loss = 3.1196727752685547 17000
training loss = 3.1195268630981445 17500
training loss = 3.119377613067627 18000
training loss = 3.1196067333221436 18500
training loss = 3.1192824840545654 19000
training loss = 3.1280293464660645 19500
training loss = 3.1188361644744873 20000
training loss = 3.1187002658843994 20500
training loss = 3.5424370765686035 21000
training loss = 4.618832111358643 21500
training loss = 3.128178358078003 22000
training loss = 3.118199348449707 22500
training loss = 3.1181752681732178 23000
training loss = 3.1179516315460205 23500
training loss = 3.1180543899536133 24000
training loss = 5.970612525939941 24500
training loss = 3.122133493423462 25000
training loss = 3.117546319961548 25500
training loss = 3.1174376010894775 26000
training loss = 3.117335319519043 26500
training loss = 3.117241621017456 27000
training loss = 3.117164134979248 27500
training loss = 3.1170578002929688 28000
training loss = 3.116992473602295 28500
training loss = 3.116889476776123 29000
training loss = 3.1168034076690674 29500
training loss = 3.1167218685150146 30000
training loss = 3.1166417598724365 30500
training loss = 3.1165623664855957 31000
training loss = 3.116488456726074 31500
training loss = 3.1164236068725586 32000
training loss = 3.1163458824157715 32500
training loss = 3.1162807941436768 33000
training loss = 3.9349524974823 33500
training loss = 3.117431640625 34000
training loss = 3.4245107173919678 34500
training loss = 3.1310131549835205 35000
(1, 38)
(1, 0)
