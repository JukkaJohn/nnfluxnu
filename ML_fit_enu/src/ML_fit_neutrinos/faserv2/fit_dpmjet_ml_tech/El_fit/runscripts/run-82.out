data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.332186377864534 4.195428465427264 65.7324588222985
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 84.26380157470703 500
training loss = 76.05889129638672 1000
training loss = 76.0353012084961 1500
training loss = 76.00471496582031 2000
training loss = 75.96035766601562 2500
training loss = 75.89366149902344 3000
training loss = 75.78331756591797 3500
training loss = 75.5528793334961 4000
training loss = 74.49686431884766 4500
training loss = 3.3631155490875244 5000
training loss = 3.5999162197113037 5500
training loss = 3.5397419929504395 6000
training loss = 2.7780959606170654 6500
training loss = 2.777047634124756 7000
training loss = 2.7763800621032715 7500
training loss = 2.7757575511932373 8000
training loss = 2.775131940841675 8500
training loss = 2.933746814727783 9000
training loss = 2.7454917430877686 9500
training loss = 2.440131187438965 10000
training loss = 2.371161937713623 10500
training loss = 2.305615186691284 11000
training loss = 2.1739871501922607 11500
training loss = 2.0084121227264404 12000
training loss = 2.8875041007995605 12500
training loss = 2.2394795417785645 13000
training loss = 2.616154432296753 13500
training loss = 1.7896851301193237 14000
training loss = 1.7632167339324951 14500
training loss = 1.7489250898361206 15000
training loss = 1.7386245727539062 15500
training loss = 1.737748622894287 16000
training loss = 1.7214630842208862 16500
training loss = 1.7269914150238037 17000
training loss = 1.7093827724456787 17500
training loss = 1.7099195718765259 18000
training loss = 1.6997119188308716 18500
training loss = 1.6956276893615723 19000
training loss = 1.6919705867767334 19500
training loss = 1.6886364221572876 20000
training loss = 1.6855906248092651 20500
training loss = 1.6828235387802124 21000
training loss = 1.6803547143936157 21500
training loss = 1.6779427528381348 22000
training loss = 1.6757757663726807 22500
training loss = 1.6737326383590698 23000
training loss = 1.6718571186065674 23500
training loss = 1.6701170206069946 24000
training loss = 1.6684428453445435 24500
training loss = 1.6674805879592896 25000
training loss = 1.7484101057052612 25500
training loss = 1.6647634506225586 26000
training loss = 1.6626616716384888 26500
training loss = 1.661342978477478 27000
training loss = 1.6601369380950928 27500
training loss = 1.6590076684951782 28000
training loss = 5.736924648284912 28500
training loss = 1.6615960597991943 29000
training loss = 1.6575217247009277 29500
training loss = 1.6547540426254272 30000
training loss = 1.653825044631958 30500
training loss = 1.6529234647750854 31000
training loss = 1.6522011756896973 31500
training loss = 1.6527860164642334 32000
training loss = 1.6551672220230103 32500
training loss = 1.6504476070404053 33000
training loss = 1.6487808227539062 33500
training loss = 1.6479688882827759 34000
training loss = 1.6472532749176025 34500
training loss = 1.646518349647522 35000
reduced chi^2 level 2 = 1.6465235948562622
Constrained alpha: 2.268602132797241
Constrained beta: 4.2202348709106445
Constrained gamma: 42.76264953613281
(1, 38)
(1, 0)
