data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.8382173375730853 16.940974553397766 53.69884232519172
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 137.06692504882812 500
training loss = 75.76968383789062 1000
training loss = 75.4814682006836 1500
training loss = 75.22330474853516 2000
training loss = 74.99266052246094 2500
training loss = 74.7536849975586 3000
training loss = 74.32823181152344 3500
training loss = 68.99417877197266 4000
training loss = 4.2973551750183105 4500
training loss = 3.9662981033325195 5000
training loss = 3.9142279624938965 5500
training loss = 3.905714273452759 6000
training loss = 3.903634786605835 6500
training loss = 3.902437686920166 7000
training loss = 3.9014089107513428 7500
training loss = 3.9004082679748535 8000
training loss = 3.8994574546813965 8500
training loss = 3.8985395431518555 9000
training loss = 3.8976409435272217 9500
training loss = 3.896771192550659 10000
training loss = 3.8962764739990234 10500
training loss = 3.9082624912261963 11000
training loss = 3.8943891525268555 11500
training loss = 3.893507242202759 12000
training loss = 3.897796154022217 12500
training loss = 5.997654438018799 13000
training loss = 7.015397548675537 13500
training loss = 3.919663190841675 14000
training loss = 4.021945953369141 14500
training loss = 4.008224964141846 15000
training loss = 3.947695732116699 15500
training loss = 3.894132137298584 16000
training loss = 4.909245491027832 16500
training loss = 3.886535167694092 17000
training loss = 3.887176036834717 17500
training loss = 3.888819694519043 18000
training loss = 4.936384677886963 18500
training loss = 3.9996912479400635 19000
training loss = 3.8868777751922607 19500
training loss = 4.533556938171387 20000
training loss = 9.175494194030762 20500
training loss = 7.460411548614502 21000
training loss = 3.901841402053833 21500
training loss = 3.8904170989990234 22000
training loss = 3.8880977630615234 22500
training loss = 3.879995584487915 23000
training loss = 3.8795430660247803 23500
training loss = 3.8791086673736572 24000
training loss = 3.8786919116973877 24500
training loss = 4.281055450439453 25000
training loss = 3.8779096603393555 25500
training loss = 4.415064811706543 26000
training loss = 3.8796169757843018 26500
training loss = 4.8751630783081055 27000
training loss = 4.023955345153809 27500
training loss = 3.8773434162139893 28000
training loss = 3.8755276203155518 28500
training loss = 3.8768510818481445 29000
training loss = 3.8749613761901855 29500
training loss = 3.8745808601379395 30000
training loss = 3.8742716312408447 30500
training loss = 3.8844902515411377 31000
training loss = 3.905724048614502 31500
training loss = 3.879178047180176 32000
training loss = 3.8733930587768555 32500
training loss = 3.8740522861480713 33000
training loss = 3.8726043701171875 33500
training loss = 3.8726344108581543 34000
training loss = 3.887274980545044 34500
training loss = 3.879641056060791 35000
(1, 38)
(1, 0)
