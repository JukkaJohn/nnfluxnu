data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.3355285131255323 1.0795623440667335 68.4641700055979
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 58.75710678100586 500
training loss = 31.164409637451172 1000
training loss = 28.0747013092041 1500
training loss = 21.666057586669922 2000
training loss = 5.31707239151001 2500
training loss = 2.189690351486206 3000
training loss = 2.1454203128814697 3500
training loss = 2.1030187606811523 4000
training loss = 2.410590648651123 4500
training loss = 1.9398066997528076 5000
training loss = 1.4681276082992554 5500
training loss = 1.415147304534912 6000
training loss = 1.242234706878662 6500
training loss = 1.184461236000061 7000
training loss = 1.136183738708496 7500
training loss = 1.0977216958999634 8000
training loss = 1.0682282447814941 8500
training loss = 1.0460516214370728 9000
training loss = 1.0297311544418335 9500
training loss = 1.0177171230316162 10000
training loss = 1.0086336135864258 10500
training loss = 1.0015983581542969 11000
training loss = 0.9959442019462585 11500
training loss = 0.99120032787323 12000
training loss = 0.9870781302452087 12500
training loss = 0.9833299517631531 13000
training loss = 0.9798868894577026 13500
training loss = 0.976703941822052 14000
training loss = 0.976859450340271 14500
training loss = 0.9711512327194214 15000
training loss = 0.9672746062278748 15500
training loss = 1.5582746267318726 16000
training loss = 0.9862335920333862 16500
training loss = 0.9577845335006714 17000
training loss = 0.9999097585678101 17500
training loss = 0.9492175579071045 18000
training loss = 0.9455787539482117 18500
training loss = 0.939527690410614 19000
training loss = 0.9344239830970764 19500
training loss = 0.9298208355903625 20000
training loss = 0.9258838295936584 20500
training loss = 0.9225831627845764 21000
training loss = 0.9198863506317139 21500
training loss = 0.9653220176696777 22000
training loss = 0.9981485605239868 22500
training loss = 0.9241325855255127 23000
training loss = 0.912851870059967 23500
training loss = 0.9115995168685913 24000
training loss = 0.9105845093727112 24500
training loss = 0.920873761177063 25000
training loss = 0.9111803770065308 25500
reduced chi^2 level 2 = 0.9109758138656616
Constrained alpha: 2.3842058181762695
Constrained beta: -0.8591539263725281
Constrained gamma: 51.99623489379883
(1, 38)
(1, 0)
