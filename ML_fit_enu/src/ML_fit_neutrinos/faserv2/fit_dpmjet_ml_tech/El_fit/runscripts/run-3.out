data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.5636510517896 7.684025576802971 80.03039251632151
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 74.1282958984375 500
training loss = 20.034738540649414 1000
training loss = 6.463130950927734 1500
training loss = 4.353736400604248 2000
training loss = 3.899348258972168 2500
training loss = 3.7299299240112305 3000
training loss = 3.655951738357544 3500
training loss = 3.443518877029419 4000
training loss = 3.3015711307525635 4500
training loss = 3.147674083709717 5000
training loss = 2.9758989810943604 5500
training loss = 6.9509758949279785 6000
training loss = 6.806822299957275 6500
training loss = 2.3617308139801025 7000
training loss = 1.997100830078125 7500
training loss = 1.9306470155715942 8000
training loss = 1.8924492597579956 8500
training loss = 1.7128022909164429 9000
training loss = 1.6185604333877563 9500
training loss = 1.5849051475524902 10000
training loss = 1.570460319519043 10500
training loss = 1.5703827142715454 11000
training loss = 1.5565531253814697 11500
training loss = 1.5524286031723022 12000
training loss = 1.5454411506652832 12500
training loss = 1.5410752296447754 13000
training loss = 1.537284016609192 13500
training loss = 1.5340855121612549 14000
training loss = 1.6179171800613403 14500
training loss = 1.5282566547393799 15000
training loss = 1.5253783464431763 15500
training loss = 1.5221314430236816 16000
training loss = 1.519116997718811 16500
training loss = 1.5162975788116455 17000
training loss = 1.5137553215026855 17500
training loss = 1.5112193822860718 18000
training loss = 1.5089248418807983 18500
training loss = 1.5064280033111572 19000
training loss = 1.5039873123168945 19500
training loss = 1.5015226602554321 20000
training loss = 1.4992344379425049 20500
training loss = 1.4968458414077759 21000
training loss = 1.494468331336975 21500
training loss = 1.492235779762268 22000
training loss = 1.4905304908752441 22500
training loss = 1.4882316589355469 23000
training loss = 1.4863451719284058 23500
training loss = 1.4840835332870483 24000
training loss = 1.4818958044052124 24500
training loss = 1.4796725511550903 25000
training loss = 1.4775991439819336 25500
training loss = 1.475513219833374 26000
training loss = 1.473394513130188 26500
training loss = 1.4712685346603394 27000
training loss = 3.614004135131836 27500
training loss = 1.592528223991394 28000
training loss = 1.7326772212982178 28500
training loss = 1.4705877304077148 29000
training loss = 1.4620110988616943 29500
training loss = 1.459783911705017 30000
training loss = 1.4578487873077393 30500
training loss = 1.4555636644363403 31000
training loss = 1.453542947769165 31500
training loss = 1.4516886472702026 32000
training loss = 1.4496874809265137 32500
training loss = 1.4475746154785156 33000
training loss = 1.445786714553833 33500
training loss = 1.4441181421279907 34000
training loss = 1.4436795711517334 34500
training loss = 1.4393070936203003 35000
reduced chi^2 level 2 = 1.4393234252929688
Constrained alpha: 3.3229331970214844
Constrained beta: 3.4715282917022705
Constrained gamma: 52.42911911010742
(1, 38)
(1, 0)
