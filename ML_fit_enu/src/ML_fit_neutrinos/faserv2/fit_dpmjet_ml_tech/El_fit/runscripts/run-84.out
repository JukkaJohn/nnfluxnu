data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.9333377314922098 11.854934519933087 92.88527744205801
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 79.34363555908203 500
training loss = 77.55070495605469 1000
training loss = 77.04313659667969 1500
training loss = 76.39248657226562 2000
training loss = 75.3363037109375 2500
training loss = 70.84822082519531 3000
training loss = 7.9570631980896 3500
training loss = 4.955977916717529 4000
training loss = 4.139448165893555 4500
training loss = 3.9970040321350098 5000
training loss = 3.974794626235962 5500
training loss = 3.969919443130493 6000
training loss = 3.9679012298583984 6500
training loss = 3.9663496017456055 7000
training loss = 3.9649107456207275 7500
training loss = 3.963505744934082 8000
training loss = 3.9650251865386963 8500
training loss = 4.088686943054199 9000
training loss = 3.9596188068389893 9500
training loss = 3.9587032794952393 10000
training loss = 3.9567952156066895 10500
training loss = 4.077306747436523 11000
training loss = 4.235428810119629 11500
training loss = 4.138942718505859 12000
training loss = 4.081706523895264 12500
training loss = 3.9542384147644043 13000
training loss = 4.112827301025391 13500
training loss = 4.248377799987793 14000
training loss = 3.947197437286377 14500
training loss = 3.9457144737243652 15000
training loss = 3.9443905353546143 15500
training loss = 3.9432766437530518 16000
training loss = 3.9421398639678955 16500
training loss = 3.942347526550293 17000
training loss = 3.940049886703491 17500
training loss = 3.938930034637451 18000
training loss = 3.937875986099243 18500
training loss = 3.9368550777435303 19000
training loss = 3.935875415802002 19500
training loss = 3.9348700046539307 20000
training loss = 3.933906078338623 20500
training loss = 3.9335079193115234 21000
training loss = 3.932037353515625 21500
training loss = 3.9311647415161133 22000
training loss = 3.9302561283111572 22500
training loss = 3.9293932914733887 23000
training loss = 3.9285552501678467 23500
training loss = 3.936901807785034 24000
training loss = 3.9269168376922607 24500
training loss = 3.9261281490325928 25000
training loss = 3.925365686416626 25500
training loss = 3.924609661102295 26000
training loss = 3.92387318611145 26500
training loss = 3.923163652420044 27000
training loss = 3.9226274490356445 27500
training loss = 3.9217891693115234 28000
training loss = 3.92112135887146 28500
training loss = 3.920562744140625 29000
training loss = 3.9227912425994873 29500
training loss = 3.9214775562286377 30000
training loss = 3.9674999713897705 30500
training loss = 4.039061069488525 31000
training loss = 3.923624277114868 31500
training loss = 4.053244590759277 32000
training loss = 3.9164621829986572 32500
training loss = 3.915858745574951 33000
training loss = 3.9352972507476807 33500
training loss = 6.439570903778076 34000
training loss = 3.914616346359253 34500
training loss = 4.0387420654296875 35000
(1, 38)
(1, 0)
