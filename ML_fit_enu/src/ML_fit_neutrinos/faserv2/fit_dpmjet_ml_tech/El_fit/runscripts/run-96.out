data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.2775329448622768 0.701335570028947 22.476461758763357
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 154.81471252441406 500
training loss = 35.691925048828125 1000
training loss = 30.801366806030273 1500
training loss = 29.42658805847168 2000
training loss = 25.96463966369629 2500
training loss = 14.711535453796387 3000
training loss = 5.4659743309021 3500
training loss = 3.1951966285705566 4000
training loss = 3.014397382736206 4500
training loss = 7.258936882019043 5000
training loss = 2.8269076347351074 5500
training loss = 2.7291629314422607 6000
training loss = 6.6330976486206055 6500
training loss = 2.6148416996002197 7000
training loss = 2.4928581714630127 7500
training loss = 2.442579984664917 8000
training loss = 2.4040932655334473 8500
training loss = 2.3826911449432373 9000
training loss = 2.350356101989746 9500
training loss = 2.3424506187438965 10000
training loss = 2.3950281143188477 10500
training loss = 2.2947211265563965 11000
training loss = 2.279101848602295 11500
training loss = 2.26405668258667 12000
training loss = 2.2496042251586914 12500
training loss = 2.2358076572418213 13000
training loss = 2.2226104736328125 13500
training loss = 2.2097935676574707 14000
training loss = 2.198047161102295 14500
training loss = 3.397864580154419 15000
training loss = 2.1733875274658203 15500
training loss = 2.1619057655334473 16000
training loss = 2.150543689727783 16500
training loss = 2.1393890380859375 17000
training loss = 2.1280438899993896 17500
training loss = 2.135556697845459 18000
training loss = 2.3270132541656494 18500
training loss = 2.091876745223999 19000
training loss = 2.0737788677215576 19500
training loss = 3.847947835922241 20000
training loss = 1.9452341794967651 20500
training loss = 1.9301284551620483 21000
training loss = 1.9189966917037964 21500
training loss = 1.9081802368164062 22000
training loss = 1.897309422492981 22500
training loss = 1.8865617513656616 23000
training loss = 1.876633882522583 23500
training loss = 3.2607924938201904 24000
training loss = 1.9226484298706055 24500
training loss = 4.856591701507568 25000
training loss = 2.316112995147705 25500
training loss = 1.8107531070709229 26000
training loss = 2.632397413253784 26500
training loss = 5.85610818862915 27000
training loss = 1.752779483795166 27500
training loss = 3.8615176677703857 28000
training loss = 2.2694098949432373 28500
training loss = 1.7939876317977905 29000
training loss = 1.6035672426223755 29500
training loss = 1.8030624389648438 30000
training loss = 1.5399017333984375 30500
training loss = 1.5287325382232666 31000
training loss = 1.4779597520828247 31500
training loss = 1.4502853155136108 32000
training loss = 1.9863393306732178 32500
training loss = 1.4491122961044312 33000
training loss = 1.400107741355896 33500
training loss = 1.3893458843231201 34000
training loss = 1.3800874948501587 34500
training loss = 1.3720256090164185 35000
reduced chi^2 level 2 = 1.372008204460144
Constrained alpha: 2.3335325717926025
Constrained beta: -0.00010973693133564666
Constrained gamma: 22.709760665893555
(1, 38)
(1, 0)
