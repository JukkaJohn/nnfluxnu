data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.9257384433644054 15.938495520856524 43.86548499227162
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 151.54684448242188 500
training loss = 70.40664672851562 1000
training loss = 69.58335876464844 1500
training loss = 68.34797668457031 2000
training loss = 64.70030975341797 2500
training loss = 5.7620344161987305 3000
training loss = 3.7408671379089355 3500
training loss = 2.873764753341675 4000
training loss = 2.6050267219543457 4500
training loss = 2.553917646408081 5000
training loss = 2.5453460216522217 5500
training loss = 2.5492193698883057 6000
training loss = 2.54036545753479 6500
training loss = 2.537961006164551 7000
training loss = 2.532259464263916 7500
training loss = 2.5997121334075928 8000
training loss = 2.5270838737487793 8500
training loss = 2.52469539642334 9000
training loss = 2.52241587638855 9500
training loss = 2.5202317237854004 10000
training loss = 2.5181357860565186 10500
training loss = 2.7463693618774414 11000
training loss = 2.578012228012085 11500
training loss = 2.512237071990967 12000
training loss = 2.5111167430877686 12500
training loss = 2.5086610317230225 13000
training loss = 2.5069167613983154 13500
training loss = 2.505657434463501 14000
training loss = 2.5041189193725586 14500
training loss = 2.502166271209717 15000
training loss = 2.5005829334259033 15500
training loss = 2.4991297721862793 16000
training loss = 2.4977309703826904 16500
training loss = 6.8311896324157715 17000
training loss = 2.7116281986236572 17500
training loss = 2.5322787761688232 18000
training loss = 2.5083653926849365 18500
training loss = 2.4914181232452393 19000
training loss = 2.493739604949951 19500
training loss = 3.8892359733581543 20000
training loss = 2.493490219116211 20500
training loss = 2.486846446990967 21000
training loss = 2.4857842922210693 21500
training loss = 2.484776496887207 22000
training loss = 2.4837939739227295 22500
training loss = 2.482828378677368 23000
training loss = 2.4828009605407715 23500
training loss = 2.480999708175659 24000
training loss = 2.4825422763824463 24500
training loss = 2.4853904247283936 25000
training loss = 2.4785146713256836 25500
training loss = 2.4776082038879395 26000
training loss = 2.4766852855682373 26500
training loss = 2.476065158843994 27000
training loss = 2.4750804901123047 27500
training loss = 2.474179267883301 28000
training loss = 2.473334312438965 28500
training loss = 2.4724557399749756 29000
training loss = 2.471529006958008 29500
training loss = 2.470529317855835 30000
training loss = 2.8119475841522217 30500
training loss = 2.467902421951294 31000
training loss = 2.465719223022461 31500
training loss = 2.4617393016815186 32000
training loss = 2.453238010406494 32500
training loss = 2.4430203437805176 33000
training loss = 2.435549259185791 33500
training loss = 2.4287607669830322 34000
training loss = 2.422241687774658 34500
training loss = 2.4158999919891357 35000
(1, 38)
(1, 0)
