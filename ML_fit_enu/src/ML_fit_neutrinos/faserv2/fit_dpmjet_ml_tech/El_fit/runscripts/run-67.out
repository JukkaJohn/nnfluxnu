data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.721604313347617 9.152406946682278 33.67490031663364
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 69.62005615234375 500
training loss = 69.20230102539062 1000
training loss = 69.17604064941406 1500
training loss = 69.12548828125 2000
training loss = 68.9928207397461 2500
training loss = 68.0725326538086 3000
training loss = 2.507082939147949 3500
training loss = 2.336777448654175 4000
training loss = 2.3318088054656982 4500
training loss = 2.328334331512451 5000
training loss = 2.325410842895508 5500
training loss = 2.3226122856140137 6000
training loss = 2.323657751083374 6500
training loss = 2.337663412094116 7000
training loss = 2.3143632411956787 7500
training loss = 2.310657262802124 8000
training loss = 2.30840802192688 8500
training loss = 2.303692102432251 9000
training loss = 2.74526047706604 9500
training loss = 2.3013105392456055 10000
training loss = 2.5688064098358154 10500
training loss = 2.304224729537964 11000
training loss = 2.2844483852386475 11500
training loss = 2.280134916305542 12000
training loss = 2.2759532928466797 12500
training loss = 2.2717807292938232 13000
training loss = 2.2675483226776123 13500
training loss = 2.2633676528930664 14000
training loss = 1.7668049335479736 14500
training loss = 1.7162492275238037 15000
training loss = 1.6992679834365845 15500
training loss = 1.6901891231536865 16000
training loss = 1.6828618049621582 16500
training loss = 1.6761703491210938 17000
training loss = 1.6700067520141602 17500
training loss = 2.128925085067749 18000
training loss = 3.6949381828308105 18500
training loss = 1.6522889137268066 19000
training loss = 1.6463968753814697 19500
training loss = 1.6948846578598022 20000
training loss = 2.909400463104248 20500
training loss = 1.6452763080596924 21000
training loss = 1.6239193677902222 21500
training loss = 1.6182464361190796 22000
training loss = 1.6135104894638062 22500
training loss = 1.6092849969863892 23000
training loss = 1.6055781841278076 23500
training loss = 1.6023545265197754 24000
training loss = 1.5994813442230225 24500
training loss = 3.805750846862793 25000
training loss = 2.4853193759918213 25500
training loss = 1.5930272340774536 26000
training loss = 1.5913866758346558 26500
training loss = 1.5899280309677124 27000
training loss = 5.527009010314941 27500
training loss = 1.5882954597473145 28000
training loss = 1.5897198915481567 28500
training loss = 1.603715419769287 29000
training loss = 1.5891857147216797 29500
training loss = 1.7619729042053223 30000
training loss = 1.5830200910568237 30500
training loss = 1.5822902917861938 31000
training loss = 2.900258779525757 31500
training loss = 1.6784570217132568 32000
training loss = 1.6103349924087524 32500
training loss = 1.5995426177978516 33000
training loss = 1.5794193744659424 33500
training loss = 1.578743815422058 34000
training loss = 1.578137993812561 34500
training loss = 1.57763671875 35000
reduced chi^2 level 2 = 1.5776320695877075
Constrained alpha: 2.3244025707244873
Constrained beta: 3.9621694087982178
Constrained gamma: 28.25457191467285
(1, 38)
(1, 0)
