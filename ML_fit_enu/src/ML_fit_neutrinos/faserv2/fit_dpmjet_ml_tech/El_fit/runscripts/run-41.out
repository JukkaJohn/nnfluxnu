data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.0544033411602496 8.703100420333637 46.19802780436335
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 81.73509979248047 500
training loss = 80.71212768554688 1000
training loss = 80.41614532470703 1500
training loss = 79.9886703491211 2000
training loss = 79.30291748046875 2500
training loss = 77.71246337890625 3000
training loss = 55.423919677734375 3500
training loss = 5.026491165161133 4000
training loss = 4.156033992767334 4500
training loss = 3.7631447315216064 5000
training loss = 3.616450309753418 5500
training loss = 3.567365884780884 6000
training loss = 3.5469970703125 6500
training loss = 3.525761604309082 7000
training loss = 3.446763753890991 7500
training loss = 4.364394187927246 8000
training loss = 3.4240500926971436 8500
training loss = 3.5495052337646484 9000
training loss = 3.382765531539917 9500
training loss = 3.3356142044067383 10000
training loss = 3.305007219314575 10500
training loss = 3.2636053562164307 11000
training loss = 3.207818031311035 11500
training loss = 3.1355488300323486 12000
training loss = 3.0494723320007324 12500
training loss = 2.9568941593170166 13000
training loss = 2.865095853805542 13500
training loss = 2.7823386192321777 14000
training loss = 2.7114686965942383 14500
training loss = 2.653923511505127 15000
training loss = 2.6089425086975098 15500
training loss = 2.575066566467285 16000
training loss = 2.5499649047851562 16500
training loss = 2.5317740440368652 17000
training loss = 2.5187036991119385 17500
training loss = 2.509322166442871 18000
training loss = 2.510800838470459 18500
training loss = 2.498434066772461 19000
training loss = 2.493938684463501 19500
training loss = 2.4911186695098877 20000
training loss = 2.4890105724334717 20500
training loss = 2.540863275527954 21000
training loss = 3.879211902618408 21500
training loss = 2.5012400150299072 22000
training loss = 2.4833197593688965 22500
training loss = 2.482379674911499 23000
training loss = 2.481417655944824 23500
training loss = 2.480564832687378 24000
training loss = 2.479764938354492 24500
training loss = 6.263179779052734 25000
training loss = 2.9183285236358643 25500
training loss = 2.486945390701294 26000
training loss = 2.476654529571533 26500
training loss = 2.47582745552063 27000
training loss = 2.475118637084961 27500
training loss = 2.474255323410034 28000
training loss = 2.473478317260742 28500
training loss = 2.472663402557373 29000
training loss = 2.4718637466430664 29500
training loss = 2.4710140228271484 30000
training loss = 2.4703564643859863 30500
training loss = 4.428768157958984 31000
training loss = 2.6548244953155518 31500
training loss = 2.467515707015991 32000
training loss = 2.4666030406951904 32500
training loss = 2.4656548500061035 33000
training loss = 2.4647316932678223 33500
training loss = 2.4637792110443115 34000
training loss = 2.4628376960754395 34500
training loss = 2.461926221847534 35000
(1, 38)
(1, 0)
