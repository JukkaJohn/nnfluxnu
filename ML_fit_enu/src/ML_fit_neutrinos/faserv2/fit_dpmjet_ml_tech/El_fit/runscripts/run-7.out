data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.2683700328953535 11.250747763094314 95.90524235692779
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 74.61931610107422 500
training loss = 73.81365966796875 1000
training loss = 72.716796875 1500
training loss = 48.0069694519043 2000
training loss = 4.763740062713623 2500
training loss = 3.846156120300293 3000
training loss = 3.436509847640991 3500
training loss = 3.291776180267334 4000
training loss = 3.249386787414551 4500
training loss = 4.573325157165527 5000
training loss = 3.228808879852295 5500
training loss = 3.3985564708709717 6000
training loss = 3.235166072845459 6500
training loss = 3.219597816467285 7000
training loss = 3.2156665325164795 7500
training loss = 3.2136600017547607 8000
training loss = 3.2118113040924072 8500
training loss = 3.2104580402374268 9000
training loss = 3.208700180053711 9500
training loss = 3.219942808151245 10000
training loss = 3.230026960372925 10500
training loss = 3.2049901485443115 11000
training loss = 3.2040557861328125 11500
training loss = 3.205742359161377 12000
training loss = 3.202559232711792 12500
training loss = 3.2010281085968018 13000
training loss = 3.200068473815918 13500
training loss = 3.199209213256836 14000
training loss = 3.198368787765503 14500
training loss = 3.1975550651550293 15000
training loss = 3.1968119144439697 15500
training loss = 3.4636054039001465 16000
training loss = 3.1990485191345215 16500
training loss = 3.9408106803894043 17000
training loss = 3.383676767349243 17500
training loss = 3.1931145191192627 18000
training loss = 3.192425012588501 18500
training loss = 3.191754102706909 19000
training loss = 3.191103219985962 19500
training loss = 3.190443277359009 20000
training loss = 3.1898157596588135 20500
training loss = 3.189190626144409 21000
training loss = 3.188596248626709 21500
training loss = 3.1879992485046387 22000
training loss = 3.187880277633667 22500
training loss = 3.18684458732605 23000
training loss = 3.1862776279449463 23500
training loss = 3.1857893466949463 24000
training loss = 3.1851933002471924 24500
training loss = 3.1846587657928467 25000
training loss = 3.1841461658477783 25500
training loss = 3.183645248413086 26000
training loss = 3.1831986904144287 26500
training loss = 3.1826703548431396 27000
training loss = 3.1822001934051514 27500
training loss = 3.1817400455474854 28000
training loss = 3.181317090988159 28500
training loss = 8.565279960632324 29000
training loss = 3.7288978099823 29500
training loss = 5.327622890472412 30000
training loss = 3.1796929836273193 30500
training loss = 3.179201364517212 31000
training loss = 3.698927640914917 31500
training loss = 7.9506354331970215 32000
training loss = 3.378727912902832 32500
training loss = 3.193608522415161 33000
training loss = 3.3203043937683105 33500
training loss = 3.177133083343506 34000
training loss = 3.205052375793457 34500
training loss = 3.2097620964050293 35000
(1, 38)
(1, 0)
