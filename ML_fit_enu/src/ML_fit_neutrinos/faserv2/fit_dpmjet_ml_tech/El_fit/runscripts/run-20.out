data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.670076105567529 12.726372708626544 75.82554413393325
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 59.24428939819336 500
training loss = 4.584349155426025 1000
training loss = 3.7645158767700195 1500
training loss = 3.3885252475738525 2000
training loss = 3.116947889328003 2500
training loss = 2.931490182876587 3000
training loss = 2.8181557655334473 3500
training loss = 2.7525222301483154 4000
training loss = 2.70766019821167 4500
training loss = 2.794173002243042 5000
training loss = 3.028380870819092 5500
training loss = 3.1288604736328125 6000
training loss = 2.5128345489501953 6500
training loss = 2.403759717941284 7000
training loss = 2.338838577270508 7500
training loss = 2.314704418182373 8000
training loss = 2.871581554412842 8500
training loss = 2.304631471633911 9000
training loss = 2.3034064769744873 9500
training loss = 2.3897528648376465 10000
training loss = 2.302396297454834 10500
training loss = 2.3026599884033203 11000
training loss = 3.4925196170806885 11500
training loss = 2.3383820056915283 12000
training loss = 2.301236629486084 12500
training loss = 2.3015332221984863 13000
training loss = 2.6707074642181396 13500
training loss = 2.679788589477539 14000
training loss = 2.3004937171936035 14500
training loss = 2.322991132736206 15000
training loss = 2.300067901611328 15500
training loss = 2.2998669147491455 16000
training loss = 2.2996907234191895 16500
training loss = 5.077809810638428 17000
training loss = 2.3400354385375977 17500
training loss = 2.352313995361328 18000
training loss = 2.299877882003784 18500
training loss = 2.2986764907836914 19000
training loss = 2.2985615730285645 19500
training loss = 3.5108587741851807 20000
training loss = 2.597923994064331 20500
training loss = 2.297915458679199 21000
training loss = 2.297722816467285 21500
training loss = 2.2975833415985107 22000
training loss = 2.297430992126465 22500
training loss = 2.297248601913452 23000
training loss = 2.297074556350708 23500
training loss = 2.311413049697876 24000
training loss = 2.2967793941497803 24500
training loss = 2.296537399291992 25000
training loss = 2.2963831424713135 25500
training loss = 2.2964348793029785 26000
training loss = 2.3104636669158936 26500
training loss = 2.5147101879119873 27000
training loss = 4.132462024688721 27500
training loss = 2.298260450363159 28000
training loss = 2.2960124015808105 28500
training loss = 4.589471340179443 29000
training loss = 2.3152403831481934 29500
training loss = 2.2956507205963135 30000
training loss = 2.2947497367858887 30500
training loss = 2.2945380210876465 31000
training loss = 2.30069899559021 31500
training loss = 2.2982213497161865 32000
training loss = 2.294219493865967 32500
training loss = 2.317716360092163 33000
training loss = 2.3014602661132812 33500
training loss = 2.3308141231536865 34000
training loss = 2.3228306770324707 34500
training loss = 2.2941019535064697 35000
(1, 38)
(1, 0)
