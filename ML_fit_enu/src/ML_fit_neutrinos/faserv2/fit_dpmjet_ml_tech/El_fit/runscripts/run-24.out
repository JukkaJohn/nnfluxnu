data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.650912040473594 10.775332130706328 87.3705650124293
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 78.69267272949219 500
training loss = 78.49198150634766 1000
training loss = 78.45526123046875 1500
training loss = 78.4112319946289 2000
training loss = 78.36083984375 2500
training loss = 78.304931640625 3000
training loss = 78.88339233398438 3500
training loss = 78.17133331298828 4000
training loss = 78.08264923095703 4500
training loss = 77.969970703125 5000
training loss = 77.82194519042969 5500
training loss = 77.62105560302734 6000
training loss = 77.33895874023438 6500
training loss = 76.92576599121094 7000
training loss = 76.28203582763672 7500
training loss = 75.2267074584961 8000
training loss = 73.08402252197266 8500
training loss = 58.823768615722656 9000
training loss = 5.821730613708496 9500
training loss = 4.652566432952881 10000
training loss = 3.6447432041168213 10500
training loss = 3.299584150314331 11000
training loss = 3.027317762374878 11500
training loss = 2.862253427505493 12000
training loss = 2.7456142902374268 12500
training loss = 2.640123128890991 13000
training loss = 2.535095691680908 13500
training loss = 2.438861846923828 14000
training loss = 2.3660507202148438 14500
training loss = 2.311596632003784 15000
training loss = 2.2703287601470947 15500
training loss = 2.409531831741333 16000
training loss = 2.375746965408325 16500
training loss = 2.2101964950561523 17000
training loss = 2.4170398712158203 17500
training loss = 2.4769248962402344 18000
training loss = 2.2559473514556885 18500
training loss = 3.635486125946045 19000
training loss = 2.2961528301239014 19500
training loss = 2.2008533477783203 20000
training loss = 2.1018881797790527 20500
training loss = 2.0960490703582764 21000
training loss = 2.086669921875 21500
training loss = 2.0802741050720215 22000
training loss = 2.074906587600708 22500
training loss = 2.0702712535858154 23000
training loss = 2.0665783882141113 23500
training loss = 2.0643198490142822 24000
training loss = 2.0733213424682617 24500
training loss = 2.058176040649414 25000
training loss = 2.055422782897949 25500
training loss = 2.0526998043060303 26000
training loss = 2.0500612258911133 26500
training loss = 2.0465633869171143 27000
training loss = 2.043337345123291 27500
training loss = 2.040485143661499 28000
training loss = 6.190523624420166 28500
training loss = 2.0619657039642334 29000
training loss = 2.0318877696990967 29500
training loss = 2.0275092124938965 30000
training loss = 2.023129463195801 30500
training loss = 2.0183722972869873 31000
training loss = 2.012803792953491 31500
training loss = 2.0070314407348633 32000
training loss = 2.001588821411133 32500
training loss = 1.9954596757888794 33000
training loss = 1.9898959398269653 33500
training loss = 1.9843709468841553 34000
training loss = 1.979168176651001 34500
training loss = 1.9737147092819214 35000
reduced chi^2 level 2 = 1.9736928939819336
Constrained alpha: 4.104698657989502
Constrained beta: 3.427976369857788
Constrained gamma: 59.75019836425781
(1, 38)
(1, 0)
