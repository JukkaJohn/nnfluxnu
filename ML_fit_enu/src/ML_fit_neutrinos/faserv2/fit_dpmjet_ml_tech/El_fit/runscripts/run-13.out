data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.7252679158019809 13.906921810875595 48.544357745764664
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 106.94696807861328 500
training loss = 75.94593811035156 1000
training loss = 74.56913757324219 1500
training loss = 72.58116912841797 2000
training loss = 68.00033569335938 2500
training loss = 12.464888572692871 3000
training loss = 5.438581943511963 3500
training loss = 3.8054609298706055 4000
training loss = 3.4680287837982178 4500
training loss = 3.5906822681427 5000
training loss = 3.9425346851348877 5500
training loss = 3.2969460487365723 6000
training loss = 3.272160053253174 6500
training loss = 3.2546191215515137 7000
training loss = 3.242079973220825 7500
training loss = 3.2329068183898926 8000
training loss = 3.2259185314178467 8500
training loss = 3.2205488681793213 9000
training loss = 3.216228485107422 9500
training loss = 3.2127535343170166 10000
training loss = 3.209728956222534 10500
training loss = 3.2071402072906494 11000
training loss = 3.2048585414886475 11500
training loss = 3.2028086185455322 12000
training loss = 3.200953483581543 12500
training loss = 3.199233293533325 13000
training loss = 3.1975982189178467 13500
training loss = 3.19606351852417 14000
training loss = 3.1946282386779785 14500
training loss = 3.1933400630950928 15000
training loss = 3.1918914318084717 15500
training loss = 3.1906027793884277 16000
training loss = 3.1906991004943848 16500
training loss = 3.203479766845703 17000
training loss = 3.2107231616973877 17500
training loss = 3.1865673065185547 18000
training loss = 3.1847758293151855 18500
training loss = 3.1837241649627686 19000
training loss = 3.1827056407928467 19500
training loss = 3.181706428527832 20000
training loss = 5.251360893249512 20500
training loss = 3.2327370643615723 21000
training loss = 3.195993661880493 21500
training loss = 3.178036689758301 22000
training loss = 3.1772053241729736 22500
training loss = 3.1763880252838135 23000
training loss = 3.1755847930908203 23500
training loss = 3.1748194694519043 24000
training loss = 3.174071788787842 24500
training loss = 3.1733508110046387 25000
training loss = 6.313161373138428 25500
training loss = 3.177902936935425 26000
training loss = 3.1713197231292725 26500
training loss = 3.1706972122192383 27000
training loss = 4.616128444671631 27500
training loss = 3.6591246128082275 28000
training loss = 3.9497358798980713 28500
training loss = 3.1823651790618896 29000
training loss = 3.167799711227417 29500
training loss = 3.167253017425537 30000
training loss = 3.1667239665985107 30500
training loss = 3.1662237644195557 31000
training loss = 3.1657392978668213 31500
training loss = 3.1652824878692627 32000
training loss = 3.1648247241973877 32500
training loss = 3.1643693447113037 33000
training loss = 3.163942575454712 33500
training loss = 3.163517951965332 34000
training loss = 3.163165807723999 34500
training loss = 3.1627442836761475 35000
(1, 38)
(1, 0)
