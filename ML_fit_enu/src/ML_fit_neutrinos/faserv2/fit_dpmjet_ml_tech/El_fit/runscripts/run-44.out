data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.2314122507059944 17.64224637739519 4.548404752120572
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 129.2864990234375 500
training loss = 76.20552825927734 1000
training loss = 76.03018188476562 1500
training loss = 75.92549133300781 2000
training loss = 75.80558013916016 2500
training loss = 75.67244720458984 3000
training loss = 75.52173614501953 3500
training loss = 75.32537841796875 4000
training loss = 74.94686126708984 4500
training loss = 73.29339599609375 5000
training loss = 5.992013454437256 5500
training loss = 3.6013259887695312 6000
training loss = 2.907589912414551 6500
training loss = 2.6959590911865234 7000
training loss = 2.627394199371338 7500
training loss = 2.6051933765411377 8000
training loss = 2.597714900970459 8500
training loss = 2.594966411590576 9000
training loss = 4.224069595336914 9500
training loss = 2.594987154006958 10000
training loss = 3.622776985168457 10500
training loss = 2.5919718742370605 11000
training loss = 2.591526985168457 11500
training loss = 2.591102123260498 12000
training loss = 2.5906944274902344 12500
training loss = 4.12222146987915 13000
training loss = 2.5902369022369385 13500
training loss = 2.58955454826355 14000
training loss = 2.5894317626953125 14500
training loss = 2.5888519287109375 15000
training loss = 2.5885157585144043 15500
training loss = 2.5881967544555664 16000
training loss = 2.587881326675415 16500
training loss = 2.5906050205230713 17000
training loss = 6.395013332366943 17500
training loss = 2.6444034576416016 18000
training loss = 2.5870444774627686 18500
training loss = 2.5864765644073486 19000
training loss = 2.586183786392212 19500
training loss = 2.585916757583618 20000
training loss = 2.585674524307251 20500
training loss = 2.5854296684265137 21000
training loss = 2.585195302963257 21500
training loss = 5.032554626464844 22000
training loss = 2.584744691848755 22500
training loss = 2.7774593830108643 23000
training loss = 2.802067756652832 23500
training loss = 2.6037967205047607 24000
training loss = 2.6926043033599854 24500
training loss = 2.7614054679870605 25000
training loss = 3.3351376056671143 25500
training loss = 2.6052985191345215 26000
training loss = 2.5846731662750244 26500
training loss = 2.5842881202697754 27000
training loss = 2.5828864574432373 27500
training loss = 2.582784652709961 28000
training loss = 2.8404476642608643 28500
training loss = 2.6597671508789062 29000
training loss = 2.6511709690093994 29500
training loss = 2.582156181335449 30000
training loss = 2.5821123123168945 30500
training loss = 2.5817954540252686 31000
training loss = 2.581660747528076 31500
training loss = 2.5815374851226807 32000
training loss = 2.5814359188079834 32500
training loss = 2.581284523010254 33000
training loss = 2.581164598464966 33500
training loss = 2.5810539722442627 34000
training loss = 2.580944776535034 34500
training loss = 2.5808358192443848 35000
(1, 38)
(1, 0)
