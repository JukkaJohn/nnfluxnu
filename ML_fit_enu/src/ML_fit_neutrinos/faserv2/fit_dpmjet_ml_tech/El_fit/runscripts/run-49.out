data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.732621875705628 16.172216885623495 46.76206605038592
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 99.23896026611328 500
training loss = 74.47883605957031 1000
training loss = 74.4027328491211 1500
training loss = 74.16365051269531 2000
training loss = 67.6817855834961 2500
training loss = 2.1614739894866943 3000
training loss = 2.0950379371643066 3500
training loss = 2.066802978515625 4000
training loss = 2.0547029972076416 4500
training loss = 2.0483975410461426 5000
training loss = 2.044739246368408 5500
training loss = 2.0424299240112305 6000
training loss = 2.0413320064544678 6500
training loss = 3.898175001144409 7000
training loss = 2.734652519226074 7500
training loss = 2.057462692260742 8000
training loss = 2.038038492202759 8500
training loss = 2.037750482559204 9000
training loss = 2.0373618602752686 9500
training loss = 2.037141799926758 10000
training loss = 2.0369603633880615 10500
training loss = 2.0368101596832275 11000
training loss = 2.0366930961608887 11500
training loss = 2.036581039428711 12000
training loss = 2.726698160171509 12500
training loss = 2.0470333099365234 13000
training loss = 2.0652143955230713 13500
training loss = 1.8877134323120117 14000
training loss = 2.5561153888702393 14500
training loss = 1.6789425611495972 15000
training loss = 1.6909579038619995 15500
training loss = 1.6308592557907104 16000
training loss = 1.5878394842147827 16500
training loss = 1.5745474100112915 17000
training loss = 1.5650269985198975 17500
training loss = 1.558127522468567 18000
training loss = 1.553095817565918 18500
training loss = 1.5492055416107178 19000
training loss = 1.5459333658218384 19500
training loss = 1.5433412790298462 20000
training loss = 1.5412477254867554 20500
training loss = 1.5393527746200562 21000
training loss = 1.5378397703170776 21500
training loss = 1.5364418029785156 22000
training loss = 1.5352469682693481 22500
training loss = 1.5341025590896606 23000
training loss = 1.5331507921218872 23500
training loss = 1.5322233438491821 24000
training loss = 1.531385898590088 24500
training loss = 1.5307549238204956 25000
training loss = 1.5752458572387695 25500
training loss = 1.5293869972229004 26000
training loss = 1.5287528038024902 26500
training loss = 1.528185248374939 27000
training loss = 1.5275503396987915 27500
training loss = 1.5269352197647095 28000
training loss = 1.526352047920227 28500
training loss = 1.5258435010910034 29000
training loss = 1.5273895263671875 29500
training loss = 1.5253636837005615 30000
training loss = 1.5241141319274902 30500
training loss = 1.523558497428894 31000
training loss = 1.5230014324188232 31500
training loss = 1.5224233865737915 32000
training loss = 1.5219676494598389 32500
training loss = 1.5214359760284424 33000
training loss = 1.5207728147506714 33500
training loss = 1.5201607942581177 34000
training loss = 1.5195624828338623 34500
training loss = 2.392441511154175 35000
(1, 38)
(1, 0)
