data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.189424346806235 14.412893522305003 10.058470735121361
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 78.81832122802734 500
training loss = 78.52008056640625 1000
training loss = 78.45596313476562 1500
training loss = 78.37913513183594 2000
training loss = 78.29298400878906 2500
training loss = 78.20118713378906 3000
training loss = 78.10778045654297 3500
training loss = 78.0162582397461 4000
training loss = 77.93004608154297 4500
training loss = 77.85681915283203 5000
training loss = 77.7966537475586 5500
training loss = 77.74342346191406 6000
training loss = 77.69337463378906 6500
training loss = 77.65638732910156 7000
training loss = 77.76576232910156 7500
training loss = 77.54956817626953 8000
training loss = 77.40216064453125 8500
training loss = 77.03288269042969 9000
training loss = 62.89373016357422 9500
training loss = 2.1945624351501465 10000
training loss = 1.973427653312683 10500
training loss = 1.9040688276290894 11000
training loss = 1.8821812868118286 11500
training loss = 1.8750098943710327 12000
training loss = 1.8727058172225952 12500
training loss = 1.8719755411148071 13000
training loss = 1.8809975385665894 13500
training loss = 1.8723015785217285 14000
training loss = 1.724501609802246 14500
training loss = 1.6331511735916138 15000
training loss = 1.595861792564392 15500
training loss = 1.5565608739852905 16000
training loss = 1.5344702005386353 16500
training loss = 1.520949363708496 17000
training loss = 2.5181655883789062 17500
training loss = 1.4933534860610962 18000
training loss = 1.5366259813308716 18500
training loss = 1.5867197513580322 19000
training loss = 1.466652274131775 19500
training loss = 1.4585063457489014 20000
training loss = 1.4510124921798706 20500
training loss = 1.4438899755477905 21000
training loss = 1.4372951984405518 21500
training loss = 1.431190848350525 22000
training loss = 1.4254015684127808 22500
training loss = 1.4199013710021973 23000
training loss = 1.4147177934646606 23500
training loss = 3.7177517414093018 24000
training loss = 2.191358804702759 24500
training loss = 1.4308315515518188 25000
training loss = 1.3950846195220947 25500
training loss = 1.3896595239639282 26000
training loss = 1.3849384784698486 26500
training loss = 1.6544638872146606 27000
training loss = 1.375419020652771 27500
training loss = 1.393762230873108 28000
training loss = 1.5510587692260742 28500
training loss = 1.3681684732437134 29000
training loss = 1.3579177856445312 29500
training loss = 2.2774503231048584 30000
training loss = 2.8244595527648926 30500
training loss = 1.34060800075531 31000
training loss = 1.3355716466903687 31500
training loss = 1.3461893796920776 32000
training loss = 1.3567616939544678 32500
training loss = 1.327978491783142 33000
training loss = 1.314520239830017 33500
training loss = 1.3090966939926147 34000
training loss = 1.3035246133804321 34500
training loss = 1.297946572303772 35000
reduced chi^2 level 2 = 1.2979322671890259
Constrained alpha: 2.3296382427215576
Constrained beta: 3.638033628463745
Constrained gamma: 14.838334083557129
(1, 38)
(1, 0)
