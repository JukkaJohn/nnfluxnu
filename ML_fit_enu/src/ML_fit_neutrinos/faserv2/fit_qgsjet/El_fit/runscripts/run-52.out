data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 4.59863711e+03 4.68015448e+03 4.75745306e+03 4.89189955e+03
 5.03306554e+03 5.25084451e+03 5.31924965e+03 5.30780862e+03
 5.49884518e+03 5.58969112e+03 5.62078229e+03 5.77803005e+03
 5.88141140e+03 5.88125978e+03 6.05927287e+03 6.13031550e+03
 6.21215635e+03 6.22466275e+03 6.28964100e+03 6.30713567e+03
 6.40962233e+03 6.46794368e+03 6.49701160e+03 6.53896370e+03
 6.53845880e+03 6.53567124e+03 6.44810357e+03 6.45820672e+03
 6.38706359e+03 6.38775651e+03 6.34925011e+03 6.28975769e+03
 6.24199150e+03 6.15593398e+03 5.98687512e+03 5.89776760e+03
 5.80819436e+03 5.70674893e+03 5.61362163e+03 5.45449633e+03
 5.27910895e+03 5.10099627e+03 4.88733892e+03 4.75496746e+03
 4.58522449e+03 4.39909823e+03 4.21432177e+03 4.01963689e+03
 3.82568239e+03 3.63143648e+03 3.42217829e+03 3.24302173e+03
 3.06838529e+03 2.82961444e+03 2.63834916e+03 2.47086875e+03
 2.26008073e+03 2.07153087e+03 1.91903063e+03 1.77062672e+03
 1.60161807e+03 1.46477172e+03 1.31482987e+03 1.16991884e+03
 1.03072866e+03 9.08732083e+02 8.01859410e+02 7.14303127e+02
 6.25649099e+02 5.29022549e+02 4.48027815e+02 3.75918663e+02
 3.08871662e+02 2.58008330e+02 2.11075907e+02 1.71977942e+02
 1.36271458e+02 1.01400294e+02 7.56636555e+01 5.34445444e+01
 3.81358484e+01 3.06122499e+01 2.46286677e+01 1.96825222e+01
 1.43512283e+01 1.02373550e+01 7.25445310e+00 4.75220210e+00
 3.46055415e+00 1.26075080e+00 4.40917380e-02 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
13
data after
[4598.637112035384, 4680.154479623603, 4757.453058924168, 4891.899553467534, 5033.065538444174, 5250.844508347476, 5319.249647355908, 5307.808624122501, 5498.8451763690655, 5589.691123323983, 5620.782289333134, 5778.03004530935, 5881.411402803338, 5881.259776552001, 6059.272868685972, 6130.315497527697, 6212.156347580102, 6224.662749256414, 6289.640998493531, 6307.135673671542, 6409.622332909766, 6467.943678683994, 6497.011599509635, 6538.963699182047, 6538.458798296524, 6535.671242347543, 6448.103568769147, 6458.206720188486, 6387.063587725059, 6387.756505370097, 6349.250114943531, 6289.757692343994, 6241.991496195412, 6155.933976227414, 5986.875123412361, 5897.767601440559, 5808.194360695562, 5706.748933532996, 5613.621626383167, 5454.4963259615315, 5279.108954913488, 5100.996268306558, 4887.338919196416, 4754.9674595152, 4585.2244907523245, 4399.098227920245, 4214.321766575668, 4019.6368906551497, 3825.682390795968, 3631.436478807638, 3422.1782904571746, 3243.021732848288, 3068.385292772373, 2829.6144403244302, 2638.349158203089, 2470.868747005664, 2260.0807280632694, 2071.5308676132636, 1919.0306260709006, 1770.6267206609336, 1601.6180686170028, 1464.7717208040392, 1314.8298664835588, 1169.9188430405156, 1030.728657142295, 908.7320834636424, 801.8594095709651, 714.3031271689983, 625.6490990497571, 529.0225487287121, 448.02781538345835, 375.9186633860295, 308.87166166488817, 258.00833044366084, 211.0759065107323, 171.97794193324273, 136.27145767929665, 101.40029422789658, 75.66365552266346, 53.44454437489857, 38.135848364429506, 30.61224985996854, 24.62866771746742, 34.033750557709794, 27.009406838029562]
16
data,sig_tot
[4598.63711204 4680.15447962 4757.45305892 4891.89955347 5033.06553844
 5250.84450835 5319.24964736 5307.80862412 5498.84517637 5589.69112332
 5620.78228933 5778.03004531 5881.4114028  5881.25977655 6059.27286869
 6130.31549753 6212.15634758 6224.66274926 6289.64099849 6307.13567367
 6409.62233291 6467.94367868 6497.01159951 6538.96369918 6538.4587983
 6535.67124235 6448.10356877 6458.20672019 6387.06358773 6387.75650537
 6349.25011494 6289.75769234 6241.9914962  6155.93397623 5986.87512341
 5897.76760144 5808.1943607  5706.74893353 5613.62162638 5454.49632596
 5279.10895491 5100.99626831 4887.3389192  4754.96745952 4585.22449075
 4399.09822792 4214.32176658 4019.63689066 3825.6823908  3631.43647881
 3422.17829046 3243.02173285 3068.38529277 2829.61444032 2638.3491582
 2470.86874701 2260.08072806 2071.53086761 1919.03062607 1770.62672066
 1601.61806862 1464.7717208  1314.82986648 1169.91884304 1030.72865714
  908.73208346  801.85940957  714.30312717  625.64909905  529.02254873
  448.02781538  375.91866339  308.87166166  258.00833044  211.07590651
  171.97794193  136.27145768  101.40029423   75.66365552   53.44454437
   38.13584836   30.61224986   24.62866772   34.03375056   27.00940684
  832.72626449  873.99479789  909.19676465  941.47448005  962.13522622
  993.40509244 1039.66626667 1088.05066004 1112.54806984 1128.18529091
 1170.71331392 1195.06458388 1253.33901639 1249.67593099 1317.22359877
 1338.19033244 1348.72499168 1381.03710185 1424.19803751 1433.22419144
 1455.88636466 1483.04192217 1499.94546688 1531.42364049 1530.57360366
 1516.92433255 1540.61663293 1546.63280156 1543.34752414 1549.76498637
 1546.32171994 1536.78196995 1539.29973856 1518.16473872 1507.62114674
 1473.99624591 1450.69937823 1425.60352814 1383.75766883 1370.8740982
 1336.89446446 1297.4037168  1248.09419384 1206.40513378 1173.84396623
 1113.88471416 1080.12537513 1031.05072614  985.03145416  932.52276845
  881.56528629  828.86028791  784.14566135  730.99829137  685.49339364
  630.19345955  581.93435263  539.03486413  490.91769247  448.82323181
  408.53215009  364.51893048  329.02110108  289.65975719  255.13824845
  222.38076109  195.38120612  170.46178533  146.75636458  122.80164965
  102.42781658   82.72280092   66.808603     53.48717775   44.68730108
   36.27075316   27.73754015   20.56293034   23.81160336   26.18315604]
[4598.63711204 4680.15447962 4757.45305892 4891.89955347 5033.06553844
 5250.84450835 5319.24964736 5307.80862412 5498.84517637 5589.69112332
 5620.78228933 5778.03004531 5881.4114028  5881.25977655 6059.27286869
 6130.31549753 6212.15634758 6224.66274926 6289.64099849 6307.13567367
 6409.62233291 6467.94367868 6497.01159951 6538.96369918 6538.4587983
 6535.67124235 6448.10356877 6458.20672019 6387.06358773 6387.75650537
 6349.25011494 6289.75769234 6241.9914962  6155.93397623 5986.87512341
 5897.76760144 5808.1943607  5706.74893353 5613.62162638 5454.49632596
 5279.10895491 5100.99626831 4887.3389192  4754.96745952 4585.22449075
 4399.09822792 4214.32176658 4019.63689066 3825.6823908  3631.43647881
 3422.17829046 3243.02173285 3068.38529277 2829.61444032 2638.3491582
 2470.86874701 2260.08072806 2071.53086761 1919.03062607 1770.62672066
 1601.61806862 1464.7717208  1314.82986648 1169.91884304 1030.72865714
  908.73208346  801.85940957  714.30312717  625.64909905  529.02254873
  448.02781538  375.91866339  308.87166166  258.00833044  211.07590651
  171.97794193  136.27145768  101.40029423   75.66365552   53.44454437
   38.13584836   30.61224986   24.62866772   34.03375056   27.00940684
  832.72626449  873.99479789  909.19676465  941.47448005  962.13522622
  993.40509244 1039.66626667 1088.05066004 1112.54806984 1128.18529091
 1170.71331392 1195.06458388 1253.33901639 1249.67593099 1317.22359877
 1338.19033244 1348.72499168 1381.03710185 1424.19803751 1433.22419144
 1455.88636466 1483.04192217 1499.94546688 1531.42364049 1530.57360366
 1516.92433255 1540.61663293 1546.63280156 1543.34752414 1549.76498637
 1546.32171994 1536.78196995 1539.29973856 1518.16473872 1507.62114674
 1473.99624591 1450.69937823 1425.60352814 1383.75766883 1370.8740982
 1336.89446446 1297.4037168  1248.09419384 1206.40513378 1173.84396623
 1113.88471416 1080.12537513 1031.05072614  985.03145416  932.52276845
  881.56528629  828.86028791  784.14566135  730.99829137  685.49339364
  630.19345955  581.93435263  539.03486413  490.91769247  448.82323181
  408.53215009  364.51893048  329.02110108  289.65975719  255.13824845
  222.38076109  195.38120612  170.46178533  146.75636458  122.80164965
  102.42781658   82.72280092   66.808603     53.48717775   44.68730108
   36.27075316   27.73754015   20.56293034   23.81160336   26.18315604]
2.9388807600792424 3.0349312910721093 53.59332074265819
val isze = 16
idinces = [ 43  63  87  20 129  68  98 104  66 133 145  10 136  24  61 155  58  70
  52 105   1  35  99  39  19  47  49 106  21  60 103 132 138  97 128 141
  69   2  56 135  38  76 116 118 131  72  82  59  55 134  83  26 137  67
  41  78  32 161 158  48 108  25  53  90 109  51  14 150  29   3 159 114
 153  37 154 152  45 144  81  42  79 130 143  12 110 124  85   6  91  95
 163 151  34 140  50 102 101 112   4   5  44 111  96  84 142 119  75   7
  46  17 121 117  71  80 127 162 164 146  74  28  11 120  94  23  22 148
  93  18  27  36  57  31  65  89  30  86  92 126  13  77 147 149  33  62
 122 107  88  54 139 100  16 115  40   0  73   8 160 157 156 123 113  64
  15 125   9]
we are doing training validation split
training loss = 17.52080535888672 500
val loss = 13.353767395019531
training loss = 11.633548736572266 1000
val loss = 15.708610534667969
training loss = 10.591911315917969 1500
val loss = 15.703391075134277
training loss = 10.409711837768555 2000
val loss = 15.621123313903809
training loss = 6.649993419647217 2500
val loss = 9.676447868347168
training loss = 2.3777875900268555 3000
val loss = 2.987489700317383
training loss = 2.136749029159546 3500
val loss = 3.194878578186035
training loss = 1.9719983339309692 4000
val loss = 3.1241724491119385
training loss = 2.51396107673645 4500
val loss = 3.0872597694396973
training loss = 1.899613380432129 5000
val loss = 3.1733622550964355
training loss = 1.8640984296798706 5500
val loss = 3.1056602001190186
training loss = 2.1287763118743896 6000
val loss = 3.7186198234558105
training loss = 1.8077178001403809 6500
val loss = 3.041293144226074
training loss = 1.7814393043518066 7000
val loss = 3.0730819702148438
training loss = 1.7715933322906494 7500
val loss = 3.037097930908203
training loss = 1.8521947860717773 8000
val loss = 3.326082468032837
training loss = 1.9017969369888306 8500
val loss = 2.903754234313965
training loss = 1.7544441223144531 9000
val loss = 3.0207130908966064
training loss = 1.835060954093933 9500
val loss = 2.889289379119873
training loss = 1.7754955291748047 10000
val loss = 2.969467878341675
training loss = 2.0593526363372803 10500
val loss = 2.937495470046997
training loss = 1.7435736656188965 11000
val loss = 3.047006130218506
training loss = 1.748176097869873 11500
val loss = 3.0124526023864746
training loss = 1.9722011089324951 12000
val loss = 3.5409836769104004
training loss = 1.7414333820343018 12500
val loss = 3.0443761348724365
training loss = 1.8851699829101562 13000
val loss = 2.885221481323242
training loss = 1.7566360235214233 13500
val loss = 2.96372652053833
training loss = 1.7397303581237793 14000
val loss = 3.0354349613189697
training loss = 1.7495747804641724 14500
val loss = 2.942481517791748
training loss = 1.7403308153152466 15000
val loss = 2.9937736988067627
training loss = 1.8634284734725952 15500
val loss = 2.904768466949463
training loss = 1.7441226243972778 16000
val loss = 3.0596671104431152
training loss = 1.7376184463500977 16500
val loss = 2.986344337463379
training loss = 1.7366564273834229 17000
val loss = 2.983872413635254
training loss = 2.2511353492736816 17500
val loss = 3.892439365386963
training loss = 1.7489591836929321 18000
val loss = 2.909973621368408
training loss = 1.736585021018982 18500
val loss = 2.9419515132904053
training loss = 2.4601755142211914 19000
val loss = 3.121581554412842
training loss = 1.7594821453094482 19500
val loss = 2.8666062355041504
training loss = 1.7359613180160522 20000
val loss = 2.936757802963257
training loss = 1.8200392723083496 20500
val loss = 2.816579818725586
training loss = 1.7362921237945557 21000
val loss = 2.9097366333007812
training loss = 1.7410129308700562 21500
val loss = 2.964590549468994
training loss = 1.7421098947525024 22000
val loss = 2.978065013885498
training loss = 2.334641218185425 22500
val loss = 2.959333896636963
training loss = 1.7361310720443726 23000
val loss = 2.906035900115967
training loss = 1.8825509548187256 23500
val loss = 2.7572312355041504
training loss = 1.7369831800460815 24000
val loss = 2.9301486015319824
training loss = 1.7446739673614502 24500
val loss = 2.9897804260253906
training loss = 1.736160397529602 25000
val loss = 2.8979697227478027
training loss = 1.737431287765503 25500
val loss = 2.92812180519104
training loss = 1.7418888807296753 26000
val loss = 2.9516327381134033
training loss = 1.7363948822021484 26500
val loss = 2.891361951828003
training loss = 1.7855603694915771 27000
val loss = 2.814138889312744
training loss = 1.7390198707580566 27500
val loss = 2.937580108642578
training loss = 1.7440102100372314 28000
val loss = 2.951781749725342
training loss = 1.7565600872039795 28500
val loss = 3.0040037631988525
training loss = 1.7384250164031982 29000
val loss = 2.9285216331481934
training loss = 1.7555495500564575 29500
val loss = 3.005988359451294
training loss = 1.737002968788147 30000
val loss = 2.8782386779785156
reduced chi^2 level 2 = 1.7371903657913208
Constrained alpha: 2.3538401126861572
Constrained beta: 3.0458054542541504
Constrained gamma: 22.503273010253906
(1, 149)
(1, 16)
