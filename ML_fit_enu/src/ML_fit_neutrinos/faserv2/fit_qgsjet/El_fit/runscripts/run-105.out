data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 4.59863711e+03 4.68015448e+03 4.75745306e+03 4.89189955e+03
 5.03306554e+03 5.25084451e+03 5.31924965e+03 5.30780862e+03
 5.49884518e+03 5.58969112e+03 5.62078229e+03 5.77803005e+03
 5.88141140e+03 5.88125978e+03 6.05927287e+03 6.13031550e+03
 6.21215635e+03 6.22466275e+03 6.28964100e+03 6.30713567e+03
 6.40962233e+03 6.46794368e+03 6.49701160e+03 6.53896370e+03
 6.53845880e+03 6.53567124e+03 6.44810357e+03 6.45820672e+03
 6.38706359e+03 6.38775651e+03 6.34925011e+03 6.28975769e+03
 6.24199150e+03 6.15593398e+03 5.98687512e+03 5.89776760e+03
 5.80819436e+03 5.70674893e+03 5.61362163e+03 5.45449633e+03
 5.27910895e+03 5.10099627e+03 4.88733892e+03 4.75496746e+03
 4.58522449e+03 4.39909823e+03 4.21432177e+03 4.01963689e+03
 3.82568239e+03 3.63143648e+03 3.42217829e+03 3.24302173e+03
 3.06838529e+03 2.82961444e+03 2.63834916e+03 2.47086875e+03
 2.26008073e+03 2.07153087e+03 1.91903063e+03 1.77062672e+03
 1.60161807e+03 1.46477172e+03 1.31482987e+03 1.16991884e+03
 1.03072866e+03 9.08732083e+02 8.01859410e+02 7.14303127e+02
 6.25649099e+02 5.29022549e+02 4.48027815e+02 3.75918663e+02
 3.08871662e+02 2.58008330e+02 2.11075907e+02 1.71977942e+02
 1.36271458e+02 1.01400294e+02 7.56636555e+01 5.34445444e+01
 3.81358484e+01 3.06122499e+01 2.46286677e+01 1.96825222e+01
 1.43512283e+01 1.02373550e+01 7.25445310e+00 4.75220210e+00
 3.46055415e+00 1.26075080e+00 4.40917380e-02 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
13
data after
[4598.637112035384, 4680.154479623603, 4757.453058924168, 4891.899553467534, 5033.065538444174, 5250.844508347476, 5319.249647355908, 5307.808624122501, 5498.8451763690655, 5589.691123323983, 5620.782289333134, 5778.03004530935, 5881.411402803338, 5881.259776552001, 6059.272868685972, 6130.315497527697, 6212.156347580102, 6224.662749256414, 6289.640998493531, 6307.135673671542, 6409.622332909766, 6467.943678683994, 6497.011599509635, 6538.963699182047, 6538.458798296524, 6535.671242347543, 6448.103568769147, 6458.206720188486, 6387.063587725059, 6387.756505370097, 6349.250114943531, 6289.757692343994, 6241.991496195412, 6155.933976227414, 5986.875123412361, 5897.767601440559, 5808.194360695562, 5706.748933532996, 5613.621626383167, 5454.4963259615315, 5279.108954913488, 5100.996268306558, 4887.338919196416, 4754.9674595152, 4585.2244907523245, 4399.098227920245, 4214.321766575668, 4019.6368906551497, 3825.682390795968, 3631.436478807638, 3422.1782904571746, 3243.021732848288, 3068.385292772373, 2829.6144403244302, 2638.349158203089, 2470.868747005664, 2260.0807280632694, 2071.5308676132636, 1919.0306260709006, 1770.6267206609336, 1601.6180686170028, 1464.7717208040392, 1314.8298664835588, 1169.9188430405156, 1030.728657142295, 908.7320834636424, 801.8594095709651, 714.3031271689983, 625.6490990497571, 529.0225487287121, 448.02781538345835, 375.9186633860295, 308.87166166488817, 258.00833044366084, 211.0759065107323, 171.97794193324273, 136.27145767929665, 101.40029422789658, 75.66365552266346, 53.44454437489857, 38.135848364429506, 30.61224985996854, 24.62866771746742, 34.033750557709794, 27.009406838029562]
16
data,sig_tot
[4598.63711204 4680.15447962 4757.45305892 4891.89955347 5033.06553844
 5250.84450835 5319.24964736 5307.80862412 5498.84517637 5589.69112332
 5620.78228933 5778.03004531 5881.4114028  5881.25977655 6059.27286869
 6130.31549753 6212.15634758 6224.66274926 6289.64099849 6307.13567367
 6409.62233291 6467.94367868 6497.01159951 6538.96369918 6538.4587983
 6535.67124235 6448.10356877 6458.20672019 6387.06358773 6387.75650537
 6349.25011494 6289.75769234 6241.9914962  6155.93397623 5986.87512341
 5897.76760144 5808.1943607  5706.74893353 5613.62162638 5454.49632596
 5279.10895491 5100.99626831 4887.3389192  4754.96745952 4585.22449075
 4399.09822792 4214.32176658 4019.63689066 3825.6823908  3631.43647881
 3422.17829046 3243.02173285 3068.38529277 2829.61444032 2638.3491582
 2470.86874701 2260.08072806 2071.53086761 1919.03062607 1770.62672066
 1601.61806862 1464.7717208  1314.82986648 1169.91884304 1030.72865714
  908.73208346  801.85940957  714.30312717  625.64909905  529.02254873
  448.02781538  375.91866339  308.87166166  258.00833044  211.07590651
  171.97794193  136.27145768  101.40029423   75.66365552   53.44454437
   38.13584836   30.61224986   24.62866772   34.03375056   27.00940684
  832.72626449  873.99479789  909.19676465  941.47448005  962.13522622
  993.40509244 1039.66626667 1088.05066004 1112.54806984 1128.18529091
 1170.71331392 1195.06458388 1253.33901639 1249.67593099 1317.22359877
 1338.19033244 1348.72499168 1381.03710185 1424.19803751 1433.22419144
 1455.88636466 1483.04192217 1499.94546688 1531.42364049 1530.57360366
 1516.92433255 1540.61663293 1546.63280156 1543.34752414 1549.76498637
 1546.32171994 1536.78196995 1539.29973856 1518.16473872 1507.62114674
 1473.99624591 1450.69937823 1425.60352814 1383.75766883 1370.8740982
 1336.89446446 1297.4037168  1248.09419384 1206.40513378 1173.84396623
 1113.88471416 1080.12537513 1031.05072614  985.03145416  932.52276845
  881.56528629  828.86028791  784.14566135  730.99829137  685.49339364
  630.19345955  581.93435263  539.03486413  490.91769247  448.82323181
  408.53215009  364.51893048  329.02110108  289.65975719  255.13824845
  222.38076109  195.38120612  170.46178533  146.75636458  122.80164965
  102.42781658   82.72280092   66.808603     53.48717775   44.68730108
   36.27075316   27.73754015   20.56293034   23.81160336   26.18315604]
[4598.63711204 4680.15447962 4757.45305892 4891.89955347 5033.06553844
 5250.84450835 5319.24964736 5307.80862412 5498.84517637 5589.69112332
 5620.78228933 5778.03004531 5881.4114028  5881.25977655 6059.27286869
 6130.31549753 6212.15634758 6224.66274926 6289.64099849 6307.13567367
 6409.62233291 6467.94367868 6497.01159951 6538.96369918 6538.4587983
 6535.67124235 6448.10356877 6458.20672019 6387.06358773 6387.75650537
 6349.25011494 6289.75769234 6241.9914962  6155.93397623 5986.87512341
 5897.76760144 5808.1943607  5706.74893353 5613.62162638 5454.49632596
 5279.10895491 5100.99626831 4887.3389192  4754.96745952 4585.22449075
 4399.09822792 4214.32176658 4019.63689066 3825.6823908  3631.43647881
 3422.17829046 3243.02173285 3068.38529277 2829.61444032 2638.3491582
 2470.86874701 2260.08072806 2071.53086761 1919.03062607 1770.62672066
 1601.61806862 1464.7717208  1314.82986648 1169.91884304 1030.72865714
  908.73208346  801.85940957  714.30312717  625.64909905  529.02254873
  448.02781538  375.91866339  308.87166166  258.00833044  211.07590651
  171.97794193  136.27145768  101.40029423   75.66365552   53.44454437
   38.13584836   30.61224986   24.62866772   34.03375056   27.00940684
  832.72626449  873.99479789  909.19676465  941.47448005  962.13522622
  993.40509244 1039.66626667 1088.05066004 1112.54806984 1128.18529091
 1170.71331392 1195.06458388 1253.33901639 1249.67593099 1317.22359877
 1338.19033244 1348.72499168 1381.03710185 1424.19803751 1433.22419144
 1455.88636466 1483.04192217 1499.94546688 1531.42364049 1530.57360366
 1516.92433255 1540.61663293 1546.63280156 1543.34752414 1549.76498637
 1546.32171994 1536.78196995 1539.29973856 1518.16473872 1507.62114674
 1473.99624591 1450.69937823 1425.60352814 1383.75766883 1370.8740982
 1336.89446446 1297.4037168  1248.09419384 1206.40513378 1173.84396623
 1113.88471416 1080.12537513 1031.05072614  985.03145416  932.52276845
  881.56528629  828.86028791  784.14566135  730.99829137  685.49339364
  630.19345955  581.93435263  539.03486413  490.91769247  448.82323181
  408.53215009  364.51893048  329.02110108  289.65975719  255.13824845
  222.38076109  195.38120612  170.46178533  146.75636458  122.80164965
  102.42781658   82.72280092   66.808603     53.48717775   44.68730108
   36.27075316   27.73754015   20.56293034   23.81160336   26.18315604]
2.9846931288947083 13.207979335922534 6.859257221374904
val isze = 16
idinces = [ 43  63  87  20 129  68  98 104  66 133 145  10 136  24  61 155  58  70
  52 105   1  35  99  39  19  47  49 106  21  60 103 132 138  97 128 141
  69   2  56 135  38  76 116 118 131  72  82  59  55 134  83  26 137  67
  41  78  32 161 158  48 108  25  53  90 109  51  14 150  29   3 159 114
 153  37 154 152  45 144  81  42  79 130 143  12 110 124  85   6  91  95
 163 151  34 140  50 102 101 112   4   5  44 111  96  84 142 119  75   7
  46  17 121 117  71  80 127 162 164 146  74  28  11 120  94  23  22 148
  93  18  27  36  57  31  65  89  30  86  92 126  13  77 147 149  33  62
 122 107  88  54 139 100  16 115  40   0  73   8 160 157 156 123 113  64
  15 125   9]
we are doing training validation split
training loss = 14.146964073181152 500
val loss = 24.29937744140625
training loss = 13.821747779846191 1000
val loss = 21.841114044189453
training loss = 13.651615142822266 1500
val loss = 20.995567321777344
training loss = 13.47414779663086 2000
val loss = 20.70989227294922
training loss = 13.289671897888184 2500
val loss = 20.516464233398438
training loss = 13.112466812133789 3000
val loss = 20.353816986083984
training loss = 12.93730640411377 3500
val loss = 20.208396911621094
training loss = 12.546542167663574 4000
val loss = 19.873252868652344
training loss = 11.002165794372559 4500
val loss = 18.719390869140625
training loss = 10.3826904296875 5000
val loss = 18.960407257080078
training loss = 2.340850591659546 5500
val loss = 4.480755805969238
training loss = 2.149829387664795 6000
val loss = 4.31144905090332
training loss = 2.1261255741119385 6500
val loss = 4.170356750488281
training loss = 2.097372055053711 7000
val loss = 4.08901309967041
training loss = 2.0780558586120605 7500
val loss = 3.9917891025543213
training loss = 2.0527992248535156 8000
val loss = 3.9683992862701416
training loss = 2.037646532058716 8500
val loss = 3.934607982635498
training loss = 2.026031732559204 9000
val loss = 3.8901963233947754
training loss = 2.0165414810180664 9500
val loss = 3.8477206230163574
training loss = 2.008610963821411 10000
val loss = 3.8089804649353027
training loss = 2.2844018936157227 10500
val loss = 3.7307028770446777
training loss = 2.1494829654693604 11000
val loss = 4.162065505981445
training loss = 2.0097296237945557 11500
val loss = 3.839747667312622
training loss = 3.1855766773223877 12000
val loss = 4.157622337341309
training loss = 2.3611090183258057 12500
val loss = 3.70194149017334
training loss = 3.1492793560028076 13000
val loss = 4.119840621948242
training loss = 2.0949137210845947 13500
val loss = 4.043052673339844
training loss = 1.978427529335022 14000
val loss = 3.7170400619506836
training loss = 1.9755414724349976 14500
val loss = 3.6909782886505127
training loss = 1.9769418239593506 15000
val loss = 3.6515731811523438
training loss = 1.9715907573699951 15500
val loss = 3.684521198272705
training loss = 1.9699426889419556 16000
val loss = 3.681720495223999
training loss = 1.9683969020843506 16500
val loss = 3.6751880645751953
training loss = 2.073465347290039 17000
val loss = 4.006155490875244
training loss = 1.9844540357589722 17500
val loss = 3.6043968200683594
training loss = 1.9673449993133545 18000
val loss = 3.706902503967285
training loss = 1.9632078409194946 18500
val loss = 3.6883742809295654
training loss = 1.9618076086044312 19000
val loss = 3.6669445037841797
training loss = 1.9604016542434692 19500
val loss = 3.6695306301116943
training loss = 1.9590827226638794 20000
val loss = 3.6745874881744385
training loss = 1.9581806659698486 20500
val loss = 3.6638870239257812
training loss = 2.212629795074463 21000
val loss = 3.5825765132904053
training loss = 1.9717085361480713 21500
val loss = 3.5878653526306152
training loss = 2.0036861896514893 22000
val loss = 3.5642881393432617
training loss = 1.9529387950897217 22500
val loss = 3.6614456176757812
training loss = 1.9516681432724 23000
val loss = 3.660521984100342
training loss = 1.9681801795959473 23500
val loss = 3.7551651000976562
training loss = 2.021549940109253 24000
val loss = 3.8936376571655273
training loss = 2.0590028762817383 24500
val loss = 3.967742443084717
training loss = 1.9482710361480713 25000
val loss = 3.6561784744262695
training loss = 2.264443874359131 25500
val loss = 4.333385944366455
training loss = 1.9528489112854004 26000
val loss = 3.7169618606567383
training loss = 1.9425456523895264 26500
val loss = 3.6646227836608887
training loss = 1.9409693479537964 27000
val loss = 3.6615638732910156
training loss = 1.9399528503417969 27500
val loss = 3.636728525161743
training loss = 1.9389058351516724 28000
val loss = 3.6274800300598145
training loss = 1.9360140562057495 28500
val loss = 3.645601272583008
training loss = 1.9343212842941284 29000
val loss = 3.650294303894043
training loss = 1.9308817386627197 29500
val loss = 3.6762914657592773
training loss = 1.9282841682434082 30000
val loss = 3.6658992767333984
reduced chi^2 level 2 = 1.9282833337783813
Constrained alpha: 2.2474920749664307
Constrained beta: 3.4213707447052
Constrained gamma: 8.52933406829834
(1, 149)
(1, 16)
