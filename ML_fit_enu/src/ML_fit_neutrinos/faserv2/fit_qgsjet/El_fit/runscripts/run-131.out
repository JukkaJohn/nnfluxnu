data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 4.59863711e+03 4.68015448e+03 4.75745306e+03 4.89189955e+03
 5.03306554e+03 5.25084451e+03 5.31924965e+03 5.30780862e+03
 5.49884518e+03 5.58969112e+03 5.62078229e+03 5.77803005e+03
 5.88141140e+03 5.88125978e+03 6.05927287e+03 6.13031550e+03
 6.21215635e+03 6.22466275e+03 6.28964100e+03 6.30713567e+03
 6.40962233e+03 6.46794368e+03 6.49701160e+03 6.53896370e+03
 6.53845880e+03 6.53567124e+03 6.44810357e+03 6.45820672e+03
 6.38706359e+03 6.38775651e+03 6.34925011e+03 6.28975769e+03
 6.24199150e+03 6.15593398e+03 5.98687512e+03 5.89776760e+03
 5.80819436e+03 5.70674893e+03 5.61362163e+03 5.45449633e+03
 5.27910895e+03 5.10099627e+03 4.88733892e+03 4.75496746e+03
 4.58522449e+03 4.39909823e+03 4.21432177e+03 4.01963689e+03
 3.82568239e+03 3.63143648e+03 3.42217829e+03 3.24302173e+03
 3.06838529e+03 2.82961444e+03 2.63834916e+03 2.47086875e+03
 2.26008073e+03 2.07153087e+03 1.91903063e+03 1.77062672e+03
 1.60161807e+03 1.46477172e+03 1.31482987e+03 1.16991884e+03
 1.03072866e+03 9.08732083e+02 8.01859410e+02 7.14303127e+02
 6.25649099e+02 5.29022549e+02 4.48027815e+02 3.75918663e+02
 3.08871662e+02 2.58008330e+02 2.11075907e+02 1.71977942e+02
 1.36271458e+02 1.01400294e+02 7.56636555e+01 5.34445444e+01
 3.81358484e+01 3.06122499e+01 2.46286677e+01 1.96825222e+01
 1.43512283e+01 1.02373550e+01 7.25445310e+00 4.75220210e+00
 3.46055415e+00 1.26075080e+00 4.40917380e-02 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
13
data after
[4598.637112035384, 4680.154479623603, 4757.453058924168, 4891.899553467534, 5033.065538444174, 5250.844508347476, 5319.249647355908, 5307.808624122501, 5498.8451763690655, 5589.691123323983, 5620.782289333134, 5778.03004530935, 5881.411402803338, 5881.259776552001, 6059.272868685972, 6130.315497527697, 6212.156347580102, 6224.662749256414, 6289.640998493531, 6307.135673671542, 6409.622332909766, 6467.943678683994, 6497.011599509635, 6538.963699182047, 6538.458798296524, 6535.671242347543, 6448.103568769147, 6458.206720188486, 6387.063587725059, 6387.756505370097, 6349.250114943531, 6289.757692343994, 6241.991496195412, 6155.933976227414, 5986.875123412361, 5897.767601440559, 5808.194360695562, 5706.748933532996, 5613.621626383167, 5454.4963259615315, 5279.108954913488, 5100.996268306558, 4887.338919196416, 4754.9674595152, 4585.2244907523245, 4399.098227920245, 4214.321766575668, 4019.6368906551497, 3825.682390795968, 3631.436478807638, 3422.1782904571746, 3243.021732848288, 3068.385292772373, 2829.6144403244302, 2638.349158203089, 2470.868747005664, 2260.0807280632694, 2071.5308676132636, 1919.0306260709006, 1770.6267206609336, 1601.6180686170028, 1464.7717208040392, 1314.8298664835588, 1169.9188430405156, 1030.728657142295, 908.7320834636424, 801.8594095709651, 714.3031271689983, 625.6490990497571, 529.0225487287121, 448.02781538345835, 375.9186633860295, 308.87166166488817, 258.00833044366084, 211.0759065107323, 171.97794193324273, 136.27145767929665, 101.40029422789658, 75.66365552266346, 53.44454437489857, 38.135848364429506, 30.61224985996854, 24.62866771746742, 34.033750557709794, 27.009406838029562]
16
data,sig_tot
[4598.63711204 4680.15447962 4757.45305892 4891.89955347 5033.06553844
 5250.84450835 5319.24964736 5307.80862412 5498.84517637 5589.69112332
 5620.78228933 5778.03004531 5881.4114028  5881.25977655 6059.27286869
 6130.31549753 6212.15634758 6224.66274926 6289.64099849 6307.13567367
 6409.62233291 6467.94367868 6497.01159951 6538.96369918 6538.4587983
 6535.67124235 6448.10356877 6458.20672019 6387.06358773 6387.75650537
 6349.25011494 6289.75769234 6241.9914962  6155.93397623 5986.87512341
 5897.76760144 5808.1943607  5706.74893353 5613.62162638 5454.49632596
 5279.10895491 5100.99626831 4887.3389192  4754.96745952 4585.22449075
 4399.09822792 4214.32176658 4019.63689066 3825.6823908  3631.43647881
 3422.17829046 3243.02173285 3068.38529277 2829.61444032 2638.3491582
 2470.86874701 2260.08072806 2071.53086761 1919.03062607 1770.62672066
 1601.61806862 1464.7717208  1314.82986648 1169.91884304 1030.72865714
  908.73208346  801.85940957  714.30312717  625.64909905  529.02254873
  448.02781538  375.91866339  308.87166166  258.00833044  211.07590651
  171.97794193  136.27145768  101.40029423   75.66365552   53.44454437
   38.13584836   30.61224986   24.62866772   34.03375056   27.00940684
  832.72626449  873.99479789  909.19676465  941.47448005  962.13522622
  993.40509244 1039.66626667 1088.05066004 1112.54806984 1128.18529091
 1170.71331392 1195.06458388 1253.33901639 1249.67593099 1317.22359877
 1338.19033244 1348.72499168 1381.03710185 1424.19803751 1433.22419144
 1455.88636466 1483.04192217 1499.94546688 1531.42364049 1530.57360366
 1516.92433255 1540.61663293 1546.63280156 1543.34752414 1549.76498637
 1546.32171994 1536.78196995 1539.29973856 1518.16473872 1507.62114674
 1473.99624591 1450.69937823 1425.60352814 1383.75766883 1370.8740982
 1336.89446446 1297.4037168  1248.09419384 1206.40513378 1173.84396623
 1113.88471416 1080.12537513 1031.05072614  985.03145416  932.52276845
  881.56528629  828.86028791  784.14566135  730.99829137  685.49339364
  630.19345955  581.93435263  539.03486413  490.91769247  448.82323181
  408.53215009  364.51893048  329.02110108  289.65975719  255.13824845
  222.38076109  195.38120612  170.46178533  146.75636458  122.80164965
  102.42781658   82.72280092   66.808603     53.48717775   44.68730108
   36.27075316   27.73754015   20.56293034   23.81160336   26.18315604]
[4598.63711204 4680.15447962 4757.45305892 4891.89955347 5033.06553844
 5250.84450835 5319.24964736 5307.80862412 5498.84517637 5589.69112332
 5620.78228933 5778.03004531 5881.4114028  5881.25977655 6059.27286869
 6130.31549753 6212.15634758 6224.66274926 6289.64099849 6307.13567367
 6409.62233291 6467.94367868 6497.01159951 6538.96369918 6538.4587983
 6535.67124235 6448.10356877 6458.20672019 6387.06358773 6387.75650537
 6349.25011494 6289.75769234 6241.9914962  6155.93397623 5986.87512341
 5897.76760144 5808.1943607  5706.74893353 5613.62162638 5454.49632596
 5279.10895491 5100.99626831 4887.3389192  4754.96745952 4585.22449075
 4399.09822792 4214.32176658 4019.63689066 3825.6823908  3631.43647881
 3422.17829046 3243.02173285 3068.38529277 2829.61444032 2638.3491582
 2470.86874701 2260.08072806 2071.53086761 1919.03062607 1770.62672066
 1601.61806862 1464.7717208  1314.82986648 1169.91884304 1030.72865714
  908.73208346  801.85940957  714.30312717  625.64909905  529.02254873
  448.02781538  375.91866339  308.87166166  258.00833044  211.07590651
  171.97794193  136.27145768  101.40029423   75.66365552   53.44454437
   38.13584836   30.61224986   24.62866772   34.03375056   27.00940684
  832.72626449  873.99479789  909.19676465  941.47448005  962.13522622
  993.40509244 1039.66626667 1088.05066004 1112.54806984 1128.18529091
 1170.71331392 1195.06458388 1253.33901639 1249.67593099 1317.22359877
 1338.19033244 1348.72499168 1381.03710185 1424.19803751 1433.22419144
 1455.88636466 1483.04192217 1499.94546688 1531.42364049 1530.57360366
 1516.92433255 1540.61663293 1546.63280156 1543.34752414 1549.76498637
 1546.32171994 1536.78196995 1539.29973856 1518.16473872 1507.62114674
 1473.99624591 1450.69937823 1425.60352814 1383.75766883 1370.8740982
 1336.89446446 1297.4037168  1248.09419384 1206.40513378 1173.84396623
 1113.88471416 1080.12537513 1031.05072614  985.03145416  932.52276845
  881.56528629  828.86028791  784.14566135  730.99829137  685.49339364
  630.19345955  581.93435263  539.03486413  490.91769247  448.82323181
  408.53215009  364.51893048  329.02110108  289.65975719  255.13824845
  222.38076109  195.38120612  170.46178533  146.75636458  122.80164965
  102.42781658   82.72280092   66.808603     53.48717775   44.68730108
   36.27075316   27.73754015   20.56293034   23.81160336   26.18315604]
0.9360123665782044 10.049348816814017 28.638853691321287
val isze = 16
idinces = [ 43  63  87  20 129  68  98 104  66 133 145  10 136  24  61 155  58  70
  52 105   1  35  99  39  19  47  49 106  21  60 103 132 138  97 128 141
  69   2  56 135  38  76 116 118 131  72  82  59  55 134  83  26 137  67
  41  78  32 161 158  48 108  25  53  90 109  51  14 150  29   3 159 114
 153  37 154 152  45 144  81  42  79 130 143  12 110 124  85   6  91  95
 163 151  34 140  50 102 101 112   4   5  44 111  96  84 142 119  75   7
  46  17 121 117  71  80 127 162 164 146  74  28  11 120  94  23  22 148
  93  18  27  36  57  31  65  89  30  86  92 126  13  77 147 149  33  62
 122 107  88  54 139 100  16 115  40   0  73   8 160 157 156 123 113  64
  15 125   9]
we are doing training validation split
training loss = 17.39322280883789 500
val loss = 33.822113037109375
training loss = 13.156240463256836 1000
val loss = 21.598562240600586
training loss = 12.277875900268555 1500
val loss = 17.239639282226562
training loss = 12.188935279846191 2000
val loss = 16.550765991210938
training loss = 12.129219055175781 2500
val loss = 16.461029052734375
training loss = 12.052855491638184 3000
val loss = 16.409160614013672
training loss = 11.885844230651855 3500
val loss = 16.284278869628906
training loss = 11.292781829833984 4000
val loss = 15.728056907653809
training loss = 8.03134822845459 4500
val loss = 11.842018127441406
training loss = 2.067006826400757 5000
val loss = 2.420778751373291
training loss = 1.784374713897705 5500
val loss = 2.3559560775756836
training loss = 1.7183067798614502 6000
val loss = 2.3381223678588867
training loss = 1.6909003257751465 6500
val loss = 2.2745606899261475
training loss = 1.668002963066101 7000
val loss = 2.2135722637176514
training loss = 1.6486313343048096 7500
val loss = 2.1617250442504883
training loss = 2.574237108230591 8000
val loss = 4.046072483062744
training loss = 1.6659016609191895 8500
val loss = 2.3810982704162598
training loss = 1.6070882081985474 9000
val loss = 2.0478484630584717
training loss = 1.5964895486831665 9500
val loss = 2.0340986251831055
training loss = 1.5866411924362183 10000
val loss = 2.0081825256347656
training loss = 1.5821689367294312 10500
val loss = 1.917608618736267
training loss = 1.569861888885498 11000
val loss = 1.9812302589416504
training loss = 1.5617115497589111 11500
val loss = 1.956010103225708
training loss = 1.5535848140716553 12000
val loss = 1.9567183256149292
training loss = 1.6245349645614624 12500
val loss = 2.336268901824951
training loss = 1.5391961336135864 13000
val loss = 1.9446275234222412
training loss = 1.5315004587173462 13500
val loss = 1.927525281906128
training loss = 1.5239406824111938 14000
val loss = 1.920598030090332
training loss = 1.5165073871612549 14500
val loss = 1.9097040891647339
training loss = 1.5090141296386719 15000
val loss = 1.8982608318328857
training loss = 1.501219630241394 15500
val loss = 1.8900763988494873
training loss = 1.493317723274231 16000
val loss = 1.8764228820800781
training loss = 1.4857444763183594 16500
val loss = 1.8641387224197388
training loss = 1.4785248041152954 17000
val loss = 1.8523041009902954
training loss = 1.4715214967727661 17500
val loss = 1.838557243347168
training loss = 1.4650185108184814 18000
val loss = 1.8263535499572754
training loss = 1.4592618942260742 18500
val loss = 1.8138341903686523
training loss = 1.4537745714187622 19000
val loss = 1.8030025959014893
training loss = 1.4494318962097168 19500
val loss = 1.8160806894302368
training loss = 1.5365232229232788 20000
val loss = 1.5038824081420898
training loss = 1.4397777318954468 20500
val loss = 1.7714219093322754
training loss = 1.4362084865570068 21000
val loss = 1.7573752403259277
training loss = 1.4330350160598755 21500
val loss = 1.7589150667190552
training loss = 1.4315640926361084 22000
val loss = 1.7866017818450928
training loss = 1.4280755519866943 22500
val loss = 1.73579740524292
training loss = 1.4261420965194702 23000
val loss = 1.7304003238677979
training loss = 1.4245413541793823 23500
val loss = 1.7271578311920166
training loss = 1.4231194257736206 24000
val loss = 1.7234727144241333
training loss = 1.4222161769866943 24500
val loss = 1.6960077285766602
training loss = 1.4207724332809448 25000
val loss = 1.7102900743484497
training loss = 1.42198646068573 25500
val loss = 1.6583759784698486
training loss = 1.7610772848129272 26000
val loss = 1.3643410205841064
training loss = 1.7080188989639282 26500
val loss = 2.578385591506958
training loss = 1.440209984779358 27000
val loss = 1.5479813814163208
training loss = 1.4363362789154053 27500
val loss = 1.5274789333343506
training loss = 1.4207050800323486 28000
val loss = 1.5818641185760498
training loss = 1.9944220781326294 28500
val loss = 1.315427303314209
training loss = 1.4356898069381714 29000
val loss = 1.5087112188339233
training loss = 1.4157708883285522 29500
val loss = 1.7366743087768555
training loss = 1.4115428924560547 30000
val loss = 1.6957814693450928
reduced chi^2 level 2 = 1.410893201828003
Constrained alpha: 2.2718803882598877
Constrained beta: 2.484480142593384
Constrained gamma: 17.626689910888672
(1, 149)
(1, 16)
