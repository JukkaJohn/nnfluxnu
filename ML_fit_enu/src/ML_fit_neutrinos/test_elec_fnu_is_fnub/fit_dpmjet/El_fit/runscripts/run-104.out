data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.47565511e+02 1.72230840e+02 1.97499956e+02 2.21795457e+02
 2.46654492e+02 2.68575559e+02 2.86674697e+02 2.98739968e+02
 3.00813673e+02 2.91920419e+02 2.71025396e+02 2.37799213e+02
 1.92828217e+02 1.37303026e+02 8.29962610e+01 3.71485721e+01
 1.14049175e+01 1.27155245e+00 8.70179549e-03 0.00000000e+00
 0.00000000e+00]
5
data after
[147.56551125643458, 172.23084018395252, 197.49995616115712, 221.79545670522518, 246.65449165227122, 268.5755594392394, 286.67469713071057, 298.7399678966218, 300.81367283944854, 291.9204192868681, 271.02539648442945, 237.79921263735164, 192.82821747421158, 137.3030264200614, 82.99626099298044, 49.83374380206182]
6
data,sig_tot
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
2.2409716079545268 10.434457858614438 95.3523545631385
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 3.32098126411438 500
training loss = 2.092353582382202 1000
training loss = 1.1877211332321167 1500
training loss = 1.1061493158340454 2000
training loss = 1.0989348888397217 2500
training loss = 1.0952943563461304 3000
training loss = 1.0897945165634155 3500
training loss = 1.0825446844100952 4000
training loss = 1.0758931636810303 4500
training loss = 1.0704386234283447 5000
reduced chi^2 level 2 = 1.0704286098480225
Constrained alpha: 2.2296950817108154
Constrained beta: 2.924067974090576
Constrained gamma: 49.4395637512207
(1, 31)
(1, 0)
