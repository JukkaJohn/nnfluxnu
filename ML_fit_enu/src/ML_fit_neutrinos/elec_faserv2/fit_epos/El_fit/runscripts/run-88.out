data before
341799.3120766666
13
data after
341799.3120766668
12
data,sig_tot
[4701.61488802 4794.88307404 4880.63321867 5022.96007778 5174.83210567
 5403.75702487 5484.3285967  5479.94484219 5685.56139019 5786.42431147
 5826.87977353 5997.03952734 6111.61127189 6120.00289282 6314.21918603
 6393.96946648 6488.95532514 6507.73738104 6584.88554107 6611.357542
 6727.02060846 6794.14329472 6832.33008524 6883.07023535 6889.97798261
 6892.59405288 6805.85090368 6823.13002919 6755.19764153 6760.12009368
 6724.68217864 6666.92058447 6621.02876806 6535.26290072 6361.28388017
 6274.38796295 6184.62147252 6083.25791251 5991.57607425 5831.76498353
 5653.0757575  5472.72752995 5256.93559567 5123.7074244  4954.91618422
 4767.66969394 4581.36906943 4384.37028734 4185.71288972 3987.35291917
 3772.17693952 3587.26519712 3410.17278209 3164.08533516 2970.74715477
 2808.67191304 2597.04856618 2414.5818751  2271.39044025 2134.37490034
 1969.09134909 1839.2544647  1690.72258547 1544.17473316 1392.50617358
 1256.91949139 1130.22399104 1023.21516166  912.20022068  790.05448549
  686.33353875  592.03819306  501.7262591   430.37252022  361.23753619
  302.43819468  248.36336026  193.91990061  153.43690413  116.10994345
   88.59143539   72.09509809   57.18948527   45.17032761   32.91651188
   23.26223598   37.57643872  779.76115757  819.33217103  853.43042069
  884.65267688  905.17657488  936.43865356  980.37304356 1027.01367349
 1051.38187235 1068.91684383 1110.52455855 1135.04707121 1192.86763646
 1191.06345875 1256.598203   1279.39982714 1292.55025959 1326.24235813
 1371.17906226 1383.39877461 1409.54882409 1440.35174162 1460.91223039
 1495.95384783 1499.85710062 1490.91907337 1518.89062738 1528.01817606
 1529.21479258 1539.75390574 1539.64933353 1533.97032378 1539.72900948
 1522.9274859  1516.18075167 1486.97149422 1468.42868357 1447.53539461
 1410.13886827 1401.85149154 1372.01665223 1337.1199038  1292.16995907
 1254.66423566 1225.9759354  1168.9167834  1138.15522019 1090.81463438
 1046.20720118  993.68326062  942.38183126  887.57792407  841.33509808
  786.62063997  740.35718354  685.13950905  638.57034564  599.23479559
  555.42460834  518.73884448  485.43492206  446.38917151  415.74835233
  378.13193845  345.07563002  310.85938377  281.28047609  252.35790433
  223.51141521  194.5085539   170.15787088  145.40697576  125.54449957
  107.71333166   95.97335247   83.7134781    70.73280236   59.5188732
   48.96558417   39.57817109   32.23744109   26.15284914   21.09817715
   28.97030946   23.63710235]
[4701.61488802 4794.88307404 4880.63321867 5022.96007778 5174.83210567
 5403.75702487 5484.3285967  5479.94484219 5685.56139019 5786.42431147
 5826.87977353 5997.03952734 6111.61127189 6120.00289282 6314.21918603
 6393.96946648 6488.95532514 6507.73738104 6584.88554107 6611.357542
 6727.02060846 6794.14329472 6832.33008524 6883.07023535 6889.97798261
 6892.59405288 6805.85090368 6823.13002919 6755.19764153 6760.12009368
 6724.68217864 6666.92058447 6621.02876806 6535.26290072 6361.28388017
 6274.38796295 6184.62147252 6083.25791251 5991.57607425 5831.76498353
 5653.0757575  5472.72752995 5256.93559567 5123.7074244  4954.91618422
 4767.66969394 4581.36906943 4384.37028734 4185.71288972 3987.35291917
 3772.17693952 3587.26519712 3410.17278209 3164.08533516 2970.74715477
 2808.67191304 2597.04856618 2414.5818751  2271.39044025 2134.37490034
 1969.09134909 1839.2544647  1690.72258547 1544.17473316 1392.50617358
 1256.91949139 1130.22399104 1023.21516166  912.20022068  790.05448549
  686.33353875  592.03819306  501.7262591   430.37252022  361.23753619
  302.43819468  248.36336026  193.91990061  153.43690413  116.10994345
   88.59143539   72.09509809   57.18948527   45.17032761   32.91651188
   23.26223598   37.57643872  779.76115757  819.33217103  853.43042069
  884.65267688  905.17657488  936.43865356  980.37304356 1027.01367349
 1051.38187235 1068.91684383 1110.52455855 1135.04707121 1192.86763646
 1191.06345875 1256.598203   1279.39982714 1292.55025959 1326.24235813
 1371.17906226 1383.39877461 1409.54882409 1440.35174162 1460.91223039
 1495.95384783 1499.85710062 1490.91907337 1518.89062738 1528.01817606
 1529.21479258 1539.75390574 1539.64933353 1533.97032378 1539.72900948
 1522.9274859  1516.18075167 1486.97149422 1468.42868357 1447.53539461
 1410.13886827 1401.85149154 1372.01665223 1337.1199038  1292.16995907
 1254.66423566 1225.9759354  1168.9167834  1138.15522019 1090.81463438
 1046.20720118  993.68326062  942.38183126  887.57792407  841.33509808
  786.62063997  740.35718354  685.13950905  638.57034564  599.23479559
  555.42460834  518.73884448  485.43492206  446.38917151  415.74835233
  378.13193845  345.07563002  310.85938377  281.28047609  252.35790433
  223.51141521  194.5085539   170.15787088  145.40697576  125.54449957
  107.71333166   95.97335247   83.7134781    70.73280236   59.5188732
   48.96558417   39.57817109   32.23744109   26.15284914   21.09817715
   28.97030946   23.63710235]
3.978866234679139 0.4383465550646215 56.95718873623188
val isze = 0
idinces = [111  19  68 105  66  67  90  47  10  49 148  98  59  99 168  20  81  39
 110   1  76 120 155 138 102 104  72 129 109 117 135 143 162 119  26  60
  55  24  58 116  21 112  61  56 114  87  91  43 145  82 134  35 132 144
   2  69  97  38  52  63 171 158 137  83  78  70 140  95  41  32 170  48
  25  53 101 121  51  14 164  29   3  23 127 160  37 161 159  45 106 130
  42  79 136 150 142  12 151  22  85   6 103 152 131 128  34 147  50 108
 163 124   4   5  44 166  96  84 149  28  75   7  46  17  11 153  71 118
  80 133 169  74  94 146  93  18  27  36  57  31  65  89  30  86  92 141
 126  13  77 154 165  33  62 122 107  88  54 139 100  16 115  40   0  73
   8 167 157 156 123 113  64  15 125   9]
training loss = 10.242669105529785 500
training loss = 3.6411962509155273 1000
training loss = 3.0413548946380615 1500
training loss = 2.684286117553711 2000
training loss = 2.5114550590515137 2500
training loss = 2.3896851539611816 3000
training loss = 2.311732769012451 3500
training loss = 2.794393301010132 4000
training loss = 2.993814468383789 4500
training loss = 2.2190420627593994 5000
training loss = 2.207211494445801 5500
training loss = 2.169107437133789 6000
training loss = 2.1634933948516846 6500
training loss = 2.1442973613739014 7000
training loss = 2.436354875564575 7500
training loss = 2.1275177001953125 8000
training loss = 2.1213347911834717 8500
training loss = 2.1161012649536133 9000
training loss = 2.111963987350464 9500
training loss = 2.5507993698120117 10000
training loss = 2.1238608360290527 10500
training loss = 2.1045970916748047 11000
training loss = 2.103085994720459 11500
training loss = 2.1019186973571777 12000
training loss = 3.3345563411712646 12500
training loss = 2.115861415863037 13000
training loss = 2.1007080078125 13500
training loss = 2.100120782852173 14000
training loss = 2.0998623371124268 14500
training loss = 2.104017734527588 15000
training loss = 2.0996763706207275 15500
training loss = 2.1000664234161377 16000
reduced chi^2 level 2 = 2.100041627883911
Constrained alpha: 2.560628890991211
Constrained beta: -1.5651064888544397e-09
Constrained gamma: 30.98084259033203
(1, 172)
(1, 0)
