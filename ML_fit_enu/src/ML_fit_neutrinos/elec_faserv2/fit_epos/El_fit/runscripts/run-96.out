data before
341799.3120766666
13
data after
341799.3120766668
12
data,sig_tot
[4701.61488802 4794.88307404 4880.63321867 5022.96007778 5174.83210567
 5403.75702487 5484.3285967  5479.94484219 5685.56139019 5786.42431147
 5826.87977353 5997.03952734 6111.61127189 6120.00289282 6314.21918603
 6393.96946648 6488.95532514 6507.73738104 6584.88554107 6611.357542
 6727.02060846 6794.14329472 6832.33008524 6883.07023535 6889.97798261
 6892.59405288 6805.85090368 6823.13002919 6755.19764153 6760.12009368
 6724.68217864 6666.92058447 6621.02876806 6535.26290072 6361.28388017
 6274.38796295 6184.62147252 6083.25791251 5991.57607425 5831.76498353
 5653.0757575  5472.72752995 5256.93559567 5123.7074244  4954.91618422
 4767.66969394 4581.36906943 4384.37028734 4185.71288972 3987.35291917
 3772.17693952 3587.26519712 3410.17278209 3164.08533516 2970.74715477
 2808.67191304 2597.04856618 2414.5818751  2271.39044025 2134.37490034
 1969.09134909 1839.2544647  1690.72258547 1544.17473316 1392.50617358
 1256.91949139 1130.22399104 1023.21516166  912.20022068  790.05448549
  686.33353875  592.03819306  501.7262591   430.37252022  361.23753619
  302.43819468  248.36336026  193.91990061  153.43690413  116.10994345
   88.59143539   72.09509809   57.18948527   45.17032761   32.91651188
   23.26223598   37.57643872  779.76115757  819.33217103  853.43042069
  884.65267688  905.17657488  936.43865356  980.37304356 1027.01367349
 1051.38187235 1068.91684383 1110.52455855 1135.04707121 1192.86763646
 1191.06345875 1256.598203   1279.39982714 1292.55025959 1326.24235813
 1371.17906226 1383.39877461 1409.54882409 1440.35174162 1460.91223039
 1495.95384783 1499.85710062 1490.91907337 1518.89062738 1528.01817606
 1529.21479258 1539.75390574 1539.64933353 1533.97032378 1539.72900948
 1522.9274859  1516.18075167 1486.97149422 1468.42868357 1447.53539461
 1410.13886827 1401.85149154 1372.01665223 1337.1199038  1292.16995907
 1254.66423566 1225.9759354  1168.9167834  1138.15522019 1090.81463438
 1046.20720118  993.68326062  942.38183126  887.57792407  841.33509808
  786.62063997  740.35718354  685.13950905  638.57034564  599.23479559
  555.42460834  518.73884448  485.43492206  446.38917151  415.74835233
  378.13193845  345.07563002  310.85938377  281.28047609  252.35790433
  223.51141521  194.5085539   170.15787088  145.40697576  125.54449957
  107.71333166   95.97335247   83.7134781    70.73280236   59.5188732
   48.96558417   39.57817109   32.23744109   26.15284914   21.09817715
   28.97030946   23.63710235]
[4701.61488802 4794.88307404 4880.63321867 5022.96007778 5174.83210567
 5403.75702487 5484.3285967  5479.94484219 5685.56139019 5786.42431147
 5826.87977353 5997.03952734 6111.61127189 6120.00289282 6314.21918603
 6393.96946648 6488.95532514 6507.73738104 6584.88554107 6611.357542
 6727.02060846 6794.14329472 6832.33008524 6883.07023535 6889.97798261
 6892.59405288 6805.85090368 6823.13002919 6755.19764153 6760.12009368
 6724.68217864 6666.92058447 6621.02876806 6535.26290072 6361.28388017
 6274.38796295 6184.62147252 6083.25791251 5991.57607425 5831.76498353
 5653.0757575  5472.72752995 5256.93559567 5123.7074244  4954.91618422
 4767.66969394 4581.36906943 4384.37028734 4185.71288972 3987.35291917
 3772.17693952 3587.26519712 3410.17278209 3164.08533516 2970.74715477
 2808.67191304 2597.04856618 2414.5818751  2271.39044025 2134.37490034
 1969.09134909 1839.2544647  1690.72258547 1544.17473316 1392.50617358
 1256.91949139 1130.22399104 1023.21516166  912.20022068  790.05448549
  686.33353875  592.03819306  501.7262591   430.37252022  361.23753619
  302.43819468  248.36336026  193.91990061  153.43690413  116.10994345
   88.59143539   72.09509809   57.18948527   45.17032761   32.91651188
   23.26223598   37.57643872  779.76115757  819.33217103  853.43042069
  884.65267688  905.17657488  936.43865356  980.37304356 1027.01367349
 1051.38187235 1068.91684383 1110.52455855 1135.04707121 1192.86763646
 1191.06345875 1256.598203   1279.39982714 1292.55025959 1326.24235813
 1371.17906226 1383.39877461 1409.54882409 1440.35174162 1460.91223039
 1495.95384783 1499.85710062 1490.91907337 1518.89062738 1528.01817606
 1529.21479258 1539.75390574 1539.64933353 1533.97032378 1539.72900948
 1522.9274859  1516.18075167 1486.97149422 1468.42868357 1447.53539461
 1410.13886827 1401.85149154 1372.01665223 1337.1199038  1292.16995907
 1254.66423566 1225.9759354  1168.9167834  1138.15522019 1090.81463438
 1046.20720118  993.68326062  942.38183126  887.57792407  841.33509808
  786.62063997  740.35718354  685.13950905  638.57034564  599.23479559
  555.42460834  518.73884448  485.43492206  446.38917151  415.74835233
  378.13193845  345.07563002  310.85938377  281.28047609  252.35790433
  223.51141521  194.5085539   170.15787088  145.40697576  125.54449957
  107.71333166   95.97335247   83.7134781    70.73280236   59.5188732
   48.96558417   39.57817109   32.23744109   26.15284914   21.09817715
   28.97030946   23.63710235]
4.858082824577756 4.9043416932310535 63.37736225446569
val isze = 0
idinces = [111  19  68 105  66  67  90  47  10  49 148  98  59  99 168  20  81  39
 110   1  76 120 155 138 102 104  72 129 109 117 135 143 162 119  26  60
  55  24  58 116  21 112  61  56 114  87  91  43 145  82 134  35 132 144
   2  69  97  38  52  63 171 158 137  83  78  70 140  95  41  32 170  48
  25  53 101 121  51  14 164  29   3  23 127 160  37 161 159  45 106 130
  42  79 136 150 142  12 151  22  85   6 103 152 131 128  34 147  50 108
 163 124   4   5  44 166  96  84 149  28  75   7  46  17  11 153  71 118
  80 133 169  74  94 146  93  18  27  36  57  31  65  89  30  86  92 141
 126  13  77 154 165  33  62 122 107  88  54 139 100  16 115  40   0  73
   8 167 157 156 123 113  64  15 125   9]
training loss = 16.669342041015625 500
training loss = 13.74153995513916 1000
training loss = 13.554443359375 1500
training loss = 13.397844314575195 2000
training loss = 13.22409439086914 2500
training loss = 13.042298316955566 3000
training loss = 12.880030632019043 3500
training loss = 12.75380802154541 4000
training loss = 8.924153327941895 4500
training loss = 2.6719319820404053 5000
training loss = 2.460519552230835 5500
training loss = 2.4970061779022217 6000
training loss = 2.504673480987549 6500
training loss = 2.413515329360962 7000
training loss = 2.4109950065612793 7500
training loss = 2.4051027297973633 8000
training loss = 2.4150919914245605 8500
training loss = 2.2616453170776367 9000
training loss = 2.1959688663482666 9500
training loss = 2.157871723175049 10000
training loss = 2.155988931655884 10500
training loss = 2.1396572589874268 11000
training loss = 2.1143686771392822 11500
training loss = 2.1056697368621826 12000
training loss = 2.1017813682556152 12500
training loss = 2.0989556312561035 13000
training loss = 2.096928119659424 13500
training loss = 2.161362648010254 14000
training loss = 2.1054885387420654 14500
training loss = 2.0936830043792725 15000
training loss = 2.092594861984253 15500
training loss = 2.0917632579803467 16000
training loss = 2.083707332611084 16500
training loss = 2.151047706604004 17000
training loss = 2.077207326889038 17500
training loss = 2.075587272644043 18000
training loss = 2.074721574783325 18500
training loss = 2.0740246772766113 19000
training loss = 2.0733563899993896 19500
training loss = 2.0726046562194824 20000
training loss = 2.071852445602417 20500
training loss = 2.071247100830078 21000
training loss = 2.1956520080566406 21500
training loss = 2.070347547531128 22000
training loss = 2.069807767868042 22500
training loss = 2.0781502723693848 23000
training loss = 2.0682575702667236 23500
training loss = 2.0678443908691406 24000
training loss = 2.0673680305480957 24500
training loss = 2.066948890686035 25000
training loss = 2.0665767192840576 25500
training loss = 2.0662293434143066 26000
training loss = 2.065983295440674 26500
training loss = 2.0877058506011963 27000
training loss = 2.065655469894409 27500
training loss = 2.0654730796813965 28000
training loss = 2.06527042388916 28500
training loss = 2.0651228427886963 29000
training loss = 2.064990997314453 29500
training loss = 2.064925193786621 30000
reduced chi^2 level 2 = 2.064922571182251
Constrained alpha: 2.418860912322998
Constrained beta: 3.555051803588867
Constrained gamma: 18.15313148498535
(1, 172)
(1, 0)
