data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.15459260368460404 9.9652227313401 15.463600922021325
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 74.47188568115234 500
training loss = 74.0975112915039 1000
training loss = 73.92994689941406 1500
training loss = 73.71900939941406 2000
training loss = 73.4569091796875 2500
training loss = 73.10887908935547 3000
training loss = 72.53260803222656 3500
training loss = 70.88386535644531 4000
training loss = 45.35000991821289 4500
training loss = 4.217379093170166 5000
training loss = 3.192054033279419 5500
training loss = 2.959712028503418 6000
training loss = 2.9066054821014404 6500
training loss = 2.8930115699768066 7000
training loss = 2.8889920711517334 7500
training loss = 2.8875339031219482 8000
training loss = 2.8868138790130615 8500
training loss = 2.886385202407837 9000
training loss = 2.8869106769561768 9500
training loss = 2.885575294494629 10000
training loss = 2.8853728771209717 10500
training loss = 3.096696376800537 11000
training loss = 2.8998489379882812 11500
training loss = 2.887042999267578 12000
training loss = 2.884284496307373 12500
training loss = 2.8838465213775635 13000
training loss = 2.8835654258728027 13500
training loss = 2.883335590362549 14000
training loss = 2.8831191062927246 14500
training loss = 2.9056506156921387 15000
training loss = 2.8827269077301025 15500
training loss = 2.889469861984253 16000
training loss = 2.9958841800689697 16500
training loss = 2.886078357696533 17000
training loss = 2.8832690715789795 17500
training loss = 2.8819985389709473 18000
training loss = 2.8818583488464355 18500
training loss = 2.8817248344421387 19000
training loss = 2.88161563873291 19500
training loss = 2.8815059661865234 20000
training loss = 2.8814337253570557 20500
training loss = 2.8813445568084717 21000
training loss = 2.8812568187713623 21500
training loss = 2.8814570903778076 22000
training loss = 2.9200806617736816 22500
training loss = 2.991769790649414 23000
training loss = 2.943513870239258 23500
training loss = 2.906604290008545 24000
training loss = 3.2352426052093506 24500
training loss = 3.351517677307129 25000
training loss = 3.2645657062530518 25500
training loss = 2.8807575702667236 26000
training loss = 2.8807191848754883 26500
training loss = 2.8841423988342285 27000
training loss = 2.884551763534546 27500
training loss = 2.8807263374328613 28000
training loss = 2.9293463230133057 28500
training loss = 2.881633758544922 29000
training loss = 2.8805816173553467 29500
training loss = 2.8805718421936035 30000
training loss = 2.8807404041290283 30500
training loss = 2.880751848220825 31000
training loss = 2.880591869354248 31500
training loss = 2.8804831504821777 32000
training loss = 2.8804709911346436 32500
training loss = 2.8804562091827393 33000
training loss = 2.880559206008911 33500
training loss = 3.1948294639587402 34000
(1, 38)
(1, 0)
