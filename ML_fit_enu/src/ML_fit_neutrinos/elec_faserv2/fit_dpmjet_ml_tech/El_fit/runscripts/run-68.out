data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.12285580627136 1.9408998467180094 26.41510432088914
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 110.31865692138672 500
training loss = 80.28575134277344 1000
training loss = 80.15595245361328 1500
training loss = 80.13123321533203 2000
training loss = 80.09748840332031 2500
training loss = 80.05113220214844 3000
training loss = 79.98695373535156 3500
training loss = 79.89579772949219 4000
training loss = 79.7602310180664 4500
training loss = 79.5452651977539 5000
training loss = 79.14962005615234 5500
training loss = 77.95911407470703 6000
training loss = 3.9392428398132324 6500
training loss = 3.510143995285034 7000
training loss = 3.437608003616333 7500
training loss = 3.424339771270752 8000
training loss = 7.190736293792725 8500
training loss = 3.467729330062866 9000
training loss = 3.4132583141326904 9500
training loss = 3.4106063842773438 10000
training loss = 3.4083685874938965 10500
training loss = 3.5478789806365967 11000
training loss = 3.412818431854248 11500
training loss = 3.4011600017547607 12000
training loss = 3.398831844329834 12500
training loss = 3.3981893062591553 13000
training loss = 3.3952271938323975 13500
training loss = 3.3925070762634277 14000
training loss = 3.3905093669891357 14500
training loss = 3.396395206451416 15000
training loss = 3.3867104053497314 15500
training loss = 3.3850460052490234 16000
training loss = 3.393878698348999 16500
training loss = 3.382794141769409 17000
training loss = 3.379912853240967 17500
training loss = 3.3822925090789795 18000
training loss = 3.376704216003418 18500
training loss = 3.375225782394409 19000
training loss = 3.3737993240356445 19500
training loss = 3.3724205493927 20000
training loss = 3.371203899383545 20500
training loss = 3.3698947429656982 21000
training loss = 3.3685784339904785 21500
training loss = 3.367394208908081 22000
training loss = 3.367227792739868 22500
training loss = 3.365143299102783 23000
training loss = 5.342867374420166 23500
training loss = 3.363067626953125 24000
training loss = 3.362071990966797 24500
training loss = 3.361178398132324 25000
training loss = 3.3603572845458984 25500
training loss = 3.3646485805511475 26000
training loss = 3.3592052459716797 26500
training loss = 3.357670545578003 27000
training loss = 3.356877565383911 27500
training loss = 3.3561339378356934 28000
training loss = 3.3554024696350098 28500
training loss = 3.3546948432922363 29000
training loss = 3.354015827178955 29500
training loss = 3.353372097015381 30000
training loss = 3.3527445793151855 30500
training loss = 3.352146863937378 31000
training loss = 3.3524911403656006 31500
training loss = 3.3510091304779053 32000
training loss = 3.3509440422058105 32500
training loss = 3.3499417304992676 33000
training loss = 3.3527066707611084 33500
training loss = 3.3790271282196045 34000
training loss = 3.349595069885254 34500
training loss = 3.8402726650238037 35000
(1, 38)
(1, 0)
