data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.1528828325304571 4.366871268818344 3.3037534984759143
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 132.7691650390625 500
training loss = 79.01214599609375 1000
training loss = 77.79330444335938 1500
training loss = 77.77599334716797 2000
training loss = 77.7636489868164 2500
training loss = 77.74829864501953 3000
training loss = 77.72994232177734 3500
training loss = 77.70806884765625 4000
training loss = 77.68247985839844 4500
training loss = 77.65234375 5000
training loss = 77.61589813232422 5500
training loss = 77.56964111328125 6000
training loss = 77.50303649902344 6500
training loss = 77.37659454345703 7000
training loss = 77.01190185546875 7500
training loss = 74.88973999023438 8000
training loss = 4.355101585388184 8500
training loss = 3.1682662963867188 9000
training loss = 3.6490800380706787 9500
training loss = 3.0013580322265625 10000
training loss = 3.2163138389587402 10500
training loss = 3.0034165382385254 11000
training loss = 3.0380849838256836 11500
training loss = 2.7847676277160645 12000
training loss = 2.784245729446411 12500
training loss = 2.863598108291626 13000
training loss = 5.665350914001465 13500
training loss = 2.937504529953003 14000
training loss = 2.987379312515259 14500
training loss = 4.386117458343506 15000
training loss = 3.06077241897583 15500
training loss = 2.787196159362793 16000
training loss = 2.7748255729675293 16500
training loss = 2.7794859409332275 17000
training loss = 2.8828558921813965 17500
training loss = 2.8747053146362305 18000
training loss = 2.87986159324646 18500
training loss = 2.7906899452209473 19000
training loss = 2.7677881717681885 19500
training loss = 2.7669782638549805 20000
training loss = 2.7661209106445312 20500
training loss = 2.765354871749878 21000
training loss = 2.7647063732147217 21500
training loss = 2.76391339302063 22000
training loss = 2.805976152420044 22500
training loss = 2.762585401535034 23000
training loss = 2.7619681358337402 23500
training loss = 2.7613630294799805 24000
training loss = 2.7607791423797607 24500
training loss = 2.760228395462036 25000
training loss = 2.759692907333374 25500
training loss = 2.797091484069824 26000
training loss = 2.758660316467285 26500
training loss = 2.758181571960449 27000
training loss = 2.757718563079834 27500
training loss = 2.757262706756592 28000
training loss = 2.7568247318267822 28500
training loss = 2.7564268112182617 29000
training loss = 4.783227443695068 29500
training loss = 4.472047328948975 30000
training loss = 2.7552008628845215 30500
training loss = 2.754855155944824 31000
training loss = 3.6615471839904785 31500
training loss = 2.809109926223755 32000
training loss = 2.754248857498169 32500
training loss = 2.759533166885376 33000
training loss = 2.7540884017944336 33500
training loss = 2.7528202533721924 34000
training loss = 2.752485990524292 34500
training loss = 2.752185106277466 35000
(1, 38)
(1, 0)
