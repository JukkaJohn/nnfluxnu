data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.680417576586812 7.121892760679163 79.64678443217828
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 76.53485107421875 500
training loss = 3.9036173820495605 1000
training loss = 3.257441520690918 1500
training loss = 3.172377109527588 2000
training loss = 3.099480390548706 2500
training loss = 3.786008834838867 3000
training loss = 3.0169320106506348 3500
training loss = 2.8867533206939697 4000
training loss = 2.8019425868988037 4500
training loss = 5.854519844055176 5000
training loss = 2.563729763031006 5500
training loss = 2.4100544452667236 6000
training loss = 2.2583653926849365 6500
training loss = 2.148786783218384 7000
training loss = 2.0366945266723633 7500
training loss = 1.9652745723724365 8000
training loss = 1.911632776260376 8500
training loss = 1.8692725896835327 9000
training loss = 1.8418635129928589 9500
training loss = 1.8187164068222046 10000
training loss = 1.7890721559524536 10500
training loss = 1.7706774473190308 11000
training loss = 1.74070143699646 11500
training loss = 1.717587947845459 12000
training loss = 1.691686987876892 12500
training loss = 1.6594178676605225 13000
training loss = 1.6249362230300903 13500
training loss = 1.6001743078231812 14000
training loss = 1.526119351387024 14500
training loss = 1.6313196420669556 15000
training loss = 1.4259706735610962 15500
training loss = 1.3852766752243042 16000
training loss = 1.355096697807312 16500
training loss = 1.3351035118103027 17000
training loss = 1.3228806257247925 17500
training loss = 1.315504789352417 18000
training loss = 1.3108251094818115 18500
training loss = 1.3076084852218628 19000
training loss = 1.3051644563674927 19500
training loss = 1.3032234907150269 20000
training loss = 1.3014625310897827 20500
training loss = 1.2999807596206665 21000
training loss = 1.2986788749694824 21500
training loss = 1.2975690364837646 22000
training loss = 1.2965574264526367 22500
training loss = 1.2957650423049927 23000
training loss = 1.3124146461486816 23500
training loss = 1.3935571908950806 24000
training loss = 1.3208985328674316 24500
training loss = 1.3174493312835693 25000
training loss = 1.2978278398513794 25500
training loss = 1.4211441278457642 26000
training loss = 1.292763113975525 26500
training loss = 1.2916349172592163 27000
training loss = 1.2913837432861328 27500
training loss = 1.291124701499939 28000
training loss = 1.2909302711486816 28500
training loss = 1.2907439470291138 29000
training loss = 1.290679693222046 29500
training loss = 1.3853874206542969 30000
training loss = 1.3772002458572388 30500
training loss = 1.2904120683670044 31000
training loss = 1.3080729246139526 31500
training loss = 1.290832281112671 32000
training loss = 1.3565019369125366 32500
training loss = 1.3356815576553345 33000
training loss = 1.312312126159668 33500
training loss = 1.4036839008331299 34000
training loss = 1.297873616218567 34500
training loss = 1.2899378538131714 35000
reduced chi^2 level 2 = 1.289786458015442
Constrained alpha: 2.3050432205200195
Constrained beta: 3.9484946727752686
Constrained gamma: 46.8025016784668
(1, 38)
(1, 0)
