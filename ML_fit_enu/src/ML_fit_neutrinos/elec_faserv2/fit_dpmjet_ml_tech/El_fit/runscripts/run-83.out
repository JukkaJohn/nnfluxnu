data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.6717475525972225 3.696265545081263 23.57584721298649
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 112.25311279296875 500
training loss = 74.77960205078125 1000
training loss = 74.3723373413086 1500
training loss = 74.32408142089844 2000
training loss = 4.7219133377075195 2500
training loss = 3.854909896850586 3000
training loss = 3.7386667728424072 3500
training loss = 3.5851809978485107 4000
training loss = 3.4520421028137207 4500
training loss = 3.310307264328003 5000
training loss = 3.170872926712036 5500
training loss = 3.025266170501709 6000
training loss = 2.8982961177825928 6500
training loss = 2.796170711517334 7000
training loss = 2.720799446105957 7500
training loss = 2.666701555252075 8000
training loss = 2.626218795776367 8500
training loss = 6.01101016998291 9000
training loss = 2.6388144493103027 9500
training loss = 2.5995194911956787 10000
training loss = 2.5007381439208984 10500
training loss = 2.465614080429077 11000
training loss = 2.425447940826416 11500
training loss = 2.3792989253997803 12000
training loss = 2.326108932495117 12500
training loss = 2.2645692825317383 13000
training loss = 2.1984405517578125 13500
training loss = 2.1291255950927734 14000
training loss = 2.4067752361297607 14500
training loss = 2.041902542114258 15000
training loss = 1.9374386072158813 15500
training loss = 2.335954189300537 16000
training loss = 1.7386025190353394 16500
training loss = 1.7731040716171265 17000
training loss = 1.6574742794036865 17500
training loss = 1.6277358531951904 18000
training loss = 3.6899847984313965 18500
training loss = 1.57882559299469 19000
training loss = 5.046050071716309 19500
training loss = 1.5548474788665771 20000
training loss = 1.770760416984558 20500
training loss = 1.695778727531433 21000
training loss = 1.4798344373703003 21500
training loss = 1.4612840414047241 22000
training loss = 1.4430512189865112 22500
training loss = 3.8897154331207275 23000
training loss = 1.407832145690918 23500
training loss = 1.3905375003814697 24000
training loss = 1.3738735914230347 24500
training loss = 1.5427366495132446 25000
training loss = 1.7432138919830322 25500
training loss = 1.4399181604385376 26000
training loss = 1.313352346420288 26500
training loss = 1.299869179725647 27000
training loss = 1.2870550155639648 27500
training loss = 1.5713831186294556 28000
training loss = 2.793712615966797 28500
training loss = 1.6068581342697144 29000
training loss = 2.0210559368133545 29500
training loss = 1.2390385866165161 30000
training loss = 1.2246700525283813 30500
training loss = 1.2165400981903076 31000
training loss = 1.2099535465240479 31500
training loss = 1.2021621465682983 32000
training loss = 1.195464849472046 32500
training loss = 1.1894465684890747 33000
training loss = 1.1838812828063965 33500
training loss = 1.8242255449295044 34000
training loss = 1.173897385597229 34500
training loss = 1.1892273426055908 35000
reduced chi^2 level 2 = 1.200703501701355
Constrained alpha: 2.201378583908081
Constrained beta: 4.183823585510254
Constrained gamma: 25.11886978149414
(1, 38)
(1, 0)
