data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.4501672838144355 16.142578282684617 40.662513609062536
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 207.37078857421875 500
training loss = 68.33794403076172 1000
training loss = 66.85966491699219 1500
training loss = 64.53280639648438 2000
training loss = 54.76015090942383 2500
training loss = 6.575106143951416 3000
training loss = 4.683294296264648 3500
training loss = 4.090622901916504 4000
training loss = 3.973958969116211 4500
training loss = 3.951054573059082 5000
training loss = 3.9399807453155518 5500
training loss = 3.9314351081848145 6000
training loss = 3.9244179725646973 6500
training loss = 3.9183266162872314 7000
training loss = 3.912853956222534 7500
training loss = 3.9079012870788574 8000
training loss = 3.903348445892334 8500
training loss = 3.899409770965576 9000
training loss = 3.8950629234313965 9500
training loss = 3.8912060260772705 10000
training loss = 3.887796640396118 10500
training loss = 3.884030818939209 11000
training loss = 3.880605697631836 11500
training loss = 3.877326488494873 12000
training loss = 3.8741772174835205 12500
training loss = 3.8786003589630127 13000
training loss = 3.8682830333709717 13500
training loss = 3.8671023845672607 14000
training loss = 3.8630013465881348 14500
training loss = 3.860011100769043 15000
training loss = 3.8575167655944824 15500
training loss = 3.8552191257476807 16000
training loss = 3.852617025375366 16500
training loss = 3.850311279296875 17000
training loss = 3.848068952560425 17500
training loss = 3.8459112644195557 18000
training loss = 3.843811511993408 18500
training loss = 3.8418142795562744 19000
training loss = 3.8398115634918213 19500
training loss = 3.8378913402557373 20000
training loss = 3.8360259532928467 20500
training loss = 3.8341643810272217 21000
training loss = 3.832383394241333 21500
training loss = 3.832587480545044 22000
training loss = 3.8697686195373535 22500
training loss = 4.150994777679443 23000
training loss = 3.8429059982299805 23500
training loss = 5.15133810043335 24000
training loss = 3.8399457931518555 24500
training loss = 3.820631265640259 25000
training loss = 3.819002628326416 25500
training loss = 3.8173940181732178 26000
training loss = 3.815779447555542 26500
training loss = 3.814143180847168 27000
training loss = 3.8125240802764893 27500
training loss = 3.8108761310577393 28000
training loss = 3.8092024326324463 28500
training loss = 3.8075144290924072 29000
training loss = 3.8057949542999268 29500
training loss = 3.8040337562561035 30000
training loss = 5.606027126312256 30500
training loss = 3.806295394897461 31000
training loss = 3.7994368076324463 31500
training loss = 3.796480655670166 32000
training loss = 3.794428825378418 32500
training loss = 3.7922844886779785 33000
training loss = 3.7900655269622803 33500
training loss = 3.9958295822143555 34000
training loss = 3.7851626873016357 34500
training loss = 3.782529830932617 35000
(1, 38)
(1, 0)
