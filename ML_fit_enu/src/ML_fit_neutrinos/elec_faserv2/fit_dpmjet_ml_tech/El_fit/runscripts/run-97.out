data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.7683093412129423 6.872807036803481 22.972408307248926
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 82.9532241821289 500
training loss = 74.1329116821289 1000
training loss = 74.09771728515625 1500
training loss = 74.05699920654297 2000
training loss = 74.00550079345703 2500
training loss = 73.94003295898438 3000
training loss = 73.849609375 3500
training loss = 73.69514465332031 4000
training loss = 73.25946044921875 4500
training loss = 63.4934196472168 5000
training loss = 4.296389579772949 5500
training loss = 3.8242716789245605 6000
training loss = 3.71846079826355 6500
training loss = 3.6937263011932373 7000
training loss = 3.687443733215332 7500
training loss = 3.6854465007781982 8000
training loss = 3.6844840049743652 8500
training loss = 3.6838130950927734 9000
training loss = 3.6832334995269775 9500
training loss = 3.6826417446136475 10000
training loss = 3.682136058807373 10500
training loss = 3.681654214859009 11000
training loss = 3.681208610534668 11500
training loss = 3.680803060531616 12000
training loss = 3.680408000946045 12500
training loss = 3.6800425052642822 13000
training loss = 3.6797008514404297 13500
training loss = 3.6793763637542725 14000
training loss = 3.6790716648101807 14500
training loss = 3.67879056930542 15000
training loss = 3.678514242172241 15500
training loss = 3.6782565116882324 16000
training loss = 3.6780214309692383 16500
training loss = 4.009995460510254 17000
training loss = 3.6832680702209473 17500
training loss = 3.679161548614502 18000
training loss = 3.6771392822265625 18500
training loss = 3.676964282989502 19000
training loss = 3.6767497062683105 19500
training loss = 3.6765682697296143 20000
training loss = 5.319502353668213 20500
training loss = 3.6790571212768555 21000
training loss = 3.6766302585601807 21500
training loss = 3.6760945320129395 22000
training loss = 3.6757869720458984 22500
training loss = 3.6756350994110107 23000
training loss = 3.6754908561706543 23500
training loss = 3.6753597259521484 24000
training loss = 3.675224781036377 24500
training loss = 4.230123996734619 25000
training loss = 3.6773715019226074 25500
training loss = 3.675640821456909 26000
training loss = 3.6993632316589355 26500
training loss = 3.9141499996185303 27000
training loss = 3.675058603286743 27500
training loss = 3.674898624420166 28000
training loss = 3.700092077255249 28500
training loss = 3.6744143962860107 29000
training loss = 3.674102783203125 29500
training loss = 3.6740128993988037 30000
training loss = 3.673903703689575 30500
training loss = 3.6738088130950928 31000
training loss = 3.6737120151519775 31500
training loss = 3.673628568649292 32000
training loss = 3.673537015914917 32500
training loss = 3.6734514236450195 33000
training loss = 3.6733603477478027 33500
training loss = 3.673271894454956 34000
training loss = 3.6731977462768555 34500
training loss = 3.6732418537139893 35000
(1, 38)
(1, 0)
