data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.359739755068359 3.5421246920873117 91.52044298237954
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 101.82366943359375 500
training loss = 76.61235809326172 1000
training loss = 76.51792907714844 1500
training loss = 76.47789001464844 2000
training loss = 76.42240142822266 2500
training loss = 76.34532165527344 3000
training loss = 76.23645782470703 3500
training loss = 76.07709503173828 4000
training loss = 75.82305908203125 4500
training loss = 75.29386901855469 5000
training loss = 61.320518493652344 5500
training loss = 2.9683351516723633 6000
training loss = 2.897108316421509 6500
training loss = 3.970954418182373 7000
training loss = 2.8985424041748047 7500
training loss = 2.8876349925994873 8000
training loss = 5.187699317932129 8500
training loss = 2.4689135551452637 9000
training loss = 2.46126389503479 9500
training loss = 2.1323280334472656 10000
training loss = 1.968127965927124 10500
training loss = 1.9183907508850098 11000
training loss = 1.9307769536972046 11500
training loss = 1.8389437198638916 12000
training loss = 2.1067006587982178 12500
training loss = 1.8396638631820679 13000
training loss = 1.7817844152450562 13500
training loss = 1.7769088745117188 14000
training loss = 1.7726701498031616 14500
training loss = 1.7696031332015991 15000
training loss = 1.7673856019973755 15500
training loss = 1.7651915550231934 16000
training loss = 1.7637883424758911 16500
training loss = 1.762670874595642 17000
training loss = 1.8445501327514648 17500
training loss = 1.8133028745651245 18000
training loss = 6.272498607635498 18500
training loss = 4.7134881019592285 19000
training loss = 1.7562501430511475 19500
training loss = 1.7543537616729736 20000
training loss = 1.7533211708068848 20500
training loss = 1.7523430585861206 21000
training loss = 1.751619815826416 21500
training loss = 1.8896634578704834 22000
training loss = 1.7561789751052856 22500
training loss = 1.7596088647842407 23000
training loss = 1.7622108459472656 23500
training loss = 1.7528738975524902 24000
training loss = 1.8268781900405884 24500
training loss = 1.7461901903152466 25000
training loss = 1.7451637983322144 25500
training loss = 1.7444325685501099 26000
training loss = 1.744185209274292 26500
training loss = 1.743173360824585 27000
training loss = 1.7425510883331299 27500
training loss = 1.7419483661651611 28000
training loss = 1.7413699626922607 28500
training loss = 1.7409001588821411 29000
training loss = 1.7726781368255615 29500
training loss = 1.7397713661193848 30000
training loss = 1.857255458831787 30500
training loss = 1.745689034461975 31000
training loss = 1.7391653060913086 31500
training loss = 1.7394318580627441 32000
training loss = 1.7370963096618652 32500
training loss = 1.736611247062683 33000
training loss = 1.736133337020874 33500
training loss = 1.735632061958313 34000
training loss = 1.735133171081543 34500
training loss = 1.7346199750900269 35000
reduced chi^2 level 2 = 1.7346223592758179
Constrained alpha: 2.300025463104248
Constrained beta: 4.016657829284668
Constrained gamma: 50.76490020751953
(1, 38)
(1, 0)
