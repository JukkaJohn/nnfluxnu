data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.6873241921324067 2.609855637747447 79.89967615135485
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 102.25895690917969 500
training loss = 75.08682250976562 1000
training loss = 74.98741912841797 1500
training loss = 74.92786407470703 2000
training loss = 74.83895111083984 2500
training loss = 74.6929702758789 3000
training loss = 74.38290405273438 3500
training loss = 72.7530288696289 4000
training loss = 3.5344390869140625 4500
training loss = 3.4684031009674072 5000
training loss = 3.606027841567993 5500
training loss = 5.236162185668945 6000
training loss = 3.4344322681427 6500
training loss = 3.428626537322998 7000
training loss = 3.43463134765625 7500
training loss = 3.4212353229522705 8000
training loss = 3.414435863494873 8500
training loss = 3.409433603286743 9000
training loss = 3.4054958820343018 9500
training loss = 3.4019153118133545 10000
training loss = 3.398313045501709 10500
training loss = 3.476966381072998 11000
training loss = 3.3905832767486572 11500
training loss = 3.134394645690918 12000
training loss = 2.4659130573272705 12500
training loss = 2.2637438774108887 13000
training loss = 2.324364423751831 13500
training loss = 2.1919350624084473 14000
training loss = 2.1707000732421875 14500
training loss = 2.1573193073272705 15000
training loss = 2.1466739177703857 15500
training loss = 2.3625364303588867 16000
training loss = 2.1313865184783936 16500
training loss = 2.124943494796753 17000
training loss = 2.120870590209961 17500
training loss = 2.1146903038024902 18000
training loss = 2.1102306842803955 18500
training loss = 2.105829954147339 19000
training loss = 2.1014657020568848 19500
training loss = 2.097494602203369 20000
training loss = 2.0933375358581543 20500
training loss = 2.0893025398254395 21000
training loss = 2.0854146480560303 21500
training loss = 2.081519603729248 22000
training loss = 2.198317050933838 22500
training loss = 2.4516024589538574 23000
training loss = 2.070307731628418 23500
training loss = 2.066815137863159 24000
training loss = 2.0633296966552734 24500
training loss = 2.063957691192627 25000
training loss = 2.057197093963623 25500
training loss = 2.0525107383728027 26000
training loss = 2.048809289932251 26500
training loss = 2.0484542846679688 27000
training loss = 2.0595197677612305 27500
training loss = 2.475867509841919 28000
training loss = 2.034409999847412 28500
training loss = 2.0308327674865723 29000
training loss = 2.026989698410034 29500
training loss = 2.0232748985290527 30000
training loss = 2.019394636154175 30500
training loss = 2.0154366493225098 31000
training loss = 2.011521100997925 31500
training loss = 2.0076067447662354 32000
training loss = 2.0035598278045654 32500
training loss = 1.9995369911193848 33000
training loss = 1.9953454732894897 33500
training loss = 1.9912399053573608 34000
training loss = 1.9871306419372559 34500
training loss = 1.9830513000488281 35000
reduced chi^2 level 2 = 1.9830410480499268
Constrained alpha: 2.3353288173675537
Constrained beta: 3.808089256286621
Constrained gamma: 47.709774017333984
(1, 38)
(1, 0)
