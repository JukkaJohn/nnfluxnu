data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.7102441103699673 10.327414967166241 87.48818302887568
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 78.17606353759766 500
training loss = 76.70704650878906 1000
training loss = 76.59230041503906 1500
training loss = 76.44139099121094 2000
training loss = 76.24137115478516 2500
training loss = 75.95055389404297 3000
training loss = 75.3817367553711 3500
training loss = 72.50347900390625 4000
training loss = 7.724386215209961 4500
training loss = 5.3389692306518555 5000
training loss = 4.61879301071167 5500
training loss = 4.337852954864502 6000
training loss = 4.296389102935791 6500
training loss = 5.53265380859375 7000
training loss = 4.1284565925598145 7500
training loss = 4.114412784576416 8000
training loss = 4.1179585456848145 8500
training loss = 4.102746486663818 9000
training loss = 4.100285530090332 9500
training loss = 4.098537921905518 10000
training loss = 4.0972580909729 10500
training loss = 4.673384189605713 11000
training loss = 4.116715908050537 11500
training loss = 4.094264984130859 12000
training loss = 4.093266487121582 12500
training loss = 4.092248439788818 13000
training loss = 4.091202735900879 13500
training loss = 4.119449615478516 14000
training loss = 4.088866233825684 14500
training loss = 4.090463161468506 15000
training loss = 4.089297771453857 15500
training loss = 4.093358039855957 16000
training loss = 4.111039161682129 16500
training loss = 4.090245246887207 17000
training loss = 4.083186149597168 17500
training loss = 4.142241477966309 18000
training loss = 4.0787672996521 18500
training loss = 4.112407684326172 19000
training loss = 4.0712504386901855 19500
training loss = 4.103794097900391 20000
training loss = 4.066648960113525 20500
training loss = 4.062778949737549 21000
training loss = 6.592899322509766 21500
training loss = 4.0591230392456055 22000
training loss = 4.051887035369873 22500
training loss = 4.052365779876709 23000
training loss = 4.0439558029174805 23500
training loss = 5.477206707000732 24000
training loss = 4.035341739654541 24500
training loss = 4.037040710449219 25000
training loss = 4.0221099853515625 25500
training loss = 4.0161285400390625 26000
training loss = 4.009912490844727 26500
training loss = 5.008998870849609 27000
training loss = 4.00232458114624 27500
training loss = 4.016091346740723 28000
training loss = 3.983818531036377 28500
training loss = 3.977311849594116 29000
training loss = 3.9873592853546143 29500
training loss = 3.9647538661956787 30000
training loss = 3.9593069553375244 30500
training loss = 3.953640937805176 31000
training loss = 3.9476659297943115 31500
training loss = 3.3255014419555664 32000
training loss = 3.05977463722229 32500
training loss = 2.855137825012207 33000
training loss = 2.6773295402526855 33500
training loss = 2.526224136352539 34000
training loss = 2.9472949504852295 34500
training loss = 2.4140713214874268 35000
(1, 38)
(1, 0)
