data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.6712048231479417 2.6724561019649196 12.173653285044207
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 132.4996337890625 500
training loss = 76.32994842529297 1000
training loss = 76.03433990478516 1500
training loss = 76.021728515625 2000
training loss = 76.0055923461914 2500
training loss = 75.98452758789062 3000
training loss = 75.9567642211914 3500
training loss = 75.91844177246094 4000
training loss = 75.86006927490234 4500
training loss = 75.74812316894531 5000
training loss = 75.36309051513672 5500
training loss = 17.729328155517578 6000
training loss = 3.3179662227630615 6500
training loss = 3.1991560459136963 7000
training loss = 3.1805837154388428 7500
training loss = 3.1769485473632812 8000
training loss = 3.1851136684417725 8500
training loss = 3.2877562046051025 9000
training loss = 3.183349132537842 9500
training loss = 3.172452211380005 10000
training loss = 3.170727014541626 10500
training loss = 3.1696255207061768 11000
training loss = 3.16825795173645 11500
training loss = 3.167912721633911 12000
training loss = 3.1659164428710938 12500
training loss = 3.1649813652038574 13000
training loss = 3.163207769393921 13500
training loss = 3.161916971206665 14000
training loss = 3.16066837310791 14500
training loss = 3.159442663192749 15000
training loss = 3.1582343578338623 15500
training loss = 3.1570653915405273 16000
training loss = 3.1559228897094727 16500
training loss = 6.080667018890381 17000
training loss = 3.1537632942199707 17500
training loss = 3.3894147872924805 18000
training loss = 3.4448130130767822 18500
training loss = 3.15236496925354 19000
training loss = 3.1502833366394043 19500
training loss = 3.1490702629089355 20000
training loss = 3.148233652114868 20500
training loss = 3.147479772567749 21000
training loss = 3.146768093109131 21500
training loss = 3.146296977996826 22000
training loss = 3.1455061435699463 22500
training loss = 3.1448538303375244 23000
training loss = 3.144280195236206 23500
training loss = 3.1437370777130127 24000
training loss = 3.1432366371154785 24500
training loss = 3.1427547931671143 25000
training loss = 3.1887166500091553 25500
training loss = 3.1423416137695312 26000
training loss = 3.1414506435394287 26500
training loss = 3.141069173812866 27000
training loss = 3.141484022140503 27500
training loss = 3.140336751937866 28000
training loss = 3.140024423599243 28500
training loss = 3.285710096359253 29000
training loss = 3.2044711112976074 29500
training loss = 3.139441728591919 30000
training loss = 3.1387267112731934 30500
training loss = 3.1384494304656982 31000
training loss = 3.1381731033325195 31500
training loss = 3.137916088104248 32000
training loss = 3.138155221939087 32500
training loss = 3.2760684490203857 33000
training loss = 7.5860819816589355 33500
training loss = 3.1369693279266357 34000
training loss = 3.1367461681365967 34500
training loss = 3.1365420818328857 35000
(1, 38)
(1, 0)
