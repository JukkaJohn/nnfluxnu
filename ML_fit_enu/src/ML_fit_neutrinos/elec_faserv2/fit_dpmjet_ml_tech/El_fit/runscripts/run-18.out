data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.5552355720921215 4.709209320294178 13.633106711372333
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 106.73262786865234 500
training loss = 76.71728515625 1000
training loss = 76.22395324707031 1500
training loss = 76.17377471923828 2000
training loss = 76.10889434814453 2500
training loss = 76.02253723144531 3000
training loss = 75.90386962890625 3500
training loss = 75.72598266601562 4000
training loss = 75.39412689208984 4500
training loss = 74.31658935546875 5000
training loss = 4.073882579803467 5500
training loss = 3.284292221069336 6000
training loss = 3.219001293182373 6500
training loss = 3.2055392265319824 7000
training loss = 3.202761173248291 7500
training loss = 3.202054500579834 8000
training loss = 3.2017271518707275 8500
training loss = 3.2014904022216797 9000
training loss = 6.110907554626465 9500
training loss = 3.2018210887908936 10000
training loss = 3.2013444900512695 10500
training loss = 3.2006704807281494 11000
training loss = 3.200486660003662 11500
training loss = 3.200305700302124 12000
training loss = 3.2001283168792725 12500
training loss = 3.199963092803955 13000
training loss = 3.1999049186706543 13500
training loss = 4.141979694366455 14000
training loss = 3.268207311630249 14500
training loss = 3.2356295585632324 15000
training loss = 3.1991991996765137 15500
training loss = 3.1990602016448975 16000
training loss = 3.19891357421875 16500
training loss = 3.1987812519073486 17000
training loss = 3.1986522674560547 17500
training loss = 3.19852876663208 18000
training loss = 3.240225076675415 18500
training loss = 3.219866991043091 19000
training loss = 3.199714183807373 19500
training loss = 3.1980504989624023 20000
training loss = 3.1979377269744873 20500
training loss = 3.1978189945220947 21000
training loss = 3.197709321975708 21500
training loss = 3.1976122856140137 22000
training loss = 3.197504997253418 22500
training loss = 3.197413444519043 23000
training loss = 3.1973071098327637 23500
training loss = 3.1972239017486572 24000
training loss = 3.19712233543396 24500
training loss = 3.197113513946533 25000
training loss = 3.196951150894165 25500
training loss = 3.1968934535980225 26000
training loss = 3.1967689990997314 26500
training loss = 3.196681022644043 27000
training loss = 3.196593761444092 27500
training loss = 3.1965155601501465 28000
training loss = 3.1964378356933594 28500
training loss = 3.1963789463043213 29000
training loss = 3.1962926387786865 29500
training loss = 3.203047037124634 30000
training loss = 3.1976897716522217 30500
training loss = 3.1960902214050293 31000
training loss = 3.195988655090332 31500
training loss = 3.1965060234069824 32000
training loss = 3.1960251331329346 32500
training loss = 3.2960338592529297 33000
training loss = 3.1957154273986816 33500
training loss = 3.1956512928009033 34000
training loss = 3.1955840587615967 34500
training loss = 3.1955225467681885 35000
(1, 38)
(1, 0)
