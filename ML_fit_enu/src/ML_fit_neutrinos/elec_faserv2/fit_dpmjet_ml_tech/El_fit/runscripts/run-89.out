data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.1130701459654215 15.919731731757729 23.90693967437437
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 113.33460235595703 500
training loss = 76.15806579589844 1000
training loss = 75.86461639404297 1500
training loss = 75.51940155029297 2000
training loss = 75.07252502441406 2500
training loss = 74.36904907226562 3000
training loss = 72.27371215820312 3500
training loss = 5.2991790771484375 4000
training loss = 4.2509236335754395 4500
training loss = 4.113391399383545 5000
training loss = 4.067579746246338 5500
training loss = 4.293795108795166 6000
training loss = 4.031414031982422 6500
training loss = 4.109163761138916 7000
training loss = 4.049966812133789 7500
training loss = 4.010524749755859 8000
training loss = 4.00662088394165 8500
training loss = 4.0032525062561035 9000
training loss = 4.000225067138672 9500
training loss = 3.9974794387817383 10000
training loss = 4.1326093673706055 10500
training loss = 3.992455005645752 11000
training loss = 3.999751567840576 11500
training loss = 4.040904521942139 12000
training loss = 4.900528430938721 12500
training loss = 3.984205961227417 13000
training loss = 3.982465982437134 13500
training loss = 3.9808173179626465 14000
training loss = 3.9793026447296143 14500
training loss = 3.977844715118408 15000
training loss = 3.9764821529388428 15500
training loss = 3.975233793258667 16000
training loss = 3.9782726764678955 16500
training loss = 3.9767630100250244 17000
training loss = 3.978940486907959 17500
training loss = 3.970916748046875 18000
training loss = 3.9700024127960205 18500
training loss = 3.96913743019104 19000
training loss = 3.968348979949951 19500
training loss = 3.967592477798462 20000
training loss = 3.9669182300567627 20500
training loss = 3.9662435054779053 21000
training loss = 3.9656131267547607 21500
training loss = 3.9650485515594482 22000
training loss = 3.9645097255706787 22500
training loss = 3.9639997482299805 23000
training loss = 3.963534355163574 23500
training loss = 6.4135541915893555 24000
training loss = 3.9626777172088623 24500
training loss = 3.962301254272461 25000
training loss = 3.961926221847534 25500
training loss = 3.9615907669067383 26000
training loss = 3.961263656616211 26500
training loss = 3.9609715938568115 27000
training loss = 3.9613640308380127 27500
training loss = 4.108098983764648 28000
training loss = 4.2267842292785645 28500
training loss = 3.9599967002868652 29000
training loss = 3.959744930267334 29500
training loss = 3.959559917449951 30000
training loss = 3.959522008895874 30500
training loss = 3.959273099899292 31000
training loss = 3.960522413253784 31500
training loss = 3.958972454071045 32000
training loss = 3.958747148513794 32500
training loss = 3.9586141109466553 33000
training loss = 3.9584944248199463 33500
training loss = 3.9583821296691895 34000
training loss = 3.958280086517334 34500
training loss = 3.95817494392395 35000
(1, 38)
(1, 0)
