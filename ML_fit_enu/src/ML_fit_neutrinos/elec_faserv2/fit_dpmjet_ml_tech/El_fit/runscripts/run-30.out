data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.7187177341328194 19.531657117382842 6.550616984481916
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 242.79217529296875 500
training loss = 77.86282348632812 1000
training loss = 77.35501861572266 1500
training loss = 77.11027526855469 2000
training loss = 76.82550048828125 2500
training loss = 76.44780731201172 3000
training loss = 75.69925689697266 3500
training loss = 71.78961181640625 4000
training loss = 6.42971134185791 4500
training loss = 4.093260288238525 5000
training loss = 3.6571462154388428 5500
training loss = 3.5815188884735107 6000
training loss = 3.565538167953491 6500
training loss = 3.558800220489502 7000
training loss = 6.118951797485352 7500
training loss = 4.2165985107421875 8000
training loss = 4.61185359954834 8500
training loss = 4.923312187194824 9000
training loss = 3.5437428951263428 9500
training loss = 3.5808446407318115 10000
training loss = 3.6118643283843994 10500
training loss = 3.539530038833618 11000
training loss = 3.6191816329956055 11500
training loss = 3.5358691215515137 12000
training loss = 3.5348927974700928 12500
training loss = 4.8483476638793945 13000
training loss = 4.811370849609375 13500
training loss = 5.2021613121032715 14000
training loss = 3.605255126953125 14500
training loss = 3.5460803508758545 15000
training loss = 3.53045654296875 15500
training loss = 3.5298521518707275 16000
training loss = 3.529360294342041 16500
training loss = 3.528771162033081 17000
training loss = 3.5282838344573975 17500
training loss = 3.528141736984253 18000
training loss = 3.5273525714874268 18500
training loss = 3.5269052982330322 19000
training loss = 3.526491403579712 19500
training loss = 3.526104688644409 20000
training loss = 3.5257132053375244 20500
training loss = 3.5253419876098633 21000
training loss = 3.5250003337860107 21500
training loss = 3.524660348892212 22000
training loss = 3.524343490600586 22500
training loss = 3.524038314819336 23000
training loss = 3.523752450942993 23500
training loss = 3.5234920978546143 24000
training loss = 3.53554105758667 24500
training loss = 3.5301601886749268 25000
training loss = 3.526843547821045 25500
training loss = 3.5264391899108887 26000
training loss = 3.523794174194336 26500
training loss = 3.5220706462860107 27000
training loss = 3.5224714279174805 27500
training loss = 3.521653890609741 28000
training loss = 3.52154541015625 28500
training loss = 3.521314859390259 29000
training loss = 3.521254777908325 29500
training loss = 3.5212771892547607 30000
training loss = 3.5460100173950195 30500
training loss = 3.520960807800293 31000
training loss = 3.5212454795837402 31500
training loss = 3.5324347019195557 32000
training loss = 3.5205535888671875 32500
training loss = 3.520128011703491 33000
training loss = 3.5388293266296387 33500
training loss = 3.6438028812408447 34000
training loss = 3.535886287689209 34500
training loss = 3.533094644546509 35000
(1, 38)
(1, 0)
