data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.785965030590513 2.1211599987548535 21.634406298586473
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 62.85273361206055 500
training loss = 28.290367126464844 1000
training loss = 25.681259155273438 1500
training loss = 13.95130729675293 2000
training loss = 2.067389965057373 2500
training loss = 1.8854496479034424 3000
training loss = 1.8429148197174072 3500
training loss = 1.8235200643539429 4000
training loss = 1.809526801109314 4500
training loss = 1.8108195066452026 5000
training loss = 1.965704321861267 5500
training loss = 1.8697048425674438 6000
training loss = 1.7824327945709229 6500
training loss = 1.7746614217758179 7000
training loss = 1.7547457218170166 7500
training loss = 1.745894432067871 8000
training loss = 1.7373380661010742 8500
training loss = 1.7272300720214844 9000
training loss = 1.7141908407211304 9500
training loss = 1.6956000328063965 10000
training loss = 1.6743046045303345 10500
training loss = 1.6511893272399902 11000
training loss = 1.629154086112976 11500
training loss = 1.6068620681762695 12000
training loss = 1.5858184099197388 12500
training loss = 1.566367745399475 13000
training loss = 1.5496697425842285 13500
training loss = 1.532731294631958 14000
training loss = 1.5185933113098145 14500
training loss = 1.5060508251190186 15000
training loss = 1.4950376749038696 15500
training loss = 1.4855334758758545 16000
training loss = 1.4772428274154663 16500
training loss = 1.469337821006775 17000
training loss = 1.4648412466049194 17500
training loss = 1.4600107669830322 18000
training loss = 1.4512791633605957 18500
training loss = 1.446122646331787 19000
training loss = 1.4418705701828003 19500
training loss = 1.7104500532150269 20000
training loss = 2.8182075023651123 20500
training loss = 1.4302566051483154 21000
training loss = 1.427243947982788 21500
training loss = 3.6896867752075195 22000
training loss = 1.5764553546905518 22500
training loss = 1.418744444847107 23000
training loss = 1.416289210319519 23500
training loss = 1.6697285175323486 24000
training loss = 1.5011956691741943 24500
training loss = 1.4293439388275146 25000
training loss = 1.411846399307251 25500
training loss = 3.3267970085144043 26000
training loss = 1.4416298866271973 26500
training loss = 1.4029163122177124 27000
training loss = 1.4053583145141602 27500
training loss = 1.3989070653915405 28000
training loss = 1.397243857383728 28500
training loss = 1.395645022392273 29000
training loss = 1.3958451747894287 29500
training loss = 1.3927026987075806 30000
training loss = 1.3917495012283325 30500
training loss = 1.39328932762146 31000
training loss = 1.3883862495422363 31500
training loss = 1.387112021446228 32000
training loss = 1.385884404182434 32500
training loss = 1.384756326675415 33000
training loss = 1.3837405443191528 33500
training loss = 1.3845230340957642 34000
training loss = 1.3816759586334229 34500
training loss = 1.3808343410491943 35000
reduced chi^2 level 2 = 1.3808273077011108
Constrained alpha: 2.334791660308838
Constrained beta: -0.008273310959339142
Constrained gamma: 22.321495056152344
(1, 38)
(1, 0)
