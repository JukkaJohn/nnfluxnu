data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.645881124162968 2.188898551476297 79.16451614936638
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 88.57404327392578 500
training loss = 77.1170883178711 1000
training loss = 77.09432220458984 1500
training loss = 77.06468200683594 2000
training loss = 77.02052307128906 2500
training loss = 76.94847106933594 3000
training loss = 76.79225158691406 3500
training loss = 75.6195297241211 4000
training loss = 4.590989589691162 4500
training loss = 4.138941764831543 5000
training loss = 4.044232368469238 5500
training loss = 4.032449722290039 6000
training loss = 6.422706127166748 6500
training loss = 4.024006366729736 7000
training loss = 4.022098064422607 7500
training loss = 4.0196733474731445 8000
training loss = 4.026246070861816 8500
training loss = 4.0109992027282715 9000
training loss = 3.9769387245178223 9500
training loss = 3.852398633956909 10000
training loss = 3.8221001625061035 10500
training loss = 3.8005194664001465 11000
training loss = 3.7783308029174805 11500
training loss = 3.7542126178741455 12000
training loss = 4.243252277374268 12500
training loss = 4.327724456787109 13000
training loss = 7.040539264678955 13500
training loss = 3.7419891357421875 14000
training loss = 4.588629245758057 14500
training loss = 3.482285261154175 15000
training loss = 3.4070188999176025 15500
training loss = 3.322840690612793 16000
training loss = 3.231217384338379 16500
training loss = 3.134445905685425 17000
training loss = 3.037600040435791 17500
training loss = 2.9448018074035645 18000
training loss = 2.8611373901367188 18500
training loss = 2.8197784423828125 19000
training loss = 2.7358317375183105 19500
training loss = 2.6911747455596924 20000
training loss = 2.6596062183380127 20500
training loss = 2.6351277828216553 21000
training loss = 2.617489814758301 21500
training loss = 2.6039929389953613 22000
training loss = 2.5937888622283936 22500
training loss = 2.5843663215637207 23000
training loss = 2.576484203338623 23500
training loss = 2.5694146156311035 24000
training loss = 2.5626823902130127 24500
training loss = 2.5752384662628174 25000
training loss = 2.5510733127593994 25500
training loss = 2.60456919670105 26000
training loss = 2.694004774093628 26500
training loss = 2.534867763519287 27000
training loss = 2.5256388187408447 27500
training loss = 2.5194895267486572 28000
training loss = 2.519604444503784 28500
training loss = 2.5145509243011475 29000
training loss = 2.509408950805664 29500
training loss = 3.641400098800659 30000
training loss = 2.6499428749084473 30500
training loss = 2.480039596557617 31000
training loss = 2.473445415496826 31500
training loss = 2.4664742946624756 32000
training loss = 2.4592702388763428 32500
training loss = 2.4519598484039307 33000
training loss = 2.4468703269958496 33500
training loss = 10.086618423461914 34000
training loss = 2.429854393005371 34500
training loss = 2.422316074371338 35000
(1, 38)
(1, 0)
