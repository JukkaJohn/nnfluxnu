data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.6821188208497855 19.31543788136788 66.31745488204577
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 486.274169921875 500
training loss = 77.13925170898438 1000
training loss = 76.97599792480469 1500
training loss = 76.80472564697266 2000
training loss = 76.5945053100586 2500
training loss = 75.95101165771484 3000
training loss = 3.795006275177002 3500
training loss = 3.361724376678467 4000
training loss = 3.3429064750671387 4500
training loss = 3.1954832077026367 5000
training loss = 3.223375082015991 5500
training loss = 3.1928656101226807 6000
training loss = 3.20339035987854 6500
training loss = 3.1956088542938232 7000
training loss = 3.1639158725738525 7500
training loss = 3.1612751483917236 8000
training loss = 3.1591293811798096 8500
training loss = 3.157369375228882 9000
training loss = 3.155836343765259 9500
training loss = 3.1545333862304688 10000
training loss = 3.153420925140381 10500
training loss = 3.177401304244995 11000
training loss = 3.1516358852386475 11500
training loss = 3.150907039642334 12000
training loss = 3.150239944458008 12500
training loss = 3.149646520614624 13000
training loss = 3.1473758220672607 13500
training loss = 3.148603916168213 14000
(1, 38)
(1, 0)
