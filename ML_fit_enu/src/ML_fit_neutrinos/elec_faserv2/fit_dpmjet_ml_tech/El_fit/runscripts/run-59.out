data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.818007323508567 5.811579118728092 35.2889597418078
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 85.80156707763672 500
training loss = 71.7291030883789 1000
training loss = 71.62579345703125 1500
training loss = 71.53253173828125 2000
training loss = 71.39755249023438 2500
training loss = 71.1905746459961 3000
training loss = 70.82225036621094 3500
training loss = 69.84248352050781 4000
training loss = 49.93195724487305 4500
training loss = 4.5626726150512695 5000
training loss = 4.03054666519165 5500
training loss = 3.9258973598480225 6000
training loss = 3.922957181930542 6500
training loss = 4.019176483154297 7000
training loss = 5.823813438415527 7500
training loss = 4.649765968322754 8000
training loss = 4.040760040283203 8500
training loss = 3.880709171295166 9000
training loss = 7.914697170257568 9500
training loss = 3.8925862312316895 10000
training loss = 3.8572065830230713 10500
training loss = 3.815589427947998 11000
training loss = 3.8242509365081787 11500
training loss = 4.131400108337402 12000
training loss = 4.78153657913208 12500
training loss = 3.8039801120758057 13000
training loss = 3.8022289276123047 13500
training loss = 3.8413097858428955 14000
training loss = 3.821049690246582 14500
training loss = 4.006385326385498 15000
training loss = 3.7881646156311035 15500
training loss = 3.794985055923462 16000
training loss = 4.472720623016357 16500
training loss = 3.7769365310668945 17000
training loss = 3.8109242916107178 17500
training loss = 3.7986080646514893 18000
training loss = 3.7813920974731445 18500
training loss = 3.7673380374908447 19000
training loss = 3.7628157138824463 19500
training loss = 3.760594129562378 20000
training loss = 3.759004831314087 20500
training loss = 3.7626595497131348 21000
training loss = 3.7555794715881348 21500
training loss = 3.75724720954895 22000
training loss = 3.75286865234375 22500
training loss = 3.7513949871063232 23000
training loss = 3.7500112056732178 23500
training loss = 3.7502920627593994 24000
training loss = 3.74843168258667 24500
training loss = 3.7463812828063965 25000
training loss = 3.7452921867370605 25500
training loss = 3.7442634105682373 26000
training loss = 3.743281841278076 26500
training loss = 3.742340087890625 27000
training loss = 3.7414286136627197 27500
training loss = 3.9399523735046387 28000
training loss = 3.7403829097747803 28500
training loss = 3.7389590740203857 29000
training loss = 3.7382266521453857 29500
training loss = 3.7374801635742188 30000
training loss = 3.736788272857666 30500
training loss = 3.7361233234405518 31000
training loss = 3.7354981899261475 31500
training loss = 3.7348873615264893 32000
training loss = 3.7342987060546875 32500
training loss = 3.733750104904175 33000
training loss = 3.757683515548706 33500
training loss = 4.943995952606201 34000
training loss = 3.9830868244171143 34500
training loss = 3.7375476360321045 35000
(1, 38)
(1, 0)
