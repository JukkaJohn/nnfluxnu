data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.221699727499368 2.583896070820717 33.1050706082522
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 107.24962615966797 500
training loss = 74.55904388427734 1000
training loss = 74.46708679199219 1500
training loss = 74.45577239990234 2000
training loss = 74.44058227539062 2500
training loss = 74.42009735107422 3000
training loss = 74.39218139648438 3500
training loss = 74.35296630859375 4000
training loss = 74.29373931884766 4500
training loss = 74.18975067138672 5000
training loss = 73.93023681640625 5500
training loss = 71.54431915283203 6000
training loss = 3.9929187297821045 6500
training loss = 3.9017481803894043 7000
training loss = 3.9140889644622803 7500
training loss = 3.8943023681640625 8000
training loss = 3.860360860824585 8500
training loss = 3.8601386547088623 9000
training loss = 3.859473466873169 9500
training loss = 3.8590281009674072 10000
training loss = 3.85860013961792 10500
training loss = 3.8583648204803467 11000
training loss = 3.5843007564544678 11500
training loss = 3.4793860912323 12000
training loss = 3.4338624477386475 12500
training loss = 3.3993899822235107 13000
training loss = 3.3668131828308105 13500
training loss = 3.3331313133239746 14000
training loss = 3.2964015007019043 14500
training loss = 3.25549578666687 15000
training loss = 3.209031343460083 15500
training loss = 3.1577820777893066 16000
training loss = 3.1041805744171143 16500
training loss = 3.049762487411499 17000
training loss = 2.9968020915985107 17500
training loss = 2.9718191623687744 18000
training loss = 2.949615001678467 18500
training loss = 2.929248571395874 19000
training loss = 2.8335390090942383 19500
training loss = 3.499861001968384 20000
training loss = 3.908663272857666 20500
training loss = 2.766686201095581 21000
training loss = 2.9424209594726562 21500
training loss = 4.351457118988037 22000
training loss = 2.7307722568511963 22500
training loss = 2.723034620285034 23000
training loss = 2.7187488079071045 23500
training loss = 2.712412118911743 24000
training loss = 2.7072529792785645 24500
training loss = 2.7036497592926025 25000
training loss = 5.1354146003723145 25500
training loss = 2.7371184825897217 26000
training loss = 2.7036519050598145 26500
training loss = 2.6940248012542725 27000
training loss = 2.692782402038574 27500
training loss = 3.2430975437164307 28000
training loss = 7.740523338317871 28500
training loss = 2.6880671977996826 29000
training loss = 2.68697452545166 29500
training loss = 2.6858713626861572 30000
training loss = 4.246227264404297 30500
training loss = 2.683931827545166 31000
training loss = 2.6830482482910156 31500
training loss = 2.682206630706787 32000
training loss = 2.6815271377563477 32500
training loss = 4.130611419677734 33000
training loss = 2.779285430908203 33500
training loss = 2.6822876930236816 34000
training loss = 2.678493022918701 34500
training loss = 2.677746534347534 35000
(1, 38)
(1, 0)
