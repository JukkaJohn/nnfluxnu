data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.329698136615488 3.2199401899335633 79.0063828333098
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 108.42305755615234 500
training loss = 79.16400146484375 1000
training loss = 78.96619415283203 1500
training loss = 78.80455780029297 2000
training loss = 78.50711822509766 2500
training loss = 77.74092102050781 3000
training loss = 71.074951171875 3500
training loss = 4.0200581550598145 4000
training loss = 4.011396408081055 4500
training loss = 4.008485317230225 5000
training loss = 4.007481575012207 5500
training loss = 4.006887435913086 6000
training loss = 4.006176948547363 6500
training loss = 4.005987167358398 7000
training loss = 4.004047393798828 7500
training loss = 4.002948760986328 8000
training loss = 4.001899242401123 8500
training loss = 4.0008673667907715 9000
training loss = 4.033165454864502 9500
training loss = 4.0040602684021 10000
training loss = 3.999539375305176 10500
training loss = 3.9980392456054688 11000
training loss = 3.995349407196045 11500
training loss = 3.994143009185791 12000
training loss = 3.99385404586792 12500
training loss = 3.9917356967926025 13000
training loss = 3.9905247688293457 13500
training loss = 3.9892866611480713 14000
training loss = 3.988023519515991 14500
training loss = 3.986737012863159 15000
training loss = 4.052921295166016 15500
training loss = 4.107366561889648 16000
training loss = 4.0383124351501465 16500
training loss = 3.9823122024536133 17000
training loss = 3.980281352996826 17500
training loss = 3.978449821472168 18000
training loss = 3.9769039154052734 18500
training loss = 3.975348949432373 19000
training loss = 3.9737608432769775 19500
training loss = 3.972095012664795 20000
training loss = 3.9703738689422607 20500
training loss = 3.968590497970581 21000
training loss = 3.967038154602051 21500
training loss = 4.424661636352539 22000
training loss = 3.968801736831665 22500
training loss = 3.96171498298645 23000
training loss = 3.9591643810272217 23500
training loss = 3.9558539390563965 24000
training loss = 3.9534716606140137 24500
training loss = 3.9502651691436768 25000
training loss = 3.948291301727295 25500
training loss = 3.9449727535247803 26000
training loss = 3.949930429458618 26500
training loss = 3.935478687286377 27000
training loss = 3.9304797649383545 27500
training loss = 3.9254114627838135 28000
training loss = 3.9181857109069824 28500
training loss = 3.9104573726654053 29000
training loss = 3.900101900100708 29500
training loss = 3.8874664306640625 30000
training loss = 3.8708207607269287 30500
training loss = 3.8480939865112305 31000
training loss = 3.8150060176849365 31500
training loss = 3.764493942260742 32000
training loss = 3.6855175495147705 32500
training loss = 3.5663206577301025 33000
training loss = 3.4164412021636963 33500
training loss = 3.2724835872650146 34000
training loss = 3.1531426906585693 34500
training loss = 3.0508365631103516 35000
(1, 38)
(1, 0)
