data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.735136841144489 10.877313382819885 83.26287444545905
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 71.89442443847656 500
training loss = 71.10819244384766 1000
training loss = 69.84091186523438 1500
training loss = 6.139843940734863 2000
training loss = 5.06523323059082 2500
training loss = 4.40021276473999 3000
training loss = 3.9896810054779053 3500
training loss = 3.8074593544006348 4000
training loss = 3.7230067253112793 4500
training loss = 3.6750681400299072 5000
training loss = 3.6440207958221436 5500
training loss = 3.6215522289276123 6000
training loss = 3.6042978763580322 6500
training loss = 3.589505672454834 7000
training loss = 3.575604200363159 7500
training loss = 3.5614519119262695 8000
training loss = 3.5464529991149902 8500
training loss = 3.5273277759552 9000
training loss = 3.5055830478668213 9500
training loss = 3.478722095489502 10000
training loss = 3.44573974609375 10500
training loss = 3.4037699699401855 11000
training loss = 3.350957155227661 11500
training loss = 3.2854561805725098 12000
training loss = 3.2109572887420654 12500
training loss = 3.1200718879699707 13000
training loss = 3.027236223220825 13500
training loss = 2.9772086143493652 14000
training loss = 2.8681952953338623 14500
training loss = 2.799581289291382 15000
training loss = 2.7477049827575684 15500
training loss = 2.706935405731201 16000
training loss = 2.6754112243652344 16500
training loss = 2.6512930393218994 17000
training loss = 2.633025646209717 17500
training loss = 2.61940598487854 18000
training loss = 2.6087746620178223 18500
training loss = 2.6011440753936768 19000
training loss = 2.595245599746704 19500
training loss = 2.590662717819214 20000
training loss = 2.587336301803589 20500
training loss = 2.5847694873809814 21000
training loss = 2.5828182697296143 21500
training loss = 2.581272602081299 22000
training loss = 2.580052614212036 22500
training loss = 2.5841565132141113 23000
training loss = 2.596571683883667 23500
training loss = 2.5943660736083984 24000
training loss = 2.6047956943511963 24500
training loss = 2.5768377780914307 25000
training loss = 2.576289176940918 25500
training loss = 2.575968027114868 26000
training loss = 2.5756924152374268 26500
training loss = 2.5754342079162598 27000
training loss = 2.5752036571502686 27500
training loss = 2.5750110149383545 28000
training loss = 2.57485032081604 28500
training loss = 2.5747039318084717 29000
training loss = 2.5745890140533447 29500
training loss = 2.5745184421539307 30000
training loss = 2.755655527114868 30500
training loss = 2.591726064682007 31000
training loss = 2.658372402191162 31500
training loss = 5.571536064147949 32000
training loss = 2.725003242492676 32500
training loss = 2.9347949028015137 33000
(1, 38)
(1, 0)
