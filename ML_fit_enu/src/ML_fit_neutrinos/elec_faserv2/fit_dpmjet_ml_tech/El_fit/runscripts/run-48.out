data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.6002538473679486 8.409735998962065 71.63995970630582
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 81.05565643310547 500
training loss = 78.0360107421875 1000
training loss = 77.98526000976562 1500
training loss = 77.92518615722656 2000
training loss = 77.85608673095703 2500
training loss = 77.77642822265625 3000
training loss = 77.67779541015625 3500
training loss = 77.51569366455078 4000
training loss = 76.38953399658203 4500
training loss = 3.1197831630706787 5000
training loss = 3.016632080078125 5500
training loss = 2.9931552410125732 6000
training loss = 2.9875025749206543 6500
training loss = 2.9859936237335205 7000
training loss = 2.988266706466675 7500
training loss = 2.9859118461608887 8000
training loss = 2.9855456352233887 8500
training loss = 2.5893683433532715 9000
training loss = 2.488572359085083 9500
training loss = 1.9500465393066406 10000
training loss = 1.9221099615097046 10500
training loss = 1.9013677835464478 11000
training loss = 1.880327582359314 11500
training loss = 1.8585418462753296 12000
training loss = 1.8384768962860107 12500
training loss = 1.819905400276184 13000
training loss = 1.8044919967651367 13500
training loss = 1.7883553504943848 14000
training loss = 1.787928581237793 14500
training loss = 1.815574049949646 15000
training loss = 1.7530887126922607 15500
training loss = 1.7437021732330322 16000
training loss = 1.7356921434402466 16500
training loss = 1.7342227697372437 17000
training loss = 1.724756121635437 17500
training loss = 1.8032636642456055 18000
training loss = 1.7203730344772339 18500
training loss = 1.7066421508789062 19000
training loss = 1.7027355432510376 19500
training loss = 1.6992175579071045 20000
training loss = 1.6950392723083496 20500
training loss = 1.6916838884353638 21000
training loss = 1.6887446641921997 21500
training loss = 1.6861836910247803 22000
training loss = 1.6837353706359863 22500
training loss = 1.6815236806869507 23000
training loss = 1.679711103439331 23500
training loss = 1.677817702293396 24000
training loss = 1.6761711835861206 24500
training loss = 1.6746810674667358 25000
training loss = 1.6732066869735718 25500
training loss = 1.9457229375839233 26000
training loss = 3.178435802459717 26500
training loss = 1.6697168350219727 27000
training loss = 4.625892162322998 27500
training loss = 4.428296089172363 28000
training loss = 1.6796976327896118 28500
training loss = 1.667211651802063 29000
training loss = 1.670441746711731 29500
training loss = 1.6652708053588867 30000
training loss = 1.670863151550293 30500
training loss = 1.8432972431182861 31000
training loss = 1.6946642398834229 31500
training loss = 1.6749871969223022 32000
training loss = 1.6627757549285889 32500
training loss = 1.66168212890625 33000
training loss = 1.6613014936447144 33500
training loss = 1.6608368158340454 34000
training loss = 1.6604328155517578 34500
training loss = 1.6600984334945679 35000
reduced chi^2 level 2 = 1.660094976425171
Constrained alpha: 2.3814857006073
Constrained beta: 4.154247760772705
Constrained gamma: 44.7170295715332
(1, 38)
(1, 0)
