data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.6054624657807817 17.20666092658089 51.69732201062879
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 271.21868896484375 500
training loss = 79.0699691772461 1000
training loss = 76.71009063720703 1500
training loss = 71.72647094726562 2000
training loss = 12.030187606811523 2500
training loss = 6.146918773651123 3000
training loss = 4.586908340454102 3500
training loss = 4.0189290046691895 4000
training loss = 3.8868584632873535 4500
training loss = 3.9093880653381348 5000
training loss = 3.8598246574401855 5500
training loss = 3.84916615486145 6000
training loss = 3.932685375213623 6500
training loss = 3.8413970470428467 7000
training loss = 3.8728694915771484 7500
training loss = 4.166111946105957 8000
training loss = 3.8276174068450928 8500
training loss = 3.824702262878418 9000
training loss = 3.8220465183258057 9500
training loss = 3.8195619583129883 10000
training loss = 3.8172736167907715 10500
training loss = 3.815129280090332 11000
training loss = 3.8131906986236572 11500
training loss = 3.8112661838531494 12000
training loss = 3.80971097946167 12500
training loss = 3.808072566986084 13000
training loss = 3.8061909675598145 13500
training loss = 3.8046553134918213 14000
training loss = 3.8031938076019287 14500
training loss = 3.801805257797241 15000
training loss = 3.8004612922668457 15500
training loss = 3.799175500869751 16000
training loss = 3.7979350090026855 16500
training loss = 3.8128185272216797 17000
training loss = 4.23288106918335 17500
training loss = 6.498409748077393 18000
training loss = 3.7945780754089355 18500
training loss = 4.25601863861084 19000
training loss = 5.239370346069336 19500
training loss = 4.498173713684082 20000
training loss = 3.8051934242248535 20500
training loss = 3.8107059001922607 21000
training loss = 6.104799270629883 21500
training loss = 3.7871837615966797 22000
training loss = 3.7864203453063965 22500
training loss = 3.785677194595337 23000
training loss = 3.784952402114868 23500
training loss = 3.784274101257324 24000
training loss = 3.783694267272949 24500
training loss = 3.782951831817627 25000
training loss = 3.782331705093384 25500
training loss = 3.7817351818084717 26000
training loss = 3.7811520099639893 26500
training loss = 3.780597448348999 27000
training loss = 3.7830309867858887 27500
training loss = 6.060640811920166 28000
training loss = 3.7790298461914062 28500
training loss = 3.780834913253784 29000
training loss = 3.7780580520629883 29500
training loss = 3.7776107788085938 30000
training loss = 3.7771666049957275 30500
training loss = 3.776742696762085 31000
training loss = 3.776327133178711 31500
training loss = 3.775925397872925 32000
training loss = 7.398411273956299 32500
training loss = 3.8188815116882324 33000
training loss = 4.478253364562988 33500
training loss = 3.785128116607666 34000
training loss = 3.774235486984253 34500
training loss = 3.7746734619140625 35000
(1, 38)
(1, 0)
