data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.2968428091538424 14.272635699110662 71.5519133675902
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 98.49871826171875 500
training loss = 72.05213928222656 1000
training loss = 70.25546264648438 1500
training loss = 62.42238998413086 2000
training loss = 5.0162129402160645 2500
training loss = 3.936689615249634 3000
training loss = 3.5816121101379395 3500
training loss = 3.4598212242126465 4000
training loss = 3.385067939758301 4500
training loss = 3.3307535648345947 5000
training loss = 3.2975122928619385 5500
training loss = 3.275991201400757 6000
training loss = 3.2609546184539795 6500
training loss = 3.24991774559021 7000
training loss = 3.241697072982788 7500
training loss = 3.2354161739349365 8000
training loss = 3.230498790740967 8500
training loss = 3.268596887588501 9000
training loss = 3.223071336746216 9500
training loss = 3.220381498336792 10000
training loss = 3.2181925773620605 10500
training loss = 3.2169277667999268 11000
training loss = 3.21464467048645 11500
training loss = 3.2132766246795654 12000
training loss = 3.2120442390441895 12500
training loss = 3.211303472518921 13000
training loss = 3.2104692459106445 13500
training loss = 3.209336519241333 14000
training loss = 3.208592414855957 14500
training loss = 3.2079432010650635 15000
training loss = 3.2073538303375244 15500
training loss = 3.2068281173706055 16000
training loss = 3.2063326835632324 16500
training loss = 3.205876111984253 17000
training loss = 3.2054555416107178 17500
training loss = 3.2050881385803223 18000
training loss = 3.204676389694214 18500
training loss = 3.4253616333007812 19000
training loss = 3.208221197128296 19500
training loss = 3.2046027183532715 20000
training loss = 3.265721559524536 20500
training loss = 5.250925064086914 21000
training loss = 3.9103715419769287 21500
training loss = 4.788477897644043 22000
training loss = 3.2321887016296387 22500
training loss = 4.36132287979126 23000
training loss = 3.601926326751709 23500
training loss = 3.3210227489471436 24000
training loss = 3.2588419914245605 24500
training loss = 3.200834274291992 25000
training loss = 3.2031209468841553 25500
training loss = 3.936206340789795 26000
training loss = 5.051903247833252 26500
training loss = 3.199793577194214 27000
training loss = 3.201115846633911 27500
training loss = 3.2176785469055176 28000
training loss = 3.5119400024414062 28500
training loss = 3.198831796646118 29000
training loss = 3.315565824508667 29500
training loss = 3.2061965465545654 30000
training loss = 3.198094129562378 30500
training loss = 3.198329448699951 31000
training loss = 3.19754695892334 31500
training loss = 3.1972525119781494 32000
training loss = 3.196962594985962 32500
training loss = 3.1966898441314697 33000
training loss = 3.1964333057403564 33500
training loss = 3.196152687072754 34000
training loss = 3.1958839893341064 34500
training loss = 3.1956286430358887 35000
(1, 38)
(1, 0)
