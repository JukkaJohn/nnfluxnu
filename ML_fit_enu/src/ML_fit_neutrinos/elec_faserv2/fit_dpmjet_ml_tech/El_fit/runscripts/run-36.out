data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.144753194129101 2.9590356440553167 97.7958170780075
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 78.5815658569336 500
training loss = 74.41860961914062 1000
training loss = 74.0414810180664 1500
training loss = 7.3465046882629395 2000
training loss = 4.943417072296143 2500
training loss = 3.6177167892456055 3000
training loss = 3.4547717571258545 3500
training loss = 3.4452455043792725 4000
training loss = 3.525847911834717 4500
training loss = 3.437873125076294 5000
training loss = 3.4347405433654785 5500
training loss = 3.431708812713623 6000
training loss = 3.4287350177764893 6500
training loss = 3.4255998134613037 7000
training loss = 3.422410726547241 7500
training loss = 3.419140100479126 8000
training loss = 3.4158084392547607 8500
training loss = 3.4122273921966553 9000
training loss = 3.4084954261779785 9500
training loss = 3.404611825942993 10000
training loss = 3.400590658187866 10500
training loss = 3.3963515758514404 11000
training loss = 3.3917489051818848 11500
training loss = 3.14729642868042 12000
training loss = 2.251796007156372 12500
training loss = 2.1806602478027344 13000
training loss = 2.160572052001953 13500
training loss = 2.1534476280212402 14000
training loss = 2.1504135131835938 14500
training loss = 2.148897886276245 15000
training loss = 2.147980213165283 15500
training loss = 2.1478524208068848 16000
training loss = 2.1469831466674805 16500
training loss = 2.148667097091675 17000
training loss = 2.1463587284088135 17500
training loss = 2.146151542663574 18000
training loss = 2.1460607051849365 18500
training loss = 2.1324944496154785 19000
training loss = 2.413020372390747 19500
training loss = 2.1034176349639893 20000
training loss = 2.0956149101257324 20500
training loss = 2.089916944503784 21000
training loss = 2.120734691619873 21500
training loss = 2.459975481033325 22000
training loss = 2.1387228965759277 22500
training loss = 2.0798892974853516 23000
training loss = 2.1214327812194824 23500
training loss = 2.076984167098999 24000
training loss = 2.4402549266815186 24500
training loss = 2.976191997528076 25000
training loss = 2.070904016494751 25500
training loss = 2.1935842037200928 26000
training loss = 2.0763542652130127 26500
training loss = 2.068575382232666 27000
training loss = 2.068434953689575 27500
training loss = 2.2780723571777344 28000
training loss = 3.1759917736053467 28500
training loss = 2.066453456878662 29000
training loss = 2.066053867340088 29500
training loss = 2.0657155513763428 30000
training loss = 2.0653228759765625 30500
training loss = 2.065023422241211 31000
training loss = 2.0647265911102295 31500
training loss = 2.064466953277588 32000
training loss = 2.0678553581237793 32500
training loss = 2.0639994144439697 33000
training loss = 2.0819382667541504 33500
training loss = 2.0636003017425537 34000
training loss = 5.985166072845459 34500
training loss = 2.064781665802002 35000
(1, 38)
(1, 0)
