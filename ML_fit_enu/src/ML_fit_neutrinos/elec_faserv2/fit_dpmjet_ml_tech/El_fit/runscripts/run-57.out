data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.5952906351343668 9.123214547598629 96.82749608856926
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 74.03355407714844 500
training loss = 73.6161117553711 1000
training loss = 73.4926986694336 1500
training loss = 73.32563018798828 2000
training loss = 73.03858947753906 2500
training loss = 71.59735107421875 3000
training loss = 5.943962574005127 3500
training loss = 3.7324211597442627 4000
training loss = 3.0805084705352783 4500
training loss = 2.7412314414978027 5000
training loss = 2.727635145187378 5500
training loss = 2.7144079208374023 6000
training loss = 2.7012650966644287 6500
training loss = 2.6878886222839355 7000
training loss = 2.6741862297058105 7500
training loss = 2.6616384983062744 8000
training loss = 2.6460704803466797 8500
training loss = 2.630516529083252 9000
training loss = 2.6150572299957275 9500
training loss = 2.5991714000701904 10000
training loss = 2.6891226768493652 10500
training loss = 2.150632858276367 11000
training loss = 1.8700007200241089 11500
training loss = 1.824763298034668 12000
training loss = 1.8038402795791626 12500
training loss = 1.7918063402175903 13000
training loss = 1.7892593145370483 13500
training loss = 1.7741003036499023 14000
training loss = 1.7685643434524536 14500
training loss = 1.763749361038208 15000
training loss = 1.7822740077972412 15500
training loss = 1.7563282251358032 16000
training loss = 1.7532328367233276 16500
training loss = 1.7504234313964844 17000
training loss = 1.7478294372558594 17500
training loss = 1.7453932762145996 18000
training loss = 1.7431215047836304 18500
training loss = 1.8054215908050537 19000
training loss = 1.7389682531356812 19500
training loss = 1.9238349199295044 20000
training loss = 1.762437105178833 20500
training loss = 1.849928379058838 21000
training loss = 2.1003317832946777 21500
training loss = 1.792785406112671 22000
training loss = 1.8277158737182617 22500
training loss = 1.7275840044021606 23000
training loss = 1.7352012395858765 23500
training loss = 1.7565484046936035 24000
training loss = 1.7315722703933716 24500
training loss = 1.7244811058044434 25000
training loss = 1.7228927612304688 25500
training loss = 1.720247745513916 26000
training loss = 1.8414620161056519 26500
training loss = 1.8320722579956055 27000
training loss = 1.7194193601608276 27500
training loss = 1.7171255350112915 28000
training loss = 1.7159532308578491 28500
training loss = 1.7157964706420898 29000
training loss = 1.7142137289047241 29500
training loss = 1.7134214639663696 30000
training loss = 3.8774518966674805 30500
training loss = 1.7119550704956055 31000
training loss = 1.7132799625396729 31500
training loss = 1.7171249389648438 32000
training loss = 1.7131541967391968 32500
training loss = 1.7354891300201416 33000
training loss = 1.7086025476455688 33500
training loss = 1.7079415321350098 34000
training loss = 1.7073240280151367 34500
training loss = 1.706717848777771 35000
reduced chi^2 level 2 = 1.7067108154296875
Constrained alpha: 2.3697049617767334
Constrained beta: 3.8680903911590576
Constrained gamma: 51.89189529418945
(1, 38)
(1, 0)
