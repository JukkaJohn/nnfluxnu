data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.7097331681798638 16.198172647934364 36.76348577671179
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 109.31490325927734 500
training loss = 74.47553253173828 1000
training loss = 74.1259994506836 1500
training loss = 73.64301300048828 2000
training loss = 72.6818618774414 2500
training loss = 60.541690826416016 3000
training loss = 4.070220947265625 3500
training loss = 3.4646952152252197 4000
training loss = 3.282465934753418 4500
training loss = 3.20155668258667 5000
training loss = 3.1619977951049805 5500
training loss = 3.1394546031951904 6000
training loss = 3.572270393371582 6500
training loss = 4.215868949890137 7000
training loss = 3.108780860900879 7500
training loss = 3.9382340908050537 8000
training loss = 3.101480722427368 8500
training loss = 3.178292751312256 9000
training loss = 3.095142126083374 9500
training loss = 3.0928807258605957 10000
training loss = 3.0986335277557373 10500
training loss = 3.0895018577575684 11000
training loss = 3.0880134105682373 11500
training loss = 3.0869832038879395 12000
training loss = 3.0860817432403564 12500
training loss = 3.085284948348999 13000
training loss = 3.0845000743865967 13500
training loss = 3.08381986618042 14000
training loss = 3.083214521408081 14500
training loss = 3.082634925842285 15000
training loss = 3.082141399383545 15500
training loss = 3.0816092491149902 16000
training loss = 3.081137180328369 16500
training loss = 3.0806963443756104 17000
training loss = 3.0802690982818604 17500
training loss = 3.0798661708831787 18000
training loss = 3.079490900039673 18500
training loss = 3.079611301422119 19000
training loss = 3.0788252353668213 19500
training loss = 3.0805349349975586 20000
training loss = 3.0781166553497314 20500
training loss = 3.0777926445007324 21000
training loss = 3.0774741172790527 21500
training loss = 3.0771758556365967 22000
training loss = 3.0769007205963135 22500
training loss = 3.0766098499298096 23000
training loss = 3.0764334201812744 23500
training loss = 3.0804877281188965 24000
training loss = 3.562403678894043 24500
training loss = 3.0756373405456543 25000
training loss = 4.834046363830566 25500
training loss = 3.07509446144104 26000
training loss = 3.7304253578186035 26500
training loss = 4.28648042678833 27000
training loss = 3.4509387016296387 27500
training loss = 4.030858039855957 28000
training loss = 3.0750906467437744 28500
training loss = 3.5083320140838623 29000
training loss = 3.0982797145843506 29500
training loss = 3.081324338912964 30000
training loss = 3.1361045837402344 30500
training loss = 3.5399363040924072 31000
training loss = 3.0761780738830566 31500
training loss = 3.2373995780944824 32000
training loss = 3.1002390384674072 32500
training loss = 7.648430347442627 33000
training loss = 3.0721495151519775 33500
training loss = 3.0719900131225586 34000
training loss = 3.071845054626465 34500
training loss = 3.0716967582702637 35000
(1, 38)
(1, 0)
