data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.5829826962279125 8.510441996261937 75.99486328328715
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 78.9345932006836 500
training loss = 78.38157653808594 1000
training loss = 78.04991149902344 1500
training loss = 77.6415023803711 2000
training loss = 76.2545394897461 2500
training loss = 4.7550764083862305 3000
training loss = 3.93548583984375 3500
training loss = 3.7795965671539307 4000
training loss = 3.7462334632873535 4500
training loss = 3.7282602787017822 5000
training loss = 3.714677572250366 5500
training loss = 3.7038824558258057 6000
training loss = 3.6951005458831787 6500
training loss = 3.6873393058776855 7000
training loss = 3.6806857585906982 7500
training loss = 3.6848068237304688 8000
training loss = 3.6693601608276367 8500
training loss = 3.6643412113189697 9000
training loss = 3.659583568572998 9500
training loss = 3.6634843349456787 10000
training loss = 4.335975170135498 10500
training loss = 3.6818766593933105 11000
training loss = 3.6424758434295654 11500
training loss = 3.6410515308380127 12000
training loss = 4.668447017669678 12500
training loss = 3.631007671356201 13000
training loss = 5.26882791519165 13500
training loss = 8.087566375732422 14000
training loss = 5.020597457885742 14500
training loss = 3.617140531539917 15000
training loss = 3.6139533519744873 15500
training loss = 3.610853910446167 16000
training loss = 3.720158576965332 16500
training loss = 3.6064791679382324 17000
training loss = 5.455003261566162 17500
training loss = 3.6040737628936768 18000
training loss = 3.5978333950042725 18500
training loss = 4.1351237297058105 19000
training loss = 3.6927144527435303 19500
training loss = 3.591454029083252 20000
training loss = 3.587191581726074 20500
training loss = 3.5849626064300537 21000
training loss = 3.5828359127044678 21500
training loss = 3.5807793140411377 22000
training loss = 3.578801155090332 22500
training loss = 3.576870918273926 23000
training loss = 4.024318218231201 23500
training loss = 3.866058349609375 24000
training loss = 3.5719637870788574 24500
training loss = 4.118673324584961 25000
training loss = 3.5692825317382812 25500
training loss = 3.5666561126708984 26000
training loss = 3.927171468734741 26500
training loss = 3.6490049362182617 27000
training loss = 3.6943037509918213 27500
training loss = 5.491398334503174 28000
training loss = 5.9766621589660645 28500
training loss = 3.557494640350342 29000
training loss = 3.5561017990112305 29500
training loss = 3.5547213554382324 30000
training loss = 3.5533952713012695 30500
training loss = 3.5521457195281982 31000
training loss = 3.550642251968384 31500
training loss = 3.549302816390991 32000
training loss = 3.548049211502075 32500
training loss = 3.5466506481170654 33000
training loss = 3.5453267097473145 33500
training loss = 3.544008731842041 34000
training loss = 3.5730714797973633 34500
training loss = 3.5413570404052734 35000
(1, 38)
(1, 0)
