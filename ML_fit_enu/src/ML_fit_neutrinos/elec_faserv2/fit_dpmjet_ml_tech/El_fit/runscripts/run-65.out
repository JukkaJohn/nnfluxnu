data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.8306870319228563 5.926309253677273 88.44678070832477
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 87.70249938964844 500
training loss = 81.61852264404297 1000
training loss = 81.44379425048828 1500
training loss = 81.14376068115234 2000
training loss = 80.37974548339844 2500
training loss = 26.123497009277344 3000
training loss = 4.079067230224609 3500
training loss = 3.9636800289154053 4000
training loss = 3.9387969970703125 4500
training loss = 3.989908456802368 5000
training loss = 3.9237077236175537 5500
training loss = 3.91691517829895 6000
training loss = 3.910902738571167 6500
training loss = 3.904195785522461 7000
training loss = 3.895634651184082 7500
training loss = 3.8846099376678467 8000
training loss = 3.8730199337005615 8500
training loss = 3.863565683364868 9000
training loss = 3.855658769607544 9500
training loss = 3.8484063148498535 10000
training loss = 3.841392993927002 10500
training loss = 3.834505558013916 11000
training loss = 3.8276238441467285 11500
training loss = 3.8207015991210938 12000
training loss = 3.8136050701141357 12500
training loss = 3.8063371181488037 13000
training loss = 3.7987587451934814 13500
training loss = 3.7908003330230713 14000
training loss = 3.7823827266693115 14500
training loss = 3.7733852863311768 15000
training loss = 3.7636983394622803 15500
training loss = 3.753153085708618 16000
training loss = 3.741715669631958 16500
training loss = 3.7291324138641357 17000
training loss = 3.715174674987793 17500
training loss = 3.699612617492676 18000
training loss = 3.6824445724487305 18500
training loss = 7.944230556488037 19000
training loss = 3.6542246341705322 19500
training loss = 3.6234889030456543 20000
training loss = 3.6371734142303467 20500
training loss = 3.579038143157959 21000
training loss = 3.5559241771698 21500
training loss = 3.5315239429473877 22000
training loss = 3.5065805912017822 22500
training loss = 3.480759382247925 23000
training loss = 3.454226016998291 23500
training loss = 3.4279379844665527 24000
training loss = 3.4010016918182373 24500
training loss = 3.3752031326293945 25000
training loss = 3.353461503982544 25500
training loss = 3.3265023231506348 26000
training loss = 3.303407907485962 26500
training loss = 3.313673257827759 27000
training loss = 3.5615949630737305 27500
training loss = 3.226881742477417 28000
training loss = 3.206202507019043 28500
training loss = 3.1865334510803223 29000
training loss = 7.076080322265625 29500
training loss = 4.050525188446045 30000
training loss = 5.85763692855835 30500
training loss = 3.1223554611206055 31000
training loss = 3.1098134517669678 31500
training loss = 3.0980780124664307 32000
training loss = 3.0871593952178955 32500
training loss = 3.077495574951172 33000
training loss = 3.0683302879333496 33500
training loss = 3.0598533153533936 34000
training loss = 3.0523016452789307 34500
training loss = 3.0459792613983154 35000
(1, 38)
(1, 0)
