data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.018298633190036 19.246609900206476 77.63570258339789
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 219.26829528808594 500
training loss = 81.31525421142578 1000
training loss = 80.81847381591797 1500
training loss = 80.31013488769531 2000
training loss = 79.70083618164062 2500
training loss = 78.93885803222656 3000
training loss = 77.8677749633789 3500
training loss = 75.88296508789062 4000
training loss = 5.860473155975342 4500
training loss = 3.9197194576263428 5000
training loss = 3.77681565284729 5500
training loss = 3.776184558868408 6000
training loss = 3.8685336112976074 6500
training loss = 2.8800556659698486 7000
training loss = 2.755902051925659 7500
training loss = 2.675950765609741 8000
training loss = 2.7231364250183105 8500
training loss = 2.5510482788085938 9000
training loss = 2.4930732250213623 9500
training loss = 4.01343297958374 10000
training loss = 2.384695053100586 10500
training loss = 2.3197267055511475 11000
training loss = 2.930102825164795 11500
training loss = 1.8407779932022095 12000
training loss = 1.7234737873077393 12500
training loss = 1.9934114217758179 13000
training loss = 1.6401684284210205 13500
training loss = 1.6184900999069214 14000
training loss = 1.6035311222076416 14500
training loss = 1.5916383266448975 15000
training loss = 1.5823047161102295 15500
training loss = 1.5745720863342285 16000
training loss = 1.5975537300109863 16500
training loss = 1.6510323286056519 17000
training loss = 2.2138354778289795 17500
training loss = 2.2128913402557373 18000
training loss = 1.5573641061782837 18500
training loss = 1.5486479997634888 19000
training loss = 1.5453696250915527 19500
training loss = 1.5416911840438843 20000
training loss = 1.5393811464309692 20500
training loss = 1.5395177602767944 21000
training loss = 1.8106920719146729 21500
training loss = 1.533764362335205 22000
training loss = 1.5322380065917969 22500
training loss = 1.5306082963943481 23000
training loss = 1.5293290615081787 23500
training loss = 1.5279779434204102 24000
training loss = 2.079824447631836 24500
training loss = 1.6809417009353638 25000
training loss = 1.6843008995056152 25500
training loss = 1.9308291673660278 26000
training loss = 1.547896385192871 26500
training loss = 1.5257881879806519 27000
training loss = 1.521634578704834 27500
training loss = 1.5264579057693481 28000
training loss = 1.520588755607605 28500
training loss = 1.5193170309066772 29000
training loss = 1.5185695886611938 29500
training loss = 1.5189337730407715 30000
training loss = 1.5180768966674805 30500
training loss = 1.5171703100204468 31000
training loss = 1.6047178506851196 31500
training loss = 1.5359406471252441 32000
training loss = 1.5271581411361694 32500
training loss = 1.5245410203933716 33000
training loss = 1.513530969619751 33500
training loss = 1.5247212648391724 34000
training loss = 1.6222058534622192 34500
training loss = 1.523017168045044 35000
reduced chi^2 level 2 = 1.515816330909729
Constrained alpha: 2.5239553451538086
Constrained beta: 4.117769241333008
Constrained gamma: 50.13593673706055
(1, 38)
(1, 0)
