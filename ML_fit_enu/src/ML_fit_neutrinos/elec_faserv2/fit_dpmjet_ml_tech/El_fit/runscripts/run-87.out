data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.870185437706226 6.8433650960725885 97.41454791109993
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 71.0499038696289 500
training loss = 70.67668151855469 1000
training loss = 69.59478759765625 1500
training loss = 7.480978965759277 2000
training loss = 2.348663091659546 2500
training loss = 2.0563642978668213 3000
training loss = 1.9121454954147339 3500
training loss = 1.8898684978485107 4000
training loss = 1.8862003087997437 4500
training loss = 1.8837558031082153 5000
training loss = 1.8815853595733643 5500
training loss = 1.8792393207550049 6000
training loss = 1.8771288394927979 6500
training loss = 1.8750228881835938 7000
training loss = 1.8729256391525269 7500
training loss = 1.8708540201187134 8000
training loss = 1.8687679767608643 8500
training loss = 3.885268449783325 9000
training loss = 1.501726508140564 9500
training loss = 1.39805006980896 10000
training loss = 1.3687776327133179 10500
training loss = 1.355849027633667 11000
training loss = 1.3472577333450317 11500
training loss = 1.3409205675125122 12000
training loss = 1.336030125617981 12500
training loss = 1.332119345664978 13000
training loss = 1.3289893865585327 13500
training loss = 1.3264753818511963 14000
training loss = 1.3731366395950317 14500
training loss = 1.3286902904510498 15000
training loss = 1.816839575767517 15500
training loss = 1.3201977014541626 16000
training loss = 1.3218711614608765 16500
training loss = 8.221877098083496 17000
training loss = 1.3176521062850952 17500
training loss = 6.597625255584717 18000
training loss = 1.3165218830108643 18500
training loss = 1.316051721572876 19000
training loss = 1.3870940208435059 19500
training loss = 1.9703953266143799 20000
training loss = 1.7995043992996216 20500
training loss = 1.3351197242736816 21000
training loss = 1.3196111917495728 21500
training loss = 1.314170002937317 22000
training loss = 1.3139135837554932 22500
training loss = 1.313741683959961 23000
training loss = 1.313524603843689 23500
training loss = 1.3133580684661865 24000
training loss = 1.3131893873214722 24500
training loss = 1.3131012916564941 25000
training loss = 1.312882900238037 25500
training loss = 1.31270432472229 26000
training loss = 1.312562108039856 26500
training loss = 1.312406301498413 27000
training loss = 1.3122729063034058 27500
training loss = 1.312178134918213 28000
training loss = 1.3120522499084473 28500
training loss = 4.880971908569336 29000
training loss = 1.3117930889129639 29500
training loss = 1.311671257019043 30000
training loss = 1.3115230798721313 30500
training loss = 1.3114100694656372 31000
training loss = 1.3113017082214355 31500
training loss = 1.3142576217651367 32000
training loss = 1.311030387878418 32500
training loss = 1.3109313249588013 33000
training loss = 1.310743808746338 33500
training loss = 1.3106049299240112 34000
training loss = 1.3104567527770996 34500
training loss = 1.3103551864624023 35000
reduced chi^2 level 2 = 1.3103574514389038
Constrained alpha: 2.2183146476745605
Constrained beta: 3.8306140899658203
Constrained gamma: 51.83290481567383
(1, 38)
(1, 0)
