data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.645943245322227 17.859030659910193 49.2037788393498
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 222.4080047607422 500
training loss = 72.7610092163086 1000
training loss = 72.70866394042969 1500
training loss = 72.60449981689453 2000
training loss = 72.21515655517578 2500
training loss = 4.812892913818359 3000
training loss = 3.426318883895874 3500
training loss = 3.0740630626678467 4000
training loss = 3.00087308883667 4500
training loss = 2.9871339797973633 5000
training loss = 2.9833476543426514 5500
training loss = 2.9808714389801025 6000
training loss = 2.9786200523376465 6500
training loss = 2.976491928100586 7000
training loss = 2.9751617908477783 7500
training loss = 3.020136833190918 8000
training loss = 2.82692289352417 8500
training loss = 2.723264217376709 9000
training loss = 2.5573275089263916 9500
training loss = 2.4150643348693848 10000
training loss = 2.3835983276367188 10500
training loss = 2.350135564804077 11000
training loss = 2.320211410522461 11500
training loss = 2.295647382736206 12000
training loss = 2.276428699493408 12500
training loss = 2.2615413665771484 13000
training loss = 2.2498724460601807 13500
training loss = 2.2406399250030518 14000
training loss = 2.2331767082214355 14500
training loss = 2.227116584777832 15000
training loss = 2.221708297729492 15500
training loss = 2.2174019813537598 16000
training loss = 2.2135844230651855 16500
training loss = 2.2102596759796143 17000
training loss = 2.207252025604248 17500
training loss = 2.204498529434204 18000
training loss = 2.6455447673797607 18500
training loss = 2.200655937194824 19000
training loss = 3.5848851203918457 19500
training loss = 2.1954803466796875 20000
training loss = 2.6544077396392822 20500
training loss = 3.2576498985290527 21000
training loss = 2.2263903617858887 21500
training loss = 2.229210376739502 22000
training loss = 2.187023878097534 22500
training loss = 2.186706781387329 23000
training loss = 2.202685832977295 23500
training loss = 2.1826586723327637 24000
training loss = 2.181108236312866 24500
training loss = 2.1798014640808105 25000
training loss = 2.178511142730713 25500
training loss = 2.177335500717163 26000
training loss = 2.176192283630371 26500
training loss = 2.1749813556671143 27000
training loss = 2.173858880996704 27500
training loss = 2.1727864742279053 28000
training loss = 2.1717002391815186 28500
training loss = 2.1707024574279785 29000
training loss = 2.1697287559509277 29500
training loss = 2.168665885925293 30000
training loss = 2.1676859855651855 30500
training loss = 2.1716103553771973 31000
training loss = 2.2517881393432617 31500
training loss = 2.2018396854400635 32000
training loss = 2.163952112197876 32500
training loss = 2.163034677505493 33000
training loss = 2.1627955436706543 33500
training loss = 2.1615731716156006 34000
training loss = 2.1604344844818115 34500
training loss = 2.1595911979675293 35000
(1, 38)
(1, 0)
