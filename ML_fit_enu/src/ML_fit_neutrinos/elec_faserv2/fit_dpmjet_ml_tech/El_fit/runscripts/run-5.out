data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.2116768816663774 12.151421045117603 63.45431781985737
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 79.2817611694336 500
training loss = 72.38814544677734 1000
training loss = 71.26803588867188 1500
training loss = 68.19053649902344 2000
training loss = 4.709314346313477 2500
training loss = 3.7746450901031494 3000
training loss = 3.3939883708953857 3500
training loss = 3.3228559494018555 4000
training loss = 3.289928674697876 4500
training loss = 3.2691962718963623 5000
training loss = 3.2541611194610596 5500
training loss = 7.808043003082275 6000
training loss = 7.0313825607299805 6500
training loss = 3.2577219009399414 7000
training loss = 3.2054152488708496 7500
training loss = 3.1956019401550293 8000
training loss = 3.1848807334899902 8500
training loss = 3.1758055686950684 9000
training loss = 3.3494741916656494 9500
training loss = 3.1601738929748535 10000
training loss = 3.148966073989868 10500
training loss = 3.1405105590820312 11000
training loss = 3.131688356399536 11500
training loss = 3.1234145164489746 12000
training loss = 3.114985466003418 12500
training loss = 3.1065938472747803 13000
training loss = 3.0983705520629883 13500
training loss = 7.109778881072998 14000
training loss = 3.537224531173706 14500
training loss = 3.0745108127593994 15000
training loss = 3.065232038497925 15500
training loss = 3.056309700012207 16000
training loss = 3.0477418899536133 16500
training loss = 3.042163372039795 17000
training loss = 3.030829668045044 17500
training loss = 3.0217254161834717 18000
training loss = 3.013087749481201 18500
training loss = 3.004493236541748 19000
training loss = 2.9959661960601807 19500
training loss = 2.9874560832977295 20000
training loss = 2.979180097579956 20500
training loss = 2.971045732498169 21000
training loss = 3.2084853649139404 21500
training loss = 2.9756124019622803 22000
training loss = 3.6458535194396973 22500
training loss = 2.9409143924713135 23000
training loss = 2.9339075088500977 23500
training loss = 2.927143096923828 24000
training loss = 2.9215292930603027 24500
training loss = 3.759645938873291 25000
training loss = 2.9092161655426025 25500
training loss = 2.90522837638855 26000
training loss = 2.8984262943267822 26500
training loss = 2.8934550285339355 27000
training loss = 2.889000654220581 27500
training loss = 2.8854970932006836 28000
training loss = 2.880256175994873 28500
training loss = 2.8763468265533447 29000
training loss = 2.8727705478668213 29500
training loss = 2.8694212436676025 30000
training loss = 2.866361141204834 30500
training loss = 2.864257335662842 31000
training loss = 2.8608429431915283 31500
training loss = 2.8596572875976562 32000
training loss = 2.855962038040161 32500
training loss = 2.853518486022949 33000
training loss = 2.8514187335968018 33500
training loss = 2.849409341812134 34000
training loss = 2.847585916519165 34500
training loss = 2.845773220062256 35000
(1, 38)
(1, 0)
