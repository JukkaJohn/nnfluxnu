data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.049810915092043 4.663310923995887 23.463179579587333
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 83.87149047851562 500
training loss = 77.69378662109375 1000
training loss = 77.68726348876953 1500
training loss = 77.67876434326172 2000
training loss = 77.66636657714844 2500
training loss = 77.64788055419922 3000
training loss = 77.61734771728516 3500
training loss = 77.55606079101562 4000
training loss = 77.37117767333984 4500
training loss = 75.60023498535156 5000
training loss = 3.060439348220825 5500
training loss = 2.924692392349243 6000
training loss = 2.9135243892669678 6500
training loss = 3.6374101638793945 7000
training loss = 2.896904468536377 7500
training loss = 2.8935630321502686 8000
training loss = 2.8902623653411865 8500
training loss = 2.8872969150543213 9000
training loss = 5.074710845947266 9500
training loss = 4.505090713500977 10000
training loss = 2.879899501800537 10500
training loss = 2.876796245574951 11000
training loss = 2.8744311332702637 11500
training loss = 2.8721327781677246 12000
training loss = 2.8698954582214355 12500
training loss = 2.8705685138702393 13000
training loss = 2.9108967781066895 13500
training loss = 2.906353712081909 14000
training loss = 3.5184712409973145 14500
training loss = 2.891543388366699 15000
training loss = 2.874530792236328 15500
training loss = 2.8563482761383057 16000
training loss = 2.854595899581909 16500
training loss = 2.8529458045959473 17000
training loss = 2.8512961864471436 17500
training loss = 2.8622677326202393 18000
training loss = 2.850715398788452 18500
training loss = 2.846952199935913 19000
training loss = 2.845571756362915 19500
training loss = 2.8442771434783936 20000
training loss = 2.843048334121704 20500
training loss = 2.8448493480682373 21000
training loss = 2.942718505859375 21500
training loss = 2.840797185897827 22000
training loss = 2.945981502532959 22500
training loss = 2.8375701904296875 23000
training loss = 2.8376924991607666 23500
training loss = 2.930157423019409 24000
training loss = 2.8587887287139893 24500
training loss = 2.913896322250366 25000
training loss = 2.8844656944274902 25500
training loss = 2.8368794918060303 26000
training loss = 2.8326914310455322 26500
training loss = 2.830796003341675 27000
training loss = 2.8300840854644775 27500
training loss = 2.829420328140259 28000
training loss = 2.828765392303467 28500
training loss = 2.828153371810913 29000
training loss = 2.8275561332702637 29500
training loss = 2.8278491497039795 30000
training loss = 2.827897548675537 30500
training loss = 2.8263981342315674 31000
training loss = 3.259355306625366 31500
training loss = 5.93745756149292 32000
training loss = 2.8729636669158936 32500
training loss = 2.8239896297454834 33000
training loss = 4.071955680847168 33500
training loss = 6.314187049865723 34000
training loss = 2.8367555141448975 34500
training loss = 2.822805404663086 35000
(1, 38)
(1, 0)
