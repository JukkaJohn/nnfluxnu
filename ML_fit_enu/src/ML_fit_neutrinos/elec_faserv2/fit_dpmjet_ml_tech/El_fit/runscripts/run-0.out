data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.4920705893218134 12.165378990513636 61.64573067729424
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 79.37815856933594 500
training loss = 72.45233154296875 1000
training loss = 71.58871459960938 1500
training loss = 70.26165771484375 2000
training loss = 67.31864929199219 2500
training loss = 16.3399658203125 3000
training loss = 4.069674968719482 3500
training loss = 3.2733874320983887 4000
training loss = 3.1123533248901367 4500
training loss = 3.0771236419677734 5000
training loss = 3.061138391494751 5500
training loss = 3.111762523651123 6000
training loss = 3.291668653488159 6500
training loss = 3.0715670585632324 7000
training loss = 3.066735029220581 7500
training loss = 3.0292856693267822 8000
training loss = 3.105293035507202 8500
training loss = 3.7842018604278564 9000
training loss = 4.203377723693848 9500
training loss = 3.0177745819091797 10000
training loss = 3.0184319019317627 10500
training loss = 3.0819408893585205 11000
training loss = 3.0134811401367188 11500
training loss = 3.01039719581604 12000
training loss = 3.0090484619140625 12500
training loss = 3.007293224334717 13000
training loss = 3.0060648918151855 13500
training loss = 3.0046370029449463 14000
training loss = 3.0033984184265137 14500
training loss = 3.0022218227386475 15000
training loss = 3.0010986328125 15500
training loss = 3.0001204013824463 16000
training loss = 2.999249219894409 16500
training loss = 2.9981558322906494 17000
training loss = 3.000808000564575 17500
training loss = 2.9969663619995117 18000
training loss = 2.9964678287506104 18500
training loss = 2.99479603767395 19000
training loss = 2.9941306114196777 19500
training loss = 2.9928269386291504 20000
training loss = 2.9921183586120605 20500
training loss = 2.9913694858551025 21000
training loss = 2.990682601928711 21500
training loss = 3.4648571014404297 22000
training loss = 3.0220601558685303 22500
training loss = 2.990554094314575 23000
training loss = 2.9892325401306152 23500
training loss = 3.022059440612793 24000
training loss = 3.0071096420288086 24500
training loss = 2.987459421157837 25000
training loss = 3.00699520111084 25500
training loss = 2.9929256439208984 26000
training loss = 3.041059970855713 26500
training loss = 4.2850117683410645 27000
training loss = 2.984243392944336 27500
training loss = 2.9902122020721436 28000
training loss = 7.775363445281982 28500
training loss = 4.122580051422119 29000
training loss = 2.996856689453125 29500
training loss = 2.981952428817749 30000
training loss = 2.981588125228882 30500
training loss = 2.981173515319824 31000
training loss = 2.9808173179626465 31500
training loss = 2.9804697036743164 32000
training loss = 2.9801595211029053 32500
training loss = 2.979748487472534 33000
training loss = 2.9793660640716553 33500
training loss = 2.979020595550537 34000
training loss = 2.9786765575408936 34500
training loss = 2.9783377647399902 35000
(1, 38)
(1, 0)
