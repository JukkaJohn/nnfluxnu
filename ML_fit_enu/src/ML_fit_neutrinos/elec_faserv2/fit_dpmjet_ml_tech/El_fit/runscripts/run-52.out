data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.0211835053478704 9.920616168890328 87.67166810766257
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 73.31837463378906 500
training loss = 72.81036376953125 1000
training loss = 72.1723403930664 1500
training loss = 70.75909423828125 2000
training loss = 34.655208587646484 2500
training loss = 4.893304824829102 3000
training loss = 3.723595142364502 3500
training loss = 3.6157355308532715 4000
training loss = 3.5655083656311035 4500
training loss = 3.633885383605957 5000
training loss = 3.5074892044067383 5500
training loss = 3.481444835662842 6000
training loss = 3.4563381671905518 6500
training loss = 3.4296934604644775 7000
training loss = 3.4012880325317383 7500
training loss = 3.3718326091766357 8000
training loss = 3.341670513153076 8500
training loss = 3.3111767768859863 9000
training loss = 3.2810301780700684 9500
training loss = 3.2515785694122314 10000
training loss = 3.222618818283081 10500
training loss = 3.194493293762207 11000
training loss = 3.166525363922119 11500
training loss = 3.139040231704712 12000
training loss = 3.119689464569092 12500
training loss = 3.0900518894195557 13000
training loss = 3.05971097946167 13500
training loss = 3.035216808319092 14000
training loss = 3.010993480682373 14500
training loss = 2.986701250076294 15000
training loss = 2.963963508605957 15500
training loss = 2.9424028396606445 16000
training loss = 2.921783924102783 16500
training loss = 2.90877103805542 17000
training loss = 2.9422190189361572 17500
training loss = 2.9077351093292236 18000
training loss = 2.918950319290161 18500
training loss = 2.8351638317108154 19000
training loss = 2.820737600326538 19500
training loss = 2.812437057495117 20000
training loss = 2.8239493370056152 20500
training loss = 2.806828737258911 21000
training loss = 2.8668501377105713 21500
training loss = 2.7628347873687744 22000
training loss = 2.750474214553833 22500
training loss = 2.7407684326171875 23000
training loss = 2.7316701412200928 23500
training loss = 2.7231709957122803 24000
training loss = 2.714813709259033 24500
training loss = 2.7077760696411133 25000
training loss = 2.699097156524658 25500
training loss = 2.691420793533325 26000
training loss = 2.684279203414917 26500
training loss = 2.677490234375 27000
training loss = 2.6702568531036377 27500
training loss = 2.6639866828918457 28000
training loss = 2.6639280319213867 28500
training loss = 2.650604486465454 29000
training loss = 2.6440653800964355 29500
training loss = 2.6377596855163574 30000
training loss = 2.632380962371826 30500
training loss = 2.6258528232574463 31000
training loss = 2.6201231479644775 31500
training loss = 2.6138980388641357 32000
training loss = 2.6080002784729004 32500
training loss = 2.6021945476531982 33000
training loss = 2.596515417098999 33500
training loss = 2.590956926345825 34000
training loss = 2.5853917598724365 34500
training loss = 3.2109713554382324 35000
(1, 38)
(1, 0)
