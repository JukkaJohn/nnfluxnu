data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.1954271760708424 2.176582918305967 32.930269197064774
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 112.58280181884766 500
training loss = 77.76224517822266 1000
training loss = 77.67041015625 1500
training loss = 77.66410064697266 2000
training loss = 77.65574645996094 2500
training loss = 77.6448974609375 3000
training loss = 77.63078308105469 3500
training loss = 77.61253356933594 4000
training loss = 77.58890533447266 4500
training loss = 77.55811309814453 5000
training loss = 77.5176010131836 5500
training loss = 77.46299743652344 6000
training loss = 77.38773345947266 6500
training loss = 77.27777862548828 7000
training loss = 77.09207916259766 7500
training loss = 4.888068199157715 8000
training loss = 4.270290851593018 8500
training loss = 3.9185895919799805 9000
training loss = 3.7453196048736572 9500
training loss = 7.1031928062438965 10000
training loss = 3.605410099029541 10500
training loss = 3.573108196258545 11000
training loss = 3.543461561203003 11500
training loss = 3.5223171710968018 12000
training loss = 3.5043840408325195 12500
training loss = 3.4834911823272705 13000
training loss = 3.4903724193573 13500
training loss = 3.4420440196990967 14000
training loss = 4.00748348236084 14500
training loss = 3.6134562492370605 15000
training loss = 4.084810256958008 15500
training loss = 3.3316144943237305 16000
training loss = 3.3481037616729736 16500
training loss = 3.3092429637908936 17000
training loss = 3.225125551223755 17500
training loss = 3.153228521347046 18000
training loss = 3.112015724182129 18500
training loss = 3.0249857902526855 19000
training loss = 2.6128740310668945 19500
training loss = 2.3943867683410645 20000
training loss = 2.28547739982605 20500
training loss = 2.2221434116363525 21000
training loss = 2.1886422634124756 21500
training loss = 2.1660258769989014 22000
training loss = 2.154045581817627 22500
training loss = 2.146531343460083 23000
training loss = 2.141087532043457 23500
training loss = 2.1368517875671387 24000
training loss = 2.1328043937683105 24500
training loss = 2.128567695617676 25000
training loss = 2.12451171875 25500
training loss = 3.6306192874908447 26000
training loss = 2.122122049331665 26500
training loss = 4.160935401916504 27000
training loss = 2.108955144882202 27500
training loss = 2.105233669281006 28000
training loss = 2.1014585494995117 28500
training loss = 2.0975987911224365 29000
training loss = 2.0938234329223633 29500
training loss = 2.089984178543091 30000
training loss = 2.0863916873931885 30500
training loss = 2.083116292953491 31000
training loss = 2.5899605751037598 31500
training loss = 4.456425189971924 32000
training loss = 2.1788299083709717 32500
training loss = 2.0673136711120605 33000
training loss = 2.0633561611175537 33500
training loss = 2.0594146251678467 34000
training loss = 2.0552287101745605 34500
training loss = 2.051095724105835 35000
(1, 38)
(1, 0)
