data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.7583575468870438 15.386173719940679 91.58759810628135
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 113.1633071899414 500
training loss = 73.21552276611328 1000
training loss = 71.74407958984375 1500
training loss = 69.51031494140625 2000
training loss = 51.48564147949219 2500
training loss = 4.4506425857543945 3000
training loss = 3.255682945251465 3500
training loss = 2.9720237255096436 4000
training loss = 2.8714652061462402 4500
training loss = 2.8238284587860107 5000
training loss = 2.7983157634735107 5500
training loss = 2.781740188598633 6000
training loss = 2.769920587539673 6500
training loss = 2.7606499195098877 7000
training loss = 2.7530131340026855 7500
training loss = 2.902411937713623 8000
training loss = 3.68672513961792 8500
training loss = 2.7319953441619873 9000
training loss = 2.723344326019287 9500
training loss = 2.715630054473877 10000
training loss = 2.708225965499878 10500
training loss = 2.700995683670044 11000
training loss = 2.6939072608947754 11500
training loss = 2.694230794906616 12000
training loss = 2.680915355682373 12500
training loss = 2.71836256980896 13000
training loss = 2.676103353500366 13500
training loss = 2.658536911010742 14000
training loss = 2.7554094791412354 14500
training loss = 3.0610597133636475 15000
training loss = 3.1735687255859375 15500
training loss = 2.6226065158843994 16000
training loss = 2.616060733795166 16500
training loss = 2.606816053390503 17000
training loss = 2.658297061920166 17500
training loss = 2.6234893798828125 18000
training loss = 2.5794460773468018 18500
training loss = 2.5679547786712646 19000
training loss = 2.5573694705963135 19500
training loss = 2.546170949935913 20000
training loss = 2.5342702865600586 20500
training loss = 2.5218701362609863 21000
training loss = 2.508831739425659 21500
training loss = 2.4949421882629395 22000
training loss = 2.4798178672790527 22500
training loss = 2.463941812515259 23000
training loss = 2.4488108158111572 23500
training loss = 2.587186574935913 24000
training loss = 6.946775913238525 24500
training loss = 2.395594835281372 25000
training loss = 2.369885206222534 25500
training loss = 2.3481030464172363 26000
training loss = 2.3261594772338867 26500
training loss = 2.303410053253174 27000
training loss = 2.2803897857666016 27500
training loss = 2.2573812007904053 28000
training loss = 2.234473705291748 28500
training loss = 2.2475643157958984 29000
training loss = 2.192131519317627 29500
training loss = 2.1730611324310303 30000
training loss = 2.1552653312683105 30500
training loss = 2.139437675476074 31000
training loss = 2.1254894733428955 31500
training loss = 2.1131696701049805 32000
training loss = 2.1029109954833984 32500
training loss = 2.093679904937744 33000
training loss = 2.0855019092559814 33500
training loss = 2.0784387588500977 34000
training loss = 2.0967464447021484 34500
training loss = 2.2506351470947266 35000
(1, 38)
(1, 0)
