data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.695738160314998 19.063062310261603 13.801622134213087
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 136.3777313232422 500
training loss = 80.900634765625 1000
training loss = 80.7533187866211 1500
training loss = 80.62223815917969 2000
training loss = 80.47667694091797 2500
training loss = 80.31903839111328 3000
training loss = 80.14469146728516 3500
training loss = 79.92890167236328 4000
training loss = 79.55792236328125 4500
training loss = 78.08670806884766 5000
training loss = 4.935303688049316 5500
training loss = 3.947664737701416 6000
training loss = 3.7390811443328857 6500
training loss = 3.6820790767669678 7000
training loss = 3.669525623321533 7500
training loss = 3.6658430099487305 8000
training loss = 3.663935661315918 8500
training loss = 3.208531379699707 9000
training loss = 3.0292975902557373 9500
training loss = 2.887753963470459 10000
training loss = 2.724536418914795 10500
training loss = 2.55552077293396 11000
training loss = 2.406407117843628 11500
training loss = 2.28486967086792 12000
training loss = 3.2267115116119385 12500
training loss = 3.035505771636963 13000
training loss = 3.1411585807800293 13500
training loss = 5.458268165588379 14000
training loss = 2.1745803356170654 14500
training loss = 2.017258882522583 15000
training loss = 1.9482876062393188 15500
training loss = 1.9348176717758179 16000
training loss = 1.9247251749038696 16500
training loss = 1.9168283939361572 17000
training loss = 1.9109346866607666 17500
training loss = 2.487607002258301 18000
training loss = 2.0479183197021484 18500
training loss = 4.53805685043335 19000
training loss = 1.8952876329421997 19500
training loss = 1.8929575681686401 20000
training loss = 1.8909547328948975 20500
training loss = 1.8898948431015015 21000
training loss = 1.8877053260803223 21500
training loss = 1.8928899765014648 22000
training loss = 3.014082670211792 22500
training loss = 1.8916678428649902 23000
training loss = 1.8892405033111572 23500
training loss = 1.8820090293884277 24000
training loss = 1.8808056116104126 24500
training loss = 1.879904866218567 25000
training loss = 1.8790134191513062 25500
training loss = 1.878173828125 26000
training loss = 1.877358317375183 26500
training loss = 1.8766114711761475 27000
training loss = 2.0907866954803467 27500
training loss = 1.879209041595459 28000
training loss = 2.274763345718384 28500
training loss = 1.8742820024490356 29000
training loss = 1.8728998899459839 29500
training loss = 1.8719652891159058 30000
training loss = 1.8712345361709595 30500
training loss = 1.8711670637130737 31000
training loss = 1.86981201171875 31500
training loss = 2.7871851921081543 32000
training loss = 2.2345924377441406 32500
training loss = 1.8682045936584473 33000
training loss = 2.291325569152832 33500
training loss = 2.338078022003174 34000
training loss = 1.9441736936569214 34500
training loss = 1.8649529218673706 35000
reduced chi^2 level 2 = 1.8649461269378662
Constrained alpha: 2.309150457382202
Constrained beta: 4.005215644836426
Constrained gamma: 17.725645065307617
(1, 38)
(1, 0)
