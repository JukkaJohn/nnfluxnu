data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.513705329424672 8.558070299307513 34.415324490360874
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 80.05049133300781 500
training loss = 78.10283660888672 1000
training loss = 78.07434844970703 1500
training loss = 78.04122924804688 2000
training loss = 78.00524139404297 2500
training loss = 77.96802520751953 3000
training loss = 77.93089294433594 3500
training loss = 77.89483642578125 4000
training loss = 77.98054504394531 4500
training loss = 77.82628631591797 5000
training loss = 77.78456115722656 5500
training loss = 77.73290252685547 6000
training loss = 77.65387725830078 6500
training loss = 77.5029296875 7000
training loss = 76.84049987792969 7500
training loss = 3.950157642364502 8000
training loss = 3.7895097732543945 8500
training loss = 3.7542195320129395 9000
training loss = 4.076150417327881 9500
training loss = 3.743664503097534 10000
training loss = 3.741757869720459 10500
training loss = 3.7418711185455322 11000
training loss = 3.752668619155884 11500
training loss = 3.7415313720703125 12000
training loss = 3.7408857345581055 12500
training loss = 3.7408840656280518 13000
training loss = 3.8850743770599365 13500
training loss = 4.015252113342285 14000
training loss = 3.415647506713867 14500
training loss = 3.33084774017334 15000
training loss = 2.8416731357574463 15500
training loss = 2.69486403465271 16000
training loss = 2.585453748703003 16500
training loss = 2.5103940963745117 17000
training loss = 2.454824686050415 17500
training loss = 2.414095401763916 18000
training loss = 2.3829188346862793 18500
training loss = 2.3743650913238525 19000
training loss = 2.3401010036468506 19500
training loss = 5.906279563903809 20000
training loss = 2.313385248184204 20500
training loss = 2.3031556606292725 21000
training loss = 2.294687271118164 21500
training loss = 2.383272171020508 22000
training loss = 2.2853498458862305 22500
training loss = 5.360644817352295 23000
training loss = 2.2726447582244873 23500
training loss = 2.4338676929473877 24000
training loss = 6.812557697296143 24500
training loss = 3.0857818126678467 25000
training loss = 2.255828380584717 25500
training loss = 2.2519092559814453 26000
training loss = 2.2492456436157227 26500
training loss = 2.247025489807129 27000
training loss = 2.243945360183716 27500
training loss = 2.2417151927948 28000
training loss = 2.2397608757019043 28500
training loss = 2.237921714782715 29000
training loss = 3.240640878677368 29500
training loss = 2.2379860877990723 30000
training loss = 2.418753147125244 30500
training loss = 2.258058547973633 31000
training loss = 2.2315917015075684 31500
training loss = 2.229679584503174 32000
training loss = 2.228663921356201 32500
training loss = 2.2349741458892822 33000
training loss = 2.3439066410064697 33500
training loss = 2.2271625995635986 34000
training loss = 2.2253262996673584 34500
training loss = 2.2274556159973145 35000
(1, 38)
(1, 0)
