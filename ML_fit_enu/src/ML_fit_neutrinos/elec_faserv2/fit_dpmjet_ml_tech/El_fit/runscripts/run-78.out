data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.18217665310242936 6.670283252905733 95.6800470402554
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 76.55741119384766 500
training loss = 75.19927215576172 1000
training loss = 74.4680404663086 1500
training loss = 73.21231842041016 2000
training loss = 70.6848373413086 2500
training loss = 63.17973327636719 3000
training loss = 7.3169264793396 3500
training loss = 5.485533237457275 4000
training loss = 4.56044864654541 4500
training loss = 4.147541522979736 5000
training loss = 3.9914543628692627 5500
training loss = 3.9437882900238037 6000
training loss = 3.928173780441284 6500
training loss = 8.610515594482422 7000
training loss = 3.9169840812683105 7500
training loss = 3.9148242473602295 8000
training loss = 3.913116455078125 8500
training loss = 3.9116156101226807 9000
training loss = 3.9103033542633057 9500
training loss = 3.9090487957000732 10000
training loss = 3.9078481197357178 10500
training loss = 3.9066274166107178 11000
training loss = 3.905416488647461 11500
training loss = 3.9041924476623535 12000
training loss = 3.991368055343628 12500
training loss = 4.475806713104248 13000
training loss = 9.035658836364746 13500
training loss = 3.8988044261932373 14000
training loss = 3.8973159790039062 14500
training loss = 3.8958117961883545 15000
training loss = 8.84846019744873 15500
training loss = 5.469586372375488 16000
training loss = 4.128715515136719 16500
training loss = 3.922116756439209 17000
training loss = 3.886916399002075 17500
training loss = 5.175774574279785 18000
training loss = 3.8826651573181152 18500
training loss = 3.8802907466888428 19000
training loss = 4.311602592468262 19500
training loss = 3.893989086151123 20000
training loss = 3.8796756267547607 20500
training loss = 4.213583946228027 21000
training loss = 3.867414712905884 21500
training loss = 3.861809253692627 22000
training loss = 3.8585293292999268 22500
training loss = 3.853224992752075 23000
training loss = 3.8482248783111572 23500
training loss = 3.8427069187164307 24000
training loss = 3.836575508117676 24500
training loss = 3.8297576904296875 25000
training loss = 3.8220643997192383 25500
training loss = 3.813507080078125 26000
training loss = 3.8039438724517822 26500
training loss = 3.793266773223877 27000
training loss = 3.781334638595581 27500
training loss = 3.7681736946105957 28000
training loss = 3.753633975982666 28500
training loss = 3.7377254962921143 29000
training loss = 3.720534563064575 29500
training loss = 3.9867730140686035 30000
training loss = 3.682802438735962 30500
training loss = 3.6626763343811035 31000
training loss = 3.641950845718384 31500
training loss = 3.620958089828491 32000
training loss = 3.599900007247925 32500
training loss = 3.5789835453033447 33000
training loss = 3.5583884716033936 33500
training loss = 3.538142204284668 34000
training loss = 3.5182712078094482 34500
training loss = 3.498767614364624 35000
(1, 38)
(1, 0)
