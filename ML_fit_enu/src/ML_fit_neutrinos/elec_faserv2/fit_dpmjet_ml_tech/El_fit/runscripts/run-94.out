data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.9943852798990687 14.529607814512413 53.5525605166366
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 100.4309310913086 500
training loss = 70.27777099609375 1000
training loss = 69.6325454711914 1500
training loss = 68.8394546508789 2000
training loss = 67.82508850097656 2500
training loss = 66.3427734375 3000
training loss = 63.08037567138672 3500
training loss = 15.159379005432129 4000
training loss = 6.286886215209961 4500
training loss = 3.9528231620788574 5000
training loss = 3.458348274230957 5500
training loss = 3.3800947666168213 6000
training loss = 3.337263345718384 6500
training loss = 3.326979160308838 7000
training loss = 3.3238682746887207 7500
training loss = 3.3216423988342285 8000
training loss = 3.3198049068450928 8500
training loss = 3.318204402923584 9000
training loss = 3.3189337253570557 9500
training loss = 3.3158445358276367 10000
training loss = 3.314504861831665 10500
training loss = 3.313474178314209 11000
training loss = 3.1122255325317383 11500
training loss = 3.0104310512542725 12000
training loss = 2.9913933277130127 12500
training loss = 2.9243831634521484 13000
training loss = 3.735459566116333 13500
training loss = 2.9706945419311523 14000
training loss = 2.819017171859741 14500
training loss = 2.780947685241699 15000
training loss = 2.742121458053589 15500
training loss = 2.7044219970703125 16000
training loss = 2.669903039932251 16500
training loss = 2.639174699783325 17000
training loss = 2.61250376701355 17500
training loss = 2.5886647701263428 18000
training loss = 2.5675387382507324 18500
training loss = 2.5489232540130615 19000
training loss = 2.5323009490966797 19500
training loss = 2.537564277648926 20000
training loss = 2.504720687866211 20500
training loss = 4.337703704833984 21000
training loss = 2.496973752975464 21500
training loss = 2.4734885692596436 22000
training loss = 2.4631810188293457 22500
training loss = 2.454993724822998 23000
training loss = 2.4475033283233643 23500
training loss = 2.6095786094665527 24000
training loss = 2.46390700340271 24500
training loss = 2.5450198650360107 25000
training loss = 3.5418813228607178 25500
training loss = 2.4318833351135254 26000
training loss = 2.569406270980835 26500
training loss = 2.4376282691955566 27000
training loss = 2.415856122970581 27500
training loss = 2.4185760021209717 28000
training loss = 3.2762069702148438 28500
training loss = 2.394620656967163 29000
training loss = 2.3922910690307617 29500
training loss = 2.389737129211426 30000
training loss = 3.4654324054718018 30500
training loss = 2.4742817878723145 31000
training loss = 2.3748831748962402 31500
training loss = 2.3898119926452637 32000
training loss = 2.3680520057678223 32500
training loss = 2.6106231212615967 33000
training loss = 2.5568795204162598 33500
training loss = 2.35815167427063 34000
training loss = 2.3551080226898193 34500
training loss = 2.352053642272949 35000
(1, 38)
(1, 0)
