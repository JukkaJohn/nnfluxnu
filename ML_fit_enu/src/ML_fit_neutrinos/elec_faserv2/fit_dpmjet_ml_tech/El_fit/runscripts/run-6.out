data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.45591652998786036 4.292376868724017 82.05624895806493
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 79.14845275878906 500
training loss = 71.37561798095703 1000
training loss = 71.31088256835938 1500
training loss = 71.28987884521484 2000
training loss = 71.25494384765625 2500
training loss = 71.19329071044922 3000
training loss = 71.07556915283203 3500
training loss = 70.82815551757812 4000
training loss = 70.2413558959961 4500
training loss = 68.5654525756836 5000
training loss = 45.96101379394531 5500
training loss = 3.697397470474243 6000
training loss = 3.092219352722168 6500
training loss = 2.991626739501953 7000
training loss = 2.9724671840667725 7500
training loss = 3.006784439086914 8000
training loss = 2.9623119831085205 8500
training loss = 2.953916072845459 9000
training loss = 2.946779727935791 9500
training loss = 2.9532034397125244 10000
training loss = 2.936250925064087 10500
training loss = 2.930792808532715 11000
training loss = 2.9262020587921143 11500
training loss = 2.921818494796753 12000
training loss = 2.9180426597595215 12500
training loss = 2.95687198638916 13000
training loss = 3.26267409324646 13500
training loss = 2.905439853668213 14000
training loss = 2.9015111923217773 14500
training loss = 2.8975863456726074 15000
training loss = 2.893778085708618 15500
training loss = 2.8898465633392334 16000
training loss = 2.8884005546569824 16500
training loss = 2.9287426471710205 17000
training loss = 2.8779642581939697 17500
training loss = 2.8737244606018066 18000
training loss = 2.8694674968719482 18500
training loss = 2.9027082920074463 19000
training loss = 2.8651039600372314 19500
training loss = 2.8583624362945557 20000
training loss = 2.8516957759857178 20500
training loss = 2.8550362586975098 21000
training loss = 2.856721878051758 21500
training loss = 2.837588310241699 22000
training loss = 2.8307206630706787 22500
training loss = 2.824460744857788 23000
training loss = 2.817736864089966 23500
training loss = 2.8103363513946533 24000
training loss = 2.8020808696746826 24500
training loss = 2.7927486896514893 25000
training loss = 2.782036304473877 25500
training loss = 2.7696948051452637 26000
training loss = 2.75518798828125 26500
training loss = 6.735164642333984 27000
training loss = 3.0204660892486572 27500
training loss = 3.665708065032959 28000
training loss = 2.686014413833618 28500
training loss = 2.604142189025879 29000
training loss = 2.5048232078552246 29500
training loss = 2.384460926055908 30000
training loss = 2.231116533279419 30500
training loss = 2.0619056224823 31000
training loss = 1.9017263650894165 31500
training loss = 1.7714711427688599 32000
training loss = 1.6766606569290161 32500
training loss = 1.6126588582992554 33000
training loss = 1.675471544265747 33500
training loss = 1.54656982421875 34000
training loss = 1.5295337438583374 34500
training loss = 1.5165742635726929 35000
reduced chi^2 level 2 = 1.5169976949691772
Constrained alpha: 2.2077908515930176
Constrained beta: 4.276041030883789
Constrained gamma: 49.712100982666016
(1, 38)
(1, 0)
