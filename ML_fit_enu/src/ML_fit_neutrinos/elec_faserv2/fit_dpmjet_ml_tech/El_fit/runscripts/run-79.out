data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.7881983704525669 16.10042822440003 2.057320393195061
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 92.86003112792969 500
training loss = 79.40499114990234 1000
training loss = 79.28605651855469 1500
training loss = 79.17565155029297 2000
training loss = 79.056640625 2500
training loss = 78.93702697753906 3000
training loss = 78.82427978515625 3500
training loss = 78.72376251220703 4000
training loss = 78.63553619384766 4500
training loss = 78.54698944091797 5000
training loss = 78.41220092773438 5500
training loss = 78.05192565917969 6000
training loss = 76.50079345703125 6500
training loss = 40.68972396850586 7000
training loss = 7.26773738861084 7500
training loss = 5.85687255859375 8000
training loss = 5.261564254760742 8500
training loss = 5.045309066772461 9000
training loss = 4.959787368774414 9500
training loss = 4.982151031494141 10000
training loss = 4.910538673400879 10500
training loss = 4.912413597106934 11000
training loss = 4.978304386138916 11500
training loss = 5.191511154174805 12000
training loss = 4.9220781326293945 12500
training loss = 5.0018696784973145 13000
training loss = 4.895206451416016 13500
training loss = 4.895195484161377 14000
training loss = 4.893126487731934 14500
training loss = 4.892154693603516 15000
training loss = 4.891242027282715 15500
training loss = 4.890282154083252 16000
training loss = 4.889309883117676 16500
training loss = 4.888322353363037 17000
training loss = 4.8873395919799805 17500
training loss = 4.8864264488220215 18000
training loss = 4.885450839996338 18500
training loss = 4.886905193328857 19000
training loss = 4.890305042266846 19500
training loss = 4.881994247436523 20000
training loss = 5.305346965789795 20500
training loss = 4.914103984832764 21000
training loss = 4.8785400390625 21500
training loss = 4.904572486877441 22000
training loss = 4.876302719116211 22500
training loss = 4.874783039093018 23000
training loss = 4.873507976531982 23500
training loss = 4.872221946716309 24000
training loss = 4.8709235191345215 24500
training loss = 4.869592189788818 25000
training loss = 4.868269920349121 25500
training loss = 4.866950035095215 26000
training loss = 4.865610122680664 26500
training loss = 4.864809513092041 27000
training loss = 4.862985610961914 27500
training loss = 4.861850738525391 28000
training loss = 4.860357284545898 28500
training loss = 4.859078884124756 29000
training loss = 4.860836029052734 29500
training loss = 4.856563568115234 30000
training loss = 4.855340480804443 30500
training loss = 4.857090473175049 31000
training loss = 4.862209320068359 31500
training loss = 4.854084491729736 32000
training loss = 4.883777618408203 32500
training loss = 4.850207328796387 33000
training loss = 4.848904609680176 33500
training loss = 4.847619533538818 34000
training loss = 4.872264862060547 34500
training loss = 4.845965385437012 35000
(1, 38)
(1, 0)
