data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
1.3006030283154506 6.20689381163722 61.54051100757102
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 89.353515625 500
training loss = 78.51847839355469 1000
training loss = 78.42735290527344 1500
training loss = 78.30780029296875 2000
training loss = 78.13230895996094 2500
training loss = 77.85931396484375 3000
training loss = 77.359619140625 3500
training loss = 75.65018463134766 4000
training loss = 5.431525230407715 4500
training loss = 4.443897247314453 5000
training loss = 3.9990508556365967 5500
training loss = 4.029428005218506 6000
training loss = 3.714243173599243 6500
training loss = 3.536283016204834 7000
training loss = 3.3334972858428955 7500
training loss = 3.13179349899292 8000
training loss = 2.955457925796509 8500
training loss = 2.8157858848571777 9000
training loss = 2.711580753326416 9500
training loss = 2.638185739517212 10000
training loss = 2.620887041091919 10500
training loss = 2.5814383029937744 11000
training loss = 2.533473014831543 11500
training loss = 2.520599126815796 12000
training loss = 2.502092123031616 12500
training loss = 2.4930639266967773 13000
training loss = 2.6027047634124756 13500
training loss = 2.4829981327056885 14000
training loss = 2.483444929122925 14500
training loss = 2.4722812175750732 15000
training loss = 2.466165542602539 15500
training loss = 2.462641716003418 16000
training loss = 2.4590792655944824 16500
training loss = 3.381972551345825 17000
training loss = 2.4524097442626953 17500
training loss = 2.5245585441589355 18000
training loss = 2.451418399810791 18500
training loss = 2.4572901725769043 19000
training loss = 2.441342353820801 19500
training loss = 2.4381356239318848 20000
training loss = 2.4355268478393555 20500
training loss = 2.8716654777526855 21000
training loss = 2.589761972427368 21500
training loss = 2.4419775009155273 22000
training loss = 2.4258217811584473 22500
training loss = 2.4234366416931152 23000
training loss = 2.421187162399292 23500
training loss = 3.47682523727417 24000
training loss = 2.4332916736602783 24500
training loss = 2.4854118824005127 25000
training loss = 2.4123148918151855 25500
training loss = 2.4101920127868652 26000
training loss = 2.4080820083618164 26500
training loss = 2.4060001373291016 27000
training loss = 2.403952121734619 27500
training loss = 2.4019155502319336 28000
training loss = 2.399906873703003 28500
training loss = 2.3979146480560303 29000
training loss = 2.395935535430908 29500
training loss = 2.3939759731292725 30000
training loss = 2.3920271396636963 30500
training loss = 2.390106678009033 31000
training loss = 2.3882038593292236 31500
training loss = 2.386361837387085 32000
training loss = 2.3845677375793457 32500
training loss = 2.3828048706054688 33000
training loss = 2.381136417388916 33500
training loss = 2.380279779434204 34000
training loss = 2.7870593070983887 34500
training loss = 2.45438814163208 35000
(1, 38)
(1, 0)
