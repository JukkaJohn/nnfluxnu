data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.4387462939643333 3.0233577445260873 88.41656860739322
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 81.60736846923828 500
training loss = 72.89849090576172 1000
training loss = 72.85995483398438 1500
training loss = 72.8187484741211 2000
training loss = 72.75148010253906 2500
training loss = 72.63096618652344 3000
training loss = 72.36598205566406 3500
training loss = 70.68893432617188 4000
training loss = 2.5790727138519287 4500
training loss = 2.5460381507873535 5000
training loss = 2.5266590118408203 5500
training loss = 2.5100789070129395 6000
training loss = 2.507458448410034 6500
training loss = 2.475383996963501 7000
training loss = 2.4559884071350098 7500
training loss = 2.4351179599761963 8000
training loss = 2.412832498550415 8500
training loss = 2.388975143432617 9000
training loss = 2.3637325763702393 9500
training loss = 2.3371694087982178 10000
training loss = 2.310490369796753 10500
training loss = 2.2836310863494873 11000
training loss = 2.4455912113189697 11500
training loss = 2.2323098182678223 12000
training loss = 2.2078843116760254 12500
training loss = 2.1858580112457275 13000
training loss = 2.1645255088806152 13500
training loss = 2.146191358566284 14000
training loss = 2.1299266815185547 14500
training loss = 2.1155006885528564 15000
training loss = 2.1033742427825928 15500
training loss = 2.0931999683380127 16000
training loss = 2.0928850173950195 16500
training loss = 2.0907657146453857 17000
training loss = 2.0710930824279785 17500
training loss = 2.06571102142334 18000
training loss = 2.1043429374694824 18500
training loss = 7.2120490074157715 19000
training loss = 2.1360464096069336 19500
training loss = 2.0886144638061523 20000
training loss = 2.0562655925750732 20500
training loss = 2.123741626739502 21000
training loss = 2.049255609512329 21500
training loss = 2.0428333282470703 22000
training loss = 2.040928602218628 22500
training loss = 2.0392305850982666 23000
training loss = 2.0376110076904297 23500
training loss = 2.036034345626831 24000
training loss = 2.0345325469970703 24500
training loss = 2.033158540725708 25000
training loss = 2.0317389965057373 25500
training loss = 2.0303759574890137 26000
training loss = 2.0290937423706055 26500
training loss = 2.0277791023254395 27000
training loss = 2.026461362838745 27500
training loss = 2.0252480506896973 28000
training loss = 2.024085521697998 28500
training loss = 2.0229432582855225 29000
training loss = 2.0213751792907715 29500
training loss = 2.0632762908935547 30000
training loss = 3.096090078353882 30500
training loss = 2.0173158645629883 31000
training loss = 2.016021728515625 31500
training loss = 2.01491641998291 32000
training loss = 2.013841390609741 32500
training loss = 2.012326717376709 33000
training loss = 2.011448621749878 33500
training loss = 2.526803493499756 34000
training loss = 2.0222055912017822 34500
training loss = 2.007242202758789 35000
(1, 38)
(1, 0)
