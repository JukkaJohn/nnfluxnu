data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.435376394963998 0.7364320951760317 82.78316451447124
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 35.94535446166992 500
training loss = 25.889060974121094 1000
training loss = 23.02470588684082 1500
training loss = 8.427059173583984 2000
training loss = 3.287787675857544 2500
training loss = 2.7591240406036377 3000
training loss = 2.3958709239959717 3500
training loss = 2.1456334590911865 4000
training loss = 1.76323401927948 4500
training loss = 1.4998847246170044 5000
training loss = 1.3101288080215454 5500
training loss = 1.6969428062438965 6000
training loss = 1.2001358270645142 6500
training loss = 1.1643060445785522 7000
training loss = 0.988305926322937 7500
training loss = 0.8779480457305908 8000
training loss = 0.8213433027267456 8500
training loss = 0.797299861907959 9000
training loss = 0.7864672541618347 9500
training loss = 0.7805707454681396 10000
training loss = 0.7766293883323669 10500
training loss = 0.7840753197669983 11000
training loss = 0.7865349054336548 11500
training loss = 0.7844153046607971 12000
training loss = 0.7700425982475281 12500
training loss = 0.7664209008216858 13000
training loss = 0.76665860414505 13500
training loss = 0.8079463243484497 14000
training loss = 0.7774800062179565 14500
training loss = 0.7692217826843262 15000
training loss = 0.7851515412330627 15500
training loss = 0.7579299807548523 16000
training loss = 0.7561613917350769 16500
training loss = 0.7545372843742371 17000
training loss = 0.753288209438324 17500
training loss = 0.7521237730979919 18000
training loss = 0.7505343556404114 18500
training loss = 0.7492769360542297 19000
training loss = 0.7770004272460938 19500
training loss = 1.6258612871170044 20000
training loss = 0.9318862557411194 20500
training loss = 0.8938896059989929 21000
training loss = 0.7539512515068054 21500
training loss = 0.7450986504554749 22000
training loss = 0.7438946962356567 22500
training loss = 0.7409816384315491 23000
training loss = 0.7394899129867554 23500
training loss = 0.7387705445289612 24000
training loss = 0.7731954455375671 24500
reduced chi^2 level 2 = 0.7436974048614502
Constrained alpha: 2.4141845703125
Constrained beta: -0.8563243746757507
Constrained gamma: 60.8648796081543
(1, 38)
(1, 0)
