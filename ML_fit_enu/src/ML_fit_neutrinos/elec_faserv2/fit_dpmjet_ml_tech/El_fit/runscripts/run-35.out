data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.5210757292588042 12.549514805188968 63.014794519088824
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 99.71137237548828 500
training loss = 75.87158203125 1000
training loss = 74.61034393310547 1500
training loss = 72.67587280273438 2000
training loss = 67.9051742553711 2500
training loss = 14.303044319152832 3000
training loss = 7.794806480407715 3500
training loss = 5.556400775909424 4000
training loss = 4.4365763664245605 4500
training loss = 3.9565768241882324 5000
training loss = 3.802337646484375 5500
training loss = 3.7534332275390625 6000
training loss = 3.7366573810577393 6500
training loss = 3.7302327156066895 7000
training loss = 3.7292702198028564 7500
training loss = 5.069958209991455 8000
training loss = 3.724524974822998 8500
training loss = 3.7237308025360107 9000
training loss = 3.7226879596710205 9500
training loss = 3.7218775749206543 10000
training loss = 3.721099853515625 10500
training loss = 3.720344066619873 11000
training loss = 3.71962571144104 11500
training loss = 3.718923807144165 12000
training loss = 3.7182517051696777 12500
training loss = 3.7176103591918945 13000
training loss = 3.716982364654541 13500
training loss = 3.716768741607666 14000
training loss = 4.125833988189697 14500
training loss = 3.7153425216674805 15000
training loss = 3.7148125171661377 15500
training loss = 3.7176530361175537 16000
training loss = 4.7982683181762695 16500
training loss = 3.7134246826171875 17000
training loss = 3.7130186557769775 17500
training loss = 3.712639331817627 18000
training loss = 3.7136693000793457 18500
training loss = 3.717101573944092 19000
training loss = 3.741246223449707 19500
training loss = 3.7118031978607178 20000
training loss = 3.7480058670043945 20500
training loss = 3.942545175552368 21000
training loss = 3.7113230228424072 21500
training loss = 3.710319995880127 22000
training loss = 3.710566520690918 22500
training loss = 3.7134101390838623 23000
training loss = 3.7096328735351562 23500
training loss = 3.709442138671875 24000
training loss = 3.7092719078063965 24500
training loss = 3.7091047763824463 25000
training loss = 3.708946704864502 25500
training loss = 3.711286783218384 26000
training loss = 3.70991587638855 26500
training loss = 3.7901668548583984 27000
training loss = 7.654796600341797 27500
training loss = 3.708275318145752 28000
training loss = 3.708974838256836 28500
training loss = 4.238854885101318 29000
training loss = 4.02723503112793 29500
training loss = 3.709026575088501 30000
training loss = 3.74812650680542 30500
training loss = 3.7085330486297607 31000
training loss = 3.7694180011749268 31500
training loss = 3.7075283527374268 32000
training loss = 3.91585111618042 32500
training loss = 3.707382917404175 33000
training loss = 3.707298755645752 33500
training loss = 3.707242965698242 34000
training loss = 3.7071692943573 34500
training loss = 3.7071075439453125 35000
(1, 38)
(1, 0)
