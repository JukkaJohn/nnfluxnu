data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
2.763727930103575 2.322220739306664 53.682895413265676
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 97.09904479980469 500
training loss = 77.04927825927734 1000
training loss = 76.9436264038086 1500
training loss = 76.7470932006836 2000
training loss = 76.11040496826172 2500
training loss = 59.93296432495117 3000
training loss = 5.11527681350708 3500
training loss = 3.968766927719116 4000
training loss = 3.5940945148468018 4500
training loss = 3.4861185550689697 5000
training loss = 4.046611785888672 5500
training loss = 3.455716609954834 6000
training loss = 3.949770927429199 6500
training loss = 3.474581718444824 7000
training loss = 3.4653337001800537 7500
training loss = 3.4518508911132812 8000
training loss = 3.4502041339874268 8500
training loss = 3.44892954826355 9000
training loss = 3.4478447437286377 9500
training loss = 3.289482831954956 10000
training loss = 3.6653759479522705 10500
training loss = 3.4082024097442627 11000
training loss = 2.753296136856079 11500
training loss = 2.702425241470337 12000
training loss = 2.6474475860595703 12500
training loss = 2.615811824798584 13000
training loss = 3.094503164291382 13500
training loss = 2.517925262451172 14000
training loss = 2.4456310272216797 14500
training loss = 2.233794927597046 15000
training loss = 2.1104423999786377 15500
training loss = 2.380479097366333 16000
training loss = 1.8634597063064575 16500
training loss = 1.5836842060089111 17000
training loss = 1.473741888999939 17500
training loss = 1.4669173955917358 18000
training loss = 1.4634921550750732 18500
training loss = 1.5163508653640747 19000
training loss = 1.4590389728546143 19500
training loss = 1.458045482635498 20000
training loss = 1.4568042755126953 20500
training loss = 1.4559358358383179 21000
training loss = 1.4557150602340698 21500
training loss = 1.6665318012237549 22000
training loss = 1.9087756872177124 22500
training loss = 3.7709944248199463 23000
training loss = 2.9505019187927246 23500
training loss = 1.452077031135559 24000
training loss = 1.506752371788025 24500
training loss = 1.591477870941162 25000
training loss = 1.4770164489746094 25500
training loss = 1.4747233390808105 26000
training loss = 1.4503196477890015 26500
training loss = 1.4488099813461304 27000
training loss = 1.4482859373092651 27500
training loss = 1.4477332830429077 28000
training loss = 1.4472018480300903 28500
training loss = 1.4466946125030518 29000
training loss = 1.4461596012115479 29500
training loss = 1.4455877542495728 30000
training loss = 1.4454833269119263 30500
training loss = 2.5909225940704346 31000
training loss = 1.4465290307998657 31500
training loss = 1.4450623989105225 32000
training loss = 1.4429491758346558 32500
training loss = 1.4423938989639282 33000
training loss = 1.4418312311172485 33500
training loss = 1.4412709474563599 34000
training loss = 1.4407418966293335 34500
training loss = 1.4419499635696411 35000
reduced chi^2 level 2 = 1.4427976608276367
Constrained alpha: 2.314554214477539
Constrained beta: 3.817957639694214
Constrained gamma: 38.34017562866211
(1, 38)
(1, 0)
