data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.719726161314457 14.594743167734887 5.312117475736966
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 53.218467712402344 500
training loss = 4.447296142578125 1000
training loss = 3.036891222000122 1500
training loss = 3.988058090209961 2000
training loss = 2.2805569171905518 2500
training loss = 2.0362772941589355 3000
training loss = 1.881372332572937 3500
training loss = 1.7925714254379272 4000
training loss = 1.7412763833999634 4500
training loss = 1.71526300907135 5000
training loss = 1.7005441188812256 5500
training loss = 1.690848469734192 6000
training loss = 1.6843944787979126 6500
training loss = 1.6809098720550537 7000
training loss = 1.6727913618087769 7500
training loss = 1.6679733991622925 8000
training loss = 1.6635644435882568 8500
training loss = 1.659883975982666 9000
training loss = 1.6564162969589233 9500
training loss = 1.652929425239563 10000
training loss = 1.6496539115905762 10500
training loss = 1.6469621658325195 11000
training loss = 1.6476253271102905 11500
training loss = 1.728904366493225 12000
training loss = 13.986257553100586 12500
training loss = 1.6349520683288574 13000
training loss = 1.6324622631072998 13500
training loss = 1.778908133506775 14000
training loss = 1.6340327262878418 14500
training loss = 1.6280797719955444 15000
training loss = 1.6273776292800903 15500
training loss = 1.6228781938552856 16000
training loss = 1.620317816734314 16500
training loss = 1.623124599456787 17000
training loss = 1.6171401739120483 17500
training loss = 1.616173267364502 18000
training loss = 1.663826823234558 18500
training loss = 2.415436029434204 19000
training loss = 1.6090060472488403 19500
training loss = 8.736353874206543 20000
training loss = 1.6128969192504883 20500
training loss = 1.605357050895691 21000
training loss = 1.6035875082015991 21500
training loss = 1.6022884845733643 22000
training loss = 1.6006569862365723 22500
training loss = 1.9515495300292969 23000
training loss = 1.601973533630371 23500
training loss = 1.9456015825271606 24000
training loss = 2.186616897583008 24500
training loss = 1.5951114892959595 25000
training loss = 1.5945155620574951 25500
training loss = 1.5935308933258057 26000
training loss = 1.5921837091445923 26500
training loss = 1.5912295579910278 27000
training loss = 1.590605616569519 27500
training loss = 1.589536190032959 28000
training loss = 1.5883859395980835 28500
training loss = 1.5873570442199707 29000
training loss = 1.5863996744155884 29500
training loss = 1.585593342781067 30000
training loss = 1.5846775770187378 30500
training loss = 1.5836619138717651 31000
training loss = 1.583452820777893 31500
training loss = 1.5825453996658325 32000
training loss = 1.5814759731292725 32500
training loss = 1.6671953201293945 33000
training loss = 1.625088095664978 33500
training loss = 4.94357967376709 34000
training loss = 4.110971450805664 34500
training loss = 1.6865298748016357 35000
reduced chi^2 level 2 = 1.7532360553741455
Constrained alpha: 3.8355226516723633
Constrained beta: 2.816838026046753
Constrained gamma: 11.413873672485352
(1, 38)
(1, 0)
