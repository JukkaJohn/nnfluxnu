data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.4915030355847366 5.4274891076581255 29.094093809299835
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 94.96537017822266 500
training loss = 83.42610168457031 1000
training loss = 83.24127960205078 1500
training loss = 83.10269165039062 2000
training loss = 82.91522979736328 2500
training loss = 82.65587615966797 3000
training loss = 82.27324676513672 3500
training loss = 81.62443542480469 4000
training loss = 80.07829284667969 4500
training loss = 60.01828384399414 5000
training loss = 4.469301700592041 5500
training loss = 4.05618953704834 6000
training loss = 3.969778060913086 6500
training loss = 3.9645402431488037 7000
training loss = 3.9636166095733643 7500
training loss = 3.963229179382324 8000
training loss = 3.9629316329956055 8500
training loss = 3.964390754699707 9000
training loss = 3.970201253890991 9500
training loss = 3.9728927612304688 10000
training loss = 3.9628922939300537 10500
training loss = 3.961883068084717 11000
training loss = 3.9619252681732178 11500
training loss = 3.961700916290283 12000
training loss = 3.9614369869232178 12500
training loss = 3.9613118171691895 13000
training loss = 3.961193323135376 13500
training loss = 3.961087703704834 14000
training loss = 3.9609949588775635 14500
training loss = 3.961148262023926 15000
training loss = 3.960831880569458 15500
training loss = 3.9607551097869873 16000
training loss = 3.960700511932373 16500
training loss = 3.960632801055908 17000
training loss = 3.9606122970581055 17500
training loss = 3.9609978199005127 18000
training loss = 3.9606099128723145 18500
training loss = 3.960451602935791 19000
training loss = 3.960439682006836 19500
training loss = 3.9603641033172607 20000
training loss = 3.960343599319458 20500
training loss = 3.960306406021118 21000
training loss = 3.9602668285369873 21500
training loss = 3.9603066444396973 22000
training loss = 4.054561138153076 22500
training loss = 3.9601943492889404 23000
training loss = 4.428184509277344 23500
training loss = 4.077868938446045 24000
training loss = 4.2049713134765625 24500
training loss = 4.025974273681641 25000
(1, 38)
(1, 0)
