data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
0.4083537383331759 8.749729276302594 70.84479923348046
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 26.23097038269043 500
training loss = 2.97147536277771 1000
training loss = 2.9129467010498047 1500
training loss = 2.8529257774353027 2000
training loss = 2.7944934368133545 2500
training loss = 2.744385004043579 3000
training loss = 2.6962666511535645 3500
training loss = 2.6589667797088623 4000
training loss = 2.6285653114318848 4500
training loss = 2.600616455078125 5000
training loss = 2.5769155025482178 5500
training loss = 2.562631845474243 6000
training loss = 2.770559549331665 6500
training loss = 2.5835845470428467 7000
training loss = 2.502117156982422 7500
training loss = 2.483153820037842 8000
training loss = 2.508668899536133 8500
training loss = 2.7861826419830322 9000
training loss = 4.574506759643555 9500
training loss = 2.396569013595581 10000
training loss = 2.3720028400421143 10500
training loss = 2.3459596633911133 11000
training loss = 2.3421835899353027 11500
training loss = 2.3249123096466064 12000
training loss = 2.277693510055542 12500
training loss = 2.228503704071045 13000
training loss = 2.197751045227051 13500
training loss = 2.1677656173706055 14000
training loss = 2.139181613922119 14500
training loss = 2.112234592437744 15000
training loss = 2.087928295135498 15500
training loss = 2.067474365234375 16000
training loss = 2.0469911098480225 16500
training loss = 2.030313491821289 17000
training loss = 2.0161826610565186 17500
training loss = 2.003925323486328 18000
training loss = 1.9933897256851196 18500
training loss = 1.984369158744812 19000
training loss = 1.978708028793335 19500
training loss = 1.9738619327545166 20000
training loss = 1.9805079698562622 20500
training loss = 1.971354603767395 21000
training loss = 1.9659740924835205 21500
training loss = 1.9462283849716187 22000
training loss = 1.9419667720794678 22500
training loss = 1.9357792139053345 23000
training loss = 1.9309026002883911 23500
training loss = 1.9269523620605469 24000
training loss = 1.9214204549789429 24500
training loss = 1.9168157577514648 25000
training loss = 1.912172794342041 25500
training loss = 1.9076277017593384 26000
training loss = 1.9030009508132935 26500
training loss = 1.8984609842300415 27000
training loss = 1.8936671018600464 27500
training loss = 1.8890295028686523 28000
training loss = 1.884383201599121 28500
training loss = 1.8797589540481567 29000
training loss = 1.875235915184021 29500
training loss = 1.8705617189407349 30000
training loss = 1.8659251928329468 30500
training loss = 1.8612333536148071 31000
training loss = 1.8565826416015625 31500
training loss = 1.8519090414047241 32000
training loss = 1.8473378419876099 32500
training loss = 1.8426798582077026 33000
training loss = 1.8380919694900513 33500
training loss = 1.8334881067276 34000
training loss = 1.8289504051208496 34500
training loss = 1.8244402408599854 35000
reduced chi^2 level 2 = 1.824432611465454
Constrained alpha: 2.2693121433258057
Constrained beta: 3.6851282119750977
Constrained gamma: 45.969417572021484
(1, 38)
(1, 0)
