data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.2691900711437882 3.1297524002017063 45.3348624955485
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 104.3028793334961 500
training loss = 75.56433868408203 1000
training loss = 75.46627044677734 1500
training loss = 75.43719482421875 2000
training loss = 75.39913177490234 2500
training loss = 75.3497543334961 3000
training loss = 75.28458404541016 3500
training loss = 75.19469451904297 4000
training loss = 75.05767822265625 4500
training loss = 74.79954528808594 5000
training loss = 73.9409408569336 5500
training loss = 4.310126781463623 6000
training loss = 3.2678611278533936 6500
training loss = 3.1232235431671143 7000
training loss = 3.0929324626922607 7500
training loss = 3.088810682296753 8000
training loss = 3.1883962154388428 8500
training loss = 3.1194868087768555 9000
training loss = 3.09000563621521 9500
training loss = 3.1658363342285156 10000
training loss = 3.0796496868133545 10500
training loss = 3.0779857635498047 11000
training loss = 3.337629556655884 11500
training loss = 3.074739694595337 12000
training loss = 3.0734126567840576 12500
training loss = 3.072584390640259 13000
training loss = 3.0704357624053955 13500
training loss = 3.069308280944824 14000
training loss = 3.067981481552124 14500
training loss = 3.0669867992401123 15000
training loss = 3.0659871101379395 15500
training loss = 3.0650885105133057 16000
training loss = 3.0639171600341797 16500
training loss = 3.06307053565979 17000
training loss = 3.062269449234009 17500
training loss = 3.0615429878234863 18000
training loss = 3.060849666595459 18500
training loss = 3.0601966381073 19000
training loss = 3.0596694946289062 19500
training loss = 3.0590147972106934 20000
training loss = 3.0585248470306396 20500
training loss = 3.057919502258301 21000
training loss = 3.0574276447296143 21500
training loss = 3.0569422245025635 22000
training loss = 3.0564751625061035 22500
training loss = 3.056051015853882 23000
training loss = 3.0556886196136475 23500
training loss = 3.0553267002105713 24000
training loss = 3.2415997982025146 24500
training loss = 3.5613579750061035 25000
training loss = 3.909545421600342 25500
training loss = 3.711275577545166 26000
training loss = 3.4177961349487305 26500
training loss = 3.090454339981079 27000
training loss = 3.053041934967041 27500
training loss = 3.2807934284210205 28000
training loss = 3.0625720024108887 28500
training loss = 6.964484691619873 29000
training loss = 3.534821033477783 29500
training loss = 3.968400716781616 30000
training loss = 3.054802656173706 30500
training loss = 3.3628828525543213 31000
training loss = 3.115811824798584 31500
training loss = 3.0671019554138184 32000
training loss = 3.0927839279174805 32500
training loss = 3.05207896232605 33000
training loss = 3.0707640647888184 33500
training loss = 3.0497677326202393 34000
training loss = 3.0491085052490234 34500
training loss = 3.048868417739868 35000
(1, 38)
(1, 0)
