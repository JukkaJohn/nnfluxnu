data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.8825579519616875 17.718557321803694 87.0248974530365
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 115.11677551269531 500
training loss = 72.7040023803711 1000
training loss = 72.25469970703125 1500
training loss = 65.16903686523438 2000
training loss = 5.108229160308838 2500
training loss = 4.5176825523376465 3000
training loss = 4.184386253356934 3500
training loss = 4.076905727386475 4000
training loss = 4.035811424255371 4500
training loss = 4.016395092010498 5000
training loss = 4.020167350769043 5500
training loss = 4.044255256652832 6000
training loss = 4.023009300231934 6500
training loss = 4.0805768966674805 7000
training loss = 4.140923023223877 7500
training loss = 4.188210964202881 8000
training loss = 4.500009536743164 8500
training loss = 4.054028034210205 9000
training loss = 3.9772560596466064 9500
training loss = 4.009227752685547 10000
training loss = 3.985076904296875 10500
training loss = 3.971228837966919 11000
training loss = 3.9697771072387695 11500
training loss = 3.9684512615203857 12000
training loss = 3.9671871662139893 12500
training loss = 3.9659624099731445 13000
training loss = 3.964754581451416 13500
training loss = 3.963545083999634 14000
training loss = 3.9622802734375 14500
training loss = 3.9608988761901855 15000
training loss = 3.9592032432556152 15500
training loss = 3.956428050994873 16000
training loss = 3.944448709487915 16500
training loss = 3.842111825942993 17000
training loss = 3.809098482131958 17500
training loss = 3.78947377204895 18000
training loss = 4.271054267883301 18500
training loss = 3.784778118133545 19000
training loss = 3.7383952140808105 19500
training loss = 3.877699136734009 20000
training loss = 3.769521713256836 20500
training loss = 3.7452845573425293 21000
training loss = 3.672762870788574 21500
training loss = 4.203258991241455 22000
training loss = 4.026173114776611 22500
training loss = 3.6339802742004395 23000
training loss = 3.785585880279541 23500
training loss = 4.106680870056152 24000
training loss = 3.5903096199035645 24500
training loss = 3.563150644302368 25000
training loss = 3.5472750663757324 25500
training loss = 3.531649589538574 26000
training loss = 3.515427350997925 26500
training loss = 3.4992454051971436 27000
training loss = 3.4833803176879883 27500
training loss = 3.4680428504943848 28000
training loss = 3.451843738555908 28500
training loss = 3.4361350536346436 29000
training loss = 3.420789957046509 29500
training loss = 3.405548095703125 30000
training loss = 3.390897750854492 30500
training loss = 3.376932382583618 31000
training loss = 3.36313533782959 31500
training loss = 3.3498799800872803 32000
training loss = 3.337376356124878 32500
training loss = 3.3253259658813477 33000
training loss = 3.3138649463653564 33500
training loss = 3.30458402633667 34000
training loss = 3.3358728885650635 34500
training loss = 3.284672498703003 35000
(1, 38)
(1, 0)
