data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
3.9960856752435503 12.823530945411008 99.9105287293759
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 75.03305053710938 500
training loss = 73.11578369140625 1000
training loss = 72.9291000366211 1500
training loss = 72.69161987304688 2000
training loss = 72.28704833984375 2500
training loss = 70.12834167480469 3000
training loss = 5.038860321044922 3500
training loss = 3.3336386680603027 4000
training loss = 3.2251510620117188 4500
training loss = 3.1721158027648926 5000
training loss = 3.1399707794189453 5500
training loss = 3.1190590858459473 6000
training loss = 3.1045215129852295 6500
training loss = 3.0938971042633057 7000
training loss = 3.085611581802368 7500
training loss = 3.0788557529449463 8000
training loss = 3.0729475021362305 8500
training loss = 3.067671060562134 9000
training loss = 3.062865734100342 9500
training loss = 3.058340549468994 10000
training loss = 3.0632777214050293 10500
training loss = 3.0504438877105713 11000
training loss = 3.0974152088165283 11500
training loss = 3.043147563934326 12000
training loss = 3.1575658321380615 12500
training loss = 3.0354843139648438 13000
training loss = 2.5075535774230957 13500
training loss = 2.0146403312683105 14000
training loss = 1.8856911659240723 14500
training loss = 1.8595577478408813 15000
training loss = 1.7822333574295044 15500
training loss = 1.7594598531723022 16000
training loss = 1.7435834407806396 16500
training loss = 1.7325462102890015 17000
training loss = 1.724381685256958 17500
training loss = 1.7183207273483276 18000
training loss = 1.7135391235351562 18500
training loss = 1.7097628116607666 19000
training loss = 1.7066869735717773 19500
training loss = 5.233485698699951 20000
training loss = 1.7297844886779785 20500
training loss = 1.7014577388763428 21000
training loss = 1.698763370513916 21500
training loss = 1.6976474523544312 22000
training loss = 1.8735805749893188 22500
training loss = 1.6954900026321411 23000
training loss = 1.6956580877304077 23500
training loss = 1.6944307088851929 24000
training loss = 5.334074974060059 24500
training loss = 2.3591055870056152 25000
training loss = 2.0319602489471436 25500
training loss = 3.877932548522949 26000
training loss = 2.7978696823120117 26500
training loss = 1.7612277269363403 27000
training loss = 1.7293809652328491 27500
training loss = 4.489439487457275 28000
training loss = 1.7981493473052979 28500
training loss = 1.6900032758712769 29000
training loss = 1.689814567565918 29500
training loss = 1.6896390914916992 30000
training loss = 1.6895908117294312 30500
training loss = 1.6924318075180054 31000
training loss = 1.6892057657241821 31500
training loss = 1.6899691820144653 32000
training loss = 1.6933473348617554 32500
training loss = 1.6962553262710571 33000
training loss = 1.689924955368042 33500
training loss = 1.6937404870986938 34000
training loss = 1.6886649131774902 34500
training loss = 1.6886285543441772 35000
reduced chi^2 level 2 = 1.6886142492294312
Constrained alpha: 2.3242979049682617
Constrained beta: 4.0935211181640625
Constrained gamma: 52.149085998535156
(1, 38)
(1, 0)
