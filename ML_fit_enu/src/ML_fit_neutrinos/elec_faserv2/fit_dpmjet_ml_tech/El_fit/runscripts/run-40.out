data before
[    0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
     0.             0.             0.             0.
 24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052     0.
     0.        ]
2
data after
[24565.213870047606, 28604.774670906983, 32612.824361310424, 36818.82904487854, 40600.64514511231, 44127.694229187015, 46580.255275896605, 48460.669028872675, 48495.667513908695, 46882.060246977235, 43455.051316412355, 38210.779037788394, 31004.513869743347, 23012.01252785225, 15620.003873895836, 10279.780883155823, 8278.323387380791, 7468.836792578029, 722.707940518114]
2
data,sig_tot
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
[24565.21387005 28604.77467091 32612.82436131 36818.82904488
 40600.64514511 44127.69422919 46580.2552759  48460.66902887
 48495.66751391 46882.06024698 43455.05131641 38210.77903779
 31004.51386974 23012.01252785 15620.0038739  10279.78088316
  8278.32338738  7468.83679258   722.70794052  4161.45696218
  5080.62616736  6090.85151958  7290.22066015  8537.66305494
  9767.44492556 10959.02289139 11989.42474127 12498.0750052
 12436.93138504 11737.24392846 10302.86374726  8313.99665736
  6016.64511467  3859.29359     2486.56000495  2049.31432809
  2182.8129729    234.34635079]
4.000539320031764 7.5210003978231725 9.415320923996262
val isze = 0
idinces = [24 32  2 12  7  5 33  3 13 18 14  1 23 20 22  6 11 35 21 30 19  4 27 10
 34 37  8 26 17 31 16 29 25 28  0 15 36  9]
training loss = 89.57820129394531 500
training loss = 77.94294738769531 1000
training loss = 77.93048858642578 1500
training loss = 77.92568969726562 2000
training loss = 77.91960144042969 2500
training loss = 77.91207122802734 3000
training loss = 77.90277862548828 3500
training loss = 77.89167785644531 4000
training loss = 77.87798309326172 4500
training loss = 77.90914154052734 5000
training loss = 77.8432846069336 5500
training loss = 79.23811340332031 6000
training loss = 77.7228775024414 6500
training loss = 77.0425033569336 7000
training loss = 4.493829250335693 7500
training loss = 9.082462310791016 8000
training loss = 4.465943813323975 8500
training loss = 4.474499225616455 9000
training loss = 4.461605072021484 9500
training loss = 4.4596734046936035 10000
training loss = 5.110760688781738 10500
training loss = 4.455531120300293 11000
training loss = 4.4529876708984375 11500
training loss = 4.450479984283447 12000
training loss = 4.447866439819336 12500
training loss = 4.44581937789917 13000
training loss = 4.443282127380371 13500
training loss = 4.439630031585693 14000
training loss = 4.446011066436768 14500
training loss = 4.4342241287231445 15000
training loss = 4.501857757568359 15500
training loss = 4.429482936859131 16000
training loss = 4.42722749710083 16500
training loss = 4.425322532653809 17000
training loss = 4.423618316650391 17500
training loss = 4.422176837921143 18000
training loss = 4.4209394454956055 18500
training loss = 4.419915199279785 19000
training loss = 4.419058799743652 19500
training loss = 4.418346881866455 20000
training loss = 4.417745590209961 20500
training loss = 4.417242527008057 21000
training loss = 4.423656463623047 21500
training loss = 4.533682823181152 22000
training loss = 4.416182518005371 22500
training loss = 4.519829750061035 23000
training loss = 4.550549030303955 23500
training loss = 4.497714519500732 24000
training loss = 4.4153008460998535 24500
training loss = 4.4149298667907715 25000
training loss = 4.414712905883789 25500
training loss = 4.414535045623779 26000
training loss = 4.425617218017578 26500
training loss = 4.414307117462158 27000
training loss = 4.414088249206543 27500
training loss = 4.4141058921813965 28000
training loss = 4.41406774520874 28500
training loss = 4.413644313812256 29000
training loss = 4.413491725921631 29500
training loss = 4.4133620262146 30000
training loss = 4.413254737854004 30500
training loss = 4.413167953491211 31000
training loss = 4.413026332855225 31500
training loss = 4.41293478012085 32000
training loss = 4.41281795501709 32500
training loss = 4.412722110748291 33000
training loss = 4.412662982940674 33500
training loss = 4.412569522857666 34000
training loss = 4.412446022033691 34500
training loss = 4.412364482879639 35000
(1, 38)
(1, 0)
