data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.46335423e+02 1.63004468e+02 1.78687863e+02 1.90852320e+02
 1.98441897e+02 1.99944266e+02 1.92374320e+02 1.78022876e+02
 1.56023482e+02 1.29501983e+02 9.82585946e+01 6.98798527e+01
 4.42672021e+01 2.43488112e+01 1.10312936e+01 3.67603913e+00
 7.60762454e-01 7.00450440e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
7
data after
[146.33542317444014, 163.00446815170642, 178.6878634608865, 190.8523196526716, 198.44189720286116, 199.94426625776453, 192.374320329303, 178.02287588680477, 156.0234816549956, 129.501983250808, 98.25859463064572, 69.87985268306507, 44.26720206443648, 39.88695144313927]
8
data,sig_tot
[146.33542317 163.00446815 178.68786346 190.85231965 198.4418972
 199.94426626 192.37432033 178.02287589 156.02348165 129.50198325
  98.25859463  69.87985268  44.26720206  39.88695144  27.10120937
  32.11532221  37.22984498  41.51728651  44.86560772  46.66192318
  46.44292695  43.67551828  38.70419843  32.2770071   24.76840728
  36.49004949]
[146.33542317 163.00446815 178.68786346 190.85231965 198.4418972
 199.94426626 192.37432033 178.02287589 156.02348165 129.50198325
  98.25859463  69.87985268  44.26720206  39.88695144  27.10120937
  32.11532221  37.22984498  41.51728651  44.86560772  46.66192318
  46.44292695  43.67551828  38.70419843  32.2770071   24.76840728
  36.49004949]
4.524846106787144 12.701003889019828 70.587281846402
val isze = 0
idinces = [19  5 14  7  1  2 11 23 13 12 20  3 24  6 18 10 22 25  8 21 16 17  0 15
  4  9]
training loss = 47.0517692565918 500
training loss = 45.79170227050781 1000
training loss = 41.74146270751953 1500
training loss = 19.07240104675293 2000
training loss = 15.8231782913208 2500
training loss = 13.015982627868652 3000
training loss = 10.373345375061035 3500
training loss = 8.056185722351074 4000
training loss = 6.206085205078125 4500
training loss = 4.961236000061035 5000
(1, 26)
(1, 0)
