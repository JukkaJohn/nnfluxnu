data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.47565511e+02 1.72230840e+02 1.97499956e+02 2.21795457e+02
 2.46654492e+02 2.68575559e+02 2.86674697e+02 2.98739968e+02
 3.00813673e+02 2.91920419e+02 2.71025396e+02 2.37799213e+02
 1.92828217e+02 1.37303026e+02 8.29962610e+01 3.71485721e+01
 1.14049175e+01 1.27155245e+00 8.70179549e-03 0.00000000e+00
 0.00000000e+00]
5
data after
[147.56551125643458, 172.23084018395252, 197.49995616115712, 221.79545670522518, 246.65449165227122, 268.5755594392394, 286.67469713071057, 298.7399678966218, 300.81367283944854, 291.9204192868681, 271.02539648442945, 237.79921263735164, 192.82821747421158, 137.3030264200614, 82.99626099298044, 49.83374380206182]
6
data,sig_tot
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
2.1875864283698228 0.7350220260336515 79.44082454824098
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 1.449831485748291 500
training loss = 0.9643568396568298 1000
training loss = 0.9443773031234741 1500
training loss = 0.9222908616065979 2000
training loss = 0.8867759704589844 2500
training loss = 0.873658299446106 3000
training loss = 0.8697375059127808 3500
training loss = 0.8681222200393677 4000
training loss = 0.8676571249961853 4500
reduced chi^2 level 2 = 0.8676683306694031
Constrained alpha: 2.447889566421509
Constrained beta: 1.2405734062194824
Constrained gamma: 38.473663330078125
(1, 31)
(1, 0)
