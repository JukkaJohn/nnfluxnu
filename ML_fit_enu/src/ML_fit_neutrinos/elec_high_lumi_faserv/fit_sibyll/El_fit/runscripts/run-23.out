data before
[  0.           0.           0.           0.           0.
   0.           0.           0.           0.           0.
   0.           0.           0.           0.           0.
   0.           0.           0.           0.           0.
 128.70353686 148.78283719 168.15354617 187.22370306 202.75516579
 214.93512338 219.19366606 217.96930231 205.64917786 184.28630033
 155.71763212 123.41973493  90.76418067  62.18380302  39.84537931
  25.29659514  19.9822009   18.03774272   1.74792816   0.
   0.        ]
3
data after
[128.70353685897828, 148.78283719064714, 168.15354617245674, 187.2237030576873, 202.75516579301834, 214.93512337660792, 219.19366606149674, 217.96930231076, 205.64917785764695, 184.2863003275919, 155.71763212081194, 123.41973492524028, 90.76418066874594, 62.183803022389114, 39.845379313286394, 25.296595141325145, 39.76787178337821]
8
data,sig_tot
[128.70353686 148.78283719 168.15354617 187.22370306 202.75516579
 214.93512338 219.19366606 217.96930231 205.64917786 184.28630033
 155.71763212 123.41973493  90.76418067  62.18380302  39.84537931
  25.29659514  39.76787178  41.12103908  26.48390527  30.94695871
  34.91469695  37.99947578  39.94603801  40.14426542  37.5465598
  32.90800041  26.5003386   43.19923573]
[128.70353686 148.78283719 168.15354617 187.22370306 202.75516579
 214.93512338 219.19366606 217.96930231 205.64917786 184.28630033
 155.71763212 123.41973493  90.76418067  62.18380302  39.84537931
  25.29659514  39.76787178  41.12103908  26.48390527  30.94695871
  34.91469695  37.99947578  39.94603801  40.14426542  37.5465598
  32.90800041  26.5003386   43.19923573]
4.392418229281824 12.393564537724018 34.34513408599912
val isze = 0
idinces = [ 7 21  5  2 13 19 11 12  1 22 25 14 18  3 26  6 20 10 24 27  8 23 16 17
  0 15  4  9]
training loss = 62.18205261230469 500
training loss = 59.77976989746094 1000
training loss = 49.675540924072266 1500
training loss = 25.852731704711914 2000
training loss = 18.584339141845703 2500
training loss = 13.401317596435547 3000
training loss = 10.658038139343262 3500
training loss = 9.1905517578125 4000
training loss = 7.818334579467773 4500
training loss = 6.551444053649902 5000
(1, 28)
(1, 0)
