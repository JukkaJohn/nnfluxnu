84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 881.5327,  881.0925,  944.3860,  997.8982,  992.9227, 1063.9354,
        1161.8394, 1090.0439, 1165.4813, 1139.6487, 1198.9592, 1183.1288,
        1225.1445, 1309.6162, 1295.9498, 1420.4675, 1400.4911, 1460.3718,
        1509.1882, 1496.1583, 1472.8940, 1540.7993, 1586.0083, 1664.7744,
        1690.4834, 1713.2421, 1609.6969, 1721.7352, 1699.9924, 1683.6599,
        1682.7620, 1679.4329, 1705.5288, 1706.0464, 1729.2251, 1724.1262,
        1615.6016, 1541.3394, 1655.5476, 1647.1566, 1645.7893, 1567.4298,
        1477.4874, 1500.9348, 1316.4633, 1344.3010, 1292.2743, 1235.6780,
        1234.9603, 1167.9214,  997.5916, 1021.2266,  949.4624,  937.2032,
         920.1284,  859.9293,  779.8041,  727.4930,  598.2333,  508.9394,
         553.4341,  455.4930,  440.4640,  400.8954,  319.5629,  344.9697,
         286.6612,  264.7052,  239.4676,  182.0009,  152.3641,  148.3943,
         150.4992,  103.5928,  106.7307,   51.0523,   51.5115,   46.9224,
          38.2771,   41.2400,   15.4934,   39.5237,   34.8167])]
2502.1680489804544
2.387827590440537 7.123239606512712 44.29427456920738
val isze = 8
idinces = [ 8 49 11 10  7 60 52  0 64 26  1 16 30 19 32 72 25  5 67 22 13 74 55 39
 82 73 76 47 44 15 28 69  3 77 37 54 12 75 79 38 40 68 81 48 27 50 57 35
 20 18 36 41 14 23 80 61 45 70 56 65 66  4 46 71 43 62 42 34 17  9 29  6
  2 63 21 33 59 53 58 78 31 24 51]
we are doing training validation split
training loss = 22.471323013305664 100
val loss = 39.648353576660156
training loss = 16.63990020751953 200
val loss = 27.996875762939453
training loss = 12.635661125183105 300
val loss = 20.935222625732422
training loss = 10.078322410583496 400
val loss = 16.043474197387695
training loss = 8.484593391418457 500
val loss = 12.69802474975586
training loss = 7.497725009918213 600
val loss = 10.384130477905273
training loss = 6.887037754058838 700
val loss = 8.760089874267578
training loss = 6.507942199707031 800
val loss = 7.6038289070129395
training loss = 6.270665168762207 900
val loss = 6.770125389099121
training loss = 6.119528770446777 1000
val loss = 6.162960052490234
training loss = 6.019998073577881 1100
val loss = 5.717118263244629
training loss = 5.950750350952148 1200
val loss = 5.387962341308594
training loss = 5.898733139038086 1300
val loss = 5.143679141998291
training loss = 5.856088161468506 1400
val loss = 4.961974143981934
training loss = 5.818138122558594 1500
val loss = 4.825956344604492
training loss = 5.782116889953613 1600
val loss = 4.723405838012695
training loss = 5.746280670166016 1700
val loss = 4.645143508911133
training loss = 5.709262371063232 1800
val loss = 4.583972454071045
training loss = 5.669435024261475 1900
val loss = 4.534505844116211
training loss = 5.623918533325195 2000
val loss = 4.492751598358154
training loss = 5.566275119781494 2100
val loss = 4.454964637756348
training loss = 5.479823112487793 2200
val loss = 4.418454170227051
training loss = 5.319801330566406 2300
val loss = 4.380332946777344
training loss = 5.009317398071289 2400
val loss = 4.31668758392334
training loss = 4.5070295333862305 2500
val loss = 4.034781455993652
training loss = 3.604952573776245 2600
val loss = 3.3181369304656982
training loss = 2.3968453407287598 2700
val loss = 1.9775493144989014
training loss = 1.9145221710205078 2800
val loss = 1.335551142692566
training loss = 1.8475093841552734 2900
val loss = 1.2135943174362183
training loss = 1.8416874408721924 3000
val loss = 1.1953140497207642
training loss = 1.8403711318969727 3100
val loss = 1.1917598247528076
training loss = 1.8393921852111816 3200
val loss = 1.1904609203338623
training loss = 1.838414192199707 3300
val loss = 1.1891696453094482
training loss = 1.837399959564209 3400
val loss = 1.1878297328948975
training loss = 1.836337924003601 3500
val loss = 1.1861121654510498
training loss = 1.8352309465408325 3600
val loss = 1.1841981410980225
training loss = 1.834077000617981 3700
val loss = 1.1819381713867188
training loss = 1.8328697681427002 3800
val loss = 1.1795037984848022
training loss = 1.8325402736663818 3900
val loss = 1.1434385776519775
training loss = 1.8303111791610718 4000
val loss = 1.1741502285003662
training loss = 1.8290324211120605 4100
val loss = 1.1801097393035889
training loss = 1.827582836151123 4200
val loss = 1.1686680316925049
training loss = 1.8261752128601074 4300
val loss = 1.16945219039917
training loss = 1.8246792554855347 4400
val loss = 1.160102367401123
training loss = 1.8232345581054688 4500
val loss = 1.1667182445526123
training loss = 1.8216004371643066 4600
val loss = 1.1532633304595947
training loss = 1.821031093597412 4700
val loss = 1.1140427589416504
training loss = 1.8183720111846924 4800
val loss = 1.1450080871582031
training loss = 1.816672444343567 4900
val loss = 1.1390416622161865
training loss = 1.8149669170379639 5000
val loss = 1.139819860458374
training loss = 1.8131612539291382 5100
val loss = 1.1295289993286133
training loss = 1.811419129371643 5200
val loss = 1.1302552223205566
training loss = 1.8095452785491943 5300
val loss = 1.1166584491729736
training loss = 1.808516025543213 5400
val loss = 1.1425694227218628
training loss = 1.8058618307113647 5500
val loss = 1.1028954982757568
training loss = 1.8079785108566284 5600
val loss = 1.1714842319488525
training loss = 1.802247405052185 5700
val loss = 1.087826132774353
training loss = 1.8004276752471924 5800
val loss = 1.0780446529388428
training loss = 1.7998576164245605 5900
val loss = 1.0336575508117676
training loss = 1.797014594078064 6000
val loss = 1.0608686208724976
training loss = 1.8031278848648071 6100
val loss = 0.9635031223297119
training loss = 1.7938908338546753 6200
val loss = 1.0416542291641235
training loss = 1.963623285293579 6300
val loss = 0.76802659034729
training loss = 1.7911570072174072 6400
val loss = 1.0229530334472656
training loss = 1.7898916006088257 6500
val loss = 1.0144221782684326
training loss = 1.789385199546814 6600
val loss = 1.0315983295440674
training loss = 1.7878679037094116 6700
val loss = 0.9980396032333374
training loss = 1.8536080121994019 6800
val loss = 1.336313247680664
training loss = 1.7862915992736816 6900
val loss = 0.9849367141723633
training loss = 1.7856076955795288 7000
val loss = 0.9753420352935791
training loss = 1.785302758216858 7100
val loss = 0.957365870475769
training loss = 1.7846533060073853 7200
val loss = 0.9640044569969177
training loss = 1.784285306930542 7300
val loss = 0.9518857002258301
training loss = 1.7840124368667603 7400
val loss = 0.9489680528640747
training loss = 1.7837011814117432 7500
val loss = 0.9498888254165649
training loss = 1.8016842603683472 7600
val loss = 0.8220024704933167
training loss = 1.7833712100982666 7700
val loss = 0.9418658018112183
training loss = 1.7832189798355103 7800
val loss = 0.9403184652328491
training loss = 1.8333935737609863 7900
val loss = 0.7607589960098267
training loss = 1.7830711603164673 8000
val loss = 0.9338735342025757
training loss = 1.7905972003936768 8100
val loss = 1.0319371223449707
training loss = 1.7830144166946411 8200
val loss = 0.9370890259742737
training loss = 1.782982587814331 8300
val loss = 0.9316878914833069
training loss = 1.7831429243087769 8400
val loss = 0.9198035001754761
training loss = 1.7830029726028442 8500
val loss = 0.9300320148468018
training loss = 1.973617434501648 8600
val loss = 0.687883198261261
training loss = 1.7830688953399658 8700
val loss = 0.9318401217460632
training loss = 1.7831264734268188 8800
val loss = 0.9234044551849365
training loss = 1.7831655740737915 8900
val loss = 0.926110565662384
training loss = 1.783197283744812 9000
val loss = 0.9285112619400024
training loss = 1.7931324243545532 9100
val loss = 1.041077733039856
training loss = 1.7833133935928345 9200
val loss = 0.9295576214790344
training loss = 1.7833715677261353 9300
val loss = 0.9302849173545837
training loss = 1.783443808555603 9400
val loss = 0.93039470911026
training loss = 1.7835028171539307 9500
val loss = 0.9298155307769775
training loss = 1.783793330192566 9600
val loss = 0.9461773633956909
training loss = 1.7836452722549438 9700
val loss = 0.9301578402519226
training loss = 1.8116858005523682 9800
val loss = 0.7921963930130005
training loss = 1.7837927341461182 9900
val loss = 0.9329563975334167
training loss = 1.7838531732559204 10000
val loss = 0.9312281012535095
training loss = 1.8708958625793457 10100
val loss = 0.7241495847702026
training loss = 1.7840126752853394 10200
val loss = 0.9302985668182373
training loss = 1.7840780019760132 10300
val loss = 0.9325953722000122
training loss = 1.784598469734192 10400
val loss = 0.9136275053024292
training loss = 1.7842293977737427 10500
val loss = 0.9337445497512817
training loss = 1.7843998670578003 10600
val loss = 0.924088716506958
training loss = 1.784404993057251 10700
val loss = 0.9311360120773315
training loss = 1.784458875656128 10800
val loss = 0.9351624250411987
training loss = 1.7872272729873657 10900
val loss = 0.9932615756988525
training loss = 1.784614086151123 11000
val loss = 0.9374265670776367
training loss = 1.7846832275390625 11100
val loss = 0.9367647767066956
training loss = 1.8100824356079102 11200
val loss = 0.8014180064201355
training loss = 1.7848432064056396 11300
val loss = 0.9378273487091064
training loss = 1.7849154472351074 11400
val loss = 0.9381150603294373
training loss = 1.7863849401474 11500
val loss = 0.903839111328125
training loss = 1.7850724458694458 11600
val loss = 0.9400416612625122
training loss = 1.7851440906524658 11700
val loss = 0.9392606616020203
training loss = 1.7855896949768066 11800
val loss = 0.9224556684494019
training loss = 1.7853024005889893 11900
val loss = 0.9408676624298096
training loss = 1.785398244857788 12000
val loss = 0.9447004795074463
training loss = 1.7854622602462769 12100
val loss = 0.9437261819839478
training loss = 1.7855627536773682 12200
val loss = 0.9482344388961792
training loss = 1.7856804132461548 12300
val loss = 0.9519087076187134
training loss = 1.7856895923614502 12400
val loss = 0.9436227083206177
training loss = 1.7937952280044556 12500
val loss = 1.0467565059661865
training loss = 1.7858500480651855 12600
val loss = 0.9445407390594482
training loss = 1.7859182357788086 12700
val loss = 0.9449084997177124
training loss = 1.7862732410430908 12800
val loss = 0.9645748138427734
training loss = 1.786075472831726 12900
val loss = 0.9464093446731567
training loss = 1.8211464881896973 13000
val loss = 0.7901365756988525
training loss = 1.7862378358840942 13100
val loss = 0.948839545249939
training loss = 1.7863017320632935 13200
val loss = 0.9477272629737854
training loss = 1.7878397703170776 13300
val loss = 0.9900384545326233
training loss = 1.7864608764648438 13400
val loss = 0.9482890367507935
training loss = 1.7866876125335693 13500
val loss = 0.9623486995697021
training loss = 1.786693811416626 13600
val loss = 0.9416772127151489
training loss = 1.786685824394226 13700
val loss = 0.9498588442802429
training loss = 1.7884374856948853 13800
val loss = 0.9113762974739075
training loss = 1.786841630935669 13900
val loss = 0.9508815407752991
training loss = 1.7994844913482666 14000
val loss = 0.848827064037323
training loss = 1.786997675895691 14100
val loss = 0.9515445232391357
training loss = 1.7893095016479492 14200
val loss = 1.0044667720794678
training loss = 1.7871735095977783 14300
val loss = 0.9494091272354126
training loss = 1.7872189283370972 14400
val loss = 0.9533243179321289
training loss = 1.787489414215088 14500
val loss = 0.9421819448471069
training loss = 1.7873759269714355 14600
val loss = 0.9556609988212585
training loss = 1.7874418497085571 14700
val loss = 0.9543678164482117
reduced chi^2 level 2 = 1.787522792816162
Constrained alpha: 2.024921417236328
Constrained beta: 3.5021111965179443
Constrained gamma: 15.595277786254883
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 824.6537,  881.7198,  958.1255,  998.8274,  997.7479, 1026.9113,
        1140.2133, 1174.1115, 1105.2081, 1126.2362, 1249.8013, 1192.3414,
        1200.2833, 1262.3574, 1264.9078, 1381.5846, 1372.3427, 1453.8521,
        1576.1407, 1542.0634, 1580.5309, 1550.6287, 1602.6405, 1560.5366,
        1742.3998, 1774.7200, 1629.1249, 1743.5176, 1763.1918, 1726.4091,
        1709.3683, 1712.6602, 1720.6398, 1729.2960, 1804.6863, 1711.6270,
        1659.3311, 1587.4292, 1712.9131, 1718.0823, 1595.1886, 1557.3400,
        1456.5109, 1467.1179, 1399.7301, 1372.8928, 1302.4951, 1243.4733,
        1099.7010, 1208.3469, 1106.3409,  999.0640,  955.9680,  882.4211,
         846.1620,  884.0603,  843.3391,  712.7322,  623.5894,  525.0200,
         553.1602,  468.9543,  444.9190,  403.5863,  370.0498,  326.5090,
         308.3003,  230.9572,  202.2872,  169.1276,  163.7819,  151.4348,
         152.5385,  100.4765,   93.4568,   84.7701,   57.3369,   40.3981,
          43.7508,   34.3416,   19.5809,   43.0029,   37.7452])]
2819.4295535020874
4.6362893093862265 2.208241863171996 71.61866369936008
val isze = 8
idinces = [45 14 28  7 75 48  6 54 13 68 20 30 12 17 23 50 19 24  9 64 15 62  5 70
 59 33 80 53 38 51 74 16  8 40 35 72 22 21 32 61 73 49 26 44 11 55 29  2
 63  0 31 25 71 57 37 79 18 52 34  4 27 81 65 47 36  1 66 41 69 42 10 46
  3 43 58 67 78 60 76 39 82 77 56]
we are doing training validation split
training loss = 247.35174560546875 100
val loss = 199.7022705078125
training loss = 174.78912353515625 200
val loss = 148.62989807128906
training loss = 7.587483882904053 300
val loss = 4.808507919311523
training loss = 4.324995040893555 400
val loss = 3.8603713512420654
training loss = 3.26214599609375 500
val loss = 3.9795448780059814
training loss = 2.765237331390381 600
val loss = 4.14827823638916
training loss = 2.459826946258545 700
val loss = 4.127109527587891
training loss = 2.2590978145599365 800
val loss = 4.1761064529418945
training loss = 2.113351583480835 900
val loss = 4.2302350997924805
training loss = 2.0085561275482178 1000
val loss = 4.2468366622924805
training loss = 1.9452364444732666 1100
val loss = 4.291532516479492
training loss = 1.884111762046814 1200
val loss = 4.249798774719238
training loss = 1.8551042079925537 1300
val loss = 4.221324443817139
training loss = 1.8124687671661377 1400
val loss = 4.234928607940674
training loss = 1.790473222732544 1500
val loss = 4.25162410736084
training loss = 1.7658295631408691 1600
val loss = 4.214255332946777
training loss = 1.7475038766860962 1700
val loss = 4.209664344787598
training loss = 1.7333744764328003 1800
val loss = 4.212162017822266
training loss = 1.7199554443359375 1900
val loss = 4.200624942779541
training loss = 1.7825757265090942 2000
val loss = 4.21901273727417
training loss = 1.6998924016952515 2100
val loss = 4.1946516036987305
training loss = 1.6914360523223877 2200
val loss = 4.190977096557617
training loss = 1.6849201917648315 2300
val loss = 4.191794395446777
training loss = 1.6780831813812256 2400
val loss = 4.183217525482178
training loss = 1.6794490814208984 2500
val loss = 4.21221399307251
training loss = 1.6677954196929932 2600
val loss = 4.179253578186035
training loss = 1.8928394317626953 2700
val loss = 4.43326473236084
training loss = 1.6600695848464966 2800
val loss = 4.18011999130249
training loss = 1.6561423540115356 2900
val loss = 4.171401023864746
training loss = 1.6582798957824707 3000
val loss = 4.186099529266357
training loss = 1.6503491401672363 3100
val loss = 4.167413234710693
training loss = 2.2831320762634277 3200
val loss = 4.448805332183838
training loss = 1.6456422805786133 3300
val loss = 4.15910530090332
training loss = 1.643067479133606 3400
val loss = 4.158096790313721
training loss = 1.6421592235565186 3500
val loss = 4.155181884765625
training loss = 1.6392682790756226 3600
val loss = 4.14925479888916
training loss = 1.6408817768096924 3700
val loss = 4.138924598693848
training loss = 1.6360294818878174 3800
val loss = 4.144472122192383
training loss = 1.712968349456787 3900
val loss = 4.408536911010742
training loss = 1.6331895589828491 4000
val loss = 4.140442848205566
training loss = 1.67960786819458 4100
val loss = 4.348731994628906
training loss = 1.6307293176651 4200
val loss = 4.131340980529785
training loss = 1.6290466785430908 4300
val loss = 4.124022483825684
training loss = 1.633247971534729 4400
val loss = 4.107622146606445
training loss = 1.6267118453979492 4500
val loss = 4.118966102600098
training loss = 1.6253317594528198 4600
val loss = 4.104812145233154
training loss = 1.6247612237930298 4700
val loss = 4.107877254486084
training loss = 1.6230953931808472 4800
val loss = 4.099359512329102
training loss = 1.6454099416732788 4900
val loss = 4.202805042266846
training loss = 1.6210147142410278 5000
val loss = 4.089818477630615
training loss = 1.6199802160263062 5100
val loss = 4.0818891525268555
training loss = 1.6194287538528442 5200
val loss = 4.083784103393555
training loss = 1.618193507194519 5300
val loss = 4.072885513305664
training loss = 3.2821390628814697 5400
val loss = 6.842931747436523
training loss = 1.6167746782302856 5500
val loss = 4.066920280456543
reduced chi^2 level 2 = 1.6160916090011597
Constrained alpha: 3.541271924972534
Constrained beta: -0.10203535854816437
Constrained gamma: 47.171024322509766
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 887.2255,  818.4756,  943.6102,  986.7536, 1018.5709, 1062.7539,
        1105.7445, 1106.5875, 1134.5404, 1143.4336, 1138.2406, 1137.1084,
        1248.0815, 1284.6624, 1278.7004, 1500.5573, 1415.9923, 1422.0797,
        1507.4539, 1594.0746, 1663.5414, 1539.4871, 1565.7474, 1613.2317,
        1629.8008, 1674.4637, 1638.0439, 1707.2792, 1676.7642, 1712.0461,
        1648.1019, 1736.3951, 1670.8823, 1767.3887, 1770.2618, 1763.6598,
        1690.6959, 1634.3715, 1693.1219, 1576.1099, 1645.5450, 1616.7921,
        1543.4161, 1460.0095, 1366.9330, 1291.8910, 1269.9196, 1300.4811,
        1169.4733, 1224.8866, 1093.8738, 1017.9856,  932.1697,  935.1459,
         937.0616,  856.6085,  833.3713,  673.5370,  591.9317,  564.8413,
         510.8418,  450.2211,  448.5195,  412.7529,  391.9530,  333.6439,
         280.1856,  270.9661,  236.3122,  179.0417,  137.1391,  153.7603,
         163.3467,  100.1514,   91.0353,   70.3725,   44.1023,   39.0269,
          43.3881,   41.8723,   24.7608,   32.6863,   36.8902])]
2900.294699782793
3.1511147290324075 18.32244074369212 24.751465536865936
val isze = 8
idinces = [10 34  4  9 19 45  3 82 37 53  5 69 73 28  2 43 32 74 35 22 25 65 12 81
 39 14 80 40  8 49 27 20 42 33 61 26 16 52 50 17 11 44 59 29 63 30  1 18
 72 54 36 58 62 68 64 51 56  7 79 41 78 24 77 71 55 76 70  0 48 13 46 21
 23 31 15 75  6 47 38 57 67 60 66]
we are doing training validation split
training loss = 430.077392578125 100
val loss = 358.8126220703125
training loss = 44.3685302734375 200
val loss = 30.585872650146484
training loss = 9.552241325378418 300
val loss = 8.541142463684082
training loss = 9.21513557434082 400
val loss = 8.380940437316895
training loss = 8.865055084228516 500
val loss = 8.307406425476074
training loss = 8.518769264221191 600
val loss = 8.264030456542969
training loss = 8.19233226776123 700
val loss = 8.257915496826172
training loss = 7.898566246032715 800
val loss = 8.291559219360352
training loss = 7.645830154418945 900
val loss = 8.361812591552734
training loss = 7.437207221984863 1000
val loss = 8.460081100463867
training loss = 7.270674228668213 1100
val loss = 8.57404899597168
training loss = 7.140288352966309 1200
val loss = 8.690023422241211
training loss = 7.038095951080322 1300
val loss = 8.79601001739502
training loss = 6.956051826477051 1400
val loss = 8.88355541229248
training loss = 6.88737154006958 1500
val loss = 8.94858169555664
training loss = 6.827084541320801 1600
val loss = 8.990823745727539
training loss = 6.771958351135254 1700
val loss = 9.012653350830078
training loss = 6.720071315765381 1800
val loss = 9.017727851867676
training loss = 6.670390605926514 1900
val loss = 9.010024070739746
training loss = 6.622373104095459 2000
val loss = 8.9930419921875
training loss = 6.575739860534668 2100
val loss = 8.969684600830078
training loss = 6.53033971786499 2200
val loss = 8.942130088806152
training loss = 6.486061096191406 2300
val loss = 8.911943435668945
training loss = 6.442823886871338 2400
val loss = 8.88011360168457
training loss = 6.400559902191162 2500
val loss = 8.847362518310547
training loss = 6.359213352203369 2600
val loss = 8.814044952392578
training loss = 6.318748950958252 2700
val loss = 8.78048324584961
training loss = 6.27915620803833 2800
val loss = 8.746746063232422
training loss = 6.2404608726501465 2900
val loss = 8.713115692138672
training loss = 6.202727794647217 3000
val loss = 8.679595947265625
training loss = 6.16609001159668 3100
val loss = 8.646439552307129
training loss = 6.130728721618652 3200
val loss = 8.61380386352539
training loss = 6.096889972686768 3300
val loss = 8.58212661743164
training loss = 6.064874172210693 3400
val loss = 8.551668167114258
training loss = 6.034999370574951 3500
val loss = 8.523012161254883
training loss = 6.007550239562988 3600
val loss = 8.496511459350586
training loss = 5.982677459716797 3700
val loss = 8.47258186340332
training loss = 5.9602837562561035 3800
val loss = 8.451324462890625
training loss = 5.939761638641357 3900
val loss = 8.43238639831543
training loss = 5.9193644523620605 4000
val loss = 8.41440200805664
training loss = 5.893562316894531 4100
val loss = 8.39274787902832
training loss = 5.837268352508545 4200
val loss = 8.348636627197266
training loss = 5.595584869384766 4300
val loss = 8.18301010131836
training loss = 4.984282970428467 4400
val loss = 7.577183246612549
training loss = 3.936493396759033 4500
val loss = 6.301074981689453
training loss = 2.59834885597229 4600
val loss = 4.190479278564453
training loss = 2.1865108013153076 4700
val loss = 3.2016687393188477
training loss = 2.146751642227173 4800
val loss = 3.094644069671631
training loss = 2.124074697494507 4900
val loss = 3.070963144302368
training loss = 2.1141090393066406 5000
val loss = 3.068708896636963
training loss = 2.096454620361328 5100
val loss = 3.0505642890930176
training loss = 2.2735564708709717 5200
val loss = 3.2370080947875977
training loss = 2.0814640522003174 5300
val loss = 3.040377140045166
training loss = 2.0765016078948975 5400
val loss = 3.036076068878174
training loss = 2.0725533962249756 5500
val loss = 3.0329251289367676
training loss = 2.0694549083709717 5600
val loss = 3.0291194915771484
training loss = 2.0678539276123047 5700
val loss = 3.0312116146087646
training loss = 2.064744234085083 5800
val loss = 3.0225770473480225
training loss = 2.0630974769592285 5900
val loss = 3.017817735671997
training loss = 2.061415195465088 6000
val loss = 3.017408609390259
training loss = 2.0600438117980957 6100
val loss = 3.0130774974823
training loss = 2.0625486373901367 6200
val loss = 3.005094528198242
training loss = 2.0577564239501953 6300
val loss = 3.006955623626709
training loss = 2.0574841499328613 6400
val loss = 3.000431776046753
training loss = 2.055891275405884 6500
val loss = 3.0003442764282227
training loss = 2.05509614944458 6600
val loss = 2.9978082180023193
training loss = 2.0562734603881836 6700
val loss = 3.004535675048828
training loss = 2.053595781326294 6800
val loss = 2.9920928478240967
training loss = 2.052957773208618 6900
val loss = 2.9893264770507812
training loss = 2.0523791313171387 7000
val loss = 2.984812021255493
training loss = 2.0516891479492188 7100
val loss = 2.983611583709717
training loss = 2.0576059818267822 7200
val loss = 3.0023679733276367
training loss = 2.0505199432373047 7300
val loss = 2.978121519088745
training loss = 2.0500032901763916 7400
val loss = 2.9756760597229004
training loss = 2.0496273040771484 7500
val loss = 2.970895290374756
training loss = 2.0489299297332764 7600
val loss = 2.9705891609191895
training loss = 2.048459529876709 7700
val loss = 2.968472480773926
training loss = 2.0478897094726562 7800
val loss = 2.9655869007110596
training loss = 2.0474419593811035 7900
val loss = 2.963263511657715
training loss = 2.048344373703003 8000
val loss = 2.9560704231262207
training loss = 2.0464558601379395 8100
val loss = 2.958895683288574
training loss = 2.0460031032562256 8200
val loss = 2.9562506675720215
training loss = 2.050895929336548 8300
val loss = 2.9729442596435547
training loss = 2.045016050338745 8400
val loss = 2.9517569541931152
training loss = 2.3143160343170166 8500
val loss = 3.3628978729248047
training loss = 2.0439977645874023 8600
val loss = 2.9466123580932617
training loss = 2.0434911251068115 8700
val loss = 2.944810390472412
training loss = 2.042964458465576 8800
val loss = 2.944068193435669
training loss = 2.0423593521118164 8900
val loss = 2.94000506401062
training loss = 2.088918447494507 9000
val loss = 3.031604290008545
training loss = 2.0410876274108887 9100
val loss = 2.9345552921295166
training loss = 2.0404298305511475 9200
val loss = 2.9320807456970215
training loss = 2.087205171585083 9300
val loss = 2.9493021965026855
training loss = 2.03885555267334 9400
val loss = 2.9256224632263184
training loss = 2.037980079650879 9500
val loss = 2.9218461513519287
training loss = 2.037299156188965 9600
val loss = 2.921163558959961
training loss = 2.0359702110290527 9700
val loss = 2.912850856781006
training loss = 2.325758934020996 9800
val loss = 3.1571261882781982
training loss = 2.0335991382598877 9900
val loss = 2.901689052581787
training loss = 2.032294988632202 10000
val loss = 2.8941071033477783
training loss = 2.0360238552093506 10100
val loss = 2.904977798461914
training loss = 2.0294783115386963 10200
val loss = 2.8777644634246826
training loss = 2.027986764907837 10300
val loss = 2.8682339191436768
training loss = 2.0264649391174316 10400
val loss = 2.857558250427246
training loss = 2.0249485969543457 10500
val loss = 2.847632884979248
training loss = 2.2149405479431152 10600
val loss = 3.147573471069336
training loss = 2.021934986114502 10700
val loss = 2.8258707523345947
training loss = 2.0204617977142334 10800
val loss = 2.8138957023620605
training loss = 2.0193779468536377 10900
val loss = 2.8068466186523438
training loss = 2.0177502632141113 11000
val loss = 2.792538642883301
training loss = 2.024616241455078 11100
val loss = 2.8084659576416016
training loss = 2.015258312225342 11200
val loss = 2.7718095779418945
training loss = 2.0140960216522217 11300
val loss = 2.76312255859375
training loss = 2.021672487258911 11400
val loss = 2.7461838722229004
training loss = 2.012021064758301 11500
val loss = 2.747077465057373
training loss = 2.011084794998169 11600
val loss = 2.7396554946899414
training loss = 2.0456979274749756 11700
val loss = 2.737276792526245
training loss = 2.009338855743408 11800
val loss = 2.726858139038086
training loss = 2.008543014526367 11900
val loss = 2.7200236320495605
training loss = 2.008059024810791 12000
val loss = 2.7185721397399902
training loss = 2.0071616172790527 12100
val loss = 2.70974063873291
training loss = 2.0065479278564453 12200
val loss = 2.7047362327575684
training loss = 2.051398992538452 12300
val loss = 2.7986655235290527
training loss = 2.0054399967193604 12400
val loss = 2.6958107948303223
training loss = 2.004956007003784 12500
val loss = 2.6918694972991943
training loss = 2.0054354667663574 12600
val loss = 2.683244228363037
training loss = 2.0041122436523438 12700
val loss = 2.6847963333129883
training loss = 2.0037429332733154 12800
val loss = 2.6810808181762695
training loss = 2.0034022331237793 12900
val loss = 2.6781392097473145
training loss = 2.0031118392944336 13000
val loss = 2.6752920150756836
training loss = 2.0164971351623535 13100
val loss = 2.666151523590088
training loss = 2.0025956630706787 13200
val loss = 2.669428586959839
training loss = 2.002375841140747 13300
val loss = 2.667293071746826
training loss = 2.0027058124542236 13400
val loss = 2.6707310676574707
training loss = 2.002011775970459 13500
val loss = 2.662818431854248
training loss = 2.11855149269104 13600
val loss = 2.727011203765869
training loss = 2.001732349395752 13700
val loss = 2.659010410308838
training loss = 2.0016212463378906 13800
val loss = 2.6565611362457275
training loss = 2.0017874240875244 13900
val loss = 2.6589224338531494
training loss = 2.001453161239624 14000
val loss = 2.653449535369873
training loss = 2.0015881061553955 14100
val loss = 2.6547653675079346
training loss = 2.0013926029205322 14200
val loss = 2.652113437652588
training loss = 2.001307487487793 14300
val loss = 2.6488914489746094
training loss = 2.016380548477173 14400
val loss = 2.690534830093384
training loss = 2.0012712478637695 14500
val loss = 2.646529197692871
training loss = 2.0012784004211426 14600
val loss = 2.6454827785491943
training loss = 2.0012834072113037 14700
val loss = 2.644719123840332
training loss = 2.001300811767578 14800
val loss = 2.6430165767669678
training loss = 2.0057027339935303 14900
val loss = 2.660179615020752
training loss = 2.0013625621795654 15000
val loss = 2.641040325164795
training loss = 2.001718282699585 15100
val loss = 2.6440694332122803
training loss = 2.0014536380767822 15200
val loss = 2.6394383907318115
training loss = 2.0015158653259277 15300
val loss = 2.638707160949707
training loss = 2.0220425128936768 15400
val loss = 2.6918153762817383
training loss = 2.0016443729400635 15500
val loss = 2.637479066848755
training loss = 2.0017240047454834 15600
val loss = 2.6366944313049316
training loss = 2.0050554275512695 15700
val loss = 2.628861904144287
training loss = 2.0018820762634277 15800
val loss = 2.6358866691589355
training loss = 2.0019733905792236 15900
val loss = 2.6351799964904785
training loss = 2.00937819480896 16000
val loss = 2.6609623432159424
training loss = 2.0021581649780273 16100
val loss = 2.6344447135925293
training loss = 2.0022621154785156 16200
val loss = 2.6339941024780273
training loss = 2.0024218559265137 16300
val loss = 2.635652542114258
training loss = 2.0024688243865967 16400
val loss = 2.6334128379821777
training loss = 2.0965685844421387 16500
val loss = 2.6831247806549072
training loss = 2.0026872158050537 16600
val loss = 2.632776975631714
training loss = 2.0028045177459717 16700
val loss = 2.6325159072875977
training loss = 2.003002643585205 16800
val loss = 2.6344990730285645
training loss = 2.0030384063720703 16900
val loss = 2.6323909759521484
training loss = 2.110285520553589 17000
val loss = 2.8278656005859375
training loss = 2.0032737255096436 17100
val loss = 2.632319927215576
training loss = 2.003401756286621 17200
val loss = 2.6320533752441406
training loss = 2.0044260025024414 17300
val loss = 2.628957986831665
training loss = 2.003649950027466 17400
val loss = 2.6324610710144043
training loss = 2.0037777423858643 17500
val loss = 2.6318655014038086
training loss = 2.0098536014556885 17600
val loss = 2.6242713928222656
training loss = 2.0040299892425537 17700
val loss = 2.6319825649261475
training loss = 2.004163980484009 17800
val loss = 2.632028579711914
training loss = 2.0044798851013184 17900
val loss = 2.635037899017334
training loss = 2.004420518875122 18000
val loss = 2.632126808166504
training loss = 2.029749631881714 18100
val loss = 2.634758710861206
training loss = 2.00468373298645 18200
val loss = 2.632082462310791
training loss = 2.004815101623535 18300
val loss = 2.6323657035827637
training loss = 2.0051140785217285 18400
val loss = 2.6356959342956543
training loss = 2.005077838897705 18500
val loss = 2.6327226161956787
training loss = 2.0988292694091797 18600
val loss = 2.798720121383667
training loss = 2.0053470134735107 18700
val loss = 2.6326141357421875
training loss = 2.00547456741333 18800
val loss = 2.633176803588867
training loss = 2.0259392261505127 18900
val loss = 2.6295132637023926
training loss = 2.005734920501709 19000
val loss = 2.63376522064209
training loss = 2.0058703422546387 19100
val loss = 2.6337265968322754
training loss = 2.006371259689331 19200
val loss = 2.6380081176757812
training loss = 2.006129264831543 19300
val loss = 2.634260892868042
training loss = 2.4351885318756104 19400
val loss = 3.2539896965026855
training loss = 2.006397247314453 19500
val loss = 2.6354331970214844
training loss = 2.006519079208374 19600
val loss = 2.634897232055664
training loss = 2.0440852642059326 19700
val loss = 2.7159743309020996
training loss = 2.006772994995117 19800
val loss = 2.63539457321167
training loss = 2.0069010257720947 19900
val loss = 2.635685920715332
training loss = 2.1850152015686035 20000
val loss = 2.767026901245117
training loss = 2.0071616172790527 20100
val loss = 2.6370012760162354
training loss = 2.007279396057129 20200
val loss = 2.6364989280700684
training loss = 2.0865304470062256 20300
val loss = 2.679964780807495
training loss = 2.0075290203094482 20400
val loss = 2.636913776397705
training loss = 2.007652521133423 20500
val loss = 2.6373565196990967
training loss = 2.007848024368286 20600
val loss = 2.636258602142334
training loss = 2.0078954696655273 20700
val loss = 2.6379153728485107
training loss = 2.008229970932007 20800
val loss = 2.6352829933166504
training loss = 2.00813889503479 20900
val loss = 2.638706684112549
training loss = 2.0084450244903564 21000
val loss = 2.6412196159362793
training loss = 2.0084307193756104 21100
val loss = 2.640707492828369
training loss = 2.0084986686706543 21200
val loss = 2.639448404312134
training loss = 2.0086116790771484 21300
val loss = 2.6396212577819824
training loss = 2.008730888366699 21400
val loss = 2.640073776245117
training loss = 2.0094985961914062 21500
val loss = 2.6453046798706055
training loss = 2.00897479057312 21600
val loss = 2.640282154083252
training loss = 2.0090768337249756 21700
val loss = 2.641031265258789
training loss = 2.0093977451324463 21800
val loss = 2.64345645904541
training loss = 2.0093002319335938 21900
val loss = 2.6417112350463867
training loss = 2.0094566345214844 22000
val loss = 2.6407716274261475
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 873.6436,  887.4630, 1002.8940,  966.0713, 1058.8112, 1057.6763,
        1044.3456, 1132.2577, 1132.4545, 1213.9408, 1186.5393, 1210.0383,
        1297.0588, 1361.6984, 1327.1842, 1404.4644, 1370.4984, 1421.4349,
        1528.8168, 1548.7657, 1633.1243, 1521.6450, 1601.4709, 1624.9698,
        1643.7427, 1781.3400, 1674.0420, 1695.5930, 1778.3575, 1647.4558,
        1656.5959, 1703.0565, 1658.5521, 1801.9160, 1641.0319, 1730.7231,
        1698.7693, 1614.2485, 1670.9739, 1639.2012, 1604.3833, 1587.6564,
        1514.5723, 1573.5398, 1373.3978, 1312.9365, 1298.5093, 1262.6019,
        1133.9762, 1178.2518, 1092.8611,  986.4808,  993.7268,  900.9395,
         887.4153,  872.7897,  792.7183,  705.6199,  606.7411,  581.8617,
         496.4096,  492.4658,  437.3902,  391.7134,  336.9011,  352.5290,
         289.0836,  244.0344,  201.5347,  176.4716,  166.5584,  151.1533,
         149.8137,  107.6569,   83.8871,   56.2047,   49.5459,   45.9128,
          23.5453,   38.0131,   21.5411,   39.8056,   37.2490])]
2544.8302084129127
3.6755020966623086 16.62543853650938 55.611280179872736
val isze = 8
idinces = [36 17 21  4 12 22 74 44 33 24 28 64 11 20  5 57 59 71 40 60 61 25 37 35
 30 80 42  3 70 66 50 29 63  7 15 47 19 62 78 77 46 54 56 69 52 16 49 23
  9 14  8 53 81  2 48 76 31 18  0 68 58  6 73 67 34 32  1 27 45 41 79 72
 75 65 43 82 55 10 51 26 13 39 38]
we are doing training validation split
training loss = 438.2636413574219 100
val loss = 306.92083740234375
training loss = 9.105799674987793 200
val loss = 5.3762617111206055
training loss = 7.586271286010742 300
val loss = 4.556949138641357
training loss = 7.374914646148682 400
val loss = 4.3057756423950195
training loss = 7.173828125 500
val loss = 4.053928852081299
training loss = 6.993575096130371 600
val loss = 3.815829277038574
training loss = 6.839312553405762 700
val loss = 3.600457191467285
training loss = 6.711475372314453 800
val loss = 3.4124999046325684
training loss = 6.607173442840576 900
val loss = 3.25296688079834
training loss = 6.521803379058838 1000
val loss = 3.1201415061950684
training loss = 6.4505157470703125 1100
val loss = 3.010690450668335
training loss = 6.389132022857666 1200
val loss = 2.9207026958465576
training loss = 6.334544658660889 1300
val loss = 2.8463294506073
training loss = 6.2846832275390625 1400
val loss = 2.7841756343841553
training loss = 6.238282203674316 1500
val loss = 2.7314558029174805
training loss = 6.194606304168701 1600
val loss = 2.685911178588867
training loss = 6.153223037719727 1700
val loss = 2.64585542678833
training loss = 6.113865852355957 1800
val loss = 2.609980583190918
training loss = 6.076345443725586 1900
val loss = 2.5773274898529053
training loss = 6.040496349334717 2000
val loss = 2.5472123622894287
training loss = 6.006178379058838 2100
val loss = 2.5191171169281006
training loss = 5.973255157470703 2200
val loss = 2.492694854736328
training loss = 5.941594123840332 2300
val loss = 2.4676666259765625
training loss = 5.911067008972168 2400
val loss = 2.4438395500183105
training loss = 5.881560325622559 2500
val loss = 2.4210541248321533
training loss = 5.8529558181762695 2600
val loss = 2.3991758823394775
training loss = 5.82515287399292 2700
val loss = 2.3780832290649414
training loss = 5.798050880432129 2800
val loss = 2.357705593109131
training loss = 5.771554470062256 2900
val loss = 2.3379464149475098
training loss = 5.7455830574035645 3000
val loss = 2.3187148571014404
training loss = 5.720054626464844 3100
val loss = 2.2999637126922607
training loss = 5.694907188415527 3200
val loss = 2.281602144241333
training loss = 5.670074939727783 3300
val loss = 2.263608694076538
training loss = 5.645505428314209 3400
val loss = 2.2459094524383545
training loss = 5.621130466461182 3500
val loss = 2.2284512519836426
training loss = 5.596863746643066 3600
val loss = 2.2111785411834717
training loss = 5.572517395019531 3700
val loss = 2.193932294845581
training loss = 5.547657012939453 3800
val loss = 2.1763954162597656
training loss = 5.534839153289795 3900
val loss = 2.200047254562378
training loss = 5.49075984954834 4000
val loss = 2.135934352874756
training loss = 5.449931621551514 4100
val loss = 2.121347427368164
training loss = 5.356855392456055 4200
val loss = 2.0373849868774414
training loss = 5.162775993347168 4300
val loss = 1.8845648765563965
training loss = 4.806414604187012 4400
val loss = 1.6710295677185059
training loss = 4.155506610870361 4500
val loss = 1.370722770690918
training loss = 2.8145203590393066 4600
val loss = 0.7263425588607788
training loss = 1.9124727249145508 4700
val loss = 0.653622567653656
training loss = 1.8523204326629639 4800
val loss = 0.7547715902328491
training loss = 1.8365650177001953 4900
val loss = 0.7672051191329956
training loss = 1.8234827518463135 5000
val loss = 0.7815850973129272
training loss = 1.8084592819213867 5100
val loss = 0.7667020559310913
training loss = 2.1223390102386475 5200
val loss = 0.8458859920501709
training loss = 1.7696863412857056 5300
val loss = 0.7437589764595032
training loss = 1.7494332790374756 5400
val loss = 0.733478844165802
training loss = 1.7338227033615112 5500
val loss = 0.7218531370162964
training loss = 1.7221639156341553 5600
val loss = 0.7250584959983826
training loss = 1.7251808643341064 5700
val loss = 0.6746426224708557
training loss = 1.7055747509002686 5800
val loss = 0.7263012528419495
training loss = 1.698728322982788 5900
val loss = 0.7269662618637085
training loss = 1.6929490566253662 6000
val loss = 0.7382741570472717
training loss = 1.6870912313461304 6100
val loss = 0.7276791930198669
training loss = 1.7099528312683105 6200
val loss = 0.66185063123703
training loss = 1.6774322986602783 6300
val loss = 0.7275209426879883
training loss = 1.673105001449585 6400
val loss = 0.727453351020813
training loss = 1.6716398000717163 6500
val loss = 0.7615407705307007
training loss = 1.6656208038330078 6600
val loss = 0.7271531820297241
training loss = 1.662265419960022 6700
val loss = 0.7307870388031006
training loss = 1.6593430042266846 6800
val loss = 0.7209228873252869
training loss = 1.6564260721206665 6900
val loss = 0.7258057594299316
training loss = 1.6565375328063965 7000
val loss = 0.7611412405967712
training loss = 1.651623010635376 7100
val loss = 0.7249574661254883
training loss = 1.7180674076080322 7200
val loss = 0.6514123678207397
training loss = 1.6476815938949585 7300
val loss = 0.7242834568023682
training loss = 1.645931363105774 7400
val loss = 0.7236387133598328
training loss = 1.652936577796936 7500
val loss = 0.7924643754959106
training loss = 1.6431150436401367 7600
val loss = 0.7233186960220337
training loss = 1.641850233078003 7700
val loss = 0.7224121689796448
training loss = 1.6427675485610962 7800
val loss = 0.6976946592330933
training loss = 1.6398645639419556 7900
val loss = 0.721581757068634
training loss = 1.667583703994751 8000
val loss = 0.653093159198761
training loss = 1.6383936405181885 8100
val loss = 0.718245804309845
training loss = 1.6376785039901733 8200
val loss = 0.7207801342010498
training loss = 1.737837791442871 8300
val loss = 1.0580328702926636
training loss = 1.6366804838180542 8400
val loss = 0.7205409407615662
training loss = 1.6362297534942627 8500
val loss = 0.7202346920967102
training loss = 1.6360729932785034 8600
val loss = 0.7139904499053955
training loss = 1.6356252431869507 8700
val loss = 0.7194477915763855
training loss = 1.7059727907180786 8800
val loss = 0.6460294723510742
training loss = 1.6352142095565796 8900
val loss = 0.7214069366455078
training loss = 1.6349927186965942 9000
val loss = 0.7190268039703369
training loss = 1.6349889039993286 9100
val loss = 0.7233139276504517
training loss = 1.6347744464874268 9200
val loss = 0.718320369720459
training loss = 1.6710915565490723 9300
val loss = 0.8876473903656006
training loss = 1.6346579790115356 9400
val loss = 0.7174040079116821
training loss = 1.6346642971038818 9500
val loss = 0.7122364044189453
training loss = 1.6346914768218994 9600
val loss = 0.7129213809967041
reduced chi^2 level 2 = 1.6345831155776978
Constrained alpha: 1.8905141353607178
Constrained beta: 3.5712902545928955
Constrained gamma: 19.33489990234375
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 910.4622,  845.9632,  954.5142,  934.8283,  941.9420, 1047.4835,
        1085.3895, 1071.7799, 1170.2437, 1196.6279, 1231.3450, 1246.2753,
        1250.2823, 1246.2701, 1372.5957, 1402.2723, 1437.6411, 1439.3816,
        1491.2706, 1486.2675, 1599.1453, 1577.2633, 1728.9257, 1578.4458,
        1649.5801, 1730.1305, 1642.5320, 1752.2445, 1708.4644, 1753.9647,
        1717.9871, 1690.2472, 1681.3903, 1643.0179, 1675.9655, 1674.7726,
        1635.7573, 1619.8809, 1652.5952, 1584.5771, 1571.7356, 1585.1691,
        1426.2574, 1490.6810, 1378.2614, 1321.4657, 1340.9188, 1205.5736,
        1128.1874, 1151.3195, 1061.9526, 1037.4025,  884.6944,  901.8402,
         935.4193,  880.2825,  794.5689,  687.5038,  607.0670,  504.6361,
         587.6507,  485.3235,  452.8250,  386.7549,  348.1526,  339.5565,
         265.6228,  294.1488,  175.5136,  178.4419,  153.4606,  157.9193,
         143.7503,  109.7905,   92.9166,   59.1922,   39.6576,   32.5320,
          41.0899,   41.6661,   19.0949,   26.8647,   40.5759])]
2459.42804415861
2.7842963728323626 18.339450486732773 27.594868313510425
val isze = 8
idinces = [26 51 27 23 70 19 47 24 72 71 53 46 35  5 14 37 78 41 64 45  7 66 25 40
 63 76 34  1 17 32 12 59 28 16 77 33 18  2 42 58 62 65 29 10 54  8 13 57
 55 15 22 68 60 43  3 52 81 67 31 44  6 75 39  0  9 82 21 61 48 80 20 73
 11 49  4 30 36 74 69 38 50 79 56]
we are doing training validation split
training loss = 399.2196960449219 100
val loss = 380.48760986328125
training loss = 42.741249084472656 200
val loss = 20.378435134887695
training loss = 9.312969207763672 300
val loss = 6.217388153076172
training loss = 8.94499397277832 400
val loss = 5.883166790008545
training loss = 8.570083618164062 500
val loss = 5.594269275665283
training loss = 8.208353042602539 600
val loss = 5.304823398590088
training loss = 7.877415657043457 700
val loss = 5.0282440185546875
training loss = 7.589768886566162 800
val loss = 4.776001930236816
training loss = 7.35156774520874 900
val loss = 4.555992126464844
training loss = 7.162309646606445 1000
val loss = 4.371543884277344
training loss = 7.016040325164795 1100
val loss = 4.221562385559082
training loss = 6.903680324554443 1200
val loss = 4.1015543937683105
training loss = 6.815638542175293 1300
val loss = 4.005412578582764
training loss = 6.743715763092041 1400
val loss = 3.927117109298706
training loss = 6.681912422180176 1500
val loss = 3.861525535583496
training loss = 6.626322269439697 1600
val loss = 3.804905891418457
training loss = 6.574642658233643 1700
val loss = 3.7546913623809814
training loss = 6.525604724884033 1800
val loss = 3.709165573120117
training loss = 6.478537082672119 1900
val loss = 3.6672277450561523
training loss = 6.433081150054932 2000
val loss = 3.6280722618103027
training loss = 6.389012813568115 2100
val loss = 3.5912842750549316
training loss = 6.346181869506836 2200
val loss = 3.556516170501709
training loss = 6.304468631744385 2300
val loss = 3.523484230041504
training loss = 6.263768672943115 2400
val loss = 3.491943359375
training loss = 6.2239789962768555 2500
val loss = 3.4618771076202393
training loss = 6.185031890869141 2600
val loss = 3.433063507080078
training loss = 6.146873950958252 2700
val loss = 3.4054641723632812
training loss = 6.109482765197754 2800
val loss = 3.3790671825408936
training loss = 6.072873592376709 2900
val loss = 3.353860855102539
training loss = 6.037109375 3000
val loss = 3.3297791481018066
training loss = 6.002309799194336 3100
val loss = 3.3070406913757324
training loss = 5.96865177154541 3200
val loss = 3.2856862545013428
training loss = 5.936339855194092 3300
val loss = 3.2658090591430664
training loss = 5.905586242675781 3400
val loss = 3.2475802898406982
training loss = 5.876489162445068 3500
val loss = 3.2308225631713867
training loss = 5.848812103271484 3600
val loss = 3.2155585289001465
training loss = 5.821400165557861 3700
val loss = 3.200892210006714
training loss = 5.7903242111206055 3800
val loss = 3.1844425201416016
training loss = 5.739589214324951 3900
val loss = 3.1570122241973877
training loss = 5.572214126586914 4000
val loss = 3.061877727508545
training loss = 5.038003921508789 4100
val loss = 2.735215663909912
training loss = 4.114933967590332 4200
val loss = 2.244295120239258
training loss = 2.9251151084899902 4300
val loss = 1.7187883853912354
training loss = 2.382014036178589 4400
val loss = 1.4840972423553467
training loss = 2.3206377029418945 4500
val loss = 1.4244438409805298
training loss = 2.300405263900757 4600
val loss = 1.3961409330368042
training loss = 2.286311626434326 4700
val loss = 1.3768377304077148
training loss = 2.2753725051879883 4800
val loss = 1.3603956699371338
training loss = 2.2674827575683594 4900
val loss = 1.3482450246810913
training loss = 2.2599658966064453 5000
val loss = 1.3374760150909424
training loss = 2.2573585510253906 5100
val loss = 1.334513783454895
training loss = 2.2496001720428467 5200
val loss = 1.3218024969100952
training loss = 2.24655818939209 5300
val loss = 1.3158851861953735
training loss = 2.242008924484253 5400
val loss = 1.3103692531585693
training loss = 2.2389302253723145 5500
val loss = 1.3057010173797607
training loss = 2.2378907203674316 5600
val loss = 1.3066309690475464
training loss = 2.2335050106048584 5700
val loss = 1.2976312637329102
training loss = 2.23111891746521 5800
val loss = 1.2941536903381348
training loss = 2.228914737701416 5900
val loss = 1.2916760444641113
training loss = 2.2267868518829346 6000
val loss = 1.2880024909973145
training loss = 2.234466314315796 6100
val loss = 1.3058693408966064
training loss = 2.2228312492370605 6200
val loss = 1.282060146331787
training loss = 2.2209761142730713 6300
val loss = 1.2799218893051147
training loss = 2.2238268852233887 6400
val loss = 1.2797703742980957
training loss = 2.217320203781128 6500
val loss = 1.2750451564788818
training loss = 2.215555191040039 6600
val loss = 1.2726504802703857
training loss = 2.2137696743011475 6700
val loss = 1.2699127197265625
training loss = 2.2120161056518555 6800
val loss = 1.2679251432418823
training loss = 2.2244277000427246 6900
val loss = 1.2727919816970825
training loss = 2.2084801197052 7000
val loss = 1.2633860111236572
training loss = 2.2067205905914307 7100
val loss = 1.2610594034194946
training loss = 2.2049543857574463 7200
val loss = 1.2579642534255981
training loss = 2.203141450881958 7300
val loss = 1.256385087966919
training loss = 2.2013351917266846 7400
val loss = 1.2541253566741943
training loss = 2.1994409561157227 7500
val loss = 1.2515186071395874
training loss = 2.1976253986358643 7600
val loss = 1.24929940700531
training loss = 2.1993863582611084 7700
val loss = 1.2603694200515747
training loss = 2.1938366889953613 7800
val loss = 1.2441915273666382
training loss = 2.191920518875122 7900
val loss = 1.2419943809509277
training loss = 2.190887212753296 8000
val loss = 1.237105369567871
training loss = 2.1880545616149902 8100
val loss = 1.2369418144226074
training loss = 2.532296657562256 8200
val loss = 1.6314611434936523
training loss = 2.184126377105713 8300
val loss = 1.2319830656051636
training loss = 2.1821680068969727 8400
val loss = 1.2292382717132568
training loss = 2.1816036701202393 8500
val loss = 1.2328095436096191
training loss = 2.1782214641571045 8600
val loss = 1.2239941358566284
training loss = 2.176225185394287 8700
val loss = 1.221410870552063
training loss = 2.1751039028167725 8800
val loss = 1.2160664796829224
training loss = 2.1722466945648193 8900
val loss = 1.2161076068878174
training loss = 2.172182321548462 9000
val loss = 1.2109330892562866
training loss = 2.1682496070861816 9100
val loss = 1.2108795642852783
training loss = 2.166233777999878 9200
val loss = 1.2083299160003662
training loss = 2.1643664836883545 9300
val loss = 1.2069005966186523
training loss = 2.162278652191162 9400
val loss = 1.2026127576828003
training loss = 2.167306661605835 9500
val loss = 1.1982536315917969
training loss = 2.158369541168213 9600
val loss = 1.1971994638442993
training loss = 2.156384229660034 9700
val loss = 1.19448983669281
training loss = 2.1545181274414062 9800
val loss = 1.1909877061843872
training loss = 2.152569055557251 9900
val loss = 1.1892800331115723
training loss = 2.284532070159912 10000
val loss = 1.3154412508010864
training loss = 2.148754596710205 10100
val loss = 1.1843839883804321
training loss = 2.1468493938446045 10200
val loss = 1.1814073324203491
training loss = 2.1456902027130127 10300
val loss = 1.1758008003234863
training loss = 2.143136501312256 10400
val loss = 1.1761369705200195
training loss = 2.1511588096618652 10500
val loss = 1.1723742485046387
training loss = 2.1394858360290527 10600
val loss = 1.1709421873092651
training loss = 2.139119863510132 10700
val loss = 1.1764642000198364
training loss = 2.135918617248535 10800
val loss = 1.1660664081573486
training loss = 2.134181261062622 10900
val loss = 1.1635284423828125
training loss = 2.152662754058838 11000
val loss = 1.1658356189727783
training loss = 2.1307473182678223 11100
val loss = 1.1587638854980469
training loss = 2.1315255165100098 11200
val loss = 1.1677820682525635
training loss = 2.1274096965789795 11300
val loss = 1.1546512842178345
training loss = 2.1257402896881104 11400
val loss = 1.1514441967010498
training loss = 2.125415802001953 11500
val loss = 1.1567991971969604
training loss = 2.1225624084472656 11600
val loss = 1.1469428539276123
training loss = 2.192620038986206 11700
val loss = 1.1940903663635254
training loss = 2.1194746494293213 11800
val loss = 1.1430635452270508
training loss = 2.1179544925689697 11900
val loss = 1.1402405500411987
training loss = 2.1164376735687256 12000
val loss = 1.1385725736618042
training loss = 2.1150062084198 12100
val loss = 1.1361076831817627
training loss = 2.1135356426239014 12200
val loss = 1.1343411207199097
training loss = 2.112262487411499 12300
val loss = 1.1299902200698853
training loss = 2.110743522644043 12400
val loss = 1.1297111511230469
training loss = 2.116834878921509 12500
val loss = 1.1533095836639404
training loss = 2.1080172061920166 12600
val loss = 1.1256853342056274
training loss = 2.110826015472412 12700
val loss = 1.1414015293121338
training loss = 2.105393886566162 12800
val loss = 1.1225674152374268
training loss = 2.104105234146118 12900
val loss = 1.119991660118103
training loss = 2.103034257888794 13000
val loss = 1.1152902841567993
training loss = 2.10160231590271 13100
val loss = 1.1163172721862793
training loss = 2.100975275039673 13200
val loss = 1.1105303764343262
training loss = 2.09920334815979 13300
val loss = 1.1116851568222046
training loss = 2.097975015640259 13400
val loss = 1.1109501123428345
training loss = 2.0991177558898926 13500
val loss = 1.1218361854553223
training loss = 2.095672369003296 13600
val loss = 1.1075421571731567
training loss = 2.1054646968841553 13700
val loss = 1.1411411762237549
training loss = 2.093416690826416 13800
val loss = 1.1038403511047363
training loss = 2.0923049449920654 13900
val loss = 1.1024165153503418
training loss = 2.0916247367858887 14000
val loss = 1.1053133010864258
training loss = 2.0901379585266113 14100
val loss = 1.0991142988204956
training loss = 2.092365264892578 14200
val loss = 1.113726019859314
training loss = 2.0879788398742676 14300
val loss = 1.0960841178894043
training loss = 2.086892604827881 14400
val loss = 1.0940582752227783
training loss = 2.086021661758423 14500
val loss = 1.0957422256469727
training loss = 2.0847690105438232 14600
val loss = 1.0908868312835693
training loss = 2.190307378768921 14700
val loss = 1.1637331247329712
training loss = 2.082620859146118 14800
val loss = 1.087109923362732
training loss = 2.081505298614502 14900
val loss = 1.0855931043624878
training loss = 2.091074228286743 15000
val loss = 1.119797706604004
training loss = 2.0791854858398438 15100
val loss = 1.0815738439559937
training loss = 2.07802414894104 15200
val loss = 1.0820579528808594
training loss = 2.0766446590423584 15300
val loss = 1.0782281160354614
training loss = 2.0752222537994385 15400
val loss = 1.074719786643982
training loss = 2.0931763648986816 15500
val loss = 1.0670137405395508
training loss = 2.072021961212158 15600
val loss = 1.0684425830841064
training loss = 2.0701067447662354 15700
val loss = 1.0649421215057373
training loss = 2.0679709911346436 15800
val loss = 1.0627119541168213
training loss = 2.0653908252716064 15900
val loss = 1.0554308891296387
training loss = 2.212618112564087 16000
val loss = 1.3335986137390137
training loss = 2.0591728687286377 16100
val loss = 1.042429804801941
training loss = 2.055473566055298 16200
val loss = 1.0348516702651978
training loss = 2.063014268875122 16300
val loss = 1.0181514024734497
training loss = 2.0475902557373047 16400
val loss = 1.0192714929580688
training loss = 2.043597459793091 16500
val loss = 1.01164972782135
training loss = 2.0414071083068848 16600
val loss = 1.0167980194091797
training loss = 2.0361406803131104 16700
val loss = 0.9987165927886963
training loss = 2.0324745178222656 16800
val loss = 0.9928041696548462
training loss = 2.0294151306152344 16900
val loss = 0.9827914237976074
training loss = 2.0256237983703613 17000
val loss = 0.9817276000976562
training loss = 2.0221211910247803 17100
val loss = 0.9760186672210693
training loss = 2.019526720046997 17200
val loss = 0.9641079902648926
training loss = 2.0154073238372803 17300
val loss = 0.9648234844207764
training loss = 2.0125513076782227 17400
val loss = 0.9528003931045532
training loss = 2.0087549686431885 17500
val loss = 0.9549328684806824
training loss = 2.0054192543029785 17600
val loss = 0.947315514087677
training loss = 2.011708974838257 17700
val loss = 0.9831330180168152
training loss = 1.9990501403808594 17800
val loss = 0.9356006979942322
training loss = 1.9958765506744385 17900
val loss = 0.9297699928283691
training loss = 1.9941174983978271 18000
val loss = 0.9154709577560425
training loss = 1.9900270700454712 18100
val loss = 0.9187670946121216
training loss = 2.0078442096710205 18200
val loss = 0.8958284854888916
training loss = 1.984531283378601 18300
val loss = 0.9094953536987305
training loss = 1.9818681478500366 18400
val loss = 0.9027279019355774
training loss = 1.9798450469970703 18500
val loss = 0.9046105146408081
training loss = 1.9771419763565063 18600
val loss = 0.8930438756942749
training loss = 2.034005641937256 18700
val loss = 1.044860601425171
training loss = 1.972908616065979 18800
val loss = 0.8843295574188232
training loss = 1.9708936214447021 18900
val loss = 0.879849910736084
training loss = 1.9694124460220337 19000
val loss = 0.8715440034866333
training loss = 1.9674278497695923 19100
val loss = 0.8726167678833008
training loss = 1.9732890129089355 19200
val loss = 0.8503447771072388
training loss = 1.9643821716308594 19300
val loss = 0.8663842678070068
training loss = 1.9629253149032593 19400
val loss = 0.8628149628639221
training loss = 1.9622522592544556 19500
val loss = 0.8688722252845764
training loss = 1.9604287147521973 19600
val loss = 0.8571553826332092
training loss = 2.060157060623169 19700
val loss = 0.8806401491165161
training loss = 1.9582219123840332 19800
val loss = 0.8529868721961975
training loss = 1.957140326499939 19900
val loss = 0.8498705625534058
training loss = 1.9569519758224487 20000
val loss = 0.8400710821151733
training loss = 1.9553251266479492 20100
val loss = 0.8458842635154724
training loss = 1.954397201538086 20200
val loss = 0.8429997563362122
training loss = 1.9537937641143799 20300
val loss = 0.8385874629020691
training loss = 1.9528357982635498 20400
val loss = 0.8404611349105835
training loss = 2.1299753189086914 20500
val loss = 0.9276515245437622
training loss = 1.951399803161621 20600
val loss = 0.8368874788284302
training loss = 1.9506535530090332 20700
val loss = 0.8358491659164429
training loss = 1.9509851932525635 20800
val loss = 0.8464853763580322
training loss = 1.949392557144165 20900
val loss = 0.8333536982536316
training loss = 1.948714256286621 21000
val loss = 0.8322219252586365
training loss = 1.9484562873840332 21100
val loss = 0.8261082172393799
training loss = 1.947595477104187 21200
val loss = 0.8297814726829529
training loss = 2.2460339069366455 21300
val loss = 1.403395175933838
training loss = 1.9465357065200806 21400
val loss = 0.8266453742980957
training loss = 1.9459477663040161 21500
val loss = 0.8266682624816895
training loss = 1.9473003149032593 21600
val loss = 0.8139700889587402
training loss = 1.9449615478515625 21700
val loss = 0.8251320719718933
training loss = 2.031604051589966 21800
val loss = 1.038328766822815
training loss = 1.9440479278564453 21900
val loss = 0.8229814767837524
training loss = 1.943540096282959 22000
val loss = 0.8226571083068848
training loss = 1.9435299634933472 22100
val loss = 0.816281795501709
training loss = 1.9427109956741333 22200
val loss = 0.8212457895278931
training loss = 2.198786973953247 22300
val loss = 0.9767917990684509
training loss = 1.9419209957122803 22400
val loss = 0.8201221227645874
training loss = 1.9414783716201782 22500
val loss = 0.8193607330322266
training loss = 1.9437317848205566 22600
val loss = 0.8400323390960693
training loss = 1.9407434463500977 22700
val loss = 0.81836998462677
training loss = 1.9605789184570312 22800
val loss = 0.8929581642150879
training loss = 1.940056562423706 22900
val loss = 0.8163825869560242
training loss = 1.9396507740020752 23000
val loss = 0.8167113065719604
training loss = 1.9433915615081787 23100
val loss = 0.843843400478363
training loss = 1.9390445947647095 23200
val loss = 0.8158912062644958
training loss = 1.93867027759552 23300
val loss = 0.8154218196868896
training loss = 1.9404691457748413 23400
val loss = 0.833861231803894
training loss = 1.9380886554718018 23500
val loss = 0.8146739602088928
training loss = 1.9377334117889404 23600
val loss = 0.8138402104377747
training loss = 1.9377766847610474 23700
val loss = 0.8091343641281128
training loss = 1.937203049659729 23800
val loss = 0.8137122392654419
training loss = 1.9606094360351562 23900
val loss = 0.897976815700531
training loss = 1.936649203300476 24000
val loss = 0.8134636878967285
training loss = 1.9363224506378174 24100
val loss = 0.8130028247833252
training loss = 1.9364769458770752 24200
val loss = 0.8066411018371582
training loss = 1.935807704925537 24300
val loss = 0.8123067617416382
training loss = 1.9667432308197021 24400
val loss = 0.7902196049690247
training loss = 1.9353121519088745 24500
val loss = 0.8118126392364502
training loss = 1.9350179433822632 24600
val loss = 0.8131860494613647
training loss = 1.9349677562713623 24700
val loss = 0.8077743649482727
training loss = 1.9345282316207886 24800
val loss = 0.8113914728164673
training loss = 1.9405615329742432 24900
val loss = 0.7921184301376343
training loss = 1.9340733289718628 25000
val loss = 0.8116757869720459
training loss = 1.93377685546875 25100
val loss = 0.8111704587936401
training loss = 1.934024691581726 25200
val loss = 0.8192227482795715
training loss = 1.933323621749878 25300
val loss = 0.8110446333885193
training loss = 2.063906192779541 25400
val loss = 0.8470784425735474
training loss = 1.9328701496124268 25500
val loss = 0.8105036020278931
training loss = 1.932578206062317 25600
val loss = 0.8109039068222046
training loss = 1.9330270290374756 25700
val loss = 0.8209813833236694
training loss = 1.9321314096450806 25800
val loss = 0.8108742833137512
training loss = 2.440736770629883 25900
val loss = 1.1983404159545898
training loss = 1.931715726852417 26000
val loss = 0.8096623420715332
training loss = 1.9314146041870117 26100
val loss = 0.8109993934631348
training loss = 2.0095434188842773 26200
val loss = 0.8100689649581909
training loss = 1.9309898614883423 26300
val loss = 0.8107481002807617
training loss = 1.930705189704895 26400
val loss = 0.8112320899963379
training loss = 1.9328292608261108 26500
val loss = 0.8323684334754944
training loss = 1.9302462339401245 26600
val loss = 0.811738133430481
training loss = 1.9388513565063477 26700
val loss = 0.789241373538971
training loss = 1.9297986030578613 26800
val loss = 0.8123505115509033
training loss = 1.9343079328536987 26900
val loss = 0.7927491664886475
training loss = 1.9293949604034424 27000
val loss = 0.8142921328544617
training loss = 1.929099202156067 27100
val loss = 0.8123284578323364
training loss = 1.9365216493606567 27200
val loss = 0.7906803488731384
training loss = 1.9286969900131226 27300
val loss = 0.8127413392066956
training loss = 1.928433895111084 27400
val loss = 0.8129099607467651
training loss = 1.9288309812545776 27500
val loss = 0.8058465719223022
training loss = 1.9280762672424316 27600
val loss = 0.8134152293205261
training loss = 1.927827000617981 27700
val loss = 0.8136977553367615
training loss = 1.9282536506652832 27800
val loss = 0.8232749700546265
training loss = 1.927484154701233 27900
val loss = 0.8140089511871338
training loss = 1.9273995161056519 28000
val loss = 0.8189733624458313
training loss = 1.9271786212921143 28100
val loss = 0.8160332441329956
training loss = 1.9269429445266724 28200
val loss = 0.8145861625671387
training loss = 1.9433374404907227 28300
val loss = 0.8838695883750916
training loss = 1.9266541004180908 28400
val loss = 0.8140221834182739
training loss = 1.9264392852783203 28500
val loss = 0.8150283694267273
training loss = 2.0449063777923584 28600
val loss = 1.0947623252868652
training loss = 1.9261661767959595 28700
val loss = 0.8154735565185547
training loss = 1.925971269607544 28800
val loss = 0.8154160976409912
training loss = 1.9309841394424438 28900
val loss = 0.8492581248283386
training loss = 1.9257153272628784 29000
val loss = 0.8155449032783508
training loss = 1.9255341291427612 29100
val loss = 0.8148708343505859
training loss = 1.9256153106689453 29200
val loss = 0.8118695616722107
training loss = 1.9253005981445312 29300
val loss = 0.8160027265548706
training loss = 2.3425796031951904 29400
val loss = 1.0992095470428467
training loss = 1.9250648021697998 29500
val loss = 0.8161953687667847
training loss = 1.9248918294906616 29600
val loss = 0.8161459565162659
training loss = 1.9250730276107788 29700
val loss = 0.8227124214172363
training loss = 1.9247020483016968 29800
val loss = 0.8162609934806824
training loss = 1.924531102180481 29900
val loss = 0.816247820854187
training loss = 1.9247456789016724 30000
val loss = 0.8234150409698486
reduced chi^2 level 2 = 1.9247876405715942
Constrained alpha: 1.8899269104003906
Constrained beta: 0.7659454941749573
Constrained gamma: 12.268753051757812
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 856.2150,  877.0271,  927.6772,  974.7048,  954.9033, 1074.3531,
        1070.5398, 1065.0052, 1112.2020, 1144.1163, 1246.8738, 1191.6677,
        1255.4113, 1295.0284, 1314.8828, 1444.6847, 1346.8678, 1432.7427,
        1486.2213, 1520.3041, 1604.3657, 1539.2507, 1605.1646, 1559.6116,
        1640.7336, 1667.1752, 1579.1351, 1622.1056, 1696.3479, 1706.4257,
        1653.8495, 1708.2295, 1750.3561, 1752.6672, 1743.7065, 1769.1886,
        1640.9762, 1651.3538, 1625.4248, 1558.5261, 1585.2126, 1538.4042,
        1558.4824, 1493.8457, 1364.0959, 1320.9774, 1354.0419, 1266.9360,
        1129.1384, 1195.1924, 1030.0781,  969.5378,  913.2085,  924.2872,
         909.0303,  878.3127,  838.2508,  724.6862,  616.4597,  497.9408,
         534.9700,  447.3455,  454.2570,  392.6336,  401.7226,  325.6007,
         260.5463,  261.6928,  238.3780,  180.8885,  188.2452,  143.1544,
         130.3831,  113.1700,   97.0052,   62.7321,   40.2694,   38.2788,
          28.5558,   52.2500,   24.4036,   41.2049,   29.0857])]
2705.8926776403687
1.7919762889537294 12.810875018295016 33.9333296870248
val isze = 8
idinces = [20 37 75 38 82 14 42 55 70  2 49  3 34  4 71 48 50 45 47 17 80 59 74 39
 43  6 31 36 81 67 72  8 65 66 25 62  1 24  5 33 16 64 19 35 78 15 12  9
 27 56 13 28 46 10 51 21 54 57 11 60 79 76 58 69 29 53 30 73 63 23  0 22
 77 61 52 40 32 44 68 18  7 41 26]
we are doing training validation split
training loss = 131.33152770996094 100
val loss = 127.42506408691406
training loss = 6.741893768310547 200
val loss = 5.844776153564453
training loss = 6.707661151885986 300
val loss = 5.952195644378662
training loss = 6.692771911621094 400
val loss = 5.930423736572266
training loss = 6.676033020019531 500
val loss = 5.904489517211914
training loss = 6.656844139099121 600
val loss = 5.876349925994873
training loss = 6.637293338775635 700
val loss = 5.899277210235596
training loss = 6.616700649261475 800
val loss = 5.801708221435547
training loss = 6.596270561218262 900
val loss = 5.868831634521484
training loss = 6.574419975280762 1000
val loss = 5.732137680053711
training loss = 6.551968097686768 1100
val loss = 5.724819183349609
training loss = 6.528762340545654 1200
val loss = 5.737438201904297
training loss = 6.504881858825684 1300
val loss = 5.717471122741699
training loss = 6.480185508728027 1400
val loss = 5.683404445648193
training loss = 6.4547624588012695 1500
val loss = 5.70389986038208
training loss = 6.428254127502441 1600
val loss = 5.6482110023498535
training loss = 6.40134859085083 1700
val loss = 5.669296741485596
training loss = 6.373156547546387 1800
val loss = 5.601393699645996
training loss = 6.34453010559082 1900
val loss = 5.582544326782227
training loss = 6.315265655517578 2000
val loss = 5.551355361938477
training loss = 6.285497188568115 2100
val loss = 5.528439044952393
training loss = 6.255324840545654 2200
val loss = 5.498286724090576
training loss = 6.224937438964844 2300
val loss = 5.459567546844482
training loss = 6.194465637207031 2400
val loss = 5.442558288574219
training loss = 6.164168357849121 2500
val loss = 5.402856826782227
training loss = 6.134193420410156 2600
val loss = 5.386882781982422
training loss = 6.104811668395996 2700
val loss = 5.360969543457031
training loss = 6.077753067016602 2800
val loss = 5.223145484924316
training loss = 6.04840612411499 2900
val loss = 5.30173397064209
training loss = 6.021555423736572 3000
val loss = 5.282626152038574
training loss = 5.99542760848999 3100
val loss = 5.238903045654297
training loss = 5.969605922698975 3200
val loss = 5.2242631912231445
training loss = 5.949136734008789 3300
val loss = 5.408376216888428
training loss = 5.913437366485596 3400
val loss = 5.163121223449707
training loss = 5.876598358154297 3500
val loss = 5.007570266723633
training loss = 5.809788703918457 3600
val loss = 5.054526329040527
training loss = 5.670188903808594 3700
val loss = 4.899056911468506
training loss = 5.3944621086120605 3800
val loss = 4.68831729888916
training loss = 4.907735347747803 3900
val loss = 4.270751476287842
training loss = 3.873676061630249 4000
val loss = 3.3147642612457275
training loss = 2.4601991176605225 4100
val loss = 2.268838405609131
training loss = 2.0699243545532227 4200
val loss = 2.483344793319702
training loss = 2.028177261352539 4300
val loss = 2.5694756507873535
training loss = 2.008030891418457 4400
val loss = 2.501447916030884
training loss = 1.9918420314788818 4500
val loss = 2.558267593383789
training loss = 1.98060142993927 4600
val loss = 2.5345818996429443
training loss = 1.9722379446029663 4700
val loss = 2.553093910217285
training loss = 1.965438723564148 4800
val loss = 2.5831308364868164
training loss = 1.959355115890503 4900
val loss = 2.5594334602355957
training loss = 1.9540997743606567 5000
val loss = 2.569355010986328
training loss = 1.9493461847305298 5100
val loss = 2.5670113563537598
training loss = 1.9446738958358765 5200
val loss = 2.5603580474853516
training loss = 1.9404590129852295 5300
val loss = 2.5762453079223633
training loss = 1.9359990358352661 5400
val loss = 2.5597360134124756
training loss = 1.9451416730880737 5500
val loss = 2.3954825401306152
training loss = 1.9277260303497314 5600
val loss = 2.5596303939819336
training loss = 1.9236438274383545 5700
val loss = 2.5617361068725586
training loss = 1.9246900081634521 5800
val loss = 2.6748623847961426
training loss = 1.916077971458435 5900
val loss = 2.563196897506714
training loss = 1.9123845100402832 6000
val loss = 2.565207004547119
training loss = 1.9091976881027222 6100
val loss = 2.5430946350097656
training loss = 1.9055932760238647 6200
val loss = 2.5696280002593994
training loss = 1.9186699390411377 6300
val loss = 2.3974761962890625
training loss = 1.8993152379989624 6400
val loss = 2.5761640071868896
training loss = 1.896560549736023 6500
val loss = 2.551753282546997
training loss = 1.8935661315917969 6600
val loss = 2.5910985469818115
training loss = 1.8907607793807983 6700
val loss = 2.5849080085754395
training loss = 1.9105689525604248 6800
val loss = 2.3894500732421875
training loss = 1.885611653327942 6900
val loss = 2.590763568878174
training loss = 1.8830868005752563 7000
val loss = 2.5937061309814453
training loss = 1.8809113502502441 7100
val loss = 2.5837950706481934
training loss = 1.878462314605713 7200
val loss = 2.6055614948272705
training loss = 1.8818795680999756 7300
val loss = 2.7265257835388184
training loss = 1.8742021322250366 7400
val loss = 2.6153392791748047
training loss = 1.8723186254501343 7500
val loss = 2.5983424186706543
training loss = 1.8702216148376465 7600
val loss = 2.6185622215270996
training loss = 1.8682554960250854 7700
val loss = 2.6326777935028076
training loss = 1.8680623769760132 7800
val loss = 2.6974120140075684
training loss = 1.8646953105926514 7900
val loss = 2.644843101501465
training loss = 1.900144338607788 8000
val loss = 2.96852445602417
training loss = 1.8614250421524048 8100
val loss = 2.6593050956726074
training loss = 1.859803557395935 8200
val loss = 2.666438579559326
training loss = 1.8589891195297241 8300
val loss = 2.638209819793701
training loss = 1.8569562435150146 8400
val loss = 2.681297540664673
training loss = 1.859793782234192 8500
val loss = 2.7874975204467773
training loss = 1.8543034791946411 8600
val loss = 2.68967342376709
training loss = 1.852963924407959 8700
val loss = 2.705909252166748
training loss = 1.854366660118103 8800
val loss = 2.788576126098633
training loss = 1.8506358861923218 8900
val loss = 2.7232227325439453
training loss = 1.8497068881988525 9000
val loss = 2.710186004638672
training loss = 1.8485127687454224 9100
val loss = 2.748991012573242
training loss = 1.847409963607788 9200
val loss = 2.7516860961914062
training loss = 1.8523308038711548 9300
val loss = 2.663757801055908
training loss = 1.8455206155776978 9400
val loss = 2.7702929973602295
training loss = 1.844557523727417 9500
val loss = 2.7762598991394043
training loss = 1.8437942266464233 9600
val loss = 2.781172752380371
training loss = 1.8428312540054321 9700
val loss = 2.8033642768859863
training loss = 1.8420844078063965 9800
val loss = 2.815322160720825
training loss = 1.8412439823150635 9900
val loss = 2.825169563293457
training loss = 2.0284249782562256 10000
val loss = 3.6288390159606934
training loss = 1.839746356010437 10100
val loss = 2.8452107906341553
training loss = 1.8389278650283813 10200
val loss = 2.8613338470458984
training loss = 1.839725375175476 10300
val loss = 2.9252805709838867
training loss = 1.8374958038330078 10400
val loss = 2.8865199089050293
reduced chi^2 level 2 = 1.8368988037109375
Constrained alpha: 1.881481409072876
Constrained beta: 2.2478456497192383
Constrained gamma: 17.49086570739746
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 855.3972,  867.4526,  928.0716,  967.8896, 1017.2581, 1073.5068,
        1127.9176, 1096.6681, 1181.4988, 1205.3213, 1204.9364, 1220.2766,
        1238.8512, 1217.2021, 1312.0420, 1354.6691, 1412.9664, 1452.0017,
        1573.6124, 1533.7238, 1591.1023, 1512.5503, 1602.1094, 1601.9926,
        1602.8435, 1771.7939, 1626.3164, 1738.3422, 1736.3251, 1719.7821,
        1626.9550, 1709.1356, 1722.6067, 1753.2373, 1778.4170, 1729.5049,
        1735.1219, 1655.4792, 1621.3566, 1646.0797, 1662.7244, 1574.3442,
        1542.1978, 1497.7971, 1328.6073, 1358.0957, 1249.4373, 1280.3673,
        1101.1444, 1153.4133, 1097.8732,  977.2460,  977.8901,  928.3365,
         890.1489,  860.0963,  806.8676,  711.2644,  594.6716,  524.8007,
         535.4350,  525.2221,  441.6084,  378.7162,  343.0203,  322.1872,
         279.1949,  235.9593,  197.2976,  156.6571,  153.9774,  153.8158,
         146.8002,   97.0947,   91.9398,   69.3032,   51.9818,   43.0159,
          35.6678,   39.3650,   24.4788,   31.7187,   38.6554])]
2599.236456096439
3.8285797358720712 9.76883358588247 90.80628489316939
val isze = 8
idinces = [13 39 50  1 60 30 21 49 10 12 27  5  3 54 19 62 23 78 79 55 75 33 61 43
 25  8 71 73  9  7 82 42 76 67 53 17  0 11 57 24  6 47 59  4 64 63 46 26
 35 44 16 41 38 52 32 40 48 37 77 36 18 80 65  2 14 51 58 29 45 15 34 31
 20 56 69 68 66 81 22 72 74 28 70]
we are doing training validation split
training loss = 78.0476303100586 100
val loss = 107.72300720214844
training loss = 9.581158638000488 200
val loss = 10.859150886535645
training loss = 8.358378410339355 300
val loss = 9.010320663452148
training loss = 7.540639877319336 400
val loss = 7.666445732116699
training loss = 7.021336555480957 500
val loss = 6.695577621459961
training loss = 6.70036506652832 600
val loss = 6.000316619873047
training loss = 6.504211902618408 700
val loss = 5.503777027130127
training loss = 6.383371829986572 800
val loss = 5.149946212768555
training loss = 6.306044578552246 900
val loss = 4.898611068725586
training loss = 6.252571105957031 1000
val loss = 4.721010684967041
training loss = 6.21128511428833 1100
val loss = 4.596466064453125
training loss = 6.175533771514893 1200
val loss = 4.509687423706055
training loss = 6.14166259765625 1300
val loss = 4.449594020843506
training loss = 6.107689380645752 1400
val loss = 4.407935619354248
training loss = 6.072402477264404 1500
val loss = 4.378763198852539
training loss = 6.0345940589904785 1600
val loss = 4.357891082763672
training loss = 5.991689682006836 1700
val loss = 4.342650413513184
training loss = 5.933843612670898 1800
val loss = 4.332595348358154
training loss = 5.789719581604004 1900
val loss = 4.339973449707031
training loss = 5.232222080230713 2000
val loss = 4.373078346252441
training loss = 4.315110683441162 2100
val loss = 3.8065593242645264
training loss = 3.0917391777038574 2200
val loss = 3.1174116134643555
training loss = 2.169323444366455 2300
val loss = 2.5651907920837402
training loss = 1.8866817951202393 2400
val loss = 2.431946277618408
training loss = 1.8451988697052002 2500
val loss = 2.4180922508239746
training loss = 1.8352444171905518 2600
val loss = 2.4117112159729004
training loss = 1.8286999464035034 2700
val loss = 2.4099881649017334
training loss = 1.8228662014007568 2800
val loss = 2.41021466255188
training loss = 1.8173490762710571 2900
val loss = 2.414964437484741
training loss = 1.8153467178344727 3000
val loss = 2.2829079627990723
training loss = 1.8069108724594116 3100
val loss = 2.4163928031921387
training loss = 1.8025200366973877 3200
val loss = 2.3581173419952393
training loss = 1.7971205711364746 3300
val loss = 2.4149270057678223
training loss = 1.7929513454437256 3400
val loss = 2.467029094696045
training loss = 1.7880562543869019 3500
val loss = 2.4099223613739014
training loss = 1.7857996225357056 3600
val loss = 2.3034472465515137
training loss = 1.7798250913619995 3700
val loss = 2.4067013263702393
training loss = 1.7789723873138428 3800
val loss = 2.2764463424682617
training loss = 1.7724720239639282 3900
val loss = 2.4022228717803955
training loss = 1.786185383796692 4000
val loss = 2.1017541885375977
training loss = 1.7659016847610474 4100
val loss = 2.3949685096740723
training loss = 1.7629623413085938 4200
val loss = 2.397188901901245
training loss = 1.7600854635238647 4300
val loss = 2.4117703437805176
training loss = 1.7573267221450806 4400
val loss = 2.393490791320801
training loss = 1.7546671628952026 4500
val loss = 2.368826150894165
training loss = 1.7520349025726318 4600
val loss = 2.390505313873291
training loss = 1.7494914531707764 4700
val loss = 2.4114856719970703
training loss = 1.746917963027954 4800
val loss = 2.384345054626465
training loss = 1.7444918155670166 4900
val loss = 2.3550972938537598
training loss = 1.741934895515442 5000
val loss = 2.379800796508789
training loss = 1.7461907863616943 5100
val loss = 2.591822385787964
training loss = 1.7371575832366943 5200
val loss = 2.377098560333252
training loss = 1.7359944581985474 5300
val loss = 2.2962019443511963
training loss = 1.7326455116271973 5400
val loss = 2.3643980026245117
training loss = 1.7304985523223877 5500
val loss = 2.370577096939087
training loss = 1.7295758724212646 5600
val loss = 2.457968235015869
training loss = 1.7262996435165405 5700
val loss = 2.367042064666748
training loss = 1.7243691682815552 5800
val loss = 2.3661537170410156
training loss = 1.7225148677825928 5900
val loss = 2.3362884521484375
training loss = 1.7204978466033936 6000
val loss = 2.3626232147216797
training loss = 1.7187286615371704 6100
val loss = 2.3621439933776855
training loss = 1.7169480323791504 6200
val loss = 2.3609087467193604
training loss = 1.7152456045150757 6300
val loss = 2.359797239303589
training loss = 1.7638767957687378 6400
val loss = 1.8567025661468506
training loss = 1.7119755744934082 6500
val loss = 2.362431764602661
training loss = 1.7104387283325195 6600
val loss = 2.355726718902588
training loss = 1.7088814973831177 6700
val loss = 2.3483214378356934
training loss = 1.7074040174484253 6800
val loss = 2.353771924972534
training loss = 1.7063311338424683 6900
val loss = 2.3086471557617188
training loss = 1.7045613527297974 7000
val loss = 2.3560633659362793
training loss = 1.7032129764556885 7100
val loss = 2.3518621921539307
training loss = 1.705367922782898 7200
val loss = 2.2074105739593506
training loss = 1.7005590200424194 7300
val loss = 2.3505988121032715
training loss = 1.700280785560608 7400
val loss = 2.429831027984619
training loss = 1.6980208158493042 7500
val loss = 2.347815990447998
training loss = 1.6968063116073608 7600
val loss = 2.3478622436523438
training loss = 1.6986124515533447 7700
val loss = 2.213107109069824
training loss = 1.6943787336349487 7800
val loss = 2.3463492393493652
training loss = 1.6932108402252197 7900
val loss = 2.3444619178771973
training loss = 1.6920514106750488 8000
val loss = 2.329575538635254
training loss = 1.6908440589904785 8100
val loss = 2.3449277877807617
training loss = 1.6982810497283936 8200
val loss = 2.589989185333252
training loss = 1.6884660720825195 8300
val loss = 2.343844175338745
training loss = 1.6873607635498047 8400
val loss = 2.3671720027923584
training loss = 1.6860370635986328 8500
val loss = 2.3477156162261963
training loss = 1.6848067045211792 8600
val loss = 2.341482162475586
training loss = 1.6836767196655273 8700
val loss = 2.367154121398926
training loss = 1.6822078227996826 8800
val loss = 2.3389344215393066
training loss = 1.680868148803711 8900
val loss = 2.3395557403564453
training loss = 1.844829797744751 9000
val loss = 3.5653700828552246
training loss = 1.6779727935791016 9100
val loss = 2.332942247390747
training loss = 1.6764156818389893 9200
val loss = 2.3378214836120605
training loss = 1.6756449937820435 9300
val loss = 2.4136695861816406
training loss = 1.6730046272277832 9400
val loss = 2.336573600769043
training loss = 1.6746563911437988 9500
val loss = 2.190051317214966
training loss = 1.6690597534179688 9600
val loss = 2.32194185256958
training loss = 1.6667810678482056 9700
val loss = 2.3326332569122314
training loss = 1.6765363216400146 9800
val loss = 2.6237664222717285
training loss = 1.6614785194396973 9900
val loss = 2.3269104957580566
training loss = 1.658323049545288 10000
val loss = 2.3289875984191895
training loss = 1.656495451927185 10100
val loss = 2.4323387145996094
training loss = 1.6510406732559204 10200
val loss = 2.3256447315216064
training loss = 1.6466726064682007 10300
val loss = 2.3135342597961426
training loss = 1.6420583724975586 10400
val loss = 2.3446662425994873
training loss = 1.6367088556289673 10500
val loss = 2.319974422454834
training loss = 1.6355339288711548 10600
val loss = 2.153423309326172
training loss = 1.6244839429855347 10700
val loss = 2.3158998489379883
training loss = 1.6457128524780273 10800
val loss = 1.9236336946487427
training loss = 1.6097869873046875 10900
val loss = 2.306241512298584
training loss = 1.6012028455734253 11000
val loss = 2.3126494884490967
training loss = 1.5925021171569824 11100
val loss = 2.3291046619415283
training loss = 1.5827538967132568 11200
val loss = 2.301769733428955
training loss = 1.5726685523986816 11300
val loss = 2.2923712730407715
training loss = 1.5622793436050415 11400
val loss = 2.2980804443359375
training loss = 1.6076966524124146 11500
val loss = 1.776780128479004
training loss = 1.5403809547424316 11600
val loss = 2.29642653465271
training loss = 1.5299197435379028 11700
val loss = 2.38024640083313
training loss = 1.518197774887085 11800
val loss = 2.283149242401123
training loss = 1.5070468187332153 11900
val loss = 2.2932515144348145
training loss = 1.5002270936965942 12000
val loss = 2.1501524448394775
training loss = 1.4863989353179932 12100
val loss = 2.294426918029785
training loss = 1.4793013334274292 12200
val loss = 2.4404618740081787
training loss = 1.4672850370407104 12300
val loss = 2.286597728729248
training loss = 1.4582710266113281 12400
val loss = 2.3023903369903564
training loss = 1.451454997062683 12500
val loss = 2.3789405822753906
training loss = 1.4432517290115356 12600
val loss = 2.3095054626464844
training loss = 1.5179754495620728 12700
val loss = 3.107071876525879
training loss = 1.430762529373169 12800
val loss = 2.326958179473877
training loss = 1.4252089262008667 12900
val loss = 2.3231871128082275
training loss = 1.4214227199554443 13000
val loss = 2.403820514678955
training loss = 1.4163527488708496 13100
val loss = 2.3358449935913086
training loss = 1.4123716354370117 13200
val loss = 2.3398966789245605
training loss = 1.4097156524658203 13300
val loss = 2.2954535484313965
training loss = 1.4062151908874512 13400
val loss = 2.34805965423584
training loss = 1.4184587001800537 13500
val loss = 2.087164878845215
training loss = 1.4014019966125488 13600
val loss = 2.3584132194519043
training loss = 1.3992325067520142 13700
val loss = 2.375805377960205
training loss = 1.3977181911468506 13800
val loss = 2.347196578979492
training loss = 1.3959184885025024 13900
val loss = 2.3710527420043945
training loss = 1.471782922744751 14000
val loss = 3.11613130569458
training loss = 1.3931728601455688 14100
val loss = 2.376246452331543
training loss = 1.3918741941452026 14200
val loss = 2.3822364807128906
training loss = 1.3909894227981567 14300
val loss = 2.36747145652771
training loss = 1.38984215259552 14400
val loss = 2.3883657455444336
training loss = 1.4001634120941162 14500
val loss = 2.156714677810669
training loss = 1.3881213665008545 14600
val loss = 2.391279697418213
training loss = 1.3872686624526978 14700
val loss = 2.396559715270996
training loss = 1.3867181539535522 14800
val loss = 2.414095401763916
training loss = 1.3859423398971558 14900
val loss = 2.4010581970214844
training loss = 1.3854743242263794 15000
val loss = 2.4377641677856445
training loss = 1.384813666343689 15100
val loss = 2.400181293487549
training loss = 1.3842073678970337 15200
val loss = 2.4056549072265625
training loss = 1.4110215902328491 15300
val loss = 2.0578579902648926
training loss = 1.383263349533081 15400
val loss = 2.4082818031311035
training loss = 1.38276207447052 15500
val loss = 2.4097907543182373
training loss = 1.3826478719711304 15600
val loss = 2.379979372024536
training loss = 1.38199782371521 15700
val loss = 2.4118499755859375
training loss = 1.3815817832946777 15800
val loss = 2.4204933643341064
training loss = 1.3813222646713257 15900
val loss = 2.4070205688476562
training loss = 1.3809155225753784 16000
val loss = 2.4137682914733887
training loss = 1.4503391981124878 16100
val loss = 3.1124134063720703
training loss = 1.3803174495697021 16200
val loss = 2.4151735305786133
training loss = 1.3799688816070557 16300
val loss = 2.4159598350524902
training loss = 1.3800218105316162 16400
val loss = 2.3836636543273926
training loss = 1.379460096359253 16500
val loss = 2.4171438217163086
training loss = 1.3791471719741821 16600
val loss = 2.416914463043213
training loss = 1.3796311616897583 16700
val loss = 2.4786124229431152
training loss = 1.3786617517471313 16800
val loss = 2.4178872108459473
training loss = 1.3790254592895508 16900
val loss = 2.47340989112854
training loss = 1.3782047033309937 17000
val loss = 2.416870594024658
training loss = 1.3780802488327026 17100
val loss = 2.3918323516845703
training loss = 1.377848744392395 17200
val loss = 2.4017999172210693
training loss = 1.3775056600570679 17300
val loss = 2.420274257659912
training loss = 1.402042031288147 17400
val loss = 2.817204713821411
training loss = 1.3771231174468994 17500
val loss = 2.418588638305664
training loss = 1.3768670558929443 17600
val loss = 2.421731472015381
training loss = 1.3767600059509277 17700
val loss = 2.4302544593811035
reduced chi^2 level 2 = 1.377044916152954
Constrained alpha: 1.781235933303833
Constrained beta: 3.465738296508789
Constrained gamma: 14.95166015625
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 812.6957,  894.2831, 1022.3751,  920.2722,  987.0335, 1066.0276,
        1116.3784, 1123.8411, 1176.9308, 1193.1302, 1177.3909, 1161.7716,
        1226.6948, 1208.0255, 1332.8246, 1390.0488, 1395.1868, 1437.1681,
        1471.4012, 1556.9521, 1619.0450, 1517.5674, 1671.6205, 1640.2518,
        1672.4623, 1712.2877, 1621.7538, 1739.9905, 1711.8242, 1674.3442,
        1608.3551, 1676.5392, 1694.7098, 1743.6066, 1716.9097, 1748.5421,
        1684.0422, 1640.5051, 1675.3562, 1675.4094, 1617.9004, 1510.3789,
        1548.1217, 1480.2587, 1344.5979, 1321.2633, 1332.5266, 1255.5731,
        1155.7863, 1158.7778, 1075.9419, 1006.4062,  916.5540,  921.2263,
         896.2072,  863.2986,  815.9323,  673.8555,  638.4769,  490.4027,
         500.2418,  433.2614,  440.2272,  404.9285,  321.2471,  369.6467,
         297.9326,  264.3283,  196.5454,  181.1629,  154.4487,  156.0522,
         142.9636,   88.8296,  101.6262,   58.2457,   45.1090,   37.1823,
          39.3512,   44.8519,   25.3319,   32.3965,   42.7654])]
2742.7843792492886
4.60677377180114 13.544505555131355 0.5538070128967565
val isze = 8
idinces = [32 10 40 19 35  2 72 47 78 16  3 23 22 61  7 59 38 77  8 51  0 37 58 56
 14 53 41 55 44 30 33 82 11 80 34 75 21 52 18 54 69 63 46 76 60 64 68 49
  9 48 57 29 39 36 28  4 15  1 27 45 26 79  6 43 65 50 62 31 66 42 25 81
 17 74 12 70 73 67 24 71  5 20 13]
we are doing training validation split
training loss = 606.600830078125 100
val loss = 797.0846557617188
training loss = 8.814064025878906 200
val loss = 10.637542724609375
training loss = 5.5306854248046875 300
val loss = 6.097156524658203
training loss = 5.3338942527771 400
val loss = 6.00048828125
training loss = 5.295168399810791 500
val loss = 5.211173057556152
training loss = 4.969168186187744 600
val loss = 5.77768611907959
training loss = 4.804230213165283 700
val loss = 5.558489799499512
training loss = 4.6382880210876465 800
val loss = 5.278544902801514
training loss = 4.471511363983154 900
val loss = 5.39190149307251
training loss = 4.440782070159912 1000
val loss = 6.192511558532715
training loss = 4.080399036407471 1100
val loss = 4.8487548828125
training loss = 3.8766798973083496 1200
val loss = 4.690650939941406
training loss = 3.6708602905273438 1300
val loss = 4.584061622619629
training loss = 3.500883102416992 1400
val loss = 3.95400071144104
training loss = 3.278644323348999 1500
val loss = 4.263626575469971
training loss = 3.117903232574463 1600
val loss = 4.394112586975098
training loss = 2.9469337463378906 1700
val loss = 3.9000768661499023
training loss = 2.812889575958252 1800
val loss = 3.8123903274536133
training loss = 2.774935722351074 1900
val loss = 2.9229631423950195
training loss = 2.595228433609009 2000
val loss = 3.4678378105163574
training loss = 2.5193450450897217 2100
val loss = 3.24385666847229
training loss = 2.452483654022217 2200
val loss = 3.245823860168457
training loss = 2.416051149368286 2300
val loss = 3.4578380584716797
training loss = 2.362048387527466 2400
val loss = 3.0939717292785645
training loss = 2.3290247917175293 2500
val loss = 2.918585777282715
training loss = 2.2883355617523193 2600
val loss = 2.8245902061462402
training loss = 2.2407889366149902 2700
val loss = 3.05615234375
training loss = 2.8363029956817627 2800
val loss = 5.6414713859558105
training loss = 2.167909860610962 2900
val loss = 2.9886326789855957
training loss = 2.245992422103882 3000
val loss = 2.332996368408203
training loss = 2.1165356636047363 3100
val loss = 2.907468557357788
training loss = 2.1022067070007324 3200
val loss = 3.072357416152954
training loss = 2.075153112411499 3300
val loss = 2.848999261856079
training loss = 2.060645818710327 3400
val loss = 2.7211320400238037
training loss = 2.066845178604126 3500
val loss = 3.214397430419922
training loss = 2.0211050510406494 3600
val loss = 2.7599196434020996
training loss = 2.1900558471679688 3700
val loss = 2.087374687194824
training loss = 1.986423134803772 3800
val loss = 2.735792636871338
training loss = 1.974613904953003 3900
val loss = 2.8279690742492676
training loss = 1.9672127962112427 4000
val loss = 2.4793057441711426
training loss = 1.9442228078842163 4100
val loss = 2.649005889892578
training loss = 1.9677832126617432 4200
val loss = 2.266869068145752
training loss = 1.9225224256515503 4300
val loss = 2.6016533374786377
training loss = 1.9209115505218506 4400
val loss = 2.429570436477661
training loss = 1.9103375673294067 4500
val loss = 2.616163730621338
training loss = 1.9062838554382324 4600
val loss = 2.514927387237549
training loss = 2.345229148864746 4700
val loss = 4.56195068359375
training loss = 1.8996301889419556 4800
val loss = 2.516794443130493
training loss = 1.8986849784851074 4900
val loss = 2.5932207107543945
training loss = 1.8947778940200806 5000
val loss = 2.5239086151123047
training loss = 1.8932896852493286 5100
val loss = 2.502023220062256
training loss = 2.2357125282287598 5200
val loss = 4.253226280212402
training loss = 1.889805555343628 5300
val loss = 2.5311381816864014
training loss = 2.014789342880249 5400
val loss = 1.875308871269226
training loss = 1.8867183923721313 5500
val loss = 2.493774890899658
training loss = 1.92348313331604 5600
val loss = 2.123333215713501
training loss = 1.8841129541397095 5700
val loss = 2.4977078437805176
training loss = 1.883528709411621 5800
val loss = 2.479320764541626
training loss = 1.881986141204834 5900
val loss = 2.481865882873535
training loss = 1.8821496963500977 6000
val loss = 2.43741774559021
training loss = 1.8953510522842407 6100
val loss = 2.7903943061828613
training loss = 1.8797787427902222 6200
val loss = 2.469503879547119
training loss = 1.9609241485595703 6300
val loss = 3.237600803375244
training loss = 1.8780099153518677 6400
val loss = 2.4949851036071777
training loss = 1.9055802822113037 6500
val loss = 2.1437954902648926
training loss = 1.8765572309494019 6600
val loss = 2.4871950149536133
training loss = 1.8831555843353271 6700
val loss = 2.6811041831970215
training loss = 1.8752175569534302 6800
val loss = 2.502793312072754
training loss = 1.9084147214889526 6900
val loss = 2.934818983078003
training loss = 1.8740700483322144 7000
val loss = 2.4960968494415283
training loss = 1.921553611755371 7100
val loss = 3.031210422515869
training loss = 1.8730980157852173 7200
val loss = 2.496483087539673
training loss = 2.1272075176239014 7300
val loss = 1.7029531002044678
training loss = 1.8722034692764282 7400
val loss = 2.49215030670166
training loss = 2.42868971824646 7500
val loss = 1.5545004606246948
training loss = 1.8713635206222534 7600
val loss = 2.486875057220459
training loss = 2.551551342010498 7700
val loss = 1.5712970495224
training loss = 1.8704931735992432 7800
val loss = 2.490252733230591
training loss = 2.388866901397705 7900
val loss = 4.741827011108398
training loss = 1.8697636127471924 8000
val loss = 2.4938483238220215
training loss = 1.9287171363830566 8100
val loss = 2.044806957244873
training loss = 1.8692009449005127 8200
val loss = 2.535787343978882
training loss = 1.8778190612792969 8300
val loss = 2.304202079772949
training loss = 1.8683931827545166 8400
val loss = 2.496803045272827
training loss = 1.86775803565979 8500
val loss = 2.518015146255493
training loss = 1.867720603942871 8600
val loss = 2.5049777030944824
training loss = 1.8670967817306519 8700
val loss = 2.509274959564209
training loss = 1.8681237697601318 8800
val loss = 2.5715949535369873
training loss = 1.8666257858276367 8900
val loss = 2.5116982460021973
training loss = 2.119921922683716 9000
val loss = 1.7087430953979492
training loss = 1.866109848022461 9100
val loss = 2.505079746246338
training loss = 2.542788028717041 9200
val loss = 1.588130235671997
training loss = 1.8657389879226685 9300
val loss = 2.4955408573150635
training loss = 1.8651241064071655 9400
val loss = 2.514300584793091
training loss = 1.86751389503479 9500
val loss = 2.6197125911712646
training loss = 1.8646513223648071 9600
val loss = 2.519352436065674
training loss = 1.8652812242507935 9700
val loss = 2.456718921661377
training loss = 1.8641849756240845 9800
val loss = 2.520880699157715
training loss = 1.8717389106750488 9900
val loss = 2.3268415927886963
training loss = 1.8636934757232666 10000
val loss = 2.5207841396331787
training loss = 1.8655176162719727 10100
val loss = 2.4231374263763428
training loss = 1.8633114099502563 10200
val loss = 2.5234439373016357
training loss = 1.8941011428833008 10300
val loss = 2.9373958110809326
training loss = 1.862905740737915 10400
val loss = 2.5202488899230957
training loss = 1.8988701105117798 10500
val loss = 2.977830648422241
training loss = 1.862528681755066 10600
val loss = 2.5253639221191406
training loss = 1.8977891206741333 10700
val loss = 2.1480965614318848
training loss = 1.862163782119751 10800
val loss = 2.5230727195739746
training loss = 1.9060100317001343 10900
val loss = 2.1126749515533447
training loss = 1.8618565797805786 11000
val loss = 2.5335681438446045
training loss = 2.3208625316619873 11100
val loss = 4.601313591003418
training loss = 1.8615520000457764 11200
val loss = 2.539656639099121
training loss = 2.370863437652588 11300
val loss = 4.741981029510498
training loss = 1.861194133758545 11400
val loss = 2.5302395820617676
training loss = 2.276658296585083 11500
val loss = 4.475373268127441
training loss = 1.860891580581665 11600
val loss = 2.5261192321777344
training loss = 1.9089893102645874 11700
val loss = 2.066798686981201
training loss = 1.860642910003662 11800
val loss = 2.5461349487304688
training loss = 4.215695381164551 11900
val loss = 9.058444023132324
training loss = 1.8603664636611938 12000
val loss = 2.550750970840454
training loss = 2.5761642456054688 12100
val loss = 1.589169979095459
training loss = 1.8600664138793945 12200
val loss = 2.5241358280181885
training loss = 2.3181910514831543 12300
val loss = 1.598064661026001
training loss = 1.8598212003707886 12400
val loss = 2.52547550201416
training loss = 2.6358988285064697 12500
val loss = 1.6088624000549316
training loss = 1.8596625328063965 12600
val loss = 2.517058849334717
training loss = 1.8811002969741821 12700
val loss = 2.9009807109832764
training loss = 1.8593404293060303 12800
val loss = 2.526902675628662
training loss = 1.8659725189208984 12900
val loss = 2.366962432861328
training loss = 1.859399676322937 13000
val loss = 2.58296537399292
training loss = 1.8588694334030151 13100
val loss = 2.5728206634521484
training loss = 1.8593435287475586 13200
val loss = 2.4959299564361572
training loss = 1.858629584312439 13300
val loss = 2.522108554840088
training loss = 1.8588513135910034 13400
val loss = 2.5787322521209717
training loss = 1.8587160110473633 13500
val loss = 2.503281593322754
training loss = 1.858598232269287 13600
val loss = 2.58115816116333
training loss = 2.6440532207489014 13700
val loss = 5.564035892486572
reduced chi^2 level 2 = 1.8767558336257935
Constrained alpha: 4.902373790740967
Constrained beta: 2.4157321453094482
Constrained gamma: 0.8273996710777283
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 831.4605,  895.9764,  927.0033,  954.8297, 1018.6934, 1051.0114,
        1141.8413, 1138.6179, 1122.3760, 1175.6744, 1206.8400, 1197.5807,
        1291.2372, 1253.9825, 1381.7643, 1360.4933, 1412.5583, 1512.1069,
        1556.6740, 1486.3184, 1604.4781, 1595.8416, 1552.0366, 1610.5038,
        1657.2090, 1673.4321, 1605.1417, 1799.1274, 1707.2917, 1623.6704,
        1624.2100, 1756.8384, 1730.1990, 1707.6066, 1707.1890, 1744.9124,
        1737.8396, 1596.1747, 1644.5341, 1653.8645, 1638.9374, 1647.0574,
        1485.4825, 1500.3397, 1367.4484, 1399.0507, 1280.0841, 1207.9066,
        1160.8116, 1146.4329, 1092.6387, 1006.3859,  957.8639,  929.5969,
         888.2811,  850.7044,  803.1754,  717.9196,  601.5565,  552.5295,
         518.3101,  471.7721,  427.4711,  388.6512,  352.8105,  349.8292,
         300.2458,  249.1665,  230.2566,  189.0479,  146.6250,  148.1871,
         134.7691,  111.4593,   99.9244,   67.6720,   40.1571,   44.6821,
          31.7336,   47.1884,   23.4683,   44.6702,   40.8329])]
2400.297972928988
1.7675202461798223 2.125132129608791 91.1241748607513
val isze = 8
idinces = [82 52 71 41 47 73 53 28 31 55 22  4 14 15 75 25  6 57 20 76 70 42 36 65
 63 21 78 45 62 29 49 10 59 40 50 81  0 51 26 35 44 12 80 48 19 79 37 39
 38  5 77 68 43 23 11  3 32 66 60 24  9 74 58 17 18 64 27  2 33 56 72  7
 16 69  1 34 61 67 46 54 13 30  8]
we are doing training validation split
training loss = 41.09189987182617 100
val loss = 45.60389709472656
training loss = 29.067516326904297 200
val loss = 27.061168670654297
training loss = 20.111690521240234 300
val loss = 16.43775749206543
training loss = 14.028657913208008 400
val loss = 10.809920310974121
training loss = 10.093805313110352 500
val loss = 8.200865745544434
training loss = 7.630044937133789 600
val loss = 7.29380989074707
training loss = 6.147363662719727 700
val loss = 7.259333610534668
training loss = 5.276154041290283 800
val loss = 7.513297080993652
training loss = 4.721404552459717 900
val loss = 7.623760223388672
training loss = 4.274406909942627 1000
val loss = 7.339879035949707
training loss = 3.840510368347168 1100
val loss = 6.683204650878906
training loss = 3.415942430496216 1200
val loss = 5.9053802490234375
training loss = 3.012083053588867 1300
val loss = 5.1549248695373535
training loss = 2.637871503829956 1400
val loss = 4.420195579528809
training loss = 2.3109545707702637 1500
val loss = 3.7187752723693848
training loss = 2.054100751876831 1600
val loss = 3.1075034141540527
training loss = 1.8794214725494385 1700
val loss = 2.6321868896484375
training loss = 1.7778029441833496 1800
val loss = 2.301147937774658
training loss = 1.726037621498108 1900
val loss = 2.090240001678467
training loss = 1.7011902332305908 2000
val loss = 1.963636040687561
training loss = 1.6882001161575317 2100
val loss = 1.8899941444396973
training loss = 1.6794599294662476 2200
val loss = 1.8471291065216064
training loss = 1.6717702150344849 2300
val loss = 1.8210006952285767
training loss = 1.6640229225158691 2400
val loss = 1.8032035827636719
training loss = 1.656019926071167 2500
val loss = 1.7884635925292969
training loss = 1.6499050855636597 2600
val loss = 1.770685076713562
training loss = 1.640019416809082 2700
val loss = 1.7567707300186157
training loss = 1.6321685314178467 2800
val loss = 1.73846435546875
training loss = 1.6247085332870483 2900
val loss = 1.7182899713516235
training loss = 1.6174558401107788 3000
val loss = 1.698441743850708
training loss = 1.610671877861023 3100
val loss = 1.680234432220459
training loss = 1.603926658630371 3200
val loss = 1.6575531959533691
training loss = 1.6037636995315552 3300
val loss = 1.6596271991729736
training loss = 1.591710090637207 3400
val loss = 1.6181159019470215
training loss = 1.5876909494400024 3500
val loss = 1.5897934436798096
training loss = 1.5807653665542603 3600
val loss = 1.5808279514312744
training loss = 1.5763617753982544 3700
val loss = 1.5566937923431396
training loss = 1.5709924697875977 3800
val loss = 1.546609878540039
training loss = 1.580672264099121 3900
val loss = 1.5060436725616455
training loss = 1.5622931718826294 4000
val loss = 1.5150214433670044
training loss = 1.5581737756729126 4100
val loss = 1.502354621887207
training loss = 1.555281639099121 4200
val loss = 1.4983105659484863
training loss = 1.5507984161376953 4300
val loss = 1.4772661924362183
training loss = 1.9483968019485474 4400
val loss = 1.9870185852050781
training loss = 1.544296383857727 4500
val loss = 1.4546725749969482
training loss = 1.5412081480026245 4600
val loss = 1.446340799331665
training loss = 1.5390307903289795 4700
val loss = 1.4456238746643066
training loss = 1.535841703414917 4800
val loss = 1.4304940700531006
training loss = 1.599726915359497 4900
val loss = 1.5742945671081543
training loss = 1.5312200784683228 5000
val loss = 1.417402982711792
training loss = 1.529016137123108 5100
val loss = 1.4123982191085815
training loss = 1.5415767431259155 5200
val loss = 1.3672964572906494
training loss = 1.5253725051879883 5300
val loss = 1.404620885848999
training loss = 1.5236306190490723 5400
val loss = 1.4013302326202393
training loss = 1.5233606100082397 5500
val loss = 1.413360357284546
training loss = 1.5208066701889038 5600
val loss = 1.3970527648925781
training loss = 1.5467820167541504 5700
val loss = 1.4904073476791382
training loss = 1.518479824066162 5800
val loss = 1.393074870109558
training loss = 1.5173227787017822 5900
val loss = 1.395467758178711
training loss = 1.5168352127075195 6000
val loss = 1.4031952619552612
training loss = 1.5155823230743408 6100
val loss = 1.3967158794403076
training loss = 1.6491107940673828 6200
val loss = 1.6700694561004639
training loss = 1.5141856670379639 6300
val loss = 1.3985909223556519
training loss = 1.513482928276062 6400
val loss = 1.4020243883132935
training loss = 1.513126254081726 6500
val loss = 1.4014127254486084
training loss = 1.5125116109848022 6600
val loss = 1.4071598052978516
training loss = 1.6113225221633911 6700
val loss = 1.643101453781128
training loss = 1.5117828845977783 6800
val loss = 1.4132342338562012
training loss = 1.5113660097122192 6900
val loss = 1.4171847105026245
training loss = 1.5156240463256836 7000
val loss = 1.4551783800125122
training loss = 1.5108938217163086 7100
val loss = 1.4243204593658447
training loss = 1.5121116638183594 7200
val loss = 1.4113678932189941
training loss = 1.5106229782104492 7300
val loss = 1.4356181621551514
training loss = 1.510331153869629 7400
val loss = 1.4378516674041748
training loss = 1.5191177129745483 7500
val loss = 1.4929862022399902
training loss = 1.5101501941680908 7600
val loss = 1.446620225906372
training loss = 1.5099745988845825 7700
val loss = 1.4523069858551025
training loss = 1.5105035305023193 7800
val loss = 1.4652268886566162
training loss = 1.5098838806152344 7900
val loss = 1.4608732461929321
training loss = 1.5097240209579468 8000
val loss = 1.4665578603744507
training loss = 1.5098450183868408 8100
val loss = 1.4656015634536743
training loss = 1.5095889568328857 8200
val loss = 1.4765956401824951
training loss = 1.5692955255508423 8300
val loss = 1.4027769565582275
training loss = 1.5093847513198853 8400
val loss = 1.485217571258545
training loss = 1.509122610092163 8500
val loss = 1.4924979209899902
training loss = 1.5092988014221191 8600
val loss = 1.5031195878982544
training loss = 1.5086816549301147 8700
val loss = 1.5023088455200195
training loss = 1.6722584962844849 8800
val loss = 1.847506046295166
training loss = 1.5079585313796997 8900
val loss = 1.5102286338806152
training loss = 1.5073068141937256 9000
val loss = 1.5187515020370483
training loss = 1.5077050924301147 9100
val loss = 1.5056979656219482
training loss = 1.5059813261032104 9200
val loss = 1.5276223421096802
training loss = 1.5053889751434326 9300
val loss = 1.5455281734466553
training loss = 1.504257321357727 9400
val loss = 1.540804386138916
training loss = 1.5030688047409058 9500
val loss = 1.546168327331543
training loss = 1.5086004734039307 9600
val loss = 1.5135977268218994
training loss = 1.500895619392395 9700
val loss = 1.560091257095337
training loss = 1.5000970363616943 9800
val loss = 1.583486795425415
training loss = 1.4984427690505981 9900
val loss = 1.5777411460876465
training loss = 1.496802568435669 10000
val loss = 1.5854358673095703
training loss = 1.4975032806396484 10100
val loss = 1.6171571016311646
training loss = 1.4935163259506226 10200
val loss = 1.6012910604476929
training loss = 1.4915555715560913 10300
val loss = 1.6149098873138428
training loss = 1.4899741411209106 10400
val loss = 1.6129541397094727
training loss = 1.4878240823745728 10500
val loss = 1.6306853294372559
training loss = 1.4882993698120117 10600
val loss = 1.6622066497802734
training loss = 1.483894944190979 10700
val loss = 1.648411512374878
training loss = 1.481581687927246 10800
val loss = 1.6611391305923462
training loss = 1.4799308776855469 10900
val loss = 1.6762970685958862
training loss = 1.4770090579986572 11000
val loss = 1.6784560680389404
training loss = 1.475008487701416 11100
val loss = 1.6731224060058594
training loss = 1.4719674587249756 11200
val loss = 1.6924868822097778
reduced chi^2 level 2 = 1.4716546535491943
Constrained alpha: 1.9343597888946533
Constrained beta: 2.1529736518859863
Constrained gamma: 21.349607467651367
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 850.5470,  841.4108,  974.9002,  929.1174, 1008.4334, 1103.4036,
        1112.0275, 1148.6797, 1192.1769, 1148.6895, 1174.7594, 1162.5674,
        1233.2393, 1264.5342, 1335.0332, 1465.8009, 1434.4816, 1431.9609,
        1532.7909, 1452.4806, 1609.8472, 1598.3511, 1675.8910, 1580.4766,
        1660.1702, 1723.3829, 1611.1564, 1691.3357, 1738.5214, 1714.0638,
        1663.0439, 1706.2665, 1774.1576, 1751.6619, 1783.5327, 1695.9243,
        1649.6288, 1586.5770, 1679.5693, 1669.2903, 1606.0770, 1567.6268,
        1429.3271, 1487.3658, 1375.8181, 1406.4962, 1324.6926, 1215.6733,
        1155.8528, 1166.5771, 1076.8124,  941.7190,  922.2214,  979.0484,
         889.2140,  835.6558,  836.2845,  756.0537,  636.4155,  550.5461,
         535.2726,  454.5168,  433.6133,  412.6675,  361.4796,  333.3837,
         285.8678,  237.6163,  200.6704,  148.4277,  146.2382,  144.7764,
         150.6350,   84.1046,   82.1821,   64.5212,   55.0414,   40.1155,
          35.1841,   39.2204,   14.5194,   37.4172,   35.9774])]
2779.71622083871
4.669715038027489 6.3270974865401985 90.94468516815876
val isze = 8
idinces = [61  1 73 62 21 53 31 35 78  8  2 60 54 56 52 11 34 13 28 58 74 50 47 20
 40 30  9  3 79 37 63 17 72 18 16  6 67 49 76  7 12 22 51 41 42 80 19 66
 26 39  4 68  5 33 57 43 65 29 77 75 81 70 82 15 55 27 59 23 36 24 71 46
 44 64 32  0 38 45 69 48 14 10 25]
we are doing training validation split
training loss = 429.7760925292969 100
val loss = 531.4639282226562
training loss = 84.63279724121094 200
val loss = 103.74256896972656
training loss = 29.68385887145996 300
val loss = 45.120445251464844
training loss = 14.419266700744629 400
val loss = 25.084209442138672
training loss = 8.238901138305664 500
val loss = 14.773560523986816
training loss = 5.77971076965332 600
val loss = 9.563966751098633
training loss = 4.103863716125488 700
val loss = 6.785104274749756
training loss = 3.181183338165283 800
val loss = 4.8021416664123535
training loss = 2.756335496902466 900
val loss = 3.784581422805786
training loss = 2.5861494541168213 1000
val loss = 3.3115463256835938
training loss = 2.4873812198638916 1100
val loss = 3.0670783519744873
training loss = 2.4058659076690674 1200
val loss = 2.932419776916504
training loss = 2.324812650680542 1300
val loss = 2.857834815979004
training loss = 2.2368006706237793 1400
val loss = 2.820436477661133
training loss = 2.149409055709839 1500
val loss = 2.836974859237671
training loss = 2.0600485801696777 1600
val loss = 2.8501710891723633
training loss = 1.9773283004760742 1700
val loss = 2.810539484024048
training loss = 1.910313367843628 1800
val loss = 2.731614589691162
training loss = 1.8598055839538574 1900
val loss = 2.651862621307373
training loss = 1.8216108083724976 2000
val loss = 2.609957218170166
training loss = 1.7890737056732178 2100
val loss = 2.5629982948303223
training loss = 1.7632733583450317 2200
val loss = 2.545689105987549
training loss = 1.7417466640472412 2300
val loss = 2.534411668777466
training loss = 1.7239114046096802 2400
val loss = 2.5236480236053467
training loss = 1.7091671228408813 2500
val loss = 2.5268101692199707
training loss = 1.6975044012069702 2600
val loss = 2.512444496154785
training loss = 1.687354564666748 2700
val loss = 2.525167942047119
training loss = 1.679465413093567 2800
val loss = 2.5241549015045166
training loss = 1.6730949878692627 2900
val loss = 2.5246996879577637
training loss = 1.669417142868042 3000
val loss = 2.4963977336883545
training loss = 1.6637948751449585 3100
val loss = 2.525425434112549
training loss = 1.6604273319244385 3200
val loss = 2.525418519973755
training loss = 1.6578925848007202 3300
val loss = 2.5330097675323486
training loss = 1.6556349992752075 3400
val loss = 2.525211811065674
training loss = 1.6538946628570557 3500
val loss = 2.523660182952881
training loss = 1.6525415182113647 3600
val loss = 2.527190923690796
training loss = 1.651443600654602 3700
val loss = 2.5252914428710938
training loss = 1.6508926153182983 3800
val loss = 2.509749412536621
training loss = 1.649950385093689 3900
val loss = 2.5248262882232666
training loss = 1.756656527519226 4000
val loss = 2.9084112644195557
training loss = 1.6491280794143677 4100
val loss = 2.5266785621643066
training loss = 1.6488990783691406 4200
val loss = 2.526364326477051
training loss = 1.648802399635315 4300
val loss = 2.521742820739746
training loss = 1.6487475633621216 4400
val loss = 2.5263924598693848
training loss = 1.6490238904953003 4500
val loss = 2.5141115188598633
training loss = 1.64889657497406 4600
val loss = 2.52841854095459
training loss = 1.6497890949249268 4700
val loss = 2.5064327716827393
training loss = 1.6492589712142944 4800
val loss = 2.5302395820617676
training loss = 1.7209409475326538 4900
val loss = 2.3679041862487793
training loss = 1.6497560739517212 5000
val loss = 2.5336005687713623
training loss = 1.6500505208969116 5100
val loss = 2.5356476306915283
training loss = 1.6503682136535645 5200
val loss = 2.5405359268188477
training loss = 1.6506247520446777 5300
val loss = 2.536116361618042
training loss = 1.6509875059127808 5400
val loss = 2.529855251312256
training loss = 1.6511752605438232 5500
val loss = 2.5387191772460938
training loss = 1.65220046043396 5600
val loss = 2.5642075538635254
training loss = 1.6516475677490234 5700
val loss = 2.54087495803833
training loss = 1.6528929471969604 5800
val loss = 2.515531539916992
training loss = 1.6520291566848755 5900
val loss = 2.541353225708008
training loss = 1.6560994386672974 6000
val loss = 2.4935715198516846
training loss = 1.6523551940917969 6100
val loss = 2.5388002395629883
training loss = 1.652448058128357 6200
val loss = 2.543921947479248
training loss = 1.6563771963119507 6300
val loss = 2.4943630695343018
training loss = 1.6525635719299316 6400
val loss = 2.543816089630127
training loss = 1.6526336669921875 6500
val loss = 2.544553518295288
training loss = 1.653650164604187 6600
val loss = 2.519301414489746
training loss = 1.6526868343353271 6700
val loss = 2.544370174407959
training loss = 1.6822550296783447 6800
val loss = 2.4248876571655273
training loss = 1.6527024507522583 6900
val loss = 2.545570135116577
training loss = 1.652707815170288 7000
val loss = 2.5454111099243164
training loss = 1.6527422666549683 7100
val loss = 2.550567626953125
training loss = 1.6526386737823486 7200
val loss = 2.5433309078216553
training loss = 1.7236979007720947 7300
val loss = 2.839603900909424
training loss = 1.652572751045227 7400
val loss = 2.5433707237243652
training loss = 1.6525330543518066 7500
val loss = 2.5428175926208496
training loss = 1.6531299352645874 7600
val loss = 2.565202236175537
training loss = 1.65242338180542 7700
val loss = 2.541590690612793
training loss = 1.6524357795715332 7800
val loss = 2.535653591156006
training loss = 1.6523737907409668 7900
val loss = 2.5467333793640137
training loss = 1.652273178100586 8000
val loss = 2.540956974029541
training loss = 1.6563806533813477 8100
val loss = 2.599928617477417
training loss = 1.652158498764038 8200
val loss = 2.539680004119873
training loss = 1.6524477005004883 8300
val loss = 2.556438446044922
training loss = 1.6520832777023315 8400
val loss = 2.5442466735839844
training loss = 1.6519771814346313 8500
val loss = 2.5388078689575195
training loss = 1.6588830947875977 8600
val loss = 2.6179535388946533
training loss = 1.6518597602844238 8700
val loss = 2.538625955581665
training loss = 1.6518014669418335 8800
val loss = 2.536123275756836
training loss = 1.651766300201416 8900
val loss = 2.5349435806274414
training loss = 1.6516708135604858 9000
val loss = 2.5368242263793945
training loss = 1.6529027223587036 9100
val loss = 2.5697717666625977
training loss = 1.651557207107544 9200
val loss = 2.536485195159912
training loss = 1.6514919996261597 9300
val loss = 2.5339510440826416
training loss = 1.651533842086792 9400
val loss = 2.5438570976257324
training loss = 1.6513670682907104 9500
val loss = 2.5352962017059326
training loss = 1.6513689756393433 9600
val loss = 2.5398497581481934
training loss = 1.6512477397918701 9700
val loss = 2.5340681076049805
training loss = 1.763940691947937 9800
val loss = 2.347813844680786
training loss = 1.6511462926864624 9900
val loss = 2.5352542400360107
training loss = 1.6510658264160156 10000
val loss = 2.533339500427246
training loss = 1.6514509916305542 10100
val loss = 2.551426410675049
training loss = 1.6509603261947632 10200
val loss = 2.532216787338257
training loss = 1.7185754776000977 10300
val loss = 2.369868278503418
training loss = 1.650861382484436 10400
val loss = 2.5322561264038086
training loss = 1.65078866481781 10500
val loss = 2.531552791595459
training loss = 1.6508989334106445 10600
val loss = 2.541921615600586
training loss = 1.6506887674331665 10700
val loss = 2.530855417251587
training loss = 1.7354683876037598 10800
val loss = 2.357213258743286
training loss = 1.6505993604660034 10900
val loss = 2.5304031372070312
training loss = 1.6505223512649536 11000
val loss = 2.530160903930664
training loss = 1.6509262323379517 11100
val loss = 2.548396587371826
training loss = 1.6504310369491577 11200
val loss = 2.529542922973633
training loss = 1.6503568887710571 11300
val loss = 2.529139757156372
training loss = 1.6512885093688965 11400
val loss = 2.55731201171875
training loss = 1.650266408920288 11500
val loss = 2.528972864151001
training loss = 1.686630129814148 11600
val loss = 2.3961310386657715
training loss = 1.6501833200454712 11700
val loss = 2.5279111862182617
training loss = 1.6501007080078125 11800
val loss = 2.5280838012695312
training loss = 1.6520063877105713 11900
val loss = 2.5686185359954834
training loss = 1.650008201599121 12000
val loss = 2.5277698040008545
training loss = 1.7182152271270752 12100
val loss = 2.822716474533081
training loss = 1.649924874305725 12200
val loss = 2.5276038646698
training loss = 1.6498358249664307 12300
val loss = 2.5267603397369385
training loss = 1.6697418689727783 12400
val loss = 2.671994209289551
training loss = 1.649757981300354 12500
val loss = 2.5270133018493652
training loss = 1.6496702432632446 12600
val loss = 2.526244640350342
training loss = 1.6519570350646973 12700
val loss = 2.4868555068969727
training loss = 1.649586796760559 12800
val loss = 2.5255889892578125
training loss = 1.6495014429092407 12900
val loss = 2.5258665084838867
training loss = 1.6495360136032104 13000
val loss = 2.5295357704162598
training loss = 1.6494168043136597 13100
val loss = 2.524876832962036
training loss = 1.7635903358459473 13200
val loss = 2.33614182472229
training loss = 1.649335503578186 13300
val loss = 2.523585081100464
training loss = 1.6492408514022827 13400
val loss = 2.5246176719665527
training loss = 1.6518985033035278 13500
val loss = 2.4823355674743652
training loss = 1.649167776107788 13600
val loss = 2.5242199897766113
training loss = 1.6490782499313354 13700
val loss = 2.5248470306396484
training loss = 1.6490857601165771 13800
val loss = 2.524815559387207
training loss = 1.6489880084991455 13900
val loss = 2.523575782775879
training loss = 1.6526457071304321 14000
val loss = 2.580681800842285
training loss = 1.6489081382751465 14100
val loss = 2.523733615875244
training loss = 1.8958059549331665 14200
val loss = 2.3197927474975586
training loss = 1.6488393545150757 14300
val loss = 2.522895574569702
training loss = 1.6487467288970947 14400
val loss = 2.5227479934692383
training loss = 1.6511499881744385 14500
val loss = 2.5690579414367676
training loss = 1.6486842632293701 14600
val loss = 2.5224974155426025
training loss = 1.6487284898757935 14700
val loss = 2.5128705501556396
training loss = 1.648639440536499 14800
val loss = 2.5196292400360107
training loss = 1.6485378742218018 14900
val loss = 2.5221567153930664
training loss = 1.6486822366714478 15000
val loss = 2.5128836631774902
training loss = 1.6484748125076294 15100
val loss = 2.5224575996398926
training loss = 1.7480324506759644 15200
val loss = 2.334566831588745
training loss = 1.648418664932251 15300
val loss = 2.524137496948242
training loss = 1.6483445167541504 15400
val loss = 2.5240869522094727
training loss = 1.6484075784683228 15500
val loss = 2.5271172523498535
training loss = 1.64829421043396 15600
val loss = 2.521730899810791
training loss = 1.648339867591858 15700
val loss = 2.5274364948272705
training loss = 1.6482653617858887 15800
val loss = 2.5236682891845703
training loss = 1.6481847763061523 15900
val loss = 2.521653413772583
training loss = 1.6552375555038452 16000
val loss = 2.4546563625335693
training loss = 1.6481454372406006 16100
val loss = 2.522125720977783
training loss = 1.6483733654022217 16200
val loss = 2.5067880153656006
training loss = 1.6481468677520752 16300
val loss = 2.5270352363586426
training loss = 1.648041009902954 16400
val loss = 2.5216569900512695
training loss = 1.6643744707107544 16500
val loss = 2.4253876209259033
training loss = 1.648003339767456 16600
val loss = 2.522602081298828
training loss = 1.6488269567489624 16700
val loss = 2.5494699478149414
training loss = 1.647986650466919 16800
val loss = 2.5186240673065186
training loss = 1.6478981971740723 16900
val loss = 2.5217082500457764
training loss = 1.648748517036438 17000
val loss = 2.549178123474121
training loss = 1.6478863954544067 17100
val loss = 2.522261142730713
training loss = 1.6478139162063599 17200
val loss = 2.521704912185669
training loss = 1.6563106775283813 17300
val loss = 2.6121268272399902
training loss = 1.6477892398834229 17400
val loss = 2.5207135677337646
training loss = 1.6477200984954834 17500
val loss = 2.52232027053833
training loss = 1.6482839584350586 17600
val loss = 2.543832778930664
training loss = 1.6476777791976929 17700
val loss = 2.5222413539886475
training loss = 1.658972978591919 17800
val loss = 2.630017042160034
training loss = 1.6476335525512695 17900
val loss = 2.5232162475585938
training loss = 1.9115564823150635 18000
val loss = 2.3137471675872803
training loss = 1.6475921869277954 18100
val loss = 2.5252087116241455
training loss = 1.647600531578064 18200
val loss = 2.531017303466797
training loss = 1.6476036310195923 18300
val loss = 2.515899658203125
training loss = 1.6474634408950806 18400
val loss = 2.5226988792419434
training loss = 1.6480746269226074 18500
val loss = 2.545743227005005
training loss = 1.6474179029464722 18600
val loss = 2.5223937034606934
training loss = 1.6544525623321533 18700
val loss = 2.4536099433898926
training loss = 1.6473973989486694 18800
val loss = 2.5277247428894043
training loss = 1.6472984552383423 18900
val loss = 2.5231943130493164
training loss = 1.6524208784103394 19000
val loss = 2.4640631675720215
training loss = 1.6472598314285278 19100
val loss = 2.523714303970337
training loss = 1.647184133529663 19200
val loss = 2.523228645324707
training loss = 1.6483430862426758 19300
val loss = 2.556049108505249
training loss = 1.6471333503723145 19400
val loss = 2.523634195327759
training loss = 1.6628443002700806 19500
val loss = 2.4248900413513184
training loss = 1.647095799446106 19600
val loss = 2.528146743774414
training loss = 1.646996021270752 19700
val loss = 2.5240848064422607
training loss = 1.647655725479126 19800
val loss = 2.5482020378112793
training loss = 1.646935224533081 19900
val loss = 2.5240354537963867
training loss = 1.6480140686035156 20000
val loss = 2.556579828262329
training loss = 1.6468968391418457 20100
val loss = 2.529831886291504
training loss = 1.6467825174331665 20200
val loss = 2.5245842933654785
training loss = 1.6479557752609253 20300
val loss = 2.5575709342956543
training loss = 1.6467151641845703 20400
val loss = 2.524982213973999
training loss = 1.646633505821228 20500
val loss = 2.5235562324523926
training loss = 1.6467896699905396 20600
val loss = 2.5148026943206787
training loss = 1.6465473175048828 20700
val loss = 2.525224208831787
training loss = 1.6550487279891968 20800
val loss = 2.4503602981567383
training loss = 1.646466851234436 20900
val loss = 2.5250613689422607
training loss = 1.6463732719421387 21000
val loss = 2.5264644622802734
training loss = 1.6465842723846436 21100
val loss = 2.5400941371917725
training loss = 1.6462703943252563 21200
val loss = 2.52575421333313
training loss = 1.6523022651672363 21300
val loss = 2.6032943725585938
training loss = 1.646165370941162 21400
val loss = 2.526304244995117
training loss = 1.6460824012756348 21500
val loss = 2.530343532562256
training loss = 1.6461079120635986 21600
val loss = 2.5308141708374023
training loss = 1.645976185798645 21700
val loss = 2.526660919189453
training loss = 1.6458733081817627 21800
val loss = 2.5265488624572754
training loss = 1.6462761163711548 21900
val loss = 2.5468647480010986
training loss = 1.6457509994506836 22000
val loss = 2.5274410247802734
training loss = 1.6617722511291504 22100
val loss = 2.4283485412597656
training loss = 1.6456189155578613 22200
val loss = 2.5291800498962402
training loss = 1.6455801725387573 22300
val loss = 2.536557674407959
training loss = 1.6454836130142212 22400
val loss = 2.530427932739258
training loss = 1.6453531980514526 22500
val loss = 2.528419017791748
training loss = 1.6488596200942993 22600
val loss = 2.4774208068847656
training loss = 1.6451927423477173 22700
val loss = 2.528381824493408
training loss = 1.7584296464920044 22800
val loss = 2.951629638671875
training loss = 1.6450234651565552 22900
val loss = 2.52955961227417
training loss = 1.6448827981948853 23000
val loss = 2.530125856399536
training loss = 1.6448482275009155 23100
val loss = 2.5326597690582275
training loss = 1.644687533378601 23200
val loss = 2.5300991535186768
training loss = 1.648476243019104 23300
val loss = 2.477181911468506
training loss = 1.644490361213684 23400
val loss = 2.5301079750061035
training loss = 1.644407033920288 23500
val loss = 2.522639274597168
training loss = 1.6443578004837036 23600
val loss = 2.5231568813323975
training loss = 1.6441080570220947 23700
val loss = 2.531040668487549
training loss = 1.6570264101028442 23800
val loss = 2.651564121246338
training loss = 1.6438713073730469 23900
val loss = 2.5312345027923584
training loss = 1.6438063383102417 24000
val loss = 2.521810531616211
training loss = 1.643613576889038 24100
val loss = 2.53141450881958
training loss = 1.6434143781661987 24200
val loss = 2.53214693069458
training loss = 1.647977590560913 24300
val loss = 2.6020493507385254
training loss = 1.6431273221969604 24400
val loss = 2.532689094543457
training loss = 1.6520166397094727 24500
val loss = 2.4512290954589844
training loss = 1.6428385972976685 24600
val loss = 2.5302836894989014
training loss = 1.6425973176956177 24700
val loss = 2.5336313247680664
training loss = 1.6435298919677734 24800
val loss = 2.5049169063568115
training loss = 1.6422513723373413 24900
val loss = 2.5344491004943848
training loss = 1.8232513666152954 25000
val loss = 2.3012232780456543
training loss = 1.6418942213058472 25100
val loss = 2.536163330078125
training loss = 1.6416305303573608 25200
val loss = 2.535071849822998
training loss = 1.6465368270874023 25300
val loss = 2.4723119735717773
training loss = 1.6412535905838013 25400
val loss = 2.5357279777526855
training loss = 1.6409696340560913 25500
val loss = 2.536083698272705
training loss = 1.6408710479736328 25600
val loss = 2.540372610092163
training loss = 1.6405590772628784 25700
val loss = 2.536386728286743
training loss = 1.9419686794281006 25800
val loss = 2.2932658195495605
training loss = 1.6401194334030151 25900
val loss = 2.5336833000183105
training loss = 1.6397960186004639 26000
val loss = 2.5380234718322754
training loss = 1.6398800611495972 26100
val loss = 2.5524919033050537
training loss = 1.6393343210220337 26200
val loss = 2.537601947784424
training loss = 1.6437617540359497 26300
val loss = 2.4756765365600586
training loss = 1.6388468742370605 26400
val loss = 2.5374670028686523
training loss = 1.6572390794754028 26500
val loss = 2.688028573989868
training loss = 1.6383776664733887 26600
val loss = 2.5408053398132324
training loss = 1.6380336284637451 26700
val loss = 2.5388545989990234
training loss = 1.6379510164260864 26800
val loss = 2.532595157623291
training loss = 1.6375794410705566 26900
val loss = 2.53859806060791
training loss = 1.977912187576294 27000
val loss = 3.453831672668457
training loss = 1.6371452808380127 27100
val loss = 2.538602828979492
training loss = 1.636803150177002 27200
val loss = 2.5393729209899902
training loss = 1.6383637189865112 27300
val loss = 2.4988982677459717
training loss = 1.636336088180542 27400
val loss = 2.5388269424438477
training loss = 1.7783390283584595 27500
val loss = 2.293654441833496
training loss = 1.6358622312545776 27600
val loss = 2.539548873901367
training loss = 1.6359846591949463 27700
val loss = 2.5643181800842285
training loss = 1.6354191303253174 27800
val loss = 2.5441431999206543
training loss = 1.6350077390670776 27900
val loss = 2.541515350341797
training loss = 1.6349875926971436 28000
val loss = 2.53197979927063
training loss = 1.63450288772583 28100
val loss = 2.5420875549316406
training loss = 1.6366264820098877 28200
val loss = 2.588160991668701
training loss = 1.6340610980987549 28300
val loss = 2.5412423610687256
training loss = 1.6334961652755737 28400
val loss = 2.5439248085021973
training loss = 1.633702278137207 28500
val loss = 2.538823366165161
training loss = 1.633222222328186 28600
val loss = 2.5429210662841797
training loss = 1.6326261758804321 28700
val loss = 2.5452327728271484
training loss = 1.6337491273880005 28800
val loss = 2.5712499618530273
training loss = 1.6324232816696167 28900
val loss = 2.5438051223754883
training loss = 1.6318392753601074 29000
val loss = 2.545064926147461
training loss = 1.643225073814392 29100
val loss = 2.655888557434082
training loss = 1.631656289100647 29200
val loss = 2.545588493347168
training loss = 1.6310899257659912 29300
val loss = 2.5452964305877686
training loss = 1.630117416381836 29400
val loss = 2.5488924980163574
training loss = 1.6310127973556519 29500
val loss = 2.5538289546966553
training loss = 1.63038170337677 29600
val loss = 2.547438144683838
training loss = 1.629620909690857 29700
val loss = 2.5463497638702393
training loss = 1.6763943433761597 29800
val loss = 2.349090099334717
training loss = 1.6297162771224976 29900
val loss = 2.5529122352600098
training loss = 1.6290254592895508 30000
val loss = 2.548476457595825
reduced chi^2 level 2 = 1.6290160417556763
Constrained alpha: 1.915190577507019
Constrained beta: 1.0413150787353516
Constrained gamma: 12.334787368774414
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 875.3207,  875.8178,  999.8779,  974.1818, 1002.3735, 1036.7812,
        1094.5709, 1151.2235, 1175.7612, 1227.8053, 1293.4777, 1160.9301,
        1255.3669, 1210.9097, 1309.0076, 1384.5424, 1354.2212, 1448.8719,
        1550.7526, 1469.5350, 1640.8605, 1535.0929, 1548.3685, 1668.5393,
        1714.9325, 1748.9510, 1684.2518, 1651.3149, 1677.3829, 1731.7961,
        1667.4873, 1838.7013, 1757.2563, 1679.3186, 1703.0773, 1711.9270,
        1713.2000, 1617.2347, 1605.6055, 1627.9243, 1685.4728, 1529.7517,
        1465.9370, 1512.5936, 1360.7913, 1373.3038, 1237.1089, 1236.9475,
        1115.4238, 1147.0420, 1055.7516,  981.5637,  953.9307,  941.2162,
         887.2151,  851.3663,  856.7352,  691.2830,  604.9299,  576.2670,
         562.8106,  491.9318,  447.6311,  371.6919,  334.2207,  359.7227,
         311.9761,  254.1184,  201.6502,  183.1592,  157.5802,  126.9965,
         134.6062,  105.4130,   87.5666,   64.6995,   51.0911,   39.5783,
          38.2172,   32.0241,   16.9491,   32.3409,   45.8962])]
2755.6035283663023
1.6044180940743908 3.0278880665539543 72.5336126788472
val isze = 8
idinces = [ 4  6 43 13 11 58 27 73 51 57 52  5 54 74 56 36 31 59 44 19 53 29 28 66
 21 16 12 67  3 68 77 48 72 42 70 46 20  9  0 25 71 69  8 35 47 32 41 65
 24 45  7 39 10 81 40 62 37 64 82 33  2 80 55 63 76 78 38 22 75 61 18 34
  1 15 60 17 50 14 30 79 49 23 26]
we are doing training validation split
training loss = 33.722049713134766 100
val loss = 48.41916275024414
training loss = 25.313589096069336 200
val loss = 36.54425811767578
training loss = 18.351526260375977 300
val loss = 27.7556095123291
training loss = 13.586243629455566 400
val loss = 21.684329986572266
training loss = 10.431717872619629 500
val loss = 17.400863647460938
training loss = 8.27871322631836 600
val loss = 14.274751663208008
training loss = 6.759281635284424 700
val loss = 11.923210144042969
training loss = 5.747591495513916 800
val loss = 9.993423461914062
training loss = 5.188881874084473 900
val loss = 8.652786254882812
training loss = 4.95060396194458 1000
val loss = 7.840616226196289
training loss = 4.824509620666504 1100
val loss = 7.421965599060059
training loss = 4.712545871734619 1200
val loss = 7.177149772644043
training loss = 4.591132164001465 1300
val loss = 6.985959053039551
training loss = 4.455289363861084 1400
val loss = 6.795719623565674
training loss = 4.301483631134033 1500
val loss = 6.5859832763671875
training loss = 4.124585151672363 1600
val loss = 6.34487247467041
training loss = 3.917118787765503 1700
val loss = 6.061742782592773
training loss = 3.6692888736724854 1800
val loss = 5.72422981262207
training loss = 3.3731038570404053 1900
val loss = 5.210472106933594
training loss = 3.0281476974487305 2000
val loss = 4.828360557556152
training loss = 2.6673028469085693 2100
val loss = 4.322464942932129
training loss = 2.357983350753784 2200
val loss = 3.7772324085235596
training loss = 2.1611297130584717 2300
val loss = 3.310563325881958
training loss = 2.0687150955200195 2400
val loss = 3.0769124031066895
training loss = 2.0339088439941406 2500
val loss = 2.8731491565704346
training loss = 2.022836446762085 2600
val loss = 2.8029563426971436
training loss = 2.0191609859466553 2700
val loss = 2.7513232231140137
training loss = 2.0179452896118164 2800
val loss = 2.751957416534424
training loss = 2.017364740371704 2900
val loss = 2.732023239135742
training loss = 2.017343044281006 3000
val loss = 2.760775089263916
training loss = 2.017001152038574 3100
val loss = 2.732081651687622
training loss = 2.0194637775421143 3200
val loss = 2.630384683609009
training loss = 2.016873836517334 3300
val loss = 2.737703800201416
training loss = 2.016787528991699 3400
val loss = 2.749885082244873
training loss = 2.016726016998291 3500
val loss = 2.734957218170166
training loss = 2.0165388584136963 3600
val loss = 2.7461748123168945
training loss = 2.0163938999176025 3700
val loss = 2.7636709213256836
training loss = 2.0160346031188965 3800
val loss = 2.7541239261627197
training loss = 2.1388299465179443 3900
val loss = 2.120358943939209
training loss = 2.0150935649871826 4000
val loss = 2.7581162452697754
training loss = 2.014374256134033 4100
val loss = 2.7634148597717285
training loss = 2.013920783996582 4200
val loss = 2.7227590084075928
training loss = 2.0123672485351562 4300
val loss = 2.7691009044647217
training loss = 2.011591911315918 4400
val loss = 2.8242886066436768
training loss = 2.009336471557617 4500
val loss = 2.7779321670532227
training loss = 2.007338047027588 4600
val loss = 2.7784085273742676
training loss = 2.0051934719085693 4700
val loss = 2.811767101287842
training loss = 2.0022788047790527 4800
val loss = 2.784303665161133
training loss = 1.999092698097229 4900
val loss = 2.7871718406677246
training loss = 1.9957294464111328 5000
val loss = 2.827119827270508
training loss = 1.9913067817687988 5100
val loss = 2.79266357421875
training loss = 1.9889675378799438 5200
val loss = 2.9095590114593506
training loss = 1.9813082218170166 5300
val loss = 2.8127522468566895
training loss = 1.9753129482269287 5400
val loss = 2.802349090576172
training loss = 1.9736675024032593 5500
val loss = 2.96970796585083
training loss = 1.9615644216537476 5600
val loss = 2.8088316917419434
training loss = 1.9539331197738647 5700
val loss = 2.858578681945801
training loss = 1.9451814889907837 5800
val loss = 2.8014631271362305
training loss = 1.9359407424926758 5900
val loss = 2.8231921195983887
training loss = 1.927918791770935 6000
val loss = 2.918349027633667
training loss = 1.9163464307785034 6100
val loss = 2.8336238861083984
training loss = 2.0314037799835205 6200
val loss = 3.7621712684631348
training loss = 1.8954895734786987 6300
val loss = 2.852935314178467
training loss = 1.8848358392715454 6400
val loss = 2.848898410797119
training loss = 1.8765077590942383 6500
val loss = 2.9612293243408203
training loss = 1.8639036417007446 6600
val loss = 2.8582491874694824
training loss = 1.9990700483322144 6700
val loss = 3.859312057495117
training loss = 1.8436737060546875 6800
val loss = 2.8732569217681885
training loss = 1.833713412284851 6900
val loss = 2.8738656044006348
training loss = 1.8246735334396362 7000
val loss = 2.824195146560669
training loss = 1.814491629600525 7100
val loss = 2.8822474479675293
training loss = 1.9507615566253662 7200
val loss = 3.8770320415496826
training loss = 1.7953996658325195 7300
val loss = 2.8880467414855957
training loss = 1.7857383489608765 7400
val loss = 2.915759325027466
training loss = 1.7764108180999756 7500
val loss = 2.887307643890381
training loss = 1.766728162765503 7600
val loss = 2.9109249114990234
training loss = 1.7573457956314087 7700
val loss = 2.9441957473754883
training loss = 1.7478320598602295 7800
val loss = 2.924609661102295
training loss = 1.7381000518798828 7900
val loss = 2.928802490234375
training loss = 1.7288776636123657 8000
val loss = 2.928521156311035
training loss = 1.7193551063537598 8100
val loss = 2.951664686203003
training loss = 1.7102470397949219 8200
val loss = 2.9809622764587402
training loss = 1.7011582851409912 8300
val loss = 2.9722325801849365
training loss = 2.1846935749053955 8400
val loss = 2.0741584300994873
training loss = 1.6840174198150635 8500
val loss = 2.999837875366211
training loss = 1.6757996082305908 8600
val loss = 3.0031487941741943
training loss = 1.669679045677185 8700
val loss = 2.945368766784668
training loss = 1.6614528894424438 8800
val loss = 3.0259909629821777
training loss = 1.757707118988037 8900
val loss = 3.783310890197754
training loss = 1.6494178771972656 9000
val loss = 3.050844192504883
training loss = 1.6441642045974731 9100
val loss = 3.0569705963134766
training loss = 1.639902114868164 9200
val loss = 3.05997371673584
training loss = 1.635949730873108 9300
val loss = 3.0772695541381836
training loss = 1.6475752592086792 9400
val loss = 2.8503737449645996
training loss = 1.6296789646148682 9500
val loss = 3.0982282161712646
training loss = 1.6270934343338013 9600
val loss = 3.10752534866333
reduced chi^2 level 2 = 1.74261474609375
Constrained alpha: 1.8737962245941162
Constrained beta: 1.8291255235671997
Constrained gamma: 24.885427474975586
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 865.0751,  813.0466,  920.1080,  968.7167, 1008.3447, 1027.0968,
        1107.2628, 1069.7477, 1154.1774, 1150.2844, 1161.5458, 1143.8424,
        1200.2156, 1229.6836, 1263.7786, 1399.5352, 1366.7673, 1432.3152,
        1522.8511, 1496.5802, 1640.7471, 1549.7151, 1569.5883, 1646.6191,
        1550.4587, 1747.5508, 1614.3779, 1721.3730, 1598.5513, 1660.5818,
        1707.0518, 1705.8146, 1743.2705, 1823.6444, 1641.6511, 1740.5923,
        1718.1649, 1600.7241, 1570.9817, 1630.3237, 1614.5376, 1547.7661,
        1483.7190, 1522.3091, 1323.5817, 1253.0144, 1357.5103, 1224.6786,
        1161.0635, 1122.0695, 1099.7832,  969.2414,  903.5573,  910.2559,
         849.6826,  831.7263,  788.2852,  685.3493,  570.3533,  554.0907,
         511.1863,  492.5539,  446.1094,  398.3812,  338.0410,  345.7439,
         307.1169,  264.9170,  216.5539,  164.8295,  163.6400,  165.6103,
         153.7199,  106.9691,  104.7242,   77.8177,   47.0623,   37.5655,
          34.1232,   46.8616,   19.4893,   40.0358,   31.6706])]
2987.5717067299925
1.221228826665146 1.5450985890673996 61.62359485930128
val isze = 8
idinces = [58 42  9 35  4 62 81 60 65 37 80 68 27 22 66 10 17 23 74 71 75 39 38 54
  3 12 11 82 59 41 26 45 13  6 24 44 32 40 28 25 21 55 49 43 63 33  8 70
 72 67 52 15 57 34 14 61  5 47  2 53 31 51 46  1 73 78 30  7 69  0 36 29
 77 16 56 19 76 64 20 50 18 48 79]
we are doing training validation split
training loss = 32.035240173339844 100
val loss = 35.01715087890625
training loss = 19.811487197875977 200
val loss = 24.788856506347656
training loss = 11.92479419708252 300
val loss = 18.108957290649414
training loss = 7.614209175109863 400
val loss = 13.538494110107422
training loss = 6.061411380767822 500
val loss = 10.702181816101074
training loss = 5.190351486206055 600
val loss = 8.614665985107422
training loss = 4.304284572601318 700
val loss = 6.528130531311035
training loss = 3.4573638439178467 800
val loss = 4.447842121124268
training loss = 2.85797119140625 900
val loss = 2.8418028354644775
training loss = 2.552001476287842 1000
val loss = 1.9384727478027344
training loss = 2.4062135219573975 1100
val loss = 1.5200963020324707
training loss = 2.319225788116455 1200
val loss = 1.344204068183899
training loss = 2.253899097442627 1300
val loss = 1.2700446844100952
training loss = 2.2021195888519287 1400
val loss = 1.2466940879821777
training loss = 2.1602799892425537 1500
val loss = 1.237943410873413
training loss = 2.126819610595703 1600
val loss = 1.2407580614089966
training loss = 2.099620819091797 1700
val loss = 1.2453120946884155
training loss = 2.0776212215423584 1800
val loss = 1.2551692724227905
training loss = 2.059518337249756 1900
val loss = 1.2676175832748413
training loss = 2.044618844985962 2000
val loss = 1.2842344045639038
training loss = 2.0318896770477295 2100
val loss = 1.2961088418960571
training loss = 2.073747158050537 2200
val loss = 1.31231689453125
training loss = 2.0116984844207764 2300
val loss = 1.3255788087844849
training loss = 2.0034468173980713 2400
val loss = 1.3392797708511353
training loss = 1.9961670637130737 2500
val loss = 1.3545408248901367
training loss = 1.9894403219223022 2600
val loss = 1.3656833171844482
training loss = 1.9833616018295288 2700
val loss = 1.3778395652770996
training loss = 1.9777417182922363 2800
val loss = 1.3894309997558594
training loss = 1.972633957862854 2900
val loss = 1.4002963304519653
training loss = 1.968367099761963 3000
val loss = 1.4137238264083862
training loss = 1.9633960723876953 3100
val loss = 1.4207096099853516
training loss = 1.9597978591918945 3200
val loss = 1.4313619136810303
training loss = 1.9553933143615723 3300
val loss = 1.439395546913147
training loss = 1.958465576171875 3400
val loss = 1.4460488557815552
training loss = 1.9484267234802246 3500
val loss = 1.4563860893249512
training loss = 1.9452917575836182 3600
val loss = 1.4638599157333374
training loss = 1.9429398775100708 3700
val loss = 1.4731364250183105
training loss = 1.9395906925201416 3800
val loss = 1.4782123565673828
training loss = 1.93708336353302 3900
val loss = 1.4850269556045532
training loss = 1.9347807168960571 4000
val loss = 1.4904541969299316
training loss = 1.932450771331787 4100
val loss = 1.496883749961853
training loss = 1.9304065704345703 4200
val loss = 1.5024125576019287
training loss = 1.9286247491836548 4300
val loss = 1.5068354606628418
training loss = 1.9266648292541504 4400
val loss = 1.5126792192459106
training loss = 1.9249882698059082 4500
val loss = 1.5154469013214111
training loss = 1.9233765602111816 4600
val loss = 1.5216320753097534
training loss = 1.9222601652145386 4700
val loss = 1.5270687341690063
training loss = 1.920535922050476 4800
val loss = 1.5300651788711548
training loss = 1.9191628694534302 4900
val loss = 1.533289909362793
training loss = 1.918230414390564 5000
val loss = 1.5378127098083496
training loss = 1.916761875152588 5100
val loss = 1.5402594804763794
training loss = 1.9156256914138794 5200
val loss = 1.5431344509124756
training loss = 1.9207251071929932 5300
val loss = 1.553626537322998
training loss = 1.9135547876358032 5400
val loss = 1.5488598346710205
training loss = 1.9126560688018799 5500
val loss = 1.551363468170166
training loss = 1.9116982221603394 5600
val loss = 1.5542654991149902
training loss = 1.910814642906189 5700
val loss = 1.5564740896224976
training loss = 1.9337102174758911 5800
val loss = 1.5738375186920166
training loss = 1.9091793298721313 5900
val loss = 1.560943841934204
training loss = 1.9084256887435913 6000
val loss = 1.563164472579956
training loss = 1.9077081680297852 6100
val loss = 1.565147876739502
training loss = 1.9069849252700806 6200
val loss = 1.567091703414917
training loss = 1.9063798189163208 6300
val loss = 1.569275975227356
training loss = 1.9057401418685913 6400
val loss = 1.5707238912582397
training loss = 1.9050401449203491 6500
val loss = 1.5726429224014282
reduced chi^2 level 2 = 1.9048711061477661
Constrained alpha: 1.7805474996566772
Constrained beta: 3.1515307426452637
Constrained gamma: 23.661596298217773
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 864.4086,  897.5377,  903.7740,  942.8760, 1005.5659, 1070.2356,
        1088.4138, 1127.6396, 1124.1719, 1158.1735, 1193.2208, 1203.3783,
        1279.1992, 1326.9761, 1342.5907, 1359.2358, 1414.1691, 1429.7710,
        1481.7007, 1573.5543, 1578.3981, 1596.0945, 1547.5834, 1608.4746,
        1594.7518, 1748.5516, 1628.8588, 1752.0519, 1780.4006, 1648.2538,
        1646.6908, 1771.1072, 1678.3324, 1770.4955, 1699.1376, 1701.1284,
        1655.6154, 1644.0106, 1638.8700, 1587.0085, 1599.6619, 1591.3092,
        1455.5835, 1497.8457, 1272.8369, 1377.6600, 1221.6458, 1233.9879,
        1168.0225, 1126.8878, 1123.6984,  998.8486,  916.0847,  867.0570,
         901.7867,  906.0887,  857.5278,  713.1134,  606.7032,  514.2003,
         546.3476,  482.3090,  414.5718,  418.5197,  355.2364,  330.6693,
         266.5114,  260.3879,  188.5189,  192.5002,  143.1557,  140.5874,
         146.2057,  119.0109,   88.2073,   56.3220,   55.4953,   50.4775,
          29.9845,   51.3668,   14.4578,   42.7777,   48.9730])]
2631.1888556883146
2.0726282425073124 19.833843458897256 85.99866658135062
val isze = 8
idinces = [50 68  5 80 72 28 17 29 65 41 46 64 22 79  0 33 11 47 74 45 81  1 21 14
 63 62  4 18 61 38 55 48 30  2 36 10 69  9 27 31 37 57 51 32 78  6 58 76
 71  3 25 49 73 75 15 70 67  7 66 53 20 59 44 42 34 40 35 54 39 16 26 77
 24 43 23 52 60 82 56 12 19 13  8]
we are doing training validation split
training loss = 367.91851806640625 100
val loss = 259.3304443359375
training loss = 46.85287857055664 200
val loss = 42.77594757080078
training loss = 13.421590805053711 300
val loss = 17.10879135131836
training loss = 12.741686820983887 400
val loss = 16.262359619140625
training loss = 12.006363868713379 500
val loss = 15.296164512634277
training loss = 11.248309135437012 600
val loss = 14.256237030029297
training loss = 10.501059532165527 700
val loss = 13.178133964538574
training loss = 9.797860145568848 800
val loss = 12.10276985168457
training loss = 9.168753623962402 900
val loss = 11.074082374572754
training loss = 8.636250495910645 1000
val loss = 10.134176254272461
training loss = 8.210824012756348 1100
val loss = 9.316418647766113
training loss = 7.88852071762085 1200
val loss = 8.638259887695312
training loss = 7.653003215789795 1300
val loss = 8.098791122436523
training loss = 7.481561183929443 1400
val loss = 7.681309700012207
training loss = 7.352159023284912 1500
val loss = 7.361565589904785
training loss = 7.247933864593506 1600
val loss = 7.115034103393555
training loss = 7.158336639404297 1700
val loss = 6.9213786125183105
training loss = 7.077882766723633 1800
val loss = 6.7656683921813965
training loss = 7.004146575927734 1900
val loss = 6.637279033660889
training loss = 6.935964584350586 2000
val loss = 6.528632164001465
training loss = 6.872416973114014 2100
val loss = 6.434047222137451
training loss = 6.812446594238281 2200
val loss = 6.349130630493164
training loss = 6.754868507385254 2300
val loss = 6.270575523376465
training loss = 6.698337078094482 2400
val loss = 6.195520877838135
training loss = 6.641196250915527 2500
val loss = 6.121203422546387
training loss = 6.581005573272705 2600
val loss = 6.043971538543701
training loss = 6.513491153717041 2700
val loss = 5.958083152770996
training loss = 6.430065155029297 2800
val loss = 5.852617263793945
training loss = 6.310927391052246 2900
val loss = 5.703609466552734
training loss = 6.102595806121826 3000
val loss = 5.45060920715332
training loss = 5.651662826538086 3100
val loss = 4.941134452819824
training loss = 4.725654602050781 3200
val loss = 4.065166473388672
training loss = 3.3965394496917725 3300
val loss = 3.103989362716675
training loss = 2.476020574569702 3400
val loss = 2.713618278503418
training loss = 2.3361411094665527 3500
val loss = 2.7986693382263184
training loss = 2.3127262592315674 3600
val loss = 2.845219612121582
training loss = 2.295475959777832 3700
val loss = 2.86322021484375
training loss = 2.281923294067383 3800
val loss = 2.8716678619384766
training loss = 2.271357774734497 3900
val loss = 2.8769917488098145
training loss = 2.2626824378967285 4000
val loss = 2.8791279792785645
training loss = 2.2550337314605713 4100
val loss = 2.892669677734375
training loss = 2.247211217880249 4200
val loss = 2.8977084159851074
training loss = 2.239366292953491 4300
val loss = 2.889539957046509
training loss = 2.229048013687134 4400
val loss = 2.920045852661133
training loss = 2.226346015930176 4500
val loss = 2.8714582920074463
training loss = 2.204399585723877 4600
val loss = 2.9513766765594482
training loss = 2.215707540512085 4700
val loss = 3.1122519969940186
training loss = 2.173553705215454 4800
val loss = 2.989081382751465
training loss = 2.1568570137023926 4900
val loss = 3.0094738006591797
training loss = 2.1411635875701904 5000
val loss = 3.051424503326416
training loss = 2.124030113220215 5100
val loss = 3.0459539890289307
training loss = 2.107555866241455 5200
val loss = 3.0588412284851074
training loss = 2.0915043354034424 5300
val loss = 3.077446460723877
training loss = 2.075404167175293 5400
val loss = 3.0883331298828125
training loss = 2.0665411949157715 5500
val loss = 3.167370557785034
training loss = 2.043300151824951 5600
val loss = 3.1121277809143066
training loss = 2.0357179641723633 5700
val loss = 3.195718288421631
training loss = 2.0117359161376953 5800
val loss = 3.13726806640625
training loss = 1.996382236480713 5900
val loss = 3.14495849609375
training loss = 1.981943964958191 6000
val loss = 3.143458604812622
training loss = 1.9675887823104858 6100
val loss = 3.1679439544677734
training loss = 1.9654488563537598 6200
val loss = 3.1181411743164062
training loss = 1.9422197341918945 6300
val loss = 3.1911349296569824
training loss = 1.9311182498931885 6400
val loss = 3.1990511417388916
training loss = 1.9215409755706787 6500
val loss = 3.2049286365509033
training loss = 1.9129387140274048 6600
val loss = 3.2213783264160156
training loss = 1.9056726694107056 6700
val loss = 3.241748809814453
training loss = 1.8990060091018677 6800
val loss = 3.239102840423584
training loss = 1.8932304382324219 6900
val loss = 3.2458014488220215
training loss = 1.8910181522369385 7000
val loss = 3.2182440757751465
training loss = 1.8834866285324097 7100
val loss = 3.255723237991333
training loss = 1.9110832214355469 7200
val loss = 3.169539451599121
training loss = 1.8753710985183716 7300
val loss = 3.259967803955078
training loss = 1.8717437982559204 7400
val loss = 3.258631706237793
training loss = 1.8690519332885742 7500
val loss = 3.2402422428131104
training loss = 1.8651574850082397 7600
val loss = 3.254296064376831
training loss = 1.9582843780517578 7700
val loss = 3.538867950439453
training loss = 1.8590930700302124 7800
val loss = 3.249289035797119
training loss = 1.856238603591919 7900
val loss = 3.2416746616363525
training loss = 1.8985185623168945 8000
val loss = 3.1389927864074707
training loss = 1.8507369756698608 8100
val loss = 3.2305386066436768
training loss = 1.8481026887893677 8200
val loss = 3.226170539855957
training loss = 1.8458964824676514 8300
val loss = 3.2335734367370605
reduced chi^2 level 2 = 1.8431936502456665
Constrained alpha: 1.8009012937545776
Constrained beta: 3.4122464656829834
Constrained gamma: 24.927326202392578
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 882.7518,  890.9308,  996.0476,  938.2693,  985.9768, 1078.0535,
        1143.6138, 1126.1461, 1143.0540, 1152.2577, 1277.2777, 1165.9744,
        1277.3407, 1200.8789, 1317.8295, 1447.5101, 1394.0282, 1457.0148,
        1593.2903, 1482.5426, 1634.0411, 1551.5685, 1645.4607, 1571.9923,
        1618.3982, 1629.3962, 1527.5411, 1728.1118, 1689.3147, 1689.0969,
        1712.8400, 1755.6719, 1762.5323, 1738.3507, 1793.0217, 1764.2527,
        1666.2588, 1610.7975, 1654.7643, 1606.2910, 1678.9680, 1539.7700,
        1505.3092, 1490.8934, 1366.7128, 1388.4558, 1223.4956, 1236.5050,
        1160.1617, 1181.0364,  990.2222, 1016.0409,  911.5454,  920.4128,
         878.6025,  880.3748,  808.1111,  716.2979,  612.7732,  556.6407,
         542.3777,  474.1051,  454.7520,  424.1551,  351.2495,  347.3243,
         302.7286,  253.3670,  217.8432,  168.6348,  152.6257,  142.0271,
         129.6881,  101.7956,   96.8313,   67.9401,   48.1403,   36.5636,
          31.0694,   41.3465,   22.2085,   34.5366,   35.3692])]
2817.0767693074204
0.6901499337496664 19.706838220063975 42.009148529363536
val isze = 8
idinces = [64 49 20 31 13 76 71 16  1 34 29 22  0 44 33 25 27 63 37 77  6 61  9 30
 39 40 58 10 36 67 82 59 60  8 75 68 80 78 38 50 69 18 46 74 72 73  2 54
 43 62 19  3 41 28 81 42 48  5 55  7 65 52 35 17 14 21 32 79 26  4 47 23
 57 24 15 12 66 51 53 45 11 56 70]
we are doing training validation split
training loss = 248.30636596679688 100
val loss = 175.62033081054688
training loss = 37.16794204711914 200
val loss = 41.79505920410156
training loss = 13.450682640075684 300
val loss = 15.377954483032227
training loss = 12.868983268737793 400
val loss = 14.527207374572754
training loss = 12.228460311889648 500
val loss = 13.811775207519531
training loss = 11.533608436584473 600
val loss = 13.019454002380371
training loss = 10.803762435913086 700
val loss = 12.168193817138672
training loss = 10.061212539672852 800
val loss = 11.279053688049316
training loss = 9.33121395111084 900
val loss = 10.377891540527344
training loss = 8.640872955322266 1000
val loss = 9.495197296142578
training loss = 8.016470909118652 1100
val loss = 8.663636207580566
training loss = 7.479753017425537 1200
val loss = 7.91438102722168
training loss = 7.043719291687012 1300
val loss = 7.2715559005737305
training loss = 6.709766864776611 1400
val loss = 6.747550010681152
training loss = 6.4677019119262695 1500
val loss = 6.3407392501831055
training loss = 6.299119472503662 1600
val loss = 6.037319660186768
training loss = 6.182750225067139 1700
val loss = 5.816396713256836
training loss = 6.099445343017578 1800
val loss = 5.655891418457031
training loss = 6.034932613372803 1900
val loss = 5.53676176071167
training loss = 5.980165958404541 2000
val loss = 5.44472599029541
training loss = 5.930210113525391 2100
val loss = 5.370055675506592
training loss = 5.882775783538818 2200
val loss = 5.306645393371582
training loss = 5.837007999420166 2300
val loss = 5.250694274902344
training loss = 5.792728900909424 2400
val loss = 5.200028896331787
training loss = 5.750000476837158 2500
val loss = 5.153347969055176
training loss = 5.708919048309326 2600
val loss = 5.1098504066467285
training loss = 5.669503688812256 2700
val loss = 5.068982124328613
training loss = 5.631644248962402 2800
val loss = 5.0303263664245605
training loss = 5.595059394836426 2900
val loss = 4.993436813354492
training loss = 5.5592498779296875 3000
val loss = 4.957680702209473
training loss = 5.523415565490723 3100
val loss = 4.922271728515625
training loss = 5.486261367797852 3200
val loss = 4.8859710693359375
training loss = 5.4455485343933105 3300
val loss = 4.846750259399414
training loss = 5.396862983703613 3400
val loss = 4.800543785095215
training loss = 5.329963684082031 3500
val loss = 4.73792839050293
training loss = 5.216703414916992 3600
val loss = 4.633141994476318
training loss = 4.975865364074707 3700
val loss = 4.415078163146973
training loss = 4.470605373382568 3800
val loss = 4.002674102783203
training loss = 3.7103118896484375 3900
val loss = 3.481868267059326
training loss = 2.797746181488037 4000
val loss = 2.9485433101654053
training loss = 2.197026014328003 4100
val loss = 2.681839942932129
training loss = 2.080989122390747 4200
val loss = 2.6399364471435547
training loss = 2.0564987659454346 4300
val loss = 2.6198062896728516
training loss = 2.039520025253296 4400
val loss = 2.6002302169799805
training loss = 2.025827646255493 4500
val loss = 2.583103656768799
training loss = 2.0145578384399414 4600
val loss = 2.5683963298797607
training loss = 2.005157947540283 4700
val loss = 2.5557079315185547
training loss = 2.2009384632110596 4800
val loss = 2.648836135864258
training loss = 1.9906946420669556 4900
val loss = 2.534654378890991
training loss = 1.9851586818695068 5000
val loss = 2.5273075103759766
training loss = 2.017225503921509 5100
val loss = 2.587853193283081
training loss = 1.9758317470550537 5200
val loss = 2.5139636993408203
training loss = 1.9719067811965942 5300
val loss = 2.508714199066162
training loss = 1.9682884216308594 5400
val loss = 2.501457691192627
training loss = 1.9649308919906616 5500
val loss = 2.4982783794403076
training loss = 1.9976664781570435 5600
val loss = 2.496323347091675
training loss = 1.9587277173995972 5700
val loss = 2.4898014068603516
training loss = 1.9558216333389282 5800
val loss = 2.4856996536254883
training loss = 1.9531804323196411 5900
val loss = 2.47930645942688
training loss = 1.950270175933838 6000
val loss = 2.4780802726745605
training loss = 1.9475293159484863 6100
val loss = 2.474703311920166
training loss = 1.9447461366653442 6200
val loss = 2.4705686569213867
training loss = 1.9420239925384521 6300
val loss = 2.467275857925415
training loss = 1.9428198337554932 6400
val loss = 2.475989818572998
training loss = 1.93637216091156 6500
val loss = 2.4601035118103027
training loss = 2.1460256576538086 6600
val loss = 2.592982292175293
training loss = 1.9304531812667847 6700
val loss = 2.452042579650879
training loss = 1.927341103553772 6800
val loss = 2.4487056732177734
training loss = 1.924302577972412 6900
val loss = 2.446855068206787
training loss = 1.9207981824874878 7000
val loss = 2.4407150745391846
training loss = 1.9617233276367188 7100
val loss = 2.4500083923339844
training loss = 1.9135644435882568 7200
val loss = 2.432431221008301
training loss = 1.9095985889434814 7300
val loss = 2.427319049835205
training loss = 1.9057434797286987 7400
val loss = 2.420334815979004
training loss = 1.9011632204055786 7500
val loss = 2.417480230331421
training loss = 1.896350383758545 7600
val loss = 2.4120867252349854
training loss = 1.8916406631469727 7700
val loss = 2.404710054397583
training loss = 1.8861584663391113 7800
val loss = 2.4007062911987305
training loss = 1.8805487155914307 7900
val loss = 2.3922841548919678
training loss = 1.8743209838867188 8000
val loss = 2.388741970062256
training loss = 1.8677465915679932 8100
val loss = 2.380916118621826
training loss = 1.886709451675415 8200
val loss = 2.378467321395874
training loss = 1.8537487983703613 8300
val loss = 2.366417169570923
training loss = 1.8461552858352661 8400
val loss = 2.358377695083618
training loss = 1.8396399021148682 8500
val loss = 2.3550350666046143
training loss = 1.8309006690979004 8600
val loss = 2.3426365852355957
training loss = 2.1838555335998535 8700
val loss = 2.6133627891540527
training loss = 1.81550133228302 8800
val loss = 2.326655149459839
training loss = 1.8078422546386719 8900
val loss = 2.3187685012817383
training loss = 1.8229326009750366 9000
val loss = 2.3485147953033447
training loss = 1.7934188842773438 9100
val loss = 2.3042116165161133
training loss = 1.7863212823867798 9200
val loss = 2.29695987701416
training loss = 1.9145838022232056 9300
val loss = 2.3793768882751465
training loss = 1.7727928161621094 9400
val loss = 2.2834136486053467
training loss = 1.7662508487701416 9500
val loss = 2.2773053646087646
training loss = 1.7610746622085571 9600
val loss = 2.2699966430664062
training loss = 1.7541967630386353 9700
val loss = 2.2658376693725586
training loss = 1.7491097450256348 9800
val loss = 2.2638046741485596
training loss = 1.7431931495666504 9900
val loss = 2.2563371658325195
training loss = 1.7379753589630127 10000
val loss = 2.2509071826934814
training loss = 1.7367722988128662 10100
val loss = 2.255678653717041
training loss = 1.7289797067642212 10200
val loss = 2.2429041862487793
training loss = 1.724704623222351 10300
val loss = 2.2390544414520264
training loss = 1.7212469577789307 10400
val loss = 2.234797716140747
training loss = 1.7174454927444458 10500
val loss = 2.2329893112182617
training loss = 1.7211649417877197 10600
val loss = 2.229036331176758
training loss = 1.711344838142395 10700
val loss = 2.228175163269043
training loss = 1.7132576704025269 10800
val loss = 2.2375130653381348
training loss = 1.706387996673584 10900
val loss = 2.223789691925049
training loss = 1.704193115234375 11000
val loss = 2.222745895385742
training loss = 1.7060378789901733 11100
val loss = 2.218205451965332
training loss = 1.7007488012313843 11200
val loss = 2.2206969261169434
training loss = 1.7027833461761475 11300
val loss = 2.216407060623169
training loss = 1.698098063468933 11400
val loss = 2.2195987701416016
training loss = 1.6969749927520752 11500
val loss = 2.218888521194458
training loss = 1.6977519989013672 11600
val loss = 2.2252659797668457
training loss = 1.6952790021896362 11700
val loss = 2.218541145324707
training loss = 1.6967101097106934 11800
val loss = 2.226097583770752
training loss = 1.6940515041351318 11900
val loss = 2.2191050052642822
training loss = 1.693526029586792 12000
val loss = 2.2189698219299316
training loss = 1.711937665939331 12100
val loss = 2.220893144607544
training loss = 1.6927624940872192 12200
val loss = 2.2195050716400146
training loss = 1.6924290657043457 12300
val loss = 2.2198355197906494
training loss = 1.6924687623977661 12400
val loss = 2.2228410243988037
training loss = 1.6919437646865845 12500
val loss = 2.2208924293518066
training loss = 1.6973834037780762 12600
val loss = 2.217008590698242
training loss = 1.6915730237960815 12700
val loss = 2.2215962409973145
training loss = 1.6913750171661377 12800
val loss = 2.2223267555236816
training loss = 1.696479082107544 12900
val loss = 2.2372283935546875
training loss = 1.691064715385437 13000
val loss = 2.223416566848755
training loss = 1.6909027099609375 13100
val loss = 2.2237870693206787
training loss = 1.6921018362045288 13200
val loss = 2.2206344604492188
training loss = 1.690615177154541 13300
val loss = 2.2248916625976562
training loss = 1.6910048723220825 13400
val loss = 2.2226381301879883
training loss = 1.6903271675109863 13500
val loss = 2.2260701656341553
training loss = 1.690158724784851 13600
val loss = 2.226259231567383
training loss = 1.7369807958602905 13700
val loss = 2.239819288253784
training loss = 1.6898425817489624 13800
val loss = 2.227292060852051
training loss = 1.6896830797195435 13900
val loss = 2.2281298637390137
training loss = 1.6895273923873901 14000
val loss = 2.2274270057678223
training loss = 1.6893192529678345 14100
val loss = 2.228470802307129
training loss = 1.7116917371749878 14200
val loss = 2.2285356521606445
training loss = 1.688951015472412 14300
val loss = 2.2292540073394775
training loss = 1.6887465715408325 14400
val loss = 2.2296528816223145
training loss = 1.6887767314910889 14500
val loss = 2.23248553276062
training loss = 1.6883454322814941 14600
val loss = 2.2304511070251465
training loss = 1.6912494897842407 14700
val loss = 2.226041316986084
training loss = 1.6879658699035645 14800
val loss = 2.2322075366973877
training loss = 1.687691330909729 14900
val loss = 2.2315874099731445
training loss = 1.6874817609786987 15000
val loss = 2.2322540283203125
training loss = 1.6872400045394897 15100
val loss = 2.2324154376983643
training loss = 1.6869922876358032 15200
val loss = 2.232588291168213
training loss = 1.6867905855178833 15300
val loss = 2.2339038848876953
training loss = 1.6865098476409912 15400
val loss = 2.2333879470825195
training loss = 2.2002146244049072 15500
val loss = 2.820286750793457
training loss = 1.6860288381576538 15600
val loss = 2.2346529960632324
training loss = 1.6857414245605469 15700
val loss = 2.2343716621398926
training loss = 1.7078427076339722 15800
val loss = 2.2362747192382812
training loss = 1.6852152347564697 15900
val loss = 2.2351040840148926
training loss = 1.6849358081817627 16000
val loss = 2.2357406616210938
training loss = 1.684781551361084 16100
val loss = 2.2372634410858154
training loss = 1.6843793392181396 16200
val loss = 2.2361066341400146
training loss = 1.7555636167526245 16300
val loss = 2.3402228355407715
training loss = 1.6838343143463135 16400
val loss = 2.237133264541626
training loss = 1.683526635169983 16500
val loss = 2.2371585369110107
training loss = 1.7413102388381958 16600
val loss = 2.3257503509521484
training loss = 1.6829721927642822 16700
val loss = 2.237335681915283
training loss = 1.6826348304748535 16800
val loss = 2.238255023956299
reduced chi^2 level 2 = 1.6825820207595825
Constrained alpha: 1.9133518934249878
Constrained beta: 1.759583592414856
Constrained gamma: 14.691685676574707
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 892.2504,  856.2655,  964.0082,  947.2480,  968.6125, 1102.7792,
        1118.4552, 1124.3027, 1129.0341, 1187.9083, 1229.7976, 1185.8842,
        1282.3398, 1237.2056, 1268.3501, 1378.3652, 1420.4504, 1483.5791,
        1580.2856, 1504.2780, 1614.8939, 1578.6552, 1530.7297, 1607.6802,
        1635.0841, 1728.7726, 1612.5850, 1733.2992, 1704.1306, 1699.6664,
        1635.2654, 1704.1609, 1723.8466, 1776.6106, 1689.1917, 1726.2766,
        1657.7902, 1592.7886, 1637.3992, 1624.6024, 1628.4655, 1526.2147,
        1496.9117, 1443.7478, 1391.1964, 1385.2476, 1302.7805, 1266.8142,
        1225.0999, 1140.2812, 1045.8120,  958.7755,  985.6955,  943.4949,
         895.1071,  869.2574,  834.7130,  740.3974,  628.4479,  589.7173,
         556.4565,  497.0034,  472.5892,  401.5168,  397.5772,  332.7885,
         292.2936,  271.8276,  216.1395,  187.9125,  151.9072,  171.0380,
         140.2140,   92.4215,   95.2883,   47.2101,   56.0800,   34.7924,
          37.5379,   45.6107,   14.2392,   47.9027,   42.4802])]
2321.4285205327146
4.140661478693588 15.787929823266664 85.34029967714916
val isze = 8
idinces = [70 40 81 14 30 18  7 23  2 39 62 43 74 42 57 52 76 59  6 55 65 27 69 61
  5 32 48 33 44 24 46  8 12 58 29 50 78 71 35 68 16 13 77 10 31 28 66  4
 54 17 56  1 11 67 38 53  0 73 36 41 51 79 63 72 15  3 22 19  9 37 21 80
 47 64 60 82 49 75 26 25 45 20 34]
we are doing training validation split
training loss = 373.08380126953125 100
val loss = 205.36648559570312
training loss = 8.149689674377441 200
val loss = 12.036652565002441
training loss = 7.603593826293945 300
val loss = 11.65088176727295
training loss = 7.077597141265869 400
val loss = 11.225362777709961
training loss = 6.599541187286377 500
val loss = 10.833200454711914
training loss = 6.2033586502075195 600
val loss = 10.502632141113281
training loss = 5.901144504547119 700
val loss = 10.244773864746094
training loss = 5.684747219085693 800
val loss = 10.05478286743164
training loss = 5.533857345581055 900
val loss = 9.917755126953125
training loss = 5.426302433013916 1000
val loss = 9.81672477722168
training loss = 5.344883441925049 1100
val loss = 9.738178253173828
training loss = 5.2789812088012695 1200
val loss = 9.67361831665039
training loss = 5.222936153411865 1300
val loss = 9.61823844909668
training loss = 5.173904895782471 1400
val loss = 9.569503784179688
training loss = 5.1303815841674805 1500
val loss = 9.526107788085938
training loss = 5.091439247131348 1600
val loss = 9.486980438232422
training loss = 5.056392669677734 1700
val loss = 9.451480865478516
training loss = 5.024684429168701 1800
val loss = 9.419048309326172
training loss = 4.995833873748779 1900
val loss = 9.389245986938477
training loss = 4.969433784484863 2000
val loss = 9.361742973327637
training loss = 4.945123195648193 2100
val loss = 9.336160659790039
training loss = 4.9225969314575195 2200
val loss = 9.312210083007812
training loss = 4.901586055755615 2300
val loss = 9.289657592773438
training loss = 4.88186502456665 2400
val loss = 9.26828384399414
training loss = 4.863225936889648 2500
val loss = 9.247879028320312
training loss = 4.845495223999023 2600
val loss = 9.22823429107666
training loss = 4.828482627868652 2700
val loss = 9.209259033203125
training loss = 4.8119964599609375 2800
val loss = 9.190696716308594
training loss = 4.795779705047607 2900
val loss = 9.172269821166992
training loss = 4.779336452484131 3000
val loss = 9.153563499450684
training loss = 4.761374473571777 3100
val loss = 9.13327407836914
training loss = 4.736715316772461 3200
val loss = 9.106658935546875
training loss = 4.666982650756836 3300
val loss = 9.040167808532715
training loss = 4.241832733154297 3400
val loss = 8.520627975463867
training loss = 3.196244716644287 3500
val loss = 7.612631320953369
training loss = 2.054701328277588 3600
val loss = 6.032985687255859
training loss = 1.7707538604736328 3700
val loss = 5.174374580383301
training loss = 1.7428219318389893 3800
val loss = 4.995943069458008
training loss = 1.7335795164108276 3900
val loss = 4.971004009246826
training loss = 1.726284384727478 4000
val loss = 4.964543342590332
training loss = 1.7202876806259155 4100
val loss = 4.960771560668945
training loss = 1.714759111404419 4200
val loss = 4.955377101898193
training loss = 1.7098966836929321 4300
val loss = 4.951148986816406
training loss = 1.7067956924438477 4400
val loss = 4.9556379318237305
training loss = 1.7011654376983643 4500
val loss = 4.94140625
training loss = 1.697780728340149 4600
val loss = 4.9320573806762695
training loss = 1.6936838626861572 4700
val loss = 4.931928634643555
training loss = 1.6902629137039185 4800
val loss = 4.92437744140625
training loss = 1.8341118097305298 4900
val loss = 5.031205177307129
training loss = 1.6838520765304565 5000
val loss = 4.913114070892334
training loss = 1.680928349494934 5100
val loss = 4.905762195587158
training loss = 1.7312825918197632 5200
val loss = 4.996226787567139
training loss = 1.67532479763031 5300
val loss = 4.8923797607421875
training loss = 1.672743320465088 5400
val loss = 4.884293079376221
training loss = 1.670111894607544 5500
val loss = 4.878413677215576
training loss = 1.6676522493362427 5600
val loss = 4.8691864013671875
training loss = 1.6658381223678589 5700
val loss = 4.861555099487305
training loss = 1.6628552675247192 5800
val loss = 4.854773044586182
training loss = 1.753707766532898 5900
val loss = 4.912352561950684
training loss = 1.6583259105682373 6000
val loss = 4.840355396270752
training loss = 1.6561851501464844 6100
val loss = 4.8316497802734375
training loss = 1.6604915857315063 6200
val loss = 4.823674201965332
training loss = 1.6519838571548462 6300
val loss = 4.816945552825928
training loss = 1.6516656875610352 6400
val loss = 4.804377555847168
training loss = 1.6479911804199219 6500
val loss = 4.802267074584961
training loss = 1.6460564136505127 6600
val loss = 4.792409420013428
training loss = 1.6441638469696045 6700
val loss = 4.787848949432373
training loss = 1.6422713994979858 6800
val loss = 4.777332782745361
training loss = 1.650909185409546 6900
val loss = 4.796469688415527
training loss = 1.638603925704956 7000
val loss = 4.763014793395996
training loss = 1.6368290185928345 7100
val loss = 4.753745079040527
training loss = 1.6809470653533936 7200
val loss = 4.773962020874023
training loss = 1.6332656145095825 7300
val loss = 4.738531112670898
training loss = 1.6315096616744995 7400
val loss = 4.729102611541748
training loss = 1.6340855360031128 7500
val loss = 4.720744609832764
training loss = 1.6279374361038208 7600
val loss = 4.712747573852539
training loss = 1.6261217594146729 7700
val loss = 4.70274543762207
training loss = 1.624269962310791 7800
val loss = 4.694941997528076
training loss = 1.6223502159118652 7900
val loss = 4.6840314865112305
training loss = 1.6657037734985352 8000
val loss = 4.702033519744873
training loss = 1.618309497833252 8100
val loss = 4.663745880126953
training loss = 1.616136908531189 8200
val loss = 4.650883197784424
training loss = 1.615154504776001 8300
val loss = 4.6375346183776855
training loss = 1.6113405227661133 8400
val loss = 4.624473571777344
training loss = 1.8837732076644897 8500
val loss = 4.9894232749938965
training loss = 1.6056784391403198 8600
val loss = 4.59181547164917
training loss = 1.6023765802383423 8700
val loss = 4.570247650146484
training loss = 1.5989538431167603 8800
val loss = 4.551072597503662
training loss = 1.594787836074829 8900
val loss = 4.521980285644531
training loss = 1.6245962381362915 9000
val loss = 4.51068639755249
training loss = 1.5854706764221191 9100
val loss = 4.459253311157227
training loss = 1.580068588256836 9200
val loss = 4.420952796936035
training loss = 1.5753332376480103 9300
val loss = 4.388561248779297
training loss = 1.5689339637756348 9400
val loss = 4.34439754486084
training loss = 1.5629150867462158 9500
val loss = 4.302094459533691
training loss = 1.557997465133667 9600
val loss = 4.266834259033203
training loss = 1.5509259700775146 9700
val loss = 4.222038269042969
training loss = 1.832689642906189 9800
val loss = 4.574582099914551
training loss = 1.5393257141113281 9900
val loss = 4.149305820465088
training loss = 1.5336662530899048 10000
val loss = 4.114609241485596
training loss = 1.5284173488616943 10100
val loss = 4.084531784057617
training loss = 1.5232173204421997 10200
val loss = 4.056145668029785
training loss = 1.5696898698806763 10300
val loss = 4.062837600708008
training loss = 1.5134750604629517 10400
val loss = 4.005670547485352
training loss = 1.5087535381317139 10500
val loss = 3.9820621013641357
training loss = 1.5229891538619995 10600
val loss = 3.9966492652893066
training loss = 1.5001013278961182 10700
val loss = 3.9428889751434326
training loss = 1.495895266532898 10800
val loss = 3.923936367034912
training loss = 1.4919712543487549 10900
val loss = 3.907900094985962
training loss = 1.487868309020996 11000
val loss = 3.8907077312469482
training loss = 1.4839632511138916 11100
val loss = 3.875059127807617
training loss = 1.4807474613189697 11200
val loss = 3.8592982292175293
training loss = 1.4766393899917603 11300
val loss = 3.847848415374756
training loss = 1.5501526594161987 11400
val loss = 3.946955442428589
training loss = 1.4696969985961914 11500
val loss = 3.8234920501708984
training loss = 1.4663091897964478 11600
val loss = 3.8112006187438965
training loss = 1.468105673789978 11700
val loss = 3.799586057662964
training loss = 1.459931492805481 11800
val loss = 3.7901315689086914
training loss = 1.457095742225647 11900
val loss = 3.7777099609375
training loss = 1.4538018703460693 12000
val loss = 3.769691228866577
training loss = 1.4507097005844116 12100
val loss = 3.7598466873168945
training loss = 1.4498910903930664 12200
val loss = 3.7495689392089844
training loss = 1.4448578357696533 12300
val loss = 3.741600513458252
training loss = 1.447762131690979 12400
val loss = 3.732546329498291
training loss = 1.4390108585357666 12500
val loss = 3.7236015796661377
training loss = 1.438773274421692 12600
val loss = 3.7209267616271973
training loss = 1.4331053495407104 12700
val loss = 3.705881118774414
training loss = 1.4299647808074951 12800
val loss = 3.6964640617370605
training loss = 1.4268742799758911 12900
val loss = 3.686307430267334
training loss = 1.4234439134597778 13000
val loss = 3.677748203277588
training loss = 1.4199793338775635 13100
val loss = 3.6656854152679443
training loss = 1.4162156581878662 13200
val loss = 3.6567091941833496
training loss = 1.4123759269714355 13300
val loss = 3.644486427307129
training loss = 1.4505075216293335 13400
val loss = 3.69128155708313
training loss = 1.4045559167861938 13500
val loss = 3.6209158897399902
training loss = 1.4006078243255615 13600
val loss = 3.610267162322998
training loss = 1.3969175815582275 13700
val loss = 3.60244083404541
training loss = 1.392919898033142 13800
val loss = 3.5933837890625
training loss = 1.3974003791809082 13900
val loss = 3.5989203453063965
training loss = 1.3849444389343262 14000
val loss = 3.578923463821411
training loss = 1.3958460092544556 14100
val loss = 3.5928268432617188
training loss = 1.3766469955444336 14200
val loss = 3.5641255378723145
training loss = 1.3722870349884033 14300
val loss = 3.5556516647338867
training loss = 1.3814765214920044 14400
val loss = 3.56524395942688
training loss = 1.3632514476776123 14500
val loss = 3.5381388664245605
training loss = 1.3584288358688354 14600
val loss = 3.5280964374542236
training loss = 1.3538676500320435 14700
val loss = 3.5187997817993164
training loss = 1.3489450216293335 14800
val loss = 3.507828712463379
training loss = 1.3468385934829712 14900
val loss = 3.5000667572021484
training loss = 1.339551568031311 15000
val loss = 3.4862236976623535
training loss = 1.6284128427505493 15100
val loss = 3.804413318634033
training loss = 1.3306751251220703 15200
val loss = 3.4643383026123047
training loss = 1.3264707326889038 15300
val loss = 3.453216552734375
training loss = 1.3321460485458374 15400
val loss = 3.452418565750122
training loss = 1.3190747499465942 15500
val loss = 3.4347567558288574
training loss = 1.3157334327697754 15600
val loss = 3.427093505859375
training loss = 1.3130073547363281 15700
val loss = 3.4222896099090576
training loss = 1.3102381229400635 15800
val loss = 3.4178783893585205
training loss = 1.3079917430877686 15900
val loss = 3.417065143585205
training loss = 1.3059096336364746 16000
val loss = 3.415215253829956
training loss = 1.30417001247406 16100
val loss = 3.416179895401001
training loss = 1.3023604154586792 16200
val loss = 3.4173552989959717
training loss = 1.3007296323776245 16300
val loss = 3.4189414978027344
training loss = 1.3023802042007446 16400
val loss = 3.4273593425750732
training loss = 1.297978162765503 16500
val loss = 3.4239940643310547
training loss = 1.3295915126800537 16600
val loss = 3.4587419033050537
training loss = 1.295574426651001 16700
val loss = 3.429687738418579
training loss = 1.2944140434265137 16800
val loss = 3.4325428009033203
training loss = 1.2942713499069214 16900
val loss = 3.4354560375213623
training loss = 1.2924023866653442 17000
val loss = 3.438232898712158
training loss = 1.2984592914581299 17100
val loss = 3.4516451358795166
training loss = 1.2906196117401123 17200
val loss = 3.4437379837036133
training loss = 1.2897517681121826 17300
val loss = 3.4464492797851562
training loss = 1.2890511751174927 17400
val loss = 3.4490773677825928
training loss = 1.2881962060928345 17500
val loss = 3.4517977237701416
training loss = 1.2906173467636108 17600
val loss = 3.460355043411255
training loss = 1.2868019342422485 17700
val loss = 3.4568839073181152
training loss = 1.287070393562317 17800
val loss = 3.4617817401885986
training loss = 1.2855651378631592 17900
val loss = 3.4621241092681885
training loss = 1.2848724126815796 18000
val loss = 3.4645421504974365
training loss = 1.3466917276382446 18100
val loss = 3.5255634784698486
training loss = 1.2837575674057007 18200
val loss = 3.4693424701690674
training loss = 1.283143401145935 18300
val loss = 3.4721386432647705
training loss = 1.284620761871338 18400
val loss = 3.4746499061584473
training loss = 1.2821308374404907 18500
val loss = 3.4768614768981934
training loss = 1.6495890617370605 18600
val loss = 3.8694701194763184
training loss = 1.2811570167541504 18700
val loss = 3.4816675186157227
training loss = 1.280588984489441 18800
val loss = 3.4847350120544434
training loss = 1.2805951833724976 18900
val loss = 3.4874801635742188
training loss = 1.2797064781188965 19000
val loss = 3.4892325401306152
training loss = 1.6050726175308228 19100
val loss = 3.862804889678955
training loss = 1.2788572311401367 19200
val loss = 3.493687152862549
training loss = 1.2783459424972534 19300
val loss = 3.4966623783111572
training loss = 1.283875584602356 19400
val loss = 3.5034866333007812
training loss = 1.2775427103042603 19500
val loss = 3.5011770725250244
training loss = 1.3905267715454102 19600
val loss = 3.622382879257202
training loss = 1.2767456769943237 19700
val loss = 3.505824327468872
training loss = 1.276230812072754 19800
val loss = 3.5089714527130127
training loss = 1.276042103767395 19900
val loss = 3.5101094245910645
training loss = 1.2755118608474731 20000
val loss = 3.513225555419922
training loss = 1.2977083921432495 20100
val loss = 3.539673328399658
training loss = 1.2747541666030884 20200
val loss = 3.517798900604248
training loss = 1.274228572845459 20300
val loss = 3.520885705947876
training loss = 1.2746944427490234 20400
val loss = 3.520261287689209
training loss = 1.2735685110092163 20500
val loss = 3.524799108505249
training loss = 1.273047924041748 20600
val loss = 3.527801513671875
training loss = 1.2734434604644775 20700
val loss = 3.5287606716156006
training loss = 1.2724039554595947 20800
val loss = 3.5315704345703125
training loss = 1.2719810009002686 20900
val loss = 3.534470319747925
training loss = 1.2717489004135132 21000
val loss = 3.5352842807769775
reduced chi^2 level 2 = 1.271636724472046
Constrained alpha: 1.9322422742843628
Constrained beta: 0.6656044721603394
Constrained gamma: 13.138399124145508
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 844.7737,  858.3870,  930.6037,  896.9110, 1002.2204, 1080.2397,
        1132.3053, 1128.8008, 1087.7529, 1116.6104, 1232.0365, 1171.7017,
        1286.0929, 1229.6125, 1298.9895, 1391.8835, 1389.6578, 1440.6675,
        1617.2283, 1508.8785, 1584.7684, 1535.9167, 1554.7880, 1587.5840,
        1662.1711, 1661.6472, 1569.4059, 1680.1487, 1710.0082, 1781.6420,
        1764.6481, 1714.0566, 1690.6257, 1755.2855, 1665.6438, 1765.8250,
        1681.3043, 1663.5598, 1610.5924, 1662.6387, 1594.7393, 1542.2616,
        1572.0873, 1524.1703, 1371.1841, 1325.0083, 1230.3477, 1199.8240,
        1129.8734, 1191.7225, 1101.1664,  972.6967,  961.3423,  890.6432,
         860.2130,  859.4203,  818.0987,  702.2425,  608.7698,  535.8856,
         567.0202,  469.6179,  444.9449,  421.9060,  369.7038,  307.8149,
         272.1176,  278.3741,  210.9156,  153.8786,  183.7259,  142.6008,
         150.5055,  111.6867,   92.1407,   62.9902,   62.2280,   43.6243,
          27.4539,   34.3290,   18.5546,   33.2845,   34.5092])]
2739.9203228254673
0.15072715401117098 8.483450522569083 76.56632151989588
val isze = 8
idinces = [21 32 55 25 61 29 79 81 53 69 65 33 14 22 68  3 72 49  9 43 70 36 50 19
 78  5 45 42 66 10 46 12 34 38 64 63 76 30  7  1 48 71 37 17 73 60 58  8
 74 23 62 39 67 57 26 54 13 56  4 40 47 51 24 16 52  6 77 11 20  0 27 15
  2 59 82 44 35 80 75 41 31 28 18]
we are doing training validation split
training loss = 10.198925971984863 100
val loss = 11.984003067016602
training loss = 6.071810722351074 200
val loss = 8.758147239685059
training loss = 6.050732612609863 300
val loss = 8.767067909240723
training loss = 6.028091907501221 400
val loss = 8.775284767150879
training loss = 6.005288600921631 500
val loss = 8.783782958984375
training loss = 5.983321666717529 600
val loss = 8.792106628417969
training loss = 5.962846755981445 700
val loss = 8.799725532531738
training loss = 5.9442033767700195 800
val loss = 8.806243896484375
training loss = 5.926605701446533 900
val loss = 8.81044864654541
training loss = 5.911049842834473 1000
val loss = 8.817590713500977
training loss = 5.89783239364624 1100
val loss = 8.82105827331543
training loss = 5.8861775398254395 1200
val loss = 8.818689346313477
training loss = 5.875412940979004 1300
val loss = 8.81280517578125
training loss = 5.865044593811035 1400
val loss = 8.806711196899414
training loss = 5.854740142822266 1500
val loss = 8.796895980834961
training loss = 5.844315052032471 1600
val loss = 8.785077095031738
training loss = 5.833661079406738 1700
val loss = 8.772156715393066
training loss = 5.822811603546143 1800
val loss = 8.759068489074707
training loss = 5.8117170333862305 1900
val loss = 8.744366645812988
training loss = 5.800206661224365 2000
val loss = 8.727727890014648
training loss = 5.788168430328369 2100
val loss = 8.708930969238281
training loss = 5.775769233703613 2200
val loss = 8.68930435180664
training loss = 5.762924671173096 2300
val loss = 8.669090270996094
training loss = 5.749602317810059 2400
val loss = 8.636150360107422
training loss = 5.73590612411499 2500
val loss = 8.59253215789795
training loss = 5.721107006072998 2600
val loss = 8.60057258605957
training loss = 5.705724239349365 2700
val loss = 8.571123123168945
training loss = 5.689321041107178 2800
val loss = 8.553217887878418
training loss = 5.671508312225342 2900
val loss = 8.523700714111328
training loss = 5.651769161224365 3000
val loss = 8.478894233703613
training loss = 5.629358768463135 3100
val loss = 8.434240341186523
training loss = 5.6035919189453125 3200
val loss = 8.336956024169922
training loss = 5.569354057312012 3300
val loss = 8.330781936645508
training loss = 5.523950099945068 3400
val loss = 8.240859985351562
training loss = 5.459560394287109 3500
val loss = 8.013949394226074
training loss = 5.343555450439453 3600
val loss = 7.9327006340026855
training loss = 5.153443813323975 3700
val loss = 7.609441757202148
training loss = 4.844581604003906 3800
val loss = 6.994849681854248
training loss = 4.343467712402344 3900
val loss = 6.238247871398926
training loss = 3.566091299057007 4000
val loss = 4.776270866394043
training loss = 2.6482346057891846 4100
val loss = 3.0410823822021484
training loss = 2.0936288833618164 4200
val loss = 1.8099353313446045
training loss = 1.956558108329773 4300
val loss = 1.5069291591644287
training loss = 1.924086093902588 4400
val loss = 1.4519741535186768
training loss = 1.904565453529358 4500
val loss = 1.5071786642074585
training loss = 1.9071171283721924 4600
val loss = 1.4284358024597168
training loss = 1.8788472414016724 4700
val loss = 1.5729806423187256
training loss = 1.8697025775909424 4800
val loss = 1.6011099815368652
training loss = 1.862640380859375 4900
val loss = 1.643451452255249
training loss = 1.8564270734786987 5000
val loss = 1.650514841079712
training loss = 1.860526204109192 5100
val loss = 1.5981638431549072
training loss = 1.847386121749878 5200
val loss = 1.6888569593429565
training loss = 1.8438265323638916 5300
val loss = 1.7047816514968872
training loss = 1.8418428897857666 5400
val loss = 1.7501072883605957
training loss = 1.8379342555999756 5500
val loss = 1.7310264110565186
training loss = 1.9183049201965332 5600
val loss = 1.5794585943222046
training loss = 1.8329527378082275 5700
val loss = 1.7533612251281738
training loss = 1.830639123916626 5800
val loss = 1.760118007659912
training loss = 1.8283874988555908 5900
val loss = 1.7622679471969604
training loss = 1.8261393308639526 6000
val loss = 1.7719441652297974
training loss = 1.843337059020996 6100
val loss = 1.6832060813903809
training loss = 1.8216137886047363 6200
val loss = 1.781099796295166
training loss = 1.819305419921875 6300
val loss = 1.7822229862213135
training loss = 1.819215178489685 6400
val loss = 1.8241336345672607
training loss = 1.8145577907562256 6500
val loss = 1.7843388319015503
training loss = 1.8122642040252686 6600
val loss = 1.7933075428009033
training loss = 1.8096704483032227 6700
val loss = 1.778526782989502
training loss = 1.8071564435958862 6800
val loss = 1.7808067798614502
training loss = 1.8113256692886353 6900
val loss = 1.8467069864273071
training loss = 1.8020741939544678 7000
val loss = 1.7751495838165283
training loss = 1.7995671033859253 7100
val loss = 1.7652995586395264
training loss = 1.7969340085983276 7200
val loss = 1.7678470611572266
training loss = 1.7943509817123413 7300
val loss = 1.7660499811172485
training loss = 1.7917451858520508 7400
val loss = 1.764866828918457
training loss = 1.789176106452942 7500
val loss = 1.7590304613113403
training loss = 1.7899638414382935 7600
val loss = 1.8010215759277344
training loss = 1.7840287685394287 7700
val loss = 1.7489742040634155
training loss = 1.7814511060714722 7800
val loss = 1.7481606006622314
training loss = 1.7862271070480347 7900
val loss = 1.6877570152282715
training loss = 1.7762850522994995 8000
val loss = 1.7390596866607666
training loss = 1.7736914157867432 8100
val loss = 1.732775330543518
training loss = 1.771186351776123 8200
val loss = 1.7270829677581787
training loss = 1.7685518264770508 8300
val loss = 1.7279512882232666
training loss = 1.7672604322433472 8400
val loss = 1.7008795738220215
training loss = 1.7635433673858643 8500
val loss = 1.719647765159607
training loss = 1.7609925270080566 8600
val loss = 1.716048240661621
training loss = 1.7586666345596313 8700
val loss = 1.703538179397583
training loss = 1.7560893297195435 8800
val loss = 1.7056958675384521
training loss = 1.7538517713546753 8900
val loss = 1.7106788158416748
training loss = 1.7512456178665161 9000
val loss = 1.6957547664642334
training loss = 1.82985258102417 9100
val loss = 1.5857017040252686
training loss = 1.7465194463729858 9200
val loss = 1.685827612876892
training loss = 1.7441682815551758 9300
val loss = 1.6833610534667969
reduced chi^2 level 2 = 1.7488399744033813
Constrained alpha: 1.7576531171798706
Constrained beta: 3.8313047885894775
Constrained gamma: 24.83514404296875
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 824.1797,  877.0880,  942.8737,  995.1108, 1049.0541, 1093.8622,
        1127.7303, 1102.8053, 1139.8928, 1174.4833, 1227.4856, 1134.6801,
        1308.6437, 1310.4968, 1323.5151, 1458.8743, 1371.1588, 1470.9475,
        1459.8014, 1478.4974, 1609.9283, 1548.0154, 1598.7023, 1610.9012,
        1639.4095, 1695.8943, 1558.1025, 1679.8256, 1756.0710, 1711.0090,
        1566.7889, 1702.9771, 1680.8665, 1722.4951, 1759.2850, 1755.7537,
        1653.6080, 1553.3986, 1633.8348, 1670.3583, 1653.2805, 1537.5527,
        1541.6058, 1478.3431, 1425.2931, 1409.0079, 1313.1595, 1277.4978,
        1132.1654, 1177.7655, 1038.6832,  968.9702,  950.7910,  873.1876,
         900.6909,  894.6052,  802.9095,  714.1150,  617.7774,  566.9279,
         613.9445,  506.9577,  452.6526,  391.6725,  372.1552,  359.4312,
         283.1013,  249.8729,  203.4188,  159.5297,  142.9419,  136.4749,
         143.8034,  102.7489,  101.6144,   69.9581,   49.3349,   52.1883,
          27.7620,   49.7333,   22.5763,   20.9067,   31.9463])]
2814.272564120264
4.935375641538212 10.056051578398229 23.431529633866145
val isze = 8
idinces = [65 82 59 80 19 11 31 15  6 46 39 56 44 69 25  3 57 61 35 63 14 42 66  7
 64 13  1 72  2 38 22 54 53 75 10  8 51 60 68 37 70 55 23 34 12 18 58 62
 40 47 30 77 76  0 48 41 36 50 73 52 67 28 29 43 78 74  5 20  9 32 33 21
 71 49 45 16 17  4 79 27 26 81 24]
we are doing training validation split
training loss = 283.8592834472656 100
val loss = 135.52294921875
training loss = 8.143450736999512 200
val loss = 8.282212257385254
training loss = 6.754019737243652 300
val loss = 8.025745391845703
training loss = 5.968791484832764 400
val loss = 7.916267395019531
training loss = 5.544816017150879 500
val loss = 7.878786087036133
training loss = 5.321627616882324 600
val loss = 7.876465797424316
training loss = 5.206031799316406 700
val loss = 7.888303756713867
training loss = 5.146409034729004 800
val loss = 7.903059959411621
training loss = 5.1149678230285645 900
val loss = 7.91541051864624
training loss = 5.097165107727051 1000
val loss = 7.923407554626465
training loss = 5.085656642913818 1100
val loss = 7.926857948303223
training loss = 5.076874256134033 1200
val loss = 7.926368713378906
training loss = 5.069178104400635 1300
val loss = 7.922815322875977
training loss = 5.061827182769775 1400
val loss = 7.916991233825684
training loss = 5.0545148849487305 1500
val loss = 7.909591197967529
training loss = 5.047105312347412 1600
val loss = 7.901074409484863
training loss = 5.039529800415039 1700
val loss = 7.891782760620117
training loss = 5.031743049621582 1800
val loss = 7.881878852844238
training loss = 5.0236921310424805 1900
val loss = 7.8714423179626465
training loss = 5.015297889709473 2000
val loss = 7.860477447509766
training loss = 5.006410121917725 2100
val loss = 7.848834991455078
training loss = 4.9967169761657715 2200
val loss = 7.836225509643555
training loss = 4.985494136810303 2300
val loss = 7.821878910064697
training loss = 4.970771789550781 2400
val loss = 7.803752422332764
training loss = 4.946084022521973 2500
val loss = 7.77507209777832
training loss = 4.884878635406494 2600
val loss = 7.706979274749756
training loss = 4.658947944641113 2700
val loss = 7.451567649841309
training loss = 3.856818914413452 2800
val loss = 6.456022262573242
training loss = 2.7639670372009277 2900
val loss = 4.777024269104004
training loss = 2.27018404006958 3000
val loss = 3.5699100494384766
training loss = 2.1840462684631348 3100
val loss = 3.1742422580718994
training loss = 2.181098699569702 3200
val loss = 3.092302083969116
training loss = 2.163846492767334 3300
val loss = 3.020305633544922
training loss = 2.16097092628479 3400
val loss = 2.998856544494629
training loss = 2.15852952003479 3500
val loss = 2.9783976078033447
training loss = 2.1567704677581787 3600
val loss = 2.9661471843719482
training loss = 2.1577157974243164 3700
val loss = 2.9645185470581055
training loss = 2.1533172130584717 3800
val loss = 2.9482760429382324
training loss = 2.151663064956665 3900
val loss = 2.9406580924987793
training loss = 2.149833917617798 4000
val loss = 2.935722827911377
training loss = 2.1614139080047607 4100
val loss = 2.9545931816101074
training loss = 2.1463003158569336 4200
val loss = 2.9259400367736816
training loss = 2.3776071071624756 4300
val loss = 3.066972017288208
training loss = 2.1428067684173584 4400
val loss = 2.917165517807007
training loss = 2.1411144733428955 4500
val loss = 2.9135451316833496
training loss = 2.1396310329437256 4600
val loss = 2.911848306655884
training loss = 2.1377878189086914 4700
val loss = 2.906381130218506
training loss = 2.1433286666870117 4800
val loss = 2.9184844493865967
training loss = 2.134598731994629 4900
val loss = 2.8998398780822754
training loss = 2.1343722343444824 5000
val loss = 2.892784833908081
training loss = 2.131542921066284 5100
val loss = 2.8936119079589844
training loss = 2.130072832107544 5200
val loss = 2.890756130218506
training loss = 2.128757953643799 5300
val loss = 2.8869056701660156
training loss = 2.1271655559539795 5400
val loss = 2.885080099105835
training loss = 2.1488256454467773 5500
val loss = 2.8853654861450195
training loss = 2.1242904663085938 5600
val loss = 2.8795647621154785
training loss = 2.122849464416504 5700
val loss = 2.8771533966064453
training loss = 2.121366262435913 5800
val loss = 2.8749284744262695
training loss = 2.1198365688323975 5900
val loss = 2.8715076446533203
training loss = 2.118279218673706 6000
val loss = 2.8706023693084717
training loss = 2.116551637649536 6100
val loss = 2.8661117553710938
training loss = 2.1147210597991943 6200
val loss = 2.86301326751709
training loss = 2.112720489501953 6300
val loss = 2.8594658374786377
training loss = 2.1104278564453125 6400
val loss = 2.8566927909851074
training loss = 2.10776686668396 6500
val loss = 2.8530495166778564
training loss = 2.105475664138794 6600
val loss = 2.847317695617676
training loss = 2.101257801055908 6700
val loss = 2.8449788093566895
training loss = 2.096904754638672 6800
val loss = 2.8399546146392822
training loss = 2.092176675796509 6900
val loss = 2.83382511138916
training loss = 2.0863049030303955 7000
val loss = 2.829094886779785
training loss = 2.3760290145874023 7100
val loss = 2.988626480102539
training loss = 2.071882486343384 7200
val loss = 2.8160388469696045
training loss = 2.0630698204040527 7300
val loss = 2.8085174560546875
training loss = 2.0538742542266846 7400
val loss = 2.8037564754486084
training loss = 2.0434932708740234 7500
val loss = 2.7932169437408447
training loss = 2.0360970497131348 7600
val loss = 2.7770659923553467
training loss = 2.021808624267578 7700
val loss = 2.7782387733459473
training loss = 2.047269105911255 7800
val loss = 2.758802890777588
training loss = 1.998555302619934 7900
val loss = 2.764286994934082
training loss = 1.9864435195922852 8000
val loss = 2.756685972213745
training loss = 1.976212978363037 8100
val loss = 2.7432758808135986
training loss = 1.9644172191619873 8200
val loss = 2.745466947555542
training loss = 1.9529064893722534 8300
val loss = 2.7405920028686523
training loss = 1.9425179958343506 8400
val loss = 2.731785774230957
training loss = 1.9320706129074097 8500
val loss = 2.7332546710968018
training loss = 1.9761136770248413 8600
val loss = 2.8518905639648438
training loss = 1.912951111793518 8700
val loss = 2.7303831577301025
training loss = 1.904050350189209 8800
val loss = 2.7285802364349365
training loss = 1.9086467027664185 8900
val loss = 2.7783870697021484
training loss = 1.8887325525283813 9000
val loss = 2.7296297550201416
training loss = 1.8816536664962769 9100
val loss = 2.7318811416625977
training loss = 1.8759269714355469 9200
val loss = 2.735011577606201
training loss = 1.8703768253326416 9300
val loss = 2.738171339035034
training loss = 1.888469934463501 9400
val loss = 2.697300910949707
training loss = 1.8613470792770386 9500
val loss = 2.7468981742858887
training loss = 1.8673999309539795 9600
val loss = 2.715614080429077
training loss = 1.8544456958770752 9700
val loss = 2.7572402954101562
training loss = 1.8515002727508545 9800
val loss = 2.7629592418670654
training loss = 1.849403977394104 9900
val loss = 2.768995761871338
training loss = 1.8472254276275635 10000
val loss = 2.774268627166748
training loss = 1.8454381227493286 10100
val loss = 2.787337064743042
training loss = 1.8438810110092163 10200
val loss = 2.7870092391967773
training loss = 1.8423864841461182 10300
val loss = 2.792583703994751
training loss = 1.841428279876709 10400
val loss = 2.7887496948242188
training loss = 1.8400486707687378 10500
val loss = 2.8036112785339355
training loss = 1.8595699071884155 10600
val loss = 2.7542736530303955
training loss = 1.8380961418151855 10700
val loss = 2.8125696182250977
training loss = 1.8371394872665405 10800
val loss = 2.820931911468506
training loss = 1.8364628553390503 10900
val loss = 2.8169734477996826
training loss = 1.835483193397522 11000
val loss = 2.8286988735198975
training loss = 1.868218183517456 11100
val loss = 2.9480836391448975
training loss = 1.8339258432388306 11200
val loss = 2.834561824798584
training loss = 1.833107590675354 11300
val loss = 2.839341878890991
training loss = 1.8494535684585571 11400
val loss = 2.7860145568847656
training loss = 1.8315120935440063 11500
val loss = 2.8432583808898926
training loss = 1.8306437730789185 11600
val loss = 2.848231792449951
training loss = 1.8299111127853394 11700
val loss = 2.8412411212921143
training loss = 1.8288358449935913 11800
val loss = 2.8504858016967773
training loss = 1.8282058238983154 11900
val loss = 2.8393964767456055
training loss = 1.8268074989318848 12000
val loss = 2.8525917530059814
training loss = 1.9479362964630127 12100
val loss = 3.128631591796875
training loss = 1.8245244026184082 12200
val loss = 2.851083755493164
training loss = 1.8233052492141724 12300
val loss = 2.855475425720215
training loss = 1.8221518993377686 12400
val loss = 2.848508596420288
training loss = 1.8209812641143799 12500
val loss = 2.850574493408203
training loss = 1.828525424003601 12600
val loss = 2.807626485824585
training loss = 1.8189178705215454 12700
val loss = 2.8491721153259277
training loss = 1.819278359413147 12800
val loss = 2.8691821098327637
training loss = 1.8171350955963135 12900
val loss = 2.847266674041748
training loss = 1.8162896633148193 13000
val loss = 2.8502745628356934
training loss = 1.815678596496582 13100
val loss = 2.8546786308288574
training loss = 1.8148454427719116 13200
val loss = 2.852938652038574
training loss = 1.81427800655365 13300
val loss = 2.8548450469970703
training loss = 1.8135770559310913 13400
val loss = 2.8552443981170654
training loss = 1.8129501342773438 13500
val loss = 2.8579201698303223
training loss = 1.8130714893341064 13600
val loss = 2.845024585723877
training loss = 1.811829924583435 13700
val loss = 2.861109495162964
training loss = 1.9063239097595215 13800
val loss = 3.0917916297912598
training loss = 1.8107998371124268 13900
val loss = 2.862767457962036
training loss = 1.810279369354248 14000
val loss = 2.866394519805908
training loss = 1.8678101301193237 14100
val loss = 3.035122871398926
reduced chi^2 level 2 = 1.8096410036087036
Constrained alpha: 1.9056899547576904
Constrained beta: 3.6136057376861572
Constrained gamma: 13.519627571105957
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 884.5475,  817.3004,  921.8481,  925.8839, 1020.1838, 1025.4904,
        1095.4886, 1110.1339, 1142.0859, 1184.8882, 1277.8312, 1099.4526,
        1236.7588, 1183.5299, 1287.6853, 1454.6458, 1360.1100, 1407.2202,
        1556.7251, 1465.3108, 1615.7253, 1586.0596, 1633.6492, 1628.2491,
        1678.3242, 1629.9955, 1705.6201, 1734.0745, 1711.2236, 1698.8444,
        1664.1311, 1675.7192, 1725.5684, 1674.8602, 1691.3147, 1787.9696,
        1616.5306, 1613.8267, 1676.3665, 1653.3317, 1650.6537, 1598.4874,
        1491.4866, 1581.2677, 1332.1661, 1354.1798, 1318.8682, 1194.4326,
        1130.9272, 1235.1429, 1048.5208, 1070.1559, 1000.6910,  923.9055,
         877.5435,  864.9443,  828.2537,  677.7225,  614.1094,  551.3005,
         506.6812,  496.5512,  402.8775,  379.0256,  388.0546,  342.5400,
         310.4142,  276.5285,  211.9485,  165.5509,  151.6501,  142.0394,
         159.1183,  102.1016,   93.7389,   80.0162,   57.9572,   40.1037,
          33.2507,   41.4770,   27.7040,   32.2396,   33.6423])]
2999.0801490401213
3.5195675053847713 0.11147613849410654 80.29530626929451
val isze = 8
idinces = [ 0 80 61 29 67 23 17 25 42 53 52 63  4 74 18 43 26 36 31 32 82 56 20 60
 49 16 41 65 12  1 51 64  2 81 45 77 19 55  6 50 13 58 78 35 66 30 44  8
 33 14 38 24 47 34 70 22 48 72 54 15  5 28 62 39 21 73 40  9 68 59 11 75
 46  3 79 69 57 37 27  7 10 71 76]
we are doing training validation split
training loss = 240.56552124023438 100
val loss = 186.5372772216797
training loss = 58.178348541259766 200
val loss = 47.91498565673828
training loss = 9.92088508605957 300
val loss = 6.7580766677856445
training loss = 5.525961875915527 400
val loss = 2.7497220039367676
training loss = 4.396732807159424 500
val loss = 1.972729206085205
training loss = 3.908118486404419 600
val loss = 1.5864996910095215
training loss = 3.602753162384033 700
val loss = 1.3295676708221436
training loss = 3.504650115966797 800
val loss = 0.9039695262908936
training loss = 3.222458839416504 900
val loss = 1.1185990571975708
training loss = 3.0955240726470947 1000
val loss = 1.112857699394226
training loss = 2.986213445663452 1100
val loss = 1.0182898044586182
training loss = 2.900245428085327 1200
val loss = 0.9590646028518677
training loss = 2.8289387226104736 1300
val loss = 0.9822497367858887
training loss = 2.7822608947753906 1400
val loss = 1.0685691833496094
training loss = 2.7289042472839355 1500
val loss = 0.9635993838310242
training loss = 2.827134370803833 1600
val loss = 1.4277303218841553
training loss = 2.6560962200164795 1700
val loss = 0.9283326864242554
training loss = 2.6253154277801514 1800
val loss = 0.9379044771194458
training loss = 2.597417116165161 1900
val loss = 0.906222939491272
training loss = 2.572442054748535 2000
val loss = 0.8813266754150391
training loss = 2.5526530742645264 2100
val loss = 0.8350617289543152
training loss = 2.5322203636169434 2200
val loss = 0.8556017875671387
training loss = 2.5164473056793213 2300
val loss = 0.8340234160423279
training loss = 2.5022220611572266 2400
val loss = 0.8374379277229309
training loss = 2.7984001636505127 2500
val loss = 0.5826497077941895
training loss = 2.479609727859497 2600
val loss = 0.8292479515075684
training loss = 2.703721523284912 2700
val loss = 0.6370472311973572
training loss = 2.4619197845458984 2800
val loss = 0.8203533291816711
training loss = 2.474663257598877 2900
val loss = 0.9477056264877319
training loss = 2.447368621826172 3000
val loss = 0.8101996779441833
training loss = 2.4425463676452637 3100
val loss = 0.838411808013916
training loss = 2.4351089000701904 3200
val loss = 0.7905996441841125
training loss = 2.4296810626983643 3300
val loss = 0.809732973575592
training loss = 2.4318156242370605 3400
val loss = 0.8676761388778687
training loss = 2.4194962978363037 3500
val loss = 0.7737643718719482
training loss = 2.414806365966797 3600
val loss = 0.7958711385726929
training loss = 2.583343982696533 3700
val loss = 0.5539677739143372
training loss = 2.406296491622925 3800
val loss = 0.7789314985275269
training loss = 2.401930093765259 3900
val loss = 0.7725325226783752
training loss = 2.4025797843933105 4000
val loss = 0.8168721795082092
training loss = 2.3945672512054443 4100
val loss = 0.765847384929657
training loss = 2.4172255992889404 4200
val loss = 0.6597329378128052
training loss = 2.387518882751465 4300
val loss = 0.7575781941413879
training loss = 2.3856494426727295 4400
val loss = 0.7299301624298096
training loss = 2.380906581878662 4500
val loss = 0.7544897198677063
training loss = 2.3779640197753906 4600
val loss = 0.748030424118042
training loss = 2.4128055572509766 4700
val loss = 0.6281740069389343
training loss = 2.541522264480591 4800
val loss = 0.5750336050987244
training loss = 2.369797706604004 4900
val loss = 0.7668398022651672
training loss = 2.366142988204956 5000
val loss = 0.746234118938446
training loss = 2.36515212059021 5100
val loss = 0.7714604139328003
training loss = 2.3612570762634277 5200
val loss = 0.7558552026748657
training loss = 2.4027984142303467 5300
val loss = 0.9351747035980225
training loss = 2.378056764602661 5400
val loss = 0.6337237358093262
training loss = 2.352672815322876 5500
val loss = 0.718889594078064
training loss = 2.350222587585449 5600
val loss = 0.7130966186523438
training loss = 2.3570733070373535 5700
val loss = 0.8044778108596802
training loss = 2.344980001449585 5800
val loss = 0.7141353487968445
training loss = 2.342500925064087 5900
val loss = 0.7209556698799133
training loss = 2.34492564201355 6000
val loss = 0.7756573557853699
training loss = 2.351392984390259 6100
val loss = 0.6396604776382446
training loss = 2.341756820678711 6200
val loss = 0.658706545829773
training loss = 2.3331615924835205 6300
val loss = 0.7124974131584167
training loss = 2.341287136077881 6400
val loss = 0.7976288795471191
training loss = 2.331885814666748 6500
val loss = 0.7532895803451538
training loss = 2.3523433208465576 6600
val loss = 0.8549100756645203
training loss = 2.351266622543335 6700
val loss = 0.6087432503700256
training loss = 2.333641767501831 6800
val loss = 0.7975666522979736
training loss = 2.3243319988250732 6900
val loss = 0.7565059661865234
training loss = 2.33316707611084 7000
val loss = 0.6195602416992188
training loss = 2.3216605186462402 7100
val loss = 0.7610166072845459
training loss = 2.3226733207702637 7200
val loss = 0.7785688638687134
training loss = 2.3555498123168945 7300
val loss = 0.5815466642379761
training loss = 2.3631749153137207 7400
val loss = 0.5727992057800293
training loss = 2.340623140335083 7500
val loss = 0.861201822757721
training loss = 2.3631961345672607 7600
val loss = 0.9248052835464478
training loss = 2.3051860332489014 7700
val loss = 0.6735091209411621
training loss = 2.3093364238739014 7800
val loss = 0.76065993309021
training loss = 2.3046436309814453 7900
val loss = 0.7419333457946777
training loss = 2.3035831451416016 8000
val loss = 0.7447035312652588
training loss = 2.3179149627685547 8100
val loss = 0.60097736120224
training loss = 2.3014261722564697 8200
val loss = 0.747799813747406
training loss = 2.317507028579712 8300
val loss = 0.8236758708953857
training loss = 2.292893409729004 8400
val loss = 0.7001485228538513
training loss = 2.311718702316284 8500
val loss = 0.8103799819946289
training loss = 2.291163206100464 8600
val loss = 0.7146914005279541
training loss = 2.3008575439453125 8700
val loss = 0.7791692614555359
training loss = 2.29351544380188 8800
val loss = 0.6238778233528137
training loss = 2.2947640419006348 8900
val loss = 0.6147398352622986
training loss = 2.3297665119171143 9000
val loss = 0.5634674429893494
training loss = 2.292659044265747 9100
val loss = 0.6112616658210754
training loss = 2.2833285331726074 9200
val loss = 0.6400821208953857
training loss = 2.29099178314209 9300
val loss = 0.768642008304596
training loss = 2.2814791202545166 9400
val loss = 0.6321011781692505
training loss = 2.2793188095092773 9500
val loss = 0.6366899609565735
training loss = 2.39884614944458 9600
val loss = 1.0483453273773193
training loss = 2.296107769012451 9700
val loss = 0.8026442527770996
training loss = 2.277282476425171 9800
val loss = 0.6246684789657593
training loss = 2.280426025390625 9900
val loss = 0.6088641881942749
training loss = 2.2700815200805664 10000
val loss = 0.6640646457672119
training loss = 2.3040268421173096 10100
val loss = 0.8380006551742554
training loss = 2.2735211849212646 10200
val loss = 0.7296301126480103
training loss = 2.2751853466033936 10300
val loss = 0.7442470788955688
training loss = 2.278794527053833 10400
val loss = 0.5939225554466248
training loss = 2.342115640640259 10500
val loss = 0.5391162633895874
training loss = 2.307600259780884 10600
val loss = 0.8600486516952515
training loss = 2.3004257678985596 10700
val loss = 0.5579197406768799
training loss = 2.298748731613159 10800
val loss = 0.5560300350189209
training loss = 2.298389434814453 10900
val loss = 0.8414449095726013
training loss = 2.275900363922119 11000
val loss = 0.7696413397789001
training loss = 2.3476407527923584 11100
val loss = 0.5347236394882202
training loss = 2.2680587768554688 11200
val loss = 0.5952395796775818
training loss = 2.258246421813965 11300
val loss = 0.6950766444206238
training loss = 2.2566745281219482 11400
val loss = 0.629833996295929
training loss = 2.258699655532837 11500
val loss = 0.6127750873565674
training loss = 2.255070924758911 11600
val loss = 0.689534068107605
training loss = 2.2857866287231445 11700
val loss = 0.8165030479431152
training loss = 2.3211889266967773 11800
val loss = 0.5369893312454224
training loss = 2.25216007232666 11900
val loss = 0.626011848449707
training loss = 2.2531533241271973 12000
val loss = 0.6985845565795898
training loss = 2.385608673095703 12100
val loss = 1.0462242364883423
training loss = 2.322633743286133 12200
val loss = 0.5300196409225464
training loss = 2.247406005859375 12300
val loss = 0.6666520833969116
training loss = 2.2506027221679688 12400
val loss = 0.6072593927383423
training loss = 2.37841796875 12500
val loss = 0.5278002619743347
training loss = 2.2665622234344482 12600
val loss = 0.5663326978683472
training loss = 2.244452953338623 12700
val loss = 0.6321409940719604
training loss = 2.38558030128479 12800
val loss = 1.0486178398132324
training loss = 2.2442986965179443 12900
val loss = 0.6816822290420532
training loss = 2.429313898086548 13000
val loss = 0.5371936559677124
training loss = 2.29618239402771 13100
val loss = 0.8632211685180664
training loss = 2.275489091873169 13200
val loss = 0.5490598678588867
training loss = 2.2483866214752197 13300
val loss = 0.7196168899536133
training loss = 2.2630834579467773 13400
val loss = 0.5577913522720337
training loss = 2.241471529006958 13500
val loss = 0.6853614449501038
training loss = 2.256544351577759 13600
val loss = 0.5647276639938354
training loss = 2.265726327896118 13700
val loss = 0.5511229038238525
training loss = 2.256845235824585 13800
val loss = 0.5624750256538391
training loss = 2.269151210784912 13900
val loss = 0.5480538606643677
training loss = 2.2355754375457764 14000
val loss = 0.6223052740097046
training loss = 2.25019907951355 14100
val loss = 0.740046501159668
training loss = 2.2412638664245605 14200
val loss = 0.7075909376144409
training loss = 2.241957187652588 14300
val loss = 0.5800149440765381
training loss = 2.2796573638916016 14400
val loss = 0.8306731581687927
training loss = 2.3152923583984375 14500
val loss = 0.5234036445617676
training loss = 2.24894642829895 14600
val loss = 0.5628037452697754
training loss = 2.2307610511779785 14700
val loss = 0.6295082569122314
training loss = 2.2903923988342285 14800
val loss = 0.8596058487892151
training loss = 2.231977701187134 14900
val loss = 0.6034895777702332
training loss = 2.230647325515747 15000
val loss = 0.6067878603935242
training loss = 2.2550323009490967 15100
val loss = 0.546390175819397
training loss = 2.254056930541992 15200
val loss = 0.5468093752861023
training loss = 2.2321488857269287 15300
val loss = 0.6842163801193237
training loss = 3.786738872528076 15400
val loss = 0.8080143332481384
training loss = 2.278717041015625 15500
val loss = 0.6185337901115417
training loss = 2.270786762237549 15600
val loss = 0.6097667217254639
training loss = 2.26518177986145 15700
val loss = 0.6028635501861572
training loss = 2.260890245437622 15800
val loss = 0.59907066822052
training loss = 2.257373809814453 15900
val loss = 0.597482442855835
training loss = 2.2543318271636963 16000
val loss = 0.5971709489822388
training loss = 2.2516109943389893 16100
val loss = 0.597879946231842
training loss = 2.2494003772735596 16200
val loss = 0.5847451686859131
training loss = 2.2479333877563477 16300
val loss = 0.5691912174224854
training loss = 2.2528035640716553 16400
val loss = 0.6978803277015686
training loss = 2.2481555938720703 16500
val loss = 0.5371174812316895
training loss = 2.2472119331359863 16600
val loss = 0.6830013394355774
training loss = 2.252502918243408 16700
val loss = 0.5091084241867065
training loss = 2.322801351547241 16800
val loss = 0.9568299651145935
training loss = 2.2389461994171143 16900
val loss = 0.566167414188385
training loss = 2.2375175952911377 17000
val loss = 0.6519730091094971
training loss = 2.259255886077881 17100
val loss = 0.7629671096801758
training loss = 2.2561886310577393 17200
val loss = 0.7529980540275574
training loss = 2.2315754890441895 17300
val loss = 0.6251669526100159
training loss = 2.25563907623291 17400
val loss = 0.7511386871337891
training loss = 2.2510459423065186 17500
val loss = 0.7394970655441284
training loss = 2.2401742935180664 17600
val loss = 0.702379584312439
training loss = 2.2384560108184814 17700
val loss = 0.5474727749824524
training loss = 2.27390193939209 17800
val loss = 0.5100999474525452
training loss = 2.240105152130127 17900
val loss = 0.5428637862205505
training loss = 2.229128122329712 18000
val loss = 0.661752462387085
training loss = 2.2869224548339844 18100
val loss = 0.8361395597457886
training loss = 2.2268898487091064 18200
val loss = 0.6596675515174866
training loss = 2.224754810333252 18300
val loss = 0.5747815370559692
training loss = 2.2332510948181152 18400
val loss = 0.692947506904602
training loss = 2.219820261001587 18500
val loss = 0.6174064874649048
training loss = 2.2327523231506348 18600
val loss = 0.54932701587677
training loss = 2.235790967941284 18700
val loss = 0.5424084067344666
training loss = 2.2279586791992188 18800
val loss = 0.6820000410079956
training loss = 2.2249324321746826 18900
val loss = 0.6745822429656982
training loss = 2.258411407470703 19000
val loss = 0.5227105617523193
training loss = 2.2199511528015137 19100
val loss = 0.6537517309188843
training loss = 2.2377731800079346 19200
val loss = 0.5337285399436951
training loss = 2.2456376552581787 19300
val loss = 0.7442200183868408
training loss = 2.214820623397827 19400
val loss = 0.5857712626457214
training loss = 2.2477529048919678 19500
val loss = 0.7555174827575684
training loss = 2.2201762199401855 19600
val loss = 0.6692633628845215
training loss = 2.221057415008545 19700
val loss = 0.554927408695221
training loss = 2.2694103717803955 19800
val loss = 0.5172399282455444
training loss = 2.2251012325286865 19900
val loss = 0.5461083650588989
training loss = 2.234581708908081 20000
val loss = 0.533473014831543
training loss = 2.210149049758911 20100
val loss = 0.5936239361763
training loss = 2.236069440841675 20200
val loss = 0.7299281358718872
training loss = 2.2100021839141846 20300
val loss = 0.6235077977180481
training loss = 2.2145299911499023 20400
val loss = 0.5635346174240112
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 871.9567,  873.1110,  940.1851,  939.0898,  977.7015, 1108.7463,
        1112.3561, 1130.1890, 1204.1039, 1161.7280, 1227.9838, 1168.5936,
        1215.3130, 1297.6228, 1354.0316, 1436.6844, 1427.2687, 1446.8278,
        1533.7590, 1500.0963, 1550.7903, 1637.8268, 1584.0839, 1551.8148,
        1670.8535, 1663.1818, 1655.3713, 1739.2869, 1687.4476, 1725.1644,
        1687.6195, 1750.6971, 1650.0850, 1745.0988, 1681.2151, 1693.3800,
        1699.8903, 1559.0439, 1655.5996, 1588.6545, 1663.3361, 1590.3140,
        1510.2513, 1508.8533, 1390.9048, 1271.8077, 1318.6655, 1241.8312,
        1138.1775, 1177.4875, 1075.5160,  984.4144,  972.4376,  925.8359,
         878.4810,  820.8209,  828.9066,  712.6138,  641.5339,  532.5995,
         532.6356,  471.5640,  437.8823,  408.4945,  365.1571,  325.6496,
         283.8468,  254.6643,  204.9702,  178.9947,  151.8159,  161.0475,
         136.4704,  107.1196,  103.5329,   68.0463,   47.4911,   34.7587,
          30.7952,   46.8029,   23.2571,   41.7553,   33.4449])]
2210.217527144312
3.8079871908673635 6.977331431759213 68.03156025742844
val isze = 8
idinces = [82 61 32  3 50 51 21 67 73 49 44 29 38 22  8 59 79 12 70 30  4 58 72 78
  5 41 62 18 45 26 56 16  0 77 25 66  9 55 76 11 34 52 10 14 69 64 65 47
 19 40 13  7  6 36 28 43 46 60 54 48 71 37 68 75 80  1  2 20 81 27 33 42
 24 53 57 23 15 31 35 39 74 63 17]
we are doing training validation split
training loss = 49.27942657470703 100
val loss = 42.721378326416016
training loss = 14.75334644317627 200
val loss = 15.822837829589844
training loss = 8.185123443603516 300
val loss = 11.256454467773438
training loss = 5.589632034301758 400
val loss = 8.840453147888184
training loss = 4.719579696655273 500
val loss = 7.651276588439941
training loss = 4.314353942871094 600
val loss = 7.0030694007873535
training loss = 3.9929919242858887 700
val loss = 6.519964694976807
training loss = 3.6880435943603516 800
val loss = 6.061346054077148
training loss = 3.374295234680176 900
val loss = 5.5748186111450195
training loss = 3.0407118797302246 1000
val loss = 5.033579349517822
training loss = 2.689826011657715 1100
val loss = 4.517843246459961
training loss = 2.364389419555664 1200
val loss = 3.650390625
training loss = 2.0311481952667236 1300
val loss = 3.294508457183838
training loss = 1.7829561233520508 1400
val loss = 2.639343738555908
training loss = 1.6287785768508911 1500
val loss = 2.1670303344726562
training loss = 1.53578519821167 1600
val loss = 1.9222006797790527
training loss = 1.49038827419281 1700
val loss = 1.7453482151031494
training loss = 1.5188521146774292 1800
val loss = 1.8712048530578613
training loss = 1.4548510313034058 1900
val loss = 1.5778825283050537
training loss = 1.448490023612976 2000
val loss = 1.5195882320404053
training loss = 1.4423606395721436 2100
val loss = 1.5377475023269653
training loss = 1.4385666847229004 2200
val loss = 1.530274510383606
training loss = 1.4357484579086304 2300
val loss = 1.5289934873580933
training loss = 1.4332783222198486 2400
val loss = 1.53244948387146
training loss = 1.4320882558822632 2500
val loss = 1.5625519752502441
training loss = 1.4294662475585938 2600
val loss = 1.5372676849365234
training loss = 1.4307879209518433 2700
val loss = 1.4955334663391113
training loss = 1.4263793230056763 2800
val loss = 1.5439672470092773
training loss = 1.4252829551696777 2900
val loss = 1.5635370016098022
training loss = 1.423666000366211 3000
val loss = 1.549876093864441
training loss = 1.4226844310760498 3100
val loss = 1.5679543018341064
training loss = 1.4211530685424805 3200
val loss = 1.5555810928344727
training loss = 2.1132450103759766 3300
val loss = 2.9239611625671387
training loss = 1.4187978506088257 3400
val loss = 1.5581772327423096
training loss = 1.4175688028335571 3500
val loss = 1.564094066619873
training loss = 1.4235179424285889 3600
val loss = 1.657463788986206
training loss = 1.415305256843567 3700
val loss = 1.568045735359192
training loss = 1.414184331893921 3800
val loss = 1.571569561958313
training loss = 1.413815975189209 3900
val loss = 1.6001863479614258
training loss = 1.4120012521743774 4000
val loss = 1.574747085571289
training loss = 1.4511289596557617 4100
val loss = 1.4139692783355713
training loss = 1.409886360168457 4200
val loss = 1.5783789157867432
training loss = 1.4088480472564697 4300
val loss = 1.5745551586151123
training loss = 1.4079344272613525 4400
val loss = 1.5916067361831665
training loss = 1.4067676067352295 4500
val loss = 1.5833121538162231
training loss = 1.4282431602478027 4600
val loss = 1.456533432006836
training loss = 1.4047976732254028 4700
val loss = 1.5857198238372803
training loss = 1.4039353132247925 4800
val loss = 1.6001964807510376
training loss = 1.4029511213302612 4900
val loss = 1.5877313613891602
training loss = 1.4019232988357544 5000
val loss = 1.5910649299621582
training loss = 1.406347632408142 5100
val loss = 1.5267689228057861
training loss = 1.4001283645629883 5200
val loss = 1.5930092334747314
training loss = 1.4009790420532227 5300
val loss = 1.5552992820739746
training loss = 1.398460030555725 5400
val loss = 1.5907750129699707
training loss = 1.3974953889846802 5500
val loss = 1.5971484184265137
training loss = 1.3967854976654053 5600
val loss = 1.595674753189087
training loss = 1.395848274230957 5700
val loss = 1.6006865501403809
training loss = 1.395819902420044 5800
val loss = 1.6298143863677979
training loss = 1.394280195236206 5900
val loss = 1.603778600692749
training loss = 1.409187912940979 6000
val loss = 1.7435567378997803
training loss = 1.3927946090698242 6100
val loss = 1.6057746410369873
training loss = 1.4399571418762207 6200
val loss = 1.8697152137756348
training loss = 1.391385793685913 6300
val loss = 1.6080358028411865
training loss = 1.5054165124893188 6400
val loss = 1.3692946434020996
training loss = 1.3900660276412964 6500
val loss = 1.6157138347625732
training loss = 1.3936312198638916 6600
val loss = 1.5518840551376343
training loss = 1.388933539390564 6700
val loss = 1.6251249313354492
training loss = 1.3881289958953857 6800
val loss = 1.6162378787994385
training loss = 1.3927812576293945 6900
val loss = 1.550122618675232
training loss = 1.3869826793670654 7000
val loss = 1.6188437938690186
training loss = 1.4041781425476074 7100
val loss = 1.5000474452972412
training loss = 1.3859068155288696 7200
val loss = 1.6198524236679077
training loss = 2.4607911109924316 7300
val loss = 3.5533175468444824
training loss = 1.3849331140518188 7400
val loss = 1.6233288049697876
reduced chi^2 level 2 = 1.3846654891967773
Constrained alpha: 3.4053409099578857
Constrained beta: 2.4076669216156006
Constrained gamma: 31.270709991455078
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 897.2717,  883.4120, 1003.7582,  986.3425, 1023.4221, 1045.4504,
        1176.7404, 1116.2117, 1184.5170, 1154.9661, 1201.8737, 1171.8933,
        1228.0907, 1263.9214, 1398.0146, 1326.7295, 1379.3042, 1418.5730,
        1495.9921, 1503.0449, 1588.8884, 1557.8154, 1611.4827, 1612.2091,
        1645.7346, 1684.5310, 1588.5619, 1774.9575, 1722.2670, 1820.7668,
        1685.5192, 1723.2062, 1706.5592, 1797.3041, 1759.9181, 1692.0140,
        1671.8337, 1642.6600, 1623.5952, 1607.0687, 1601.2588, 1564.1100,
        1487.2339, 1573.7264, 1386.4907, 1347.0803, 1235.2229, 1282.4083,
        1142.3551, 1174.6078, 1127.7594,  937.7579,  947.4172,  929.7930,
         877.5173,  796.5894,  801.0487,  689.3668,  651.9280,  543.1619,
         538.4376,  449.9258,  445.2501,  391.9384,  360.4854,  319.7643,
         282.7124,  245.1929,  217.9129,  190.8765,  158.6309,  122.7331,
         156.9370,  121.0327,   80.5480,   58.2045,   61.6931,   34.7777,
          39.8922,   50.7120,   26.6498,   36.2147,   25.7641])]
2381.9542321518256
3.99131913978886 3.8559361914684342 67.93595982141883
val isze = 8
idinces = [49 16 60  3 42 12  4 45 74 11  0 73 26  6 23 69 38 79 40 47 25 75 51 17
 27 80  8 62 10 35 76 44 67 15 46 56 30 14 71 52 32  9 21 50 58 63 41 64
 66 19 36  2 57 72 55 34 18 82 54 81 20 53 24 13 78 37 43 28 39 22  1 65
 70 48 68 29 61  5 33  7 59 77 31]
we are doing training validation split
training loss = 58.88331604003906 100
val loss = 87.83448791503906
training loss = 25.39992332458496 200
val loss = 34.068687438964844
training loss = 14.260133743286133 300
val loss = 18.094013214111328
training loss = 9.650988578796387 400
val loss = 10.932280540466309
training loss = 7.568817138671875 500
val loss = 7.245020866394043
training loss = 6.575708389282227 600
val loss = 5.224971771240234
training loss = 5.931993007659912 700
val loss = 4.124129295349121
training loss = 5.214115619659424 800
val loss = 3.5634679794311523
training loss = 4.502159118652344 900
val loss = 2.9141221046447754
training loss = 3.7760539054870605 1000
val loss = 2.314150333404541
training loss = 3.0857582092285156 1100
val loss = 1.802765965461731
training loss = 2.538490056991577 1200
val loss = 1.4136587381362915
training loss = 2.205017566680908 1300
val loss = 1.1771067380905151
training loss = 2.0508673191070557 1400
val loss = 1.067214012145996
training loss = 1.991051435470581 1500
val loss = 1.024242639541626
training loss = 1.9663007259368896 1600
val loss = 1.0052651166915894
training loss = 1.9525173902511597 1700
val loss = 0.9929186701774597
training loss = 1.9424247741699219 1800
val loss = 0.9824227094650269
training loss = 1.934011697769165 1900
val loss = 0.9727983474731445
training loss = 1.926639199256897 2000
val loss = 0.9640172123908997
training loss = 1.9200366735458374 2100
val loss = 0.9559592604637146
training loss = 1.9140552282333374 2200
val loss = 0.9485280513763428
training loss = 1.9086291790008545 2300
val loss = 0.9501563906669617
training loss = 1.9035804271697998 2400
val loss = 0.9360747337341309
training loss = 1.8989835977554321 2500
val loss = 0.9253888726234436
training loss = 1.8947782516479492 2600
val loss = 0.935114860534668
training loss = 1.8907662630081177 2700
val loss = 0.9214599132537842
training loss = 1.8872302770614624 2800
val loss = 0.9304443001747131
training loss = 1.8836904764175415 2900
val loss = 0.9127113223075867
training loss = 1.8805298805236816 3000
val loss = 0.9167258143424988
training loss = 1.8775262832641602 3100
val loss = 0.9060401320457458
training loss = 1.8747349977493286 3200
val loss = 0.9078365564346313
training loss = 1.872125267982483 3300
val loss = 0.9005553126335144
training loss = 1.8696573972702026 3400
val loss = 0.8945649862289429
training loss = 1.8673832416534424 3500
val loss = 0.8952021598815918
training loss = 1.8652279376983643 3600
val loss = 0.8876544237136841
training loss = 1.863227128982544 3700
val loss = 0.890561580657959
training loss = 1.8613346815109253 3800
val loss = 0.8854851722717285
training loss = 1.8595994710922241 3900
val loss = 0.8863412737846375
training loss = 1.8584743738174438 4000
val loss = 0.9127413034439087
training loss = 1.856429934501648 4100
val loss = 0.8834060430526733
training loss = 1.8597791194915771 4200
val loss = 0.8061710596084595
training loss = 1.8536232709884644 4300
val loss = 0.880978524684906
training loss = 1.8525798320770264 4400
val loss = 0.8957490921020508
training loss = 1.8511074781417847 4500
val loss = 0.8821439743041992
training loss = 1.8499542474746704 4600
val loss = 0.8807317614555359
training loss = 1.8486943244934082 4700
val loss = 0.8744198083877563
training loss = 1.847564697265625 4800
val loss = 0.8738107681274414
training loss = 1.8466618061065674 4900
val loss = 0.894454300403595
training loss = 1.845206379890442 5000
val loss = 0.8719615936279297
training loss = 1.868730902671814 5100
val loss = 0.7143324017524719
training loss = 1.8428890705108643 5200
val loss = 0.8695400357246399
training loss = 1.841812014579773 5300
val loss = 0.8684087991714478
training loss = 1.8414093255996704 5400
val loss = 0.835034191608429
training loss = 1.8394804000854492 5500
val loss = 0.8663077354431152
training loss = 1.838402509689331 5600
val loss = 0.8651655316352844
training loss = 1.8409733772277832 5700
val loss = 0.7967844605445862
training loss = 1.8361092805862427 5800
val loss = 0.8631980419158936
training loss = 1.8349851369857788 5900
val loss = 0.8566994667053223
training loss = 1.8337582349777222 6000
val loss = 0.8510730266571045
training loss = 1.832391381263733 6100
val loss = 0.8591428399085999
training loss = 1.8341472148895264 6200
val loss = 0.7954283952713013
training loss = 1.829357385635376 6300
val loss = 0.855625569820404
training loss = 1.8286784887313843 6400
val loss = 0.8941234350204468
training loss = 1.8255876302719116 6500
val loss = 0.8467960357666016
training loss = 1.8233561515808105 6600
val loss = 0.8491496443748474
training loss = 1.830533504486084 6700
val loss = 0.7448239326477051
training loss = 1.8183730840682983 6800
val loss = 0.8436095118522644
training loss = 1.8157719373703003 6900
val loss = 0.8400219678878784
training loss = 1.8133172988891602 7000
val loss = 0.8247970342636108
training loss = 1.8107377290725708 7100
val loss = 0.8336900472640991
training loss = 1.8082859516143799 7200
val loss = 0.8301050662994385
training loss = 1.8060696125030518 7300
val loss = 0.8445062637329102
training loss = 1.8035087585449219 7400
val loss = 0.8255881667137146
training loss = 1.8031716346740723 7500
val loss = 0.7756026983261108
training loss = 1.79878830909729 7600
val loss = 0.8201175332069397
training loss = 1.9338220357894897 7700
val loss = 1.3894572257995605
training loss = 1.7940549850463867 7800
val loss = 0.8122105598449707
training loss = 1.7916451692581177 7900
val loss = 0.8087006211280823
training loss = 1.7893272638320923 8000
val loss = 0.8199280500411987
training loss = 1.7868200540542603 8100
val loss = 0.807137131690979
training loss = 1.795861840248108 8200
val loss = 0.7040544748306274
training loss = 1.7818872928619385 8300
val loss = 0.8026458621025085
training loss = 1.779632568359375 8400
val loss = 0.7813531756401062
training loss = 1.776872992515564 8500
val loss = 0.7938699722290039
training loss = 1.7743335962295532 8600
val loss = 0.7937146425247192
training loss = 1.7733927965164185 8700
val loss = 0.8349548578262329
training loss = 1.7692729234695435 8800
val loss = 0.7879656553268433
training loss = 1.7925264835357666 8900
val loss = 0.9819780588150024
training loss = 1.7642453908920288 9000
val loss = 0.7797038555145264
training loss = 1.7617098093032837 9100
val loss = 0.7808405160903931
training loss = 1.760077953338623 9200
val loss = 0.7507017254829407
training loss = 1.7568433284759521 9300
val loss = 0.7755255699157715
training loss = 1.7855210304260254 9400
val loss = 0.9864767789840698
training loss = 1.7521981000900269 9500
val loss = 0.7723137140274048
training loss = 1.74990975856781 9600
val loss = 0.7690912485122681
training loss = 1.7479549646377563 9700
val loss = 0.7517905831336975
training loss = 1.7455912828445435 9800
val loss = 0.7652440071105957
training loss = 1.7962168455123901 9900
val loss = 0.6039188504219055
training loss = 1.7415704727172852 10000
val loss = 0.7577962875366211
training loss = 1.7396165132522583 10100
val loss = 0.759462833404541
training loss = 1.7417495250701904 10200
val loss = 0.8224113583564758
training loss = 1.7360844612121582 10300
val loss = 0.755989134311676
training loss = 1.7343697547912598 10400
val loss = 0.7589254975318909
training loss = 1.7329260110855103 10500
val loss = 0.7616966962814331
training loss = 1.7313495874404907 10600
val loss = 0.7521607279777527
training loss = 1.7913596630096436 10700
val loss = 1.0552438497543335
training loss = 1.7286243438720703 10800
val loss = 0.7499473094940186
training loss = 1.72732675075531 10900
val loss = 0.7485134601593018
training loss = 1.7264490127563477 11000
val loss = 0.7633630037307739
training loss = 1.7250782251358032 11100
val loss = 0.7474220991134644
training loss = 1.7270430326461792 11200
val loss = 0.8008837699890137
training loss = 1.723124623298645 11300
val loss = 0.7468281388282776
training loss = 1.722192406654358 11400
val loss = 0.7447496652603149
training loss = 1.7217175960540771 11500
val loss = 0.7610270977020264
training loss = 1.720629334449768 11600
val loss = 0.7439002394676208
training loss = 1.720160722732544 11700
val loss = 0.7563773393630981
training loss = 1.7192875146865845 11800
val loss = 0.7435482144355774
training loss = 1.7186440229415894 11900
val loss = 0.7425327301025391
training loss = 1.7184802293777466 12000
val loss = 0.7270717620849609
training loss = 1.7176188230514526 12100
val loss = 0.7422481775283813
training loss = 1.7177765369415283 12200
val loss = 0.7197005152702332
reduced chi^2 level 2 = 1.7460683584213257
Constrained alpha: 1.885578989982605
Constrained beta: 3.4336395263671875
Constrained gamma: 16.829345703125
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 931.5413,  894.2728,  896.7365,  899.0032, 1048.9698, 1049.9880,
        1074.8790, 1096.8522, 1112.9774, 1206.7561, 1212.5914, 1244.0685,
        1274.2507, 1293.8177, 1357.8036, 1444.6661, 1402.7412, 1435.5995,
        1579.3438, 1514.8414, 1565.1147, 1521.5288, 1671.6185, 1559.6902,
        1652.2734, 1692.7903, 1578.0896, 1714.5792, 1753.8307, 1678.5121,
        1672.4994, 1714.5275, 1707.4895, 1699.2665, 1635.5377, 1741.1125,
        1684.4714, 1596.5764, 1661.9144, 1685.3748, 1652.8876, 1560.8744,
        1523.6051, 1508.5674, 1435.8896, 1285.2914, 1256.6284, 1265.1449,
        1160.7395, 1142.4366, 1072.2271,  972.0872,  935.3907,  876.9937,
         907.9382,  888.2331,  842.3994,  695.1476,  634.7491,  575.0141,
         523.9191,  487.9213,  432.6153,  362.4886,  388.5830,  341.3362,
         309.6942,  235.2567,  195.5565,  196.0566,  157.1312,  149.0479,
         163.9266,   93.4078,  103.3548,   62.2494,   44.8784,   47.7424,
          52.2668,   35.6828,   25.6330,   45.0870,   35.1522])]
2599.6747395363045
3.9922715964296787 9.11610899319112 14.2058577165322
val isze = 8
idinces = [58 64 21  4 78  5 53 55 80 75 46 61 13 26 52  1 59 69 14  9 77 82 39 28
 37 36 54 31  8 68 79 12  7 48 65 16 49 35 43 44 27 41  3  6 81 32 22 25
 47 56 33 51 70  2  0 45 20 50 71 23 10 63 67 66 24 76 30 42 57 60 34 72
 40 18 11 73 74 62 15 17 19 38 29]
we are doing training validation split
training loss = 261.8695373535156 100
val loss = 300.4329528808594
training loss = 17.685287475585938 200
val loss = 12.459457397460938
training loss = 12.248680114746094 300
val loss = 9.750436782836914
training loss = 9.478058815002441 400
val loss = 8.53988265991211
training loss = 8.034562110900879 500
val loss = 7.99685001373291
training loss = 7.269108295440674 600
val loss = 7.760583877563477
training loss = 6.85852575302124 700
val loss = 7.668614864349365
training loss = 6.636611461639404 800
val loss = 7.642910480499268
training loss = 6.515479564666748 900
val loss = 7.645126819610596
training loss = 6.447862148284912 1000
val loss = 7.655857086181641
training loss = 6.4081878662109375 1100
val loss = 7.666217803955078
training loss = 6.382729530334473 1200
val loss = 7.672326564788818
training loss = 6.3642497062683105 1300
val loss = 7.6732096672058105
training loss = 6.349026203155518 1400
val loss = 7.669178485870361
training loss = 6.335230827331543 1500
val loss = 7.660928249359131
training loss = 6.32198429107666 1600
val loss = 7.649564266204834
training loss = 6.30888032913208 1700
val loss = 7.635807514190674
training loss = 6.295743942260742 1800
val loss = 7.620410919189453
training loss = 6.2824907302856445 1900
val loss = 7.603915691375732
training loss = 6.269075870513916 2000
val loss = 7.58656120300293
training loss = 6.255484580993652 2100
val loss = 7.568637847900391
training loss = 6.241695880889893 2200
val loss = 7.550254821777344
training loss = 6.2276930809021 2300
val loss = 7.531499862670898
training loss = 6.213458061218262 2400
val loss = 7.512272834777832
training loss = 6.198976516723633 2500
val loss = 7.492827415466309
training loss = 6.184228897094727 2600
val loss = 7.472862243652344
training loss = 6.169196128845215 2700
val loss = 7.4525017738342285
training loss = 6.153862476348877 2800
val loss = 7.431646347045898
training loss = 6.138221263885498 2900
val loss = 7.410384178161621
training loss = 6.122276306152344 3000
val loss = 7.388655662536621
training loss = 6.1060590744018555 3100
val loss = 7.366450786590576
training loss = 6.0896382331848145 3200
val loss = 7.34389066696167
training loss = 6.073139667510986 3300
val loss = 7.321200370788574
training loss = 6.056783199310303 3400
val loss = 7.298749923706055
training loss = 6.0409016609191895 3500
val loss = 7.276529312133789
training loss = 6.02594518661499 3600
val loss = 7.255837917327881
training loss = 6.012473106384277 3700
val loss = 7.23654317855835
training loss = 6.001467704772949 3800
val loss = 7.191936016082764
training loss = 5.992623329162598 3900
val loss = 7.207987308502197
training loss = 5.98647403717041 4000
val loss = 7.188615322113037
training loss = 5.9823222160339355 4100
val loss = 7.192868232727051
training loss = 5.98175048828125 4200
val loss = 7.096210479736328
training loss = 5.977739334106445 4300
val loss = 7.184001445770264
training loss = 5.976650714874268 4400
val loss = 7.212230205535889
training loss = 5.975473880767822 4500
val loss = 7.1786346435546875
training loss = 5.974938869476318 4600
val loss = 7.143858432769775
training loss = 5.973867893218994 4700
val loss = 7.177271842956543
training loss = 5.973154067993164 4800
val loss = 7.192269325256348
training loss = 5.972391128540039 4900
val loss = 7.173824310302734
training loss = 5.971912860870361 5000
val loss = 7.136187553405762
training loss = 5.970734596252441 5100
val loss = 7.171919345855713
training loss = 5.986221790313721 5200
val loss = 6.930835247039795
training loss = 5.96872091293335 5300
val loss = 7.169076919555664
training loss = 5.96940279006958 5400
val loss = 7.082736968994141
training loss = 5.966017246246338 5500
val loss = 7.1696953773498535
training loss = 5.964167594909668 5600
val loss = 7.160005569458008
training loss = 5.962018966674805 5700
val loss = 7.181008815765381
training loss = 5.958946704864502 5800
val loss = 7.1551737785339355
training loss = 6.041698455810547 5900
val loss = 7.812889099121094
training loss = 5.9474029541015625 6000
val loss = 7.141622066497803
training loss = 6.107903003692627 6100
val loss = 8.099678993225098
training loss = 5.895011901855469 6200
val loss = 7.054026126861572
training loss = 5.703693389892578 6300
val loss = 7.066445350646973
training loss = 4.199363708496094 6400
val loss = 4.378039836883545
training loss = 2.6236636638641357 6500
val loss = 2.7229933738708496
training loss = 2.269230842590332 6600
val loss = 2.600369691848755
training loss = 2.1277666091918945 6700
val loss = 2.4101057052612305
training loss = 2.072158098220825 6800
val loss = 2.3868255615234375
training loss = 2.008866310119629 6900
val loss = 2.402562379837036
training loss = 1.9660359621047974 7000
val loss = 2.3881373405456543
training loss = 1.9319132566452026 7100
val loss = 2.373291015625
training loss = 1.902691125869751 7200
val loss = 2.353749990463257
training loss = 1.8795589208602905 7300
val loss = 2.3495535850524902
training loss = 1.8581576347351074 7400
val loss = 2.3158817291259766
training loss = 1.8595577478408813 7500
val loss = 2.376804828643799
training loss = 1.8234448432922363 7600
val loss = 2.275883674621582
training loss = 1.806997299194336 7700
val loss = 2.268165111541748
training loss = 1.7932206392288208 7800
val loss = 2.270977020263672
training loss = 1.779581904411316 7900
val loss = 2.2543764114379883
training loss = 1.7689251899719238 8000
val loss = 2.2338829040527344
training loss = 1.757891058921814 8100
val loss = 2.2323038578033447
training loss = 1.782031536102295 8200
val loss = 2.3394570350646973
training loss = 1.7431602478027344 8300
val loss = 2.215423107147217
training loss = 1.7378294467926025 8400
val loss = 2.211695909500122
training loss = 1.7336831092834473 8500
val loss = 2.2090744972229004
training loss = 1.729923963546753 8600
val loss = 2.209596633911133
training loss = 1.7428447008132935 8700
val loss = 2.1644859313964844
training loss = 1.7238562107086182 8800
val loss = 2.2092630863189697
training loss = 1.9941926002502441 8900
val loss = 2.164292812347412
training loss = 1.7190908193588257 9000
val loss = 2.2112669944763184
training loss = 1.717056155204773 9100
val loss = 2.2073898315429688
training loss = 1.7153422832489014 9200
val loss = 2.211538314819336
training loss = 1.713790774345398 9300
val loss = 2.2093725204467773
training loss = 1.7207703590393066 9400
val loss = 2.174656867980957
training loss = 1.7113397121429443 9500
val loss = 2.2100753784179688
training loss = 1.7116365432739258 9600
val loss = 2.192800998687744
training loss = 1.709490418434143 9700
val loss = 2.207782506942749
training loss = 1.7086564302444458 9800
val loss = 2.209719657897949
training loss = 1.7100228071212769 9900
val loss = 2.2334113121032715
training loss = 1.7073787450790405 10000
val loss = 2.2101240158081055
training loss = 1.9013561010360718 10100
val loss = 2.5647058486938477
training loss = 1.7063876390457153 10200
val loss = 2.209662437438965
training loss = 1.7059438228607178 10300
val loss = 2.210411310195923
training loss = 1.706553339958191 10400
val loss = 2.1968374252319336
training loss = 1.7052321434020996 10500
val loss = 2.2106246948242188
training loss = 1.7061938047409058 10600
val loss = 2.1978297233581543
training loss = 1.7046583890914917 10700
val loss = 2.2109365463256836
training loss = 1.7054901123046875 10800
val loss = 2.2275948524475098
training loss = 1.704197883605957 10900
val loss = 2.2130136489868164
training loss = 1.703964114189148 11000
val loss = 2.2113635540008545
training loss = 1.7147985696792603 11100
val loss = 2.1725754737854004
training loss = 1.7036062479019165 11200
val loss = 2.2121548652648926
training loss = 1.7034320831298828 11300
val loss = 2.2112956047058105
training loss = 1.703545331954956 11400
val loss = 2.2053070068359375
training loss = 1.703141689300537 11500
val loss = 2.211888313293457
training loss = 1.712074875831604 11600
val loss = 2.176041603088379
training loss = 1.7028888463974 11700
val loss = 2.212364673614502
training loss = 1.7027685642242432 11800
val loss = 2.213294267654419
training loss = 1.702690601348877 11900
val loss = 2.214937925338745
training loss = 1.7025493383407593 12000
val loss = 2.2127323150634766
training loss = 1.7135980129241943 12100
val loss = 2.1733803749084473
training loss = 1.7023613452911377 12200
val loss = 2.2131476402282715
training loss = 1.7022607326507568 12300
val loss = 2.212890863418579
training loss = 1.7024593353271484 12400
val loss = 2.2217557430267334
training loss = 1.7020971775054932 12500
val loss = 2.2134251594543457
training loss = 1.8286601305007935 12600
val loss = 2.133681297302246
training loss = 1.7019708156585693 12700
val loss = 2.212118625640869
training loss = 1.701865792274475 12800
val loss = 2.2133612632751465
training loss = 1.7019363641738892 12900
val loss = 2.2091732025146484
training loss = 1.7017314434051514 13000
val loss = 2.213792562484741
training loss = 1.7539384365081787 13100
val loss = 2.3675377368927
training loss = 1.7016119956970215 13200
val loss = 2.2138023376464844
training loss = 1.7015376091003418 13300
val loss = 2.213858127593994
training loss = 1.7032719850540161 13400
val loss = 2.23764705657959
training loss = 1.7014241218566895 13500
val loss = 2.2142982482910156
training loss = 1.7013779878616333 13600
val loss = 2.2157859802246094
training loss = 1.7014213800430298 13700
val loss = 2.2199172973632812
training loss = 1.7012567520141602 13800
val loss = 2.2144553661346436
training loss = 1.7066837549209595 13900
val loss = 2.1801795959472656
training loss = 1.7011653184890747 14000
val loss = 2.2141740322113037
training loss = 1.701101303100586 14100
val loss = 2.213825225830078
training loss = 1.701094388961792 14200
val loss = 2.213531494140625
training loss = 1.7009987831115723 14300
val loss = 2.214954376220703
training loss = 1.7015295028686523 14400
val loss = 2.206475019454956
training loss = 1.7008987665176392 14500
val loss = 2.2153704166412354
training loss = 1.7208954095840454 14600
val loss = 2.2997069358825684
training loss = 1.7008014917373657 14700
val loss = 2.216850757598877
training loss = 1.7007293701171875 14800
val loss = 2.215820074081421
training loss = 1.700710654258728 14900
val loss = 2.2184832096099854
training loss = 1.7006064653396606 15000
val loss = 2.215817928314209
training loss = 1.7239142656326294 15100
val loss = 2.1575918197631836
training loss = 1.700467824935913 15200
val loss = 2.216266393661499
training loss = 1.700410008430481 15300
val loss = 2.219033718109131
training loss = 1.7002989053726196 15400
val loss = 2.2169647216796875
training loss = 1.7001683712005615 15500
val loss = 2.2169604301452637
training loss = 1.7002689838409424 15600
val loss = 2.213301420211792
training loss = 1.6998939514160156 15700
val loss = 2.2181153297424316
training loss = 1.6997007131576538 15800
val loss = 2.2180957794189453
training loss = 1.7003158330917358 15900
val loss = 2.2062244415283203
training loss = 1.699233055114746 16000
val loss = 2.2198219299316406
reduced chi^2 level 2 = 1.699016809463501
Constrained alpha: 1.8447734117507935
Constrained beta: 3.4875240325927734
Constrained gamma: 13.2328519821167
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 840.9080,  828.6138,  892.9760,  933.8395, 1026.9102, 1080.5422,
        1066.8314, 1154.5133, 1153.0421, 1144.6949, 1189.7913, 1195.0259,
        1263.4890, 1278.3539, 1338.6055, 1474.2854, 1337.1787, 1397.2609,
        1524.6533, 1504.7738, 1646.8167, 1566.4225, 1603.5092, 1590.1295,
        1645.3115, 1697.9446, 1671.4980, 1651.8002, 1767.1477, 1687.9543,
        1712.9078, 1731.9385, 1714.1262, 1761.7419, 1737.2692, 1846.0295,
        1615.0548, 1541.7225, 1662.6354, 1631.4077, 1628.0201, 1568.1810,
        1478.8156, 1479.5887, 1463.4238, 1305.3672, 1169.9561, 1202.6671,
        1103.4801, 1187.8954, 1040.2589,  967.0239,  929.8866,  962.0860,
         890.7625,  864.1323,  833.4481,  771.3889,  629.4490,  523.1104,
         545.5979,  490.5091,  442.4181,  402.1925,  351.8630,  334.9519,
         273.0209,  234.3359,  225.8183,  180.5537,  164.0361,  139.5997,
         148.4934,  102.5132,   81.6114,   60.4647,   51.7949,   41.4735,
          34.4379,   43.9975,   17.3990,   41.4075,   23.1653])]
2986.0109680473115
4.615398649518972 3.376081229517358 31.738906010090563
val isze = 8
idinces = [49 28 78  1 76 11 33  2 62 46 35 52 72 70 51 64 47 75 24 39 32 40 69 21
  7 79 13 82 29  8 67 48 65 16 14 37 77  3 68 50 22 44  6 38  9 80 12 71
 61  4 57 30 20 42 36 19 53 26 18 59 74 63 31 66 45 58 10 56 23 41 73 60
  0 55 43 34 81 27 25 15  5 17 54]
we are doing training validation split
training loss = 56.06123352050781 100
val loss = 124.96334838867188
training loss = 23.580537796020508 200
val loss = 54.466861724853516
training loss = 13.909972190856934 300
val loss = 33.020545959472656
training loss = 9.874943733215332 400
val loss = 23.768753051757812
training loss = 7.933474540710449 500
val loss = 18.985177993774414
training loss = 6.9311981201171875 600
val loss = 16.222930908203125
training loss = 6.394808292388916 700
val loss = 14.507959365844727
training loss = 6.102885723114014 800
val loss = 13.389776229858398
training loss = 5.943126201629639 900
val loss = 12.636293411254883
training loss = 5.855679035186768 1000
val loss = 12.117716789245605
training loss = 5.8077497482299805 1100
val loss = 11.756712913513184
training loss = 5.781189918518066 1200
val loss = 11.504226684570312
training loss = 5.765971660614014 1300
val loss = 11.328014373779297
training loss = 5.756635665893555 1400
val loss = 11.205821990966797
training loss = 5.750269412994385 1500
val loss = 11.121857643127441
training loss = 5.745367050170898 1600
val loss = 11.064716339111328
training loss = 5.741165637969971 1700
val loss = 11.026104927062988
training loss = 5.737305164337158 1800
val loss = 11.000150680541992
training loss = 5.733600616455078 1900
val loss = 10.98247241973877
training loss = 5.729982852935791 2000
val loss = 10.970173835754395
training loss = 5.726418972015381 2100
val loss = 10.961244583129883
training loss = 5.722908020019531 2200
val loss = 10.954472541809082
training loss = 5.719450950622559 2300
val loss = 10.949000358581543
training loss = 5.716058254241943 2400
val loss = 10.944387435913086
training loss = 5.712748050689697 2500
val loss = 10.940376281738281
training loss = 5.7095255851745605 2600
val loss = 10.936930656433105
training loss = 5.7064008712768555 2700
val loss = 10.933868408203125
training loss = 5.703362464904785 2800
val loss = 10.931344032287598
training loss = 5.700392246246338 2900
val loss = 10.929153442382812
training loss = 5.697444438934326 3000
val loss = 10.927292823791504
training loss = 5.694416522979736 3100
val loss = 10.92569351196289
training loss = 5.691137790679932 3200
val loss = 10.92413330078125
training loss = 5.687231540679932 3300
val loss = 10.92216682434082
training loss = 5.681857585906982 3400
val loss = 10.91899585723877
training loss = 5.672670364379883 3500
val loss = 10.912550926208496
training loss = 5.650681972503662 3600
val loss = 10.895576477050781
training loss = 5.570686340332031 3700
val loss = 10.78466510772705
training loss = 5.332531929016113 3800
val loss = 10.576505661010742
training loss = 5.007759094238281 3900
val loss = 9.90324592590332
training loss = 4.496255874633789 4000
val loss = 8.821258544921875
training loss = 3.741959810256958 4100
val loss = 7.334558486938477
training loss = 2.976121187210083 4200
val loss = 5.628524303436279
training loss = 2.6748485565185547 4300
val loss = 4.7615966796875
training loss = 2.6208903789520264 4400
val loss = 4.44871711730957
training loss = 2.602675437927246 4500
val loss = 4.317852973937988
training loss = 2.5892651081085205 4600
val loss = 4.213671684265137
training loss = 2.5797712802886963 4700
val loss = 4.16849422454834
training loss = 2.5694563388824463 4800
val loss = 4.072875022888184
training loss = 2.5616161823272705 4900
val loss = 4.020553112030029
training loss = 2.5549213886260986 5000
val loss = 3.963662624359131
training loss = 2.5487020015716553 5100
val loss = 3.9351558685302734
training loss = 2.543154001235962 5200
val loss = 3.900552272796631
training loss = 2.5382063388824463 5300
val loss = 3.866302251815796
training loss = 2.533587694168091 5400
val loss = 3.8443052768707275
training loss = 2.5628325939178467 5500
val loss = 3.990002155303955
training loss = 2.5252368450164795 5600
val loss = 3.7985026836395264
training loss = 2.5214133262634277 5700
val loss = 3.7803845405578613
training loss = 2.5180530548095703 5800
val loss = 3.7735772132873535
training loss = 2.5144076347351074 5900
val loss = 3.7464685440063477
training loss = 2.51179838180542 6000
val loss = 3.7448883056640625
training loss = 2.5079712867736816 6100
val loss = 3.716099500656128
training loss = 2.5049171447753906 6200
val loss = 3.705103874206543
training loss = 2.501948356628418 6300
val loss = 3.693942070007324
training loss = 2.4990837574005127 6400
val loss = 3.68153977394104
training loss = 2.5186007022857666 6500
val loss = 3.7956933975219727
training loss = 2.493537664413452 6600
val loss = 3.6613826751708984
training loss = 2.490812063217163 6700
val loss = 3.65240740776062
training loss = 2.4881694316864014 6800
val loss = 3.6434011459350586
training loss = 2.4855377674102783 6900
val loss = 3.632873296737671
training loss = 2.483783483505249 7000
val loss = 3.636977434158325
training loss = 2.480342149734497 7100
val loss = 3.6149258613586426
training loss = 2.4777398109436035 7200
val loss = 3.6075358390808105
training loss = 2.5446548461914062 7300
val loss = 3.484334945678711
training loss = 2.4724674224853516 7400
val loss = 3.5904476642608643
training loss = 2.469763994216919 7500
val loss = 3.584120750427246
training loss = 2.4669899940490723 7600
val loss = 3.5804195404052734
training loss = 2.4642205238342285 7700
val loss = 3.56860613822937
training loss = 2.4612886905670166 7800
val loss = 3.5627565383911133
training loss = 2.4583568572998047 7900
val loss = 3.553997039794922
training loss = 2.455331563949585 8000
val loss = 3.5469298362731934
training loss = 2.4581141471862793 8100
val loss = 3.4877729415893555
training loss = 2.449038028717041 8200
val loss = 3.5314974784851074
training loss = 2.4457523822784424 8300
val loss = 3.5248260498046875
training loss = 2.44706392288208 8400
val loss = 3.5675747394561768
training loss = 2.4389796257019043 8500
val loss = 3.5097503662109375
training loss = 2.5856804847717285 8600
val loss = 3.4014546871185303
training loss = 2.4319844245910645 8700
val loss = 3.493211030960083
training loss = 2.4283971786499023 8800
val loss = 3.483365058898926
training loss = 2.424943208694458 8900
val loss = 3.4714787006378174
training loss = 2.4213197231292725 9000
val loss = 3.467728614807129
training loss = 2.419066905975342 9100
val loss = 3.437753915786743
training loss = 2.414332866668701 9200
val loss = 3.449422597885132
training loss = 2.441467046737671 9300
val loss = 3.5795400142669678
training loss = 2.4075069427490234 9400
val loss = 3.432541608810425
training loss = 2.404160261154175 9500
val loss = 3.4209282398223877
training loss = 2.4046053886413574 9600
val loss = 3.372032880783081
training loss = 2.397585868835449 9700
val loss = 3.402557134628296
training loss = 2.3943209648132324 9800
val loss = 3.39131236076355
training loss = 2.3916125297546387 9900
val loss = 3.3694663047790527
training loss = 2.388148307800293 10000
val loss = 3.3715014457702637
training loss = 2.390941619873047 10100
val loss = 3.4128332138061523
training loss = 2.382140636444092 10200
val loss = 3.348871946334839
training loss = 2.379143476486206 10300
val loss = 3.341048002243042
training loss = 2.378207206726074 10400
val loss = 3.358696222305298
training loss = 2.3734848499298096 10500
val loss = 3.3209826946258545
training loss = 2.3780863285064697 10600
val loss = 3.3635571002960205
training loss = 2.3680248260498047 10700
val loss = 3.3006153106689453
training loss = 2.3653063774108887 10800
val loss = 3.291046619415283
training loss = 2.3628101348876953 10900
val loss = 3.279721736907959
training loss = 2.3602993488311768 11000
val loss = 3.271710157394409
training loss = 2.427846908569336 11100
val loss = 3.4783129692077637
training loss = 2.3555235862731934 11200
val loss = 3.2542526721954346
training loss = 2.353172779083252 11300
val loss = 3.2442736625671387
training loss = 2.352539539337158 11400
val loss = 3.2147834300994873
training loss = 2.3487613201141357 11500
val loss = 3.226665496826172
training loss = 2.3714590072631836 11600
val loss = 3.1567375659942627
training loss = 2.3446006774902344 11700
val loss = 3.2123146057128906
training loss = 2.342562675476074 11800
val loss = 3.201941728591919
training loss = 2.374155282974243 11900
val loss = 3.125886917114258
training loss = 2.3386878967285156 12000
val loss = 3.1857290267944336
training loss = 2.336758613586426 12100
val loss = 3.177767276763916
training loss = 2.335001230239868 12200
val loss = 3.1735634803771973
training loss = 2.333183765411377 12300
val loss = 3.163529396057129
training loss = 2.38376522064209 12400
val loss = 3.0830702781677246
training loss = 2.329685926437378 12500
val loss = 3.1482536792755127
training loss = 2.327916383743286 12600
val loss = 3.141646385192871
training loss = 2.3262362480163574 12700
val loss = 3.1357624530792236
training loss = 2.324519395828247 12800
val loss = 3.1278133392333984
training loss = 2.3705785274505615 12900
val loss = 3.0514118671417236
training loss = 2.320993423461914 13000
val loss = 3.114215135574341
training loss = 2.3192121982574463 13100
val loss = 3.112229585647583
training loss = 2.3172078132629395 13200
val loss = 3.097172498703003
training loss = 2.3151021003723145 13300
val loss = 3.091574192047119
training loss = 2.313030242919922 13400
val loss = 3.090531826019287
training loss = 2.3105099201202393 13500
val loss = 3.076082706451416
training loss = 2.3078935146331787 13600
val loss = 3.067312479019165
training loss = 2.5294370651245117 13700
val loss = 3.041250705718994
training loss = 2.3021183013916016 13800
val loss = 3.0482120513916016
training loss = 2.2990472316741943 13900
val loss = 3.036402940750122
training loss = 2.302457809448242 14000
val loss = 2.9903926849365234
training loss = 2.2926294803619385 14100
val loss = 3.0100882053375244
training loss = 2.289100170135498 14200
val loss = 2.9953043460845947
training loss = 2.2856063842773438 14300
val loss = 2.974928855895996
training loss = 2.281564235687256 14400
val loss = 2.9648637771606445
training loss = 2.297393798828125 14500
val loss = 2.8982913494110107
training loss = 2.2728846073150635 14600
val loss = 2.931798219680786
training loss = 2.268394947052002 14700
val loss = 2.903980255126953
training loss = 2.262833595275879 14800
val loss = 2.897416830062866
training loss = 2.2571611404418945 14900
val loss = 2.876007080078125
training loss = 2.2515084743499756 15000
val loss = 2.8637421131134033
training loss = 2.2449803352355957 15100
val loss = 2.835625171661377
training loss = 2.2964015007019043 15200
val loss = 2.972383975982666
training loss = 2.2312893867492676 15300
val loss = 2.7918410301208496
training loss = 2.2238452434539795 15400
val loss = 2.7702784538269043
training loss = 2.217630386352539 15500
val loss = 2.7645750045776367
training loss = 2.208688497543335 15600
val loss = 2.7264351844787598
training loss = 2.2011048793792725 15700
val loss = 2.6940619945526123
training loss = 2.192950487136841 15800
val loss = 2.682206869125366
training loss = 2.1850850582122803 15900
val loss = 2.6628873348236084
training loss = 2.2494654655456543 16000
val loss = 2.6091690063476562
training loss = 2.1694984436035156 16100
val loss = 2.6251800060272217
training loss = 2.161756753921509 16200
val loss = 2.6061882972717285
training loss = 2.1553757190704346 16300
val loss = 2.5791730880737305
training loss = 2.1472291946411133 16400
val loss = 2.574233055114746
training loss = 2.142326593399048 16500
val loss = 2.542753219604492
training loss = 2.1337740421295166 16600
val loss = 2.548011541366577
training loss = 2.127216100692749 16700
val loss = 2.535550594329834
training loss = 2.1218178272247314 16800
val loss = 2.5337038040161133
training loss = 2.115668296813965 16900
val loss = 2.5165882110595703
training loss = 2.1100618839263916 17000
val loss = 2.5122921466827393
training loss = 2.1048433780670166 17100
val loss = 2.502272605895996
training loss = 2.09971022605896 17200
val loss = 2.4942984580993652
training loss = 2.09501051902771 17300
val loss = 2.4922385215759277
training loss = 2.0903072357177734 17400
val loss = 2.484050989151001
training loss = 2.0987048149108887 17500
val loss = 2.4548416137695312
training loss = 2.0817131996154785 17600
val loss = 2.4763872623443604
training loss = 2.0774495601654053 17700
val loss = 2.472306251525879
training loss = 2.0738720893859863 17800
val loss = 2.4660727977752686
training loss = 2.069688320159912 17900
val loss = 2.4672188758850098
training loss = 2.080437421798706 18000
val loss = 2.444822311401367
training loss = 2.0623340606689453 18100
val loss = 2.463033676147461
training loss = 2.0584468841552734 18200
val loss = 2.4606101512908936
training loss = 2.0556466579437256 18300
val loss = 2.4531941413879395
training loss = 2.0512185096740723 18400
val loss = 2.457105875015259
training loss = 2.0516722202301025 18500
val loss = 2.438469409942627
training loss = 2.043830156326294 18600
val loss = 2.453434467315674
training loss = 2.03969407081604 18700
val loss = 2.4514808654785156
training loss = 2.036226749420166 18800
val loss = 2.4475483894348145
training loss = 2.031856060028076 18900
val loss = 2.4479658603668213
training loss = 2.0293924808502197 19000
val loss = 2.459089994430542
training loss = 2.023493766784668 19100
val loss = 2.4448351860046387
training loss = 2.0305168628692627 19200
val loss = 2.488400459289551
training loss = 2.0140652656555176 19300
val loss = 2.440378427505493
training loss = 2.0069820880889893 19400
val loss = 2.4333667755126953
training loss = 2.001408100128174 19500
val loss = 2.4292917251586914
training loss = 1.993470549583435 19600
val loss = 2.4262781143188477
training loss = 1.9892245531082153 19700
val loss = 2.437983512878418
training loss = 1.9792214632034302 19800
val loss = 2.4270989894866943
training loss = 2.458984136581421 19900
val loss = 3.0563430786132812
training loss = 1.9645880460739136 20000
val loss = 2.4373676776885986
training loss = 1.9570797681808472 20100
val loss = 2.448369264602661
training loss = 1.9516891241073608 20200
val loss = 2.462205410003662
training loss = 1.9456026554107666 20300
val loss = 2.473050117492676
training loss = 1.941860556602478 20400
val loss = 2.483452558517456
training loss = 1.9370211362838745 20500
val loss = 2.502218723297119
training loss = 1.9359207153320312 20600
val loss = 2.506687641143799
training loss = 1.930882215499878 20700
val loss = 2.531064033508301
training loss = 2.0206503868103027 20800
val loss = 2.492169141769409
training loss = 1.9266440868377686 20900
val loss = 2.5590620040893555
training loss = 1.9246106147766113 21000
val loss = 2.570775032043457
training loss = 1.923791527748108 21100
val loss = 2.589175224304199
training loss = 1.9222404956817627 21200
val loss = 2.5946927070617676
training loss = 1.9373401403427124 21300
val loss = 2.646853446960449
training loss = 1.9205025434494019 21400
val loss = 2.6153616905212402
training loss = 1.919348955154419 21500
val loss = 2.6262099742889404
training loss = 1.91968834400177 21600
val loss = 2.6244187355041504
training loss = 1.9182300567626953 21700
val loss = 2.6453769207000732
training loss = 1.9172402620315552 21800
val loss = 2.654573917388916
training loss = 1.917159080505371 21900
val loss = 2.66503643989563
training loss = 1.9162063598632812 22000
val loss = 2.6702780723571777
training loss = 1.9233970642089844 22100
val loss = 2.646782398223877
training loss = 1.91524338722229 22200
val loss = 2.6863818168640137
training loss = 1.9395800828933716 22300
val loss = 2.7983763217926025
training loss = 1.9142533540725708 22400
val loss = 2.7012836933135986
training loss = 1.9132418632507324 22500
val loss = 2.7072391510009766
training loss = 1.9175310134887695 22600
val loss = 2.6854655742645264
training loss = 1.912500262260437 22700
val loss = 2.7211034297943115
training loss = 1.9114341735839844 22800
val loss = 2.727024555206299
training loss = 1.9119877815246582 22900
val loss = 2.748779296875
training loss = 1.9104801416397095 23000
val loss = 2.740645408630371
training loss = 1.9209614992141724 23100
val loss = 2.7100706100463867
training loss = 1.9095631837844849 23200
val loss = 2.7532920837402344
training loss = 1.9226632118225098 23300
val loss = 2.6990694999694824
training loss = 1.9086804389953613 23400
val loss = 2.771202564239502
training loss = 1.9075984954833984 23500
val loss = 2.7722795009613037
training loss = 1.908226728439331 23600
val loss = 2.768421173095703
training loss = 1.9067598581314087 23700
val loss = 2.7850189208984375
training loss = 1.9076412916183472 23800
val loss = 2.8183557987213135
training loss = 1.9063133001327515 23900
val loss = 2.7962870597839355
training loss = 1.9053316116333008 24000
val loss = 2.8021390438079834
training loss = 1.905708909034729 24100
val loss = 2.8029026985168457
training loss = 1.9049394130706787 24200
val loss = 2.8157143592834473
training loss = 1.905005693435669 24300
val loss = 2.8211636543273926
training loss = 1.9069218635559082 24400
val loss = 2.803926944732666
training loss = 1.9054484367370605 24500
val loss = 2.8318686485290527
training loss = 1.9129903316497803 24600
val loss = 2.7913122177124023
training loss = 1.9056620597839355 24700
val loss = 2.8403708934783936
training loss = 1.9628934860229492 24800
val loss = 3.062382221221924
training loss = 1.905687928199768 24900
val loss = 2.8486685752868652
reduced chi^2 level 2 = 1.9056689739227295
Constrained alpha: 1.9298149347305298
Constrained beta: -0.0033137125428766012
Constrained gamma: 12.447086334228516
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 842.5087,  852.3597,  972.0799, 1013.3231, 1030.8130, 1067.5889,
        1097.9244, 1174.4862, 1158.7853, 1195.1190, 1152.6931, 1218.5597,
        1177.3236, 1287.8390, 1306.3820, 1494.8492, 1414.7410, 1405.0812,
        1577.3633, 1476.9135, 1582.5269, 1543.0195, 1591.4061, 1556.7653,
        1621.4481, 1631.9670, 1614.2164, 1787.1530, 1758.4838, 1641.8580,
        1764.2592, 1776.2169, 1724.4075, 1751.1246, 1682.7297, 1777.0682,
        1726.0750, 1602.7609, 1683.2855, 1652.7810, 1624.0250, 1530.3169,
        1464.9257, 1507.0825, 1389.4037, 1320.9011, 1290.5397, 1279.2009,
        1151.5087, 1155.9496, 1071.4441,  970.7951,  994.8244,  900.6073,
         907.7654,  848.7534,  858.0715,  704.0051,  567.0597,  548.5743,
         589.2155,  474.1407,  457.4039,  422.3329,  364.6418,  327.3229,
         280.8092,  243.9286,  225.0904,  182.1560,  152.5664,  130.8556,
         142.8015,  117.7475,   91.5721,   70.1742,   58.6917,   46.8914,
          29.7467,   35.2005,   27.9958,   31.6486,   49.2834])]
2839.349475226805
1.9992449052307943 10.89598505186644 5.915513735264644
val isze = 8
idinces = [14 50 11 47 45 82 66 70 40 10 26 81 72 38 71  0  4  1 60 76 24 57 77 41
 20 12 65  3 80 79 64 34 42 75 23 56 35 74 19 33 49  7 61 25 53 37 48 52
  5 16 22 43 68 73 31 67 46 13 28 62 54 15  2  6 29 21 51 78 55 27 59 18
 69 44 32 17  8 39 30 58 36  9 63]
we are doing training validation split
training loss = 158.9800567626953 100
val loss = 159.64404296875
training loss = 11.300398826599121 200
val loss = 9.048715591430664
training loss = 10.00226879119873 300
val loss = 8.446651458740234
training loss = 8.961986541748047 400
val loss = 8.029618263244629
training loss = 8.1774320602417 500
val loss = 7.781031608581543
training loss = 7.6042304039001465 600
val loss = 7.662426471710205
training loss = 7.192648410797119 700
val loss = 7.634581565856934
training loss = 6.899921417236328 800
val loss = 7.6650238037109375
training loss = 6.692595958709717 900
val loss = 7.729057312011719
training loss = 6.545615196228027 1000
val loss = 7.809317588806152
training loss = 6.4406633377075195 1100
val loss = 7.893611907958984
training loss = 6.364542007446289 1200
val loss = 7.974090099334717
training loss = 6.307854175567627 1300
val loss = 8.045909881591797
training loss = 6.26399040222168 1400
val loss = 8.106550216674805
training loss = 6.228358745574951 1500
val loss = 8.155031204223633
training loss = 6.197836875915527 1600
val loss = 8.191473007202148
training loss = 6.170316219329834 1700
val loss = 8.216395378112793
training loss = 6.144418716430664 1800
val loss = 8.231095314025879
training loss = 6.1192731857299805 1900
val loss = 8.236997604370117
training loss = 6.0943427085876465 2000
val loss = 8.235252380371094
training loss = 6.069307327270508 2100
val loss = 8.227517127990723
training loss = 6.044008731842041 2200
val loss = 8.214740753173828
training loss = 6.018375396728516 2300
val loss = 8.198019981384277
training loss = 5.992395877838135 2400
val loss = 8.178397178649902
training loss = 5.966124057769775 2500
val loss = 8.156497955322266
training loss = 5.939624786376953 2600
val loss = 8.132817268371582
training loss = 5.912993907928467 2700
val loss = 8.107934951782227
training loss = 5.886348724365234 2800
val loss = 8.082019805908203
training loss = 5.859788417816162 2900
val loss = 8.055418014526367
training loss = 5.833395481109619 3000
val loss = 8.02808666229248
training loss = 5.807150840759277 3100
val loss = 7.999949932098389
training loss = 5.780851364135742 3200
val loss = 7.970500946044922
training loss = 5.753888130187988 3300
val loss = 7.938869476318359
training loss = 5.724698066711426 3400
val loss = 7.902340888977051
training loss = 5.689222812652588 3500
val loss = 7.855027198791504
training loss = 5.635344505310059 3600
val loss = 7.778863906860352
training loss = 5.519069194793701 3700
val loss = 7.610198020935059
training loss = 5.2084832191467285 3800
val loss = 7.182352542877197
training loss = 4.6966423988342285 3900
val loss = 6.662818908691406
training loss = 4.015092372894287 4000
val loss = 6.073807716369629
training loss = 3.1640403270721436 4100
val loss = 5.124146938323975
training loss = 2.4354050159454346 4200
val loss = 3.9397614002227783
training loss = 2.129347085952759 4300
val loss = 3.0917201042175293
training loss = 2.0654211044311523 4400
val loss = 2.7897348403930664
training loss = 2.0475099086761475 4500
val loss = 2.7452030181884766
training loss = 2.036212682723999 4600
val loss = 2.7671687602996826
training loss = 2.027287244796753 4700
val loss = 2.7980153560638428
training loss = 2.0199482440948486 4800
val loss = 2.825035810470581
training loss = 2.013824462890625 4900
val loss = 2.846402168273926
training loss = 2.0086591243743896 5000
val loss = 2.862351894378662
training loss = 2.0043656826019287 5100
val loss = 2.8752450942993164
training loss = 2.000814914703369 5200
val loss = 2.880406618118286
training loss = 1.997725009918213 5300
val loss = 2.88018798828125
training loss = 1.9950960874557495 5400
val loss = 2.883909225463867
training loss = 1.9928483963012695 5500
val loss = 2.886812210083008
training loss = 1.9941760301589966 5600
val loss = 2.7946841716766357
training loss = 1.9890824556350708 5700
val loss = 2.8843860626220703
training loss = 1.9923503398895264 5800
val loss = 2.772526502609253
training loss = 1.9860937595367432 5900
val loss = 2.878718137741089
training loss = 1.9887986183166504 6000
val loss = 2.979320526123047
training loss = 1.983640193939209 6100
val loss = 2.875962734222412
training loss = 1.9825599193572998 6200
val loss = 2.86250638961792
training loss = 1.9815692901611328 6300
val loss = 2.847506284713745
training loss = 1.9806010723114014 6400
val loss = 2.852936029434204
training loss = 1.9891239404678345 6500
val loss = 3.0170698165893555
training loss = 1.9788562059402466 6600
val loss = 2.842123508453369
training loss = 1.9787718057632446 6700
val loss = 2.8813374042510986
training loss = 1.9772738218307495 6800
val loss = 2.836064338684082
training loss = 1.9765613079071045 6900
val loss = 2.8264822959899902
training loss = 1.976190209388733 7000
val loss = 2.7911486625671387
training loss = 1.9751551151275635 7100
val loss = 2.8155665397644043
training loss = 1.974522352218628 7200
val loss = 2.816068649291992
training loss = 1.9738457202911377 7300
val loss = 2.813600540161133
training loss = 1.9732328653335571 7400
val loss = 2.799976110458374
training loss = 1.9763261079788208 7500
val loss = 2.9004716873168945
training loss = 1.9720008373260498 7600
val loss = 2.7902591228485107
training loss = 1.9714512825012207 7700
val loss = 2.7845211029052734
training loss = 1.9720680713653564 7800
val loss = 2.724874258041382
training loss = 1.9703049659729004 7900
val loss = 2.7751646041870117
training loss = 2.1032919883728027 8000
val loss = 3.4746503829956055
training loss = 1.9691919088363647 8100
val loss = 2.7618324756622314
training loss = 1.9686877727508545 8200
val loss = 2.758340358734131
training loss = 1.9725948572158813 8300
val loss = 2.867917776107788
training loss = 1.9676294326782227 8400
val loss = 2.748994827270508
training loss = 1.9671498537063599 8500
val loss = 2.7485437393188477
training loss = 1.9665950536727905 8600
val loss = 2.7403669357299805
training loss = 1.9661343097686768 8700
val loss = 2.7339425086975098
training loss = 1.9749915599822998 8800
val loss = 2.5815589427948
training loss = 1.9651416540145874 8900
val loss = 2.7237913608551025
training loss = 1.9647022485733032 9000
val loss = 2.7261219024658203
training loss = 1.9642333984375 9100
val loss = 2.72845458984375
training loss = 1.9637333154678345 9200
val loss = 2.7097015380859375
training loss = 1.9667863845825195 9300
val loss = 2.612487316131592
training loss = 1.9627941846847534 9400
val loss = 2.70041823387146
training loss = 1.9623887538909912 9500
val loss = 2.6871790885925293
training loss = 1.9619218111038208 9600
val loss = 2.7037463188171387
training loss = 1.961456298828125 9700
val loss = 2.6858010292053223
training loss = 1.9861159324645996 9800
val loss = 2.960304021835327
training loss = 1.9605565071105957 9900
val loss = 2.675551414489746
training loss = 1.9601473808288574 10000
val loss = 2.6715054512023926
training loss = 1.9603259563446045 10100
val loss = 2.7106800079345703
training loss = 1.9592736959457397 10200
val loss = 2.6622023582458496
training loss = 1.959240198135376 10300
val loss = 2.6273276805877686
training loss = 1.9584062099456787 10400
val loss = 2.656243324279785
training loss = 1.9580212831497192 10500
val loss = 2.648699998855591
training loss = 1.9582222700119019 10600
val loss = 2.60481858253479
training loss = 1.9571806192398071 10700
val loss = 2.6407010555267334
training loss = 1.9687128067016602 10800
val loss = 2.8208839893341064
training loss = 1.9563721418380737 10900
val loss = 2.6237571239471436
training loss = 1.9559749364852905 11000
val loss = 2.626040458679199
training loss = 1.957035779953003 11100
val loss = 2.6864266395568848
training loss = 1.9551689624786377 11200
val loss = 2.616931915283203
training loss = 2.060910224914551 11300
val loss = 3.2258973121643066
training loss = 1.9543672800064087 11400
val loss = 2.608182668685913
training loss = 1.9540067911148071 11500
val loss = 2.6042263507843018
training loss = 1.9535861015319824 11600
val loss = 2.6058919429779053
training loss = 1.9532291889190674 11700
val loss = 2.5958094596862793
training loss = 2.0115950107574463 11800
val loss = 3.031907081604004
training loss = 1.9524593353271484 11900
val loss = 2.585948944091797
training loss = 1.9521106481552124 12000
val loss = 2.5826337337493896
training loss = 1.9517484903335571 12100
val loss = 2.591582775115967
training loss = 1.9513609409332275 12200
val loss = 2.574082851409912
training loss = 2.0795328617095947 12300
val loss = 3.2524847984313965
training loss = 1.9506170749664307 12400
val loss = 2.567286491394043
training loss = 1.950284719467163 12500
val loss = 2.5621654987335205
training loss = 1.9499545097351074 12600
val loss = 2.5719151496887207
training loss = 1.9495652914047241 12700
val loss = 2.553152084350586
training loss = 1.9496145248413086 12800
val loss = 2.5173473358154297
training loss = 1.9488530158996582 12900
val loss = 2.5469751358032227
training loss = 1.9486019611358643 13000
val loss = 2.555614471435547
training loss = 1.948154330253601 13100
val loss = 2.540972948074341
training loss = 1.9478381872177124 13200
val loss = 2.5329997539520264
training loss = 1.9481298923492432 13300
val loss = 2.572615623474121
training loss = 1.9471553564071655 13400
val loss = 2.525738000869751
training loss = 2.0120482444763184 13500
val loss = 2.1693196296691895
training loss = 1.9464815855026245 13600
val loss = 2.5190608501434326
training loss = 1.9461801052093506 13700
val loss = 2.511688232421875
training loss = 1.9458614587783813 13800
val loss = 2.5001561641693115
training loss = 1.9455223083496094 13900
val loss = 2.506500720977783
training loss = 2.0030393600463867 14000
val loss = 2.9366796016693115
training loss = 1.9448699951171875 14100
val loss = 2.4962246417999268
training loss = 1.9445834159851074 14200
val loss = 2.495473861694336
training loss = 1.9676936864852905 14300
val loss = 2.7574615478515625
training loss = 1.9439486265182495 14400
val loss = 2.4884581565856934
training loss = 1.9436662197113037 14500
val loss = 2.4851479530334473
training loss = 1.9434560537338257 14600
val loss = 2.5007927417755127
training loss = 1.9430514574050903 14700
val loss = 2.4770846366882324
training loss = 1.949053406715393 14800
val loss = 2.3544108867645264
training loss = 1.9424424171447754 14900
val loss = 2.472168445587158
training loss = 1.9421716928482056 15000
val loss = 2.4688401222229004
training loss = 1.941841721534729 15100
val loss = 2.4622011184692383
training loss = 1.9415820837020874 15200
val loss = 2.4595701694488525
training loss = 1.9461084604263306 15300
val loss = 2.349835157394409
training loss = 1.9409985542297363 15400
val loss = 2.452390193939209
training loss = 1.9476420879364014 15500
val loss = 2.322781562805176
training loss = 1.9404345750808716 15600
val loss = 2.441680431365967
training loss = 1.940170168876648 15700
val loss = 2.442440986633301
training loss = 1.940915822982788 15800
val loss = 2.3890435695648193
training loss = 1.939613699913025 15900
val loss = 2.436300277709961
training loss = 2.063502550125122 16000
val loss = 3.096996784210205
training loss = 1.9390558004379272 16100
val loss = 2.4288504123687744
training loss = 1.9388173818588257 16200
val loss = 2.425868272781372
training loss = 1.942911982536316 16300
val loss = 2.5324087142944336
training loss = 1.9382781982421875 16400
val loss = 2.420344829559326
training loss = 1.9391199350357056 16500
val loss = 2.4695279598236084
training loss = 1.9377702474594116 16600
val loss = 2.4052889347076416
training loss = 1.937514305114746 16700
val loss = 2.409682273864746
training loss = 1.954943299293518 16800
val loss = 2.633955478668213
training loss = 1.9369971752166748 16900
val loss = 2.404766798019409
training loss = 1.9370312690734863 17000
val loss = 2.374807834625244
training loss = 1.936479926109314 17100
val loss = 2.395099639892578
training loss = 1.936262607574463 17200
val loss = 2.3943185806274414
training loss = 1.9727054834365845 17300
val loss = 2.117706775665283
training loss = 1.93575119972229 17400
val loss = 2.390435218811035
training loss = 1.9355419874191284 17500
val loss = 2.385342597961426
training loss = 1.9948220252990723 17600
val loss = 2.8203396797180176
training loss = 1.9350529909133911 17700
val loss = 2.377788543701172
training loss = 1.9348417520523071 17800
val loss = 2.375842571258545
training loss = 1.9392448663711548 17900
val loss = 2.486114978790283
training loss = 1.9343644380569458 18000
val loss = 2.369861125946045
training loss = 1.9341685771942139 18100
val loss = 2.374013900756836
training loss = 1.9338916540145874 18200
val loss = 2.3618693351745605
training loss = 1.9336901903152466 18300
val loss = 2.3614444732666016
training loss = 1.965340256690979 18400
val loss = 2.669311761856079
training loss = 1.9332283735275269 18500
val loss = 2.358018159866333
training loss = 1.9330304861068726 18600
val loss = 2.350557565689087
training loss = 1.9329075813293457 18700
val loss = 2.36968731880188
training loss = 1.9325814247131348 18800
val loss = 2.3468070030212402
training loss = 1.9329266548156738 18900
val loss = 2.306603193283081
training loss = 1.932135820388794 19000
val loss = 2.3407697677612305
training loss = 1.9319401979446411 19100
val loss = 2.338376522064209
training loss = 1.9323410987854004 19200
val loss = 2.2972116470336914
training loss = 1.9315006732940674 19300
val loss = 2.3339133262634277
training loss = 1.931311845779419 19400
val loss = 2.3301892280578613
training loss = 1.9314061403274536 19500
val loss = 2.299638271331787
training loss = 1.9308805465698242 19600
val loss = 2.325270414352417
training loss = 1.9327820539474487 19700
val loss = 2.3965659141540527
training loss = 1.9304561614990234 19800
val loss = 2.3230791091918945
training loss = 1.930271863937378 19900
val loss = 2.316709280014038
training loss = 1.9300458431243896 20000
val loss = 2.3083648681640625
training loss = 1.9298533201217651 20100
val loss = 2.311863422393799
training loss = 2.2705047130584717 20200
val loss = 3.513230800628662
training loss = 1.929430365562439 20300
val loss = 2.3117575645446777
training loss = 1.9292501211166382 20400
val loss = 2.30366849899292
training loss = 1.970320224761963 20500
val loss = 2.016446352005005
training loss = 1.9288358688354492 20600
val loss = 2.300799608230591
training loss = 1.9287739992141724 20700
val loss = 2.278207778930664
training loss = 1.9284802675247192 20800
val loss = 2.280811309814453
training loss = 1.9282337427139282 20900
val loss = 2.290009021759033
training loss = 1.946337342262268 21000
val loss = 2.5177831649780273
training loss = 1.9278085231781006 21100
val loss = 2.2867932319641113
training loss = 1.927620530128479 21200
val loss = 2.282367706298828
training loss = 1.9275370836257935 21300
val loss = 2.2604403495788574
training loss = 1.9271883964538574 21400
val loss = 2.2768993377685547
training loss = 2.062105655670166 21500
val loss = 1.8084688186645508
training loss = 1.9267396926879883 21600
val loss = 2.2754034996032715
training loss = 1.9265328645706177 21700
val loss = 2.2681775093078613
training loss = 1.9304063320159912 21800
val loss = 2.37078857421875
training loss = 1.9260526895523071 21900
val loss = 2.262166738510132
training loss = 1.9259395599365234 22000
val loss = 2.242650270462036
training loss = 1.9256010055541992 22100
val loss = 2.24446964263916
training loss = 1.9252840280532837 22200
val loss = 2.253356456756592
training loss = 1.9477100372314453 22300
val loss = 2.507620334625244
training loss = 1.9246745109558105 22400
val loss = 2.247213363647461
training loss = 1.924364447593689 22500
val loss = 2.2400946617126465
training loss = 1.9240047931671143 22600
val loss = 2.23091197013855
training loss = 1.923587679862976 22700
val loss = 2.235898971557617
training loss = 1.9231305122375488 22800
val loss = 2.2250301837921143
training loss = 1.9226237535476685 22900
val loss = 2.229402542114258
training loss = 1.9220601320266724 23000
val loss = 2.2219014167785645
training loss = 1.9224849939346313 23100
val loss = 2.1662299633026123
training loss = 1.9206162691116333 23200
val loss = 2.211127519607544
training loss = 1.919882893562317 23300
val loss = 2.182743549346924
training loss = 1.9186651706695557 23400
val loss = 2.1871347427368164
training loss = 1.9174292087554932 23500
val loss = 2.1849212646484375
training loss = 1.9199811220169067 23600
val loss = 2.079777240753174
training loss = 1.9144684076309204 23700
val loss = 2.1604690551757812
training loss = 1.9186421632766724 23800
val loss = 2.0334737300872803
training loss = 1.9110093116760254 23900
val loss = 2.133708953857422
training loss = 1.9092352390289307 24000
val loss = 2.1180787086486816
training loss = 1.9161330461502075 24100
val loss = 2.2549448013305664
training loss = 1.9056390523910522 24200
val loss = 2.090662717819214
training loss = 1.9054720401763916 24300
val loss = 2.142171621322632
training loss = 1.902058720588684 24400
val loss = 2.0611424446105957
training loss = 1.9002132415771484 24500
val loss = 2.0586187839508057
training loss = 1.9174827337265015 24600
val loss = 1.8576533794403076
training loss = 1.8964886665344238 24700
val loss = 2.041572093963623
training loss = 1.8945528268814087 24800
val loss = 2.030768394470215
training loss = 1.8965177536010742 24900
val loss = 2.119654655456543
training loss = 1.8906038999557495 25000
val loss = 2.011751174926758
training loss = 1.8885172605514526 25100
val loss = 2.000443935394287
training loss = 1.8864785432815552 25200
val loss = 1.9782179594039917
training loss = 1.8843506574630737 25300
val loss = 1.9789495468139648
training loss = 1.8821542263031006 25400
val loss = 1.965933084487915
training loss = 1.883423924446106 25500
val loss = 1.8709957599639893
training loss = 1.8777332305908203 25600
val loss = 1.9415867328643799
training loss = 1.8754361867904663 25700
val loss = 1.9261183738708496
training loss = 1.875354290008545 25800
val loss = 1.8473745584487915
training loss = 1.870863676071167 25900
val loss = 1.8969717025756836
training loss = 1.8711644411087036 26000
val loss = 1.809992790222168
training loss = 1.866320013999939 26100
val loss = 1.876230239868164
training loss = 1.8640406131744385 26200
val loss = 1.8513262271881104
training loss = 1.893785834312439 26300
val loss = 2.111114501953125
training loss = 1.85956609249115 26400
val loss = 1.8136787414550781
training loss = 1.8573331832885742 26500
val loss = 1.8024487495422363
training loss = 1.8675787448883057 26600
val loss = 1.9510703086853027
training loss = 1.8530656099319458 26700
val loss = 1.7711585760116577
training loss = 1.8509377241134644 26800
val loss = 1.7529845237731934
training loss = 1.8498444557189941 26900
val loss = 1.6971313953399658
training loss = 1.8469057083129883 27000
val loss = 1.719778060913086
training loss = 1.8642932176589966 27100
val loss = 1.5344064235687256
training loss = 1.843140721321106 27200
val loss = 1.6888744831085205
training loss = 1.8412708044052124 27300
val loss = 1.6710870265960693
training loss = 1.8396626710891724 27400
val loss = 1.6429762840270996
training loss = 1.8379384279251099 27500
val loss = 1.641075849533081
training loss = 1.8362791538238525 27600
val loss = 1.6256232261657715
training loss = 1.8748944997787476 27700
val loss = 1.90425705909729
training loss = 1.8331413269042969 27800
val loss = 1.594679355621338
training loss = 1.8315824270248413 27900
val loss = 1.5815340280532837
training loss = 1.8302441835403442 28000
val loss = 1.5703167915344238
training loss = 1.8288838863372803 28100
val loss = 1.5537118911743164
training loss = 1.8274894952774048 28200
val loss = 1.5419347286224365
training loss = 1.826322317123413 28300
val loss = 1.5355701446533203
training loss = 1.8250489234924316 28400
val loss = 1.5137288570404053
training loss = 2.075066328048706 28500
val loss = 1.0838547945022583
training loss = 1.822730302810669 28600
val loss = 1.4927188158035278
training loss = 1.8215768337249756 28700
val loss = 1.4759858846664429
training loss = 1.820563554763794 28800
val loss = 1.4694910049438477
training loss = 1.819533109664917 28900
val loss = 1.453840732574463
training loss = 1.8185763359069824 29000
val loss = 1.4300179481506348
training loss = 1.8176199197769165 29100
val loss = 1.4310648441314697
training loss = 1.8166829347610474 29200
val loss = 1.4215359687805176
training loss = 1.8785429000854492 29300
val loss = 1.7640419006347656
training loss = 1.8149813413619995 29400
val loss = 1.4077529907226562
training loss = 1.8141158819198608 29500
val loss = 1.392170786857605
training loss = 1.8886369466781616 29600
val loss = 1.7713115215301514
training loss = 1.8125427961349487 29700
val loss = 1.3760602474212646
training loss = 1.8117570877075195 29800
val loss = 1.3700671195983887
training loss = 1.8110872507095337 29900
val loss = 1.364506483078003
training loss = 1.8103092908859253 30000
val loss = 1.348356008529663
reduced chi^2 level 2 = 1.8103020191192627
Constrained alpha: 1.8903381824493408
Constrained beta: 2.408405065536499
Constrained gamma: 11.655241966247559
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 836.1929,  906.5276,  937.5508,  964.9799, 1042.9337, 1077.2057,
        1100.1038, 1085.4285, 1188.5840, 1144.0906, 1129.8419, 1177.8293,
        1228.1632, 1331.2632, 1338.9061, 1429.3763, 1377.8840, 1557.5917,
        1566.7494, 1559.1707, 1627.2035, 1550.8007, 1624.2024, 1644.5117,
        1638.3914, 1688.1527, 1613.5033, 1726.0652, 1711.7876, 1744.2250,
        1721.9651, 1771.5460, 1741.8374, 1790.7146, 1658.6718, 1752.9572,
        1703.9008, 1651.1537, 1615.5568, 1610.0111, 1627.7487, 1543.9523,
        1513.4144, 1517.5574, 1404.6659, 1378.9711, 1268.9030, 1260.1891,
        1192.4640, 1229.6975, 1088.6306, 1041.8888,  915.1315,  929.6432,
         879.7665,  875.7813,  816.9560,  703.2137,  593.0866,  542.0438,
         507.9185,  482.6696,  440.1399,  402.8631,  356.1312,  381.2320,
         271.0947,  252.0131,  245.7181,  153.9495,  153.7496,  145.0865,
         141.6346,  110.9204,   93.3140,   39.2727,   48.6493,   42.6987,
          40.4946,   41.6239,   23.2150,   40.7164,   32.6107])]
2376.162330701419
0.6815371797731307 12.588269414962932 97.04828976743082
val isze = 8
idinces = [76 16  5 26 21  3 20 46  0 50 25 29 40 27 62 17 31 12 49 32  9 37 18 10
 80  8 81  1  6 65 71 28 13 78 75 35 24 15 73 64 38 23 47 57 67 70 63 58
 44 11 60 72 68  7  2 79  4 22 56 14 66 52 69 55 39 51 45 34 48 82 54 36
 61 53 74 43 30 19 77 41 42 33 59]
we are doing training validation split
training loss = 40.859981536865234 100
val loss = 32.06905746459961
training loss = 7.190627574920654 200
val loss = 4.264919757843018
training loss = 7.087414741516113 300
val loss = 4.177777290344238
training loss = 6.979625225067139 400
val loss = 4.095356464385986
training loss = 6.872838497161865 500
val loss = 4.012722969055176
training loss = 6.772571086883545 600
val loss = 3.9341320991516113
training loss = 6.682504177093506 700
val loss = 3.862575054168701
training loss = 6.604565620422363 800
val loss = 3.7997968196868896
training loss = 6.53916597366333 900
val loss = 3.7464253902435303
training loss = 6.4854912757873535 1000
val loss = 3.702099561691284
training loss = 6.441919326782227 1100
val loss = 3.6658809185028076
training loss = 6.40644645690918 1200
val loss = 3.636345624923706
training loss = 6.377033233642578 1300
val loss = 3.612025737762451
training loss = 6.351866245269775 1400
val loss = 3.591505289077759
training loss = 6.329489707946777 1500
val loss = 3.5736842155456543
training loss = 6.308806419372559 1600
val loss = 3.5575947761535645
training loss = 6.289060115814209 1700
val loss = 3.542665481567383
training loss = 6.269719123840332 1800
val loss = 3.528353214263916
training loss = 6.250396251678467 1900
val loss = 3.514375686645508
training loss = 6.2307586669921875 2000
val loss = 3.500378370285034
training loss = 6.21045446395874 2100
val loss = 3.486161708831787
training loss = 6.189052581787109 2200
val loss = 3.471327304840088
training loss = 6.165950775146484 2300
val loss = 3.455453395843506
training loss = 6.140278339385986 2400
val loss = 3.4379498958587646
training loss = 6.110733985900879 2500
val loss = 3.417973041534424
training loss = 6.0753092765808105 2600
val loss = 3.3941826820373535
training loss = 6.030789375305176 2700
val loss = 3.3644280433654785
training loss = 5.971850872039795 2800
val loss = 3.3253040313720703
training loss = 5.88922643661499 2900
val loss = 3.270887851715088
training loss = 5.765876770019531 3000
val loss = 3.1905393600463867
training loss = 5.5703325271606445 3100
val loss = 3.0652949810028076
training loss = 5.253951072692871 3200
val loss = 2.8688864707946777
training loss = 4.760359287261963 3300
val loss = 2.5819733142852783
training loss = 4.019711494445801 3400
val loss = 2.206251859664917
training loss = 3.039252519607544 3500
val loss = 1.8468036651611328
training loss = 2.2209103107452393 3600
val loss = 1.7950735092163086
training loss = 1.9535342454910278 3700
val loss = 1.9942374229431152
training loss = 1.921970009803772 3800
val loss = 2.098137378692627
training loss = 1.9170563220977783 3900
val loss = 2.1183063983917236
training loss = 1.913931131362915 4000
val loss = 2.11794376373291
training loss = 1.9109948873519897 4100
val loss = 2.1141796112060547
training loss = 1.9080705642700195 4200
val loss = 2.1099581718444824
training loss = 1.9051027297973633 4300
val loss = 2.105597496032715
training loss = 1.9020459651947021 4400
val loss = 2.1011252403259277
training loss = 1.8988627195358276 4500
val loss = 2.096475601196289
training loss = 1.895514726638794 4600
val loss = 2.0916154384613037
training loss = 1.8919697999954224 4700
val loss = 2.08636474609375
training loss = 1.8887256383895874 4800
val loss = 2.0552728176116943
training loss = 1.8844834566116333 4900
val loss = 2.075521469116211
training loss = 1.8896737098693848 5000
val loss = 1.9716190099716187
training loss = 1.8766145706176758 5100
val loss = 2.062835454940796
training loss = 1.8895933628082275 5200
val loss = 2.2286415100097656
training loss = 1.86836838722229 5300
val loss = 2.053016424179077
training loss = 1.9082690477371216 5400
val loss = 2.341646671295166
training loss = 1.8597497940063477 5500
val loss = 2.040700912475586
training loss = 1.8552449941635132 5600
val loss = 2.0346055030822754
training loss = 1.8509938716888428 5700
val loss = 2.008599281311035
training loss = 1.8460644483566284 5800
val loss = 2.0208702087402344
training loss = 1.8423177003860474 5900
val loss = 2.050339460372925
training loss = 1.8367211818695068 6000
val loss = 2.006700038909912
training loss = 1.9172109365463257 6100
val loss = 2.438425302505493
training loss = 1.8275214433670044 6200
val loss = 1.9973797798156738
training loss = 1.8230135440826416 6300
val loss = 1.9866399765014648
training loss = 1.8189046382904053 6400
val loss = 1.9979455471038818
training loss = 1.8144887685775757 6500
val loss = 1.9739500284194946
training loss = 1.8289239406585693 6600
val loss = 1.838073968887329
training loss = 1.8066517114639282 6700
val loss = 1.9618240594863892
training loss = 1.802918791770935 6800
val loss = 1.9553136825561523
training loss = 1.803391933441162 6900
val loss = 2.0262818336486816
training loss = 1.7960233688354492 7000
val loss = 1.9428377151489258
training loss = 1.7927486896514893 7100
val loss = 1.942859172821045
training loss = 1.7898621559143066 7200
val loss = 1.9435549974441528
training loss = 1.7868741750717163 7300
val loss = 1.9262527227401733
training loss = 1.8285233974456787 7400
val loss = 2.2121100425720215
training loss = 1.7816239595413208 7500
val loss = 1.9159955978393555
training loss = 1.7791825532913208 7600
val loss = 1.909895896911621
training loss = 1.7775919437408447 7700
val loss = 1.8781136274337769
training loss = 1.774848222732544 7800
val loss = 1.899851679801941
training loss = 1.7771358489990234 7900
val loss = 1.9741612672805786
training loss = 1.7710803747177124 8000
val loss = 1.8884294033050537
training loss = 1.7693475484848022 8100
val loss = 1.885714054107666
training loss = 1.7678825855255127 8200
val loss = 1.8844842910766602
training loss = 1.7664746046066284 8300
val loss = 1.8772579431533813
training loss = 1.765137791633606 8400
val loss = 1.8727961778640747
training loss = 1.763978123664856 8500
val loss = 1.870398998260498
training loss = 1.7628906965255737 8600
val loss = 1.8652212619781494
training loss = 1.9975980520248413 8700
val loss = 2.687681198120117
training loss = 1.7610453367233276 8800
val loss = 1.8563140630722046
training loss = 1.7602344751358032 8900
val loss = 1.854182243347168
training loss = 1.762895107269287 9000
val loss = 1.7923552989959717
training loss = 1.7589139938354492 9100
val loss = 1.8481359481811523
training loss = 2.354063034057617 9200
val loss = 1.7421143054962158
training loss = 1.7579246759414673 9300
val loss = 1.8387291431427002
training loss = 1.7574899196624756 9400
val loss = 1.8390564918518066
training loss = 1.7595666646957397 9500
val loss = 1.7854899168014526
training loss = 1.7568453550338745 9600
val loss = 1.8337724208831787
training loss = 1.7565871477127075 9700
val loss = 1.8309859037399292
training loss = 1.7569726705551147 9800
val loss = 1.8561931848526
training loss = 1.7562388181686401 9900
val loss = 1.8268442153930664
training loss = 1.849145531654358 10000
val loss = 2.271662712097168
training loss = 1.7560465335845947 10100
val loss = 1.822656273841858
training loss = 1.75599205493927 10200
val loss = 1.8204153776168823
training loss = 1.7560205459594727 10300
val loss = 1.8258610963821411
training loss = 1.755981683731079 10400
val loss = 1.817121982574463
training loss = 1.8825539350509644 10500
val loss = 1.5857913494110107
training loss = 1.7560703754425049 10600
val loss = 1.8121857643127441
training loss = 1.7561345100402832 10700
val loss = 1.8122782707214355
training loss = 1.7562443017959595 10800
val loss = 1.8154172897338867
training loss = 1.7563233375549316 10900
val loss = 1.809631109237671
training loss = 1.7565723657608032 11000
val loss = 1.796160101890564
training loss = 1.7566331624984741 11100
val loss = 1.8167831897735596
training loss = 1.7566829919815063 11200
val loss = 1.805922508239746
training loss = 1.7568249702453613 11300
val loss = 1.8048722743988037
training loss = 1.7575501203536987 11400
val loss = 1.8308082818984985
training loss = 1.7571232318878174 11500
val loss = 1.8027781248092651
training loss = 2.062077045440674 11600
val loss = 2.771076202392578
training loss = 1.7574414014816284 11700
val loss = 1.800316572189331
training loss = 1.7576007843017578 11800
val loss = 1.7999238967895508
training loss = 1.7582497596740723 11900
val loss = 1.824721097946167
training loss = 1.7579221725463867 12000
val loss = 1.79947829246521
training loss = 1.7580891847610474 12100
val loss = 1.7983245849609375
training loss = 1.7586565017700195 12200
val loss = 1.819857120513916
training loss = 1.7584115266799927 12300
val loss = 1.7965654134750366
training loss = 1.7796952724456787 12400
val loss = 1.6675833463668823
training loss = 1.7587217092514038 12500
val loss = 1.7969446182250977
training loss = 1.7588750123977661 12600
val loss = 1.7948150634765625
training loss = 1.7669579982757568 12700
val loss = 1.9007967710494995
training loss = 1.7591642141342163 12800
val loss = 1.7934370040893555
training loss = 1.7593071460723877 12900
val loss = 1.7933740615844727
training loss = 1.7639679908752441 13000
val loss = 1.871556282043457
training loss = 1.759572982788086 13100
val loss = 1.7929716110229492
training loss = 1.7597064971923828 13200
val loss = 1.792555809020996
training loss = 1.7600195407867432 13300
val loss = 1.8072948455810547
training loss = 1.7599515914916992 13400
val loss = 1.7912583351135254
training loss = 1.7780200242996216 13500
val loss = 1.6707093715667725
training loss = 1.760179042816162 13600
val loss = 1.7904239892959595
training loss = 1.8505936861038208 13700
val loss = 1.5782908201217651
training loss = 1.760393738746643 13800
val loss = 1.7928416728973389
training loss = 1.7604897022247314 13900
val loss = 1.789785385131836
training loss = 1.7606419324874878 14000
val loss = 1.7986739873886108
training loss = 1.7606626749038696 14100
val loss = 1.789414644241333
training loss = 1.760874629020691 14200
val loss = 1.777248740196228
training loss = 1.7608999013900757 14300
val loss = 1.798642635345459
training loss = 1.7608988285064697 14400
val loss = 1.7884289026260376
training loss = 1.7654386758804321 14500
val loss = 1.722501277923584
training loss = 1.7610317468643188 14600
val loss = 1.7877910137176514
training loss = 1.7612954378128052 14700
val loss = 1.7729699611663818
training loss = 1.7612067461013794 14800
val loss = 1.7800066471099854
training loss = 1.7611944675445557 14900
val loss = 1.7873482704162598
training loss = 1.8084198236465454 15000
val loss = 2.0782673358917236
training loss = 1.7612873315811157 15100
val loss = 1.786428689956665
training loss = 1.7613260746002197 15200
val loss = 1.7873328924179077
training loss = 1.7617195844650269 15300
val loss = 1.8077104091644287
training loss = 1.7613948583602905 15400
val loss = 1.7864181995391846
training loss = 1.7791200876235962 15500
val loss = 1.6655967235565186
training loss = 1.7614498138427734 15600
val loss = 1.7849750518798828
training loss = 1.761467695236206 15700
val loss = 1.785548448562622
training loss = 1.7620011568069458 15800
val loss = 1.8111152648925781
training loss = 1.7614995241165161 15900
val loss = 1.7857592105865479
training loss = 1.7615079879760742 16000
val loss = 1.7862052917480469
training loss = 1.761566162109375 16100
val loss = 1.793053150177002
training loss = 1.7615212202072144 16200
val loss = 1.7849373817443848
training loss = 1.7622461318969727 16300
val loss = 1.7583694458007812
training loss = 1.7615251541137695 16400
val loss = 1.784805417060852
training loss = 1.7615180015563965 16500
val loss = 1.7842254638671875
training loss = 1.7650110721588135 16600
val loss = 1.8524528741836548
training loss = 1.7615057229995728 16700
val loss = 1.7836143970489502
training loss = 1.761501669883728 16800
val loss = 1.7872494459152222
training loss = 1.7614953517913818 16900
val loss = 1.7803082466125488
training loss = 1.7614628076553345 17000
val loss = 1.7834991216659546
training loss = 1.7684448957443237 17100
val loss = 1.7051873207092285
training loss = 1.761427879333496 17200
val loss = 1.7823686599731445
training loss = 1.7613993883132935 17300
val loss = 1.7829091548919678
training loss = 1.7617580890655518 17400
val loss = 1.7628439664840698
training loss = 1.7613505125045776 17500
val loss = 1.7823338508605957
training loss = 1.7818249464035034 17600
val loss = 1.9619380235671997
training loss = 1.7612980604171753 17700
val loss = 1.7850234508514404
training loss = 1.7612563371658325 17800
val loss = 1.781501054763794
training loss = 1.761855125427246 17900
val loss = 1.809978723526001
training loss = 1.761186957359314 18000
val loss = 1.781506061553955
training loss = 1.7613855600357056 18100
val loss = 1.7997357845306396
training loss = 1.7611138820648193 18200
val loss = 1.781267762184143
training loss = 1.7610677480697632 18300
val loss = 1.7811344861984253
training loss = 1.7628270387649536 18400
val loss = 1.8291752338409424
training loss = 1.7609856128692627 18500
val loss = 1.780341386795044
training loss = 1.7627114057540894 18600
val loss = 1.8280918598175049
training loss = 1.7609281539916992 18700
val loss = 1.7869900465011597
training loss = 1.7608411312103271 18800
val loss = 1.7801450490951538
training loss = 1.7645269632339478 18900
val loss = 1.8506572246551514
training loss = 1.7607481479644775 19000
val loss = 1.7800785303115845
training loss = 1.7606960535049438 19100
val loss = 1.777308464050293
training loss = 1.760793924331665 19200
val loss = 1.767042875289917
training loss = 1.7605880498886108 19300
val loss = 1.7790309190750122
training loss = 1.7690964937210083 19400
val loss = 1.6912322044372559
training loss = 1.7604798078536987 19500
val loss = 1.7792279720306396
training loss = 1.8504194021224976 19600
val loss = 1.5658855438232422
training loss = 1.7603813409805298 19700
val loss = 1.7748432159423828
training loss = 1.7603036165237427 19800
val loss = 1.7795573472976685
training loss = 1.7604714632034302 19900
val loss = 1.7947213649749756
training loss = 1.760187029838562 20000
val loss = 1.7778754234313965
training loss = 1.848877191543579 20100
val loss = 2.205927848815918
training loss = 1.7600802183151245 20200
val loss = 1.775404691696167
training loss = 1.7600048780441284 20300
val loss = 1.7773349285125732
training loss = 1.8428280353546143 20400
val loss = 1.569854974746704
training loss = 1.7598905563354492 20500
val loss = 1.7801052331924438
training loss = 1.7598130702972412 20600
val loss = 1.7765945196151733
training loss = 1.760048508644104 20700
val loss = 1.7590885162353516
training loss = 1.7596887350082397 20800
val loss = 1.7762212753295898
training loss = 1.7596596479415894 20900
val loss = 1.7834972143173218
training loss = 1.7596193552017212 21000
val loss = 1.7846581935882568
training loss = 1.7594889402389526 21100
val loss = 1.7755866050720215
training loss = 1.7603410482406616 21200
val loss = 1.8106273412704468
training loss = 1.7593579292297363 21300
val loss = 1.7743868827819824
training loss = 1.7742960453033447 21400
val loss = 1.661747694015503
training loss = 1.7592496871948242 21500
val loss = 1.7807780504226685
training loss = 1.7591469287872314 21600
val loss = 1.7744311094284058
training loss = 1.7612762451171875 21700
val loss = 1.8278768062591553
training loss = 1.7590179443359375 21800
val loss = 1.7748064994812012
training loss = 1.758945345878601 21900
val loss = 1.7770227193832397
training loss = 1.7589318752288818 22000
val loss = 1.7819843292236328
training loss = 1.7588045597076416 22100
val loss = 1.773484706878662
training loss = 1.773825764656067 22200
val loss = 1.6608095169067383
training loss = 1.7586723566055298 22300
val loss = 1.7724847793579102
training loss = 1.758592963218689 22400
val loss = 1.7716877460479736
training loss = 1.7587711811065674 22500
val loss = 1.7901480197906494
training loss = 1.758453369140625 22600
val loss = 1.7726027965545654
training loss = 1.7681009769439697 22700
val loss = 1.8905668258666992
training loss = 1.7583166360855103 22800
val loss = 1.7728058099746704
training loss = 1.9810715913772583 22900
val loss = 1.5343613624572754
training loss = 1.7581787109375 23000
val loss = 1.772082805633545
training loss = 1.7580974102020264 23100
val loss = 1.7708827257156372
training loss = 1.758036732673645 23200
val loss = 1.771126389503479
training loss = 1.7579580545425415 23300
val loss = 1.7713329792022705
training loss = 1.767120599746704 23400
val loss = 1.886387825012207
training loss = 1.757824182510376 23500
val loss = 1.771531581878662
training loss = 1.7577401399612427 23600
val loss = 1.7711189985275269
training loss = 1.757816195487976 23700
val loss = 1.7586724758148193
training loss = 1.757605791091919 23800
val loss = 1.7699668407440186
training loss = 1.7617472410202026 23900
val loss = 1.8455636501312256
training loss = 1.7574703693389893 24000
val loss = 1.7684299945831299
training loss = 1.7574810981750488 24100
val loss = 1.7800884246826172
training loss = 1.7573364973068237 24200
val loss = 1.7688119411468506
training loss = 1.7572553157806396 24300
val loss = 1.7688459157943726
training loss = 1.8056832551956177 24400
val loss = 2.06471586227417
training loss = 1.7571191787719727 24500
val loss = 1.7687944173812866
training loss = 1.7570650577545166 24600
val loss = 1.7628494501113892
training loss = 1.7569843530654907 24700
val loss = 1.7689106464385986
training loss = 1.756903886795044 24800
val loss = 1.7679483890533447
training loss = 1.7581276893615723 24900
val loss = 1.8082208633422852
training loss = 1.756768822669983 25000
val loss = 1.767991542816162
training loss = 2.0588436126708984 25100
val loss = 2.719031810760498
training loss = 1.756646990776062 25200
val loss = 1.7709808349609375
training loss = 1.7565548419952393 25300
val loss = 1.7667522430419922
training loss = 1.7580090761184692 25400
val loss = 1.7273871898651123
training loss = 1.7564301490783691 25500
val loss = 1.766566276550293
training loss = 1.756347894668579 25600
val loss = 1.7666773796081543
training loss = 1.7566542625427246 25700
val loss = 1.7872140407562256
training loss = 1.7562235593795776 25800
val loss = 1.7660595178604126
training loss = 2.0858302116394043 25900
val loss = 2.7759041786193848
training loss = 1.7561005353927612 26000
val loss = 1.7684760093688965
training loss = 1.7560125589370728 26100
val loss = 1.7653920650482178
training loss = 1.7559770345687866 26200
val loss = 1.7696577310562134
training loss = 1.7558858394622803 26300
val loss = 1.7647475004196167
training loss = 1.7619096040725708 26400
val loss = 1.6894444227218628
training loss = 1.7557576894760132 26500
val loss = 1.7640568017959595
training loss = 1.7559330463409424 26600
val loss = 1.7817575931549072
training loss = 1.7556720972061157 26700
val loss = 1.7714707851409912
training loss = 1.7555527687072754 26800
val loss = 1.7637226581573486
training loss = 1.7571675777435303 26900
val loss = 1.8105241060256958
training loss = 1.7554314136505127 27000
val loss = 1.7644412517547607
training loss = 1.7553503513336182 27100
val loss = 1.7628793716430664
training loss = 1.7559682130813599 27200
val loss = 1.7369260787963867
training loss = 1.7552356719970703 27300
val loss = 1.7627836465835571
training loss = 1.7551571130752563 27400
val loss = 1.763698697090149
training loss = 1.755117654800415 27500
val loss = 1.7615809440612793
training loss = 1.7550374269485474 27600
val loss = 1.7621086835861206
training loss = 1.9507451057434082 27700
val loss = 1.5249431133270264
training loss = 1.7549235820770264 27800
val loss = 1.7645217180252075
training loss = 1.7548421621322632 27900
val loss = 1.7610740661621094
training loss = 1.7561416625976562 28000
val loss = 1.7239534854888916
training loss = 1.7547192573547363 28100
val loss = 1.7615221738815308
training loss = 1.804071068763733 28200
val loss = 2.0598347187042236
training loss = 1.7546032667160034 28300
val loss = 1.7618310451507568
training loss = 1.754523515701294 28400
val loss = 1.760837435722351
training loss = 1.7552103996276855 28500
val loss = 1.7327368259429932
training loss = 1.7544031143188477 28600
val loss = 1.7599594593048096
training loss = 1.785590410232544 28700
val loss = 1.9879326820373535
training loss = 1.7543132305145264 28800
val loss = 1.7572498321533203
training loss = 1.7542246580123901 28900
val loss = 1.7597558498382568
training loss = 1.7541486024856567 29000
val loss = 1.7588319778442383
training loss = 1.7541773319244385 29100
val loss = 1.7512778043746948
training loss = 1.7540360689163208 29200
val loss = 1.759397268295288
training loss = 1.8967489004135132 29300
val loss = 2.3341903686523438
training loss = 1.7539323568344116 29400
val loss = 1.757023811340332
training loss = 1.753848910331726 29500
val loss = 1.7586199045181274
training loss = 1.7970123291015625 29600
val loss = 1.588318109512329
training loss = 1.7537339925765991 29700
val loss = 1.7598798274993896
training loss = 1.7536842823028564 29800
val loss = 1.7524223327636719
training loss = 1.7536511421203613 29900
val loss = 1.7526222467422485
training loss = 1.7535427808761597 30000
val loss = 1.7575781345367432
reduced chi^2 level 2 = 1.7535414695739746
Constrained alpha: 1.8861325979232788
Constrained beta: 1.9121359586715698
Constrained gamma: 13.163717269897461
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 864.3588,  932.3702,  916.0961,  984.8082, 1015.8145, 1067.6060,
        1106.3413, 1145.4006, 1186.0793, 1179.2424, 1152.6880, 1199.4197,
        1264.0601, 1309.2341, 1374.7201, 1411.0206, 1345.2275, 1448.0043,
        1501.4242, 1452.0320, 1555.4779, 1539.4916, 1587.9668, 1683.7744,
        1676.3481, 1722.9478, 1666.7419, 1605.9176, 1691.7336, 1580.3397,
        1638.0638, 1702.0648, 1658.7805, 1808.9658, 1694.6376, 1725.9905,
        1665.8123, 1695.2523, 1652.0486, 1600.8981, 1703.0460, 1518.5985,
        1528.4425, 1486.5964, 1404.8893, 1298.9211, 1286.0640, 1272.6692,
        1126.0314, 1196.8212, 1104.8501,  984.0532, 1019.9376,  927.6666,
         876.1125,  867.2675,  798.6327,  703.4031,  574.1194,  482.7014,
         594.3467,  458.4152,  441.0641,  427.7052,  362.3443,  323.3478,
         284.2221,  249.0489,  209.6045,  177.0366,  157.7120,  159.3418,
         143.4785,  109.8839,  103.7755,   74.3363,   44.0851,   32.7435,
          38.3860,   38.8343,   23.5901,   24.5986,   36.4285])]
2535.4579013968514
4.854920809822482 19.335204644264017 52.28962985493057
val isze = 8
idinces = [28 46 78 77  6 54 56 49 61 20 72 40 73 31 17 53 63 57 66 47 11 19 25 10
 30 24 12 13 75  7 33 55 37 43  9 18  5 38 26  0  3 39 67 74 79 80 45 69
  2 58 29 35 36 71 23 14 15 82 41 59 52  1 60 27 21 68 81 76 42  8 32  4
 34 16 65 44 62 64 70 22 51 50 48]
we are doing training validation split
training loss = 680.079833984375 100
val loss = 693.891357421875
training loss = 628.2598266601562 200
val loss = 669.5142822265625
training loss = 254.23675537109375 300
val loss = 336.0914306640625
training loss = 15.698932647705078 400
val loss = 9.930444717407227
training loss = 11.35647201538086 500
val loss = 7.756345748901367
training loss = 8.203124046325684 600
val loss = 5.99315071105957
training loss = 6.915274620056152 700
val loss = 5.124507427215576
training loss = 6.655901908874512 800
val loss = 4.803197860717773
training loss = 6.573668956756592 900
val loss = 4.659367561340332
training loss = 6.510432720184326 1000
val loss = 4.575838088989258
training loss = 6.444492340087891 1100
val loss = 4.509827613830566
training loss = 6.371115684509277 1200
val loss = 4.443401336669922
training loss = 6.287566184997559 1300
val loss = 4.3685302734375
training loss = 6.19086217880249 1400
val loss = 4.2802629470825195
training loss = 6.077432155609131 1500
val loss = 4.174355506896973
training loss = 5.942956924438477 1600
val loss = 4.0460920333862305
training loss = 5.782299995422363 1700
val loss = 3.889718532562256
training loss = 5.589763164520264 1800
val loss = 3.6987850666046143
training loss = 5.372776985168457 1900
val loss = 3.4648027420043945
training loss = 5.118672847747803 2000
val loss = 3.2264037132263184
training loss = 4.856544017791748 2100
val loss = 2.9413137435913086
training loss = 4.61628532409668 2200
val loss = 2.672280788421631
training loss = 4.974125385284424 2300
val loss = 2.7636799812316895
training loss = 3.8105900287628174 2400
val loss = 1.8482491970062256
training loss = 3.471388816833496 2500
val loss = 1.4593459367752075
training loss = 3.028099536895752 2600
val loss = 1.1775920391082764
training loss = 2.632432222366333 2700
val loss = 1.0302602052688599
training loss = 2.2716150283813477 2800
val loss = 1.0422108173370361
training loss = 2.044538974761963 2900
val loss = 1.059389591217041
training loss = 1.8729209899902344 3000
val loss = 1.3968396186828613
training loss = 1.7950859069824219 3100
val loss = 1.5609638690948486
training loss = 1.7580678462982178 3200
val loss = 1.637952446937561
training loss = 1.7389111518859863 3300
val loss = 1.7571921348571777
training loss = 1.7655408382415771 3400
val loss = 1.6281275749206543
training loss = 1.724271535873413 3500
val loss = 1.840348243713379
training loss = 2.215831756591797 3600
val loss = 1.6292121410369873
training loss = 1.7191898822784424 3700
val loss = 1.8729681968688965
training loss = 1.7178205251693726 3800
val loss = 1.882657766342163
training loss = 1.718461513519287 3900
val loss = 1.93463933467865
training loss = 1.7160085439682007 4000
val loss = 1.8937923908233643
training loss = 1.865527629852295 4100
val loss = 1.7069756984710693
training loss = 1.714865803718567 4200
val loss = 1.899707317352295
training loss = 1.714475393295288 4300
val loss = 1.8996591567993164
training loss = 1.714085578918457 4400
val loss = 1.9165160655975342
training loss = 1.7137614488601685 4500
val loss = 1.9018452167510986
training loss = 1.8894624710083008 4600
val loss = 2.2019197940826416
training loss = 1.71320378780365 4700
val loss = 1.9061623811721802
training loss = 1.7130247354507446 4800
val loss = 1.9037284851074219
training loss = 1.7145931720733643 4900
val loss = 1.954704999923706
training loss = 1.712572693824768 5000
val loss = 1.9078004360198975
training loss = 2.098428249359131 5100
val loss = 2.6360275745391846
training loss = 1.7122018337249756 5200
val loss = 1.90785551071167
training loss = 1.7120829820632935 5300
val loss = 1.9089940786361694
training loss = 1.7127419710159302 5400
val loss = 1.8906340599060059
training loss = 1.7117668390274048 5500
val loss = 1.9120515584945679
training loss = 1.7774484157562256 5600
val loss = 1.6982483863830566
training loss = 1.7114681005477905 5700
val loss = 1.917423963546753
training loss = 1.7115637063980103 5800
val loss = 1.9040260314941406
training loss = 1.711477279663086 5900
val loss = 1.911986231803894
training loss = 1.7110726833343506 6000
val loss = 1.9189889430999756
training loss = 1.7629632949829102 6100
val loss = 1.7848879098892212
training loss = 1.7108404636383057 6200
val loss = 1.9186021089553833
training loss = 1.7107696533203125 6300
val loss = 1.919769287109375
training loss = 1.7122843265533447 6400
val loss = 1.9009217023849487
training loss = 1.7105249166488647 6500
val loss = 1.9238203763961792
training loss = 1.7674809694290161 6600
val loss = 1.7212779521942139
training loss = 1.7103028297424316 6700
val loss = 1.927245020866394
training loss = 1.7104344367980957 6800
val loss = 1.9332317113876343
training loss = 1.710433840751648 6900
val loss = 1.9204740524291992
training loss = 1.710080862045288 7000
val loss = 1.9275540113449097
training loss = 1.7144358158111572 7100
val loss = 1.9935059547424316
training loss = 1.7098581790924072 7200
val loss = 1.9321882724761963
training loss = 1.7171846628189087 7300
val loss = 1.9961879253387451
training loss = 1.7096478939056396 7400
val loss = 1.9360649585723877
training loss = 1.709632158279419 7500
val loss = 1.9334355592727661
training loss = 1.7446573972702026 7600
val loss = 1.9918862581253052
training loss = 1.7094593048095703 7700
val loss = 1.9395368099212646
training loss = 1.7104414701461792 7800
val loss = 1.910909652709961
reduced chi^2 level 2 = 1.7121561765670776
Constrained alpha: 3.7439827919006348
Constrained beta: 2.964465618133545
Constrained gamma: 31.70526123046875
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 921.3849,  891.7862,  962.3751,  978.0829, 1011.9189, 1056.0400,
        1114.9452, 1129.2617, 1155.9806, 1122.9756, 1225.7061, 1168.0405,
        1246.9952, 1244.0374, 1349.8762, 1379.0914, 1421.3361, 1499.9883,
        1587.4535, 1537.4467, 1590.6594, 1600.5005, 1578.9994, 1588.2831,
        1611.2242, 1696.4406, 1633.7550, 1741.6412, 1743.5629, 1699.6299,
        1632.1631, 1678.1897, 1709.9698, 1706.8811, 1728.3464, 1633.5956,
        1663.9927, 1647.7091, 1615.9977, 1669.3944, 1559.9955, 1613.4202,
        1509.2427, 1507.0894, 1290.6693, 1315.0848, 1240.4540, 1260.5221,
        1074.0616, 1169.0699, 1033.2795,  955.4501,  905.3805,  914.2023,
         892.2759,  841.2452,  824.8027,  707.5704,  580.6785,  536.6068,
         541.8247,  515.1620,  464.5522,  396.4565,  352.1107,  340.1126,
         263.0429,  232.3417,  228.6837,  179.2784,  160.8873,  155.6398,
         136.1827,  109.5331,   86.9255,   79.8673,   48.5901,   44.5146,
          27.2792,   44.5313,   17.5265,   44.3207,   38.3057])]
2687.8402275373824
1.4687419145742342 19.74037736128835 88.18340619764068
val isze = 8
idinces = [34 75 77 44 64 13 46 62  3 37 15 50 80 54 70 39 71 58  1 53 23 28 40 17
 73 72 35 49 20  7 16 14 12 57 51 55 67 59 82 11 56 63 42 19 43 61 29 76
 32 18 21 68 22 52  9 48 33 31 10 25  5  6 26 47 38 65  8 74 60  2 27 41
 30 79  4 81 24 45 69 66 78 36  0]
we are doing training validation split
training loss = 273.97406005859375 100
val loss = 299.10699462890625
training loss = 45.7208137512207 200
val loss = 60.097190856933594
training loss = 13.472579002380371 300
val loss = 24.334880828857422
training loss = 12.874776840209961 400
val loss = 23.123607635498047
training loss = 12.216263771057129 500
val loss = 21.917123794555664
training loss = 11.50587272644043 600
val loss = 20.59143829345703
training loss = 10.765933990478516 700
val loss = 19.178081512451172
training loss = 10.022233009338379 800
val loss = 17.713911056518555
training loss = 9.30351734161377 900
val loss = 16.242294311523438
training loss = 8.639321327209473 1000
val loss = 14.811732292175293
training loss = 8.055923461914062 1100
val loss = 13.470976829528809
training loss = 7.5713300704956055 1200
val loss = 12.263427734375
training loss = 7.191110134124756 1300
val loss = 11.219499588012695
training loss = 6.907403945922852 1400
val loss = 10.351547241210938
training loss = 6.702322483062744 1500
val loss = 9.653876304626465
training loss = 6.5541157722473145 1600
val loss = 9.107192993164062
training loss = 6.443027973175049 1700
val loss = 8.685171127319336
training loss = 6.354524612426758 1800
val loss = 8.361056327819824
training loss = 6.279594898223877 1900
val loss = 8.111154556274414
training loss = 6.213448524475098 2000
val loss = 7.916377067565918
training loss = 6.153838634490967 2100
val loss = 7.762096405029297
training loss = 6.099725723266602 2200
val loss = 7.637520790100098
training loss = 6.0504655838012695 2300
val loss = 7.534511089324951
training loss = 6.0053629875183105 2400
val loss = 7.446900367736816
training loss = 5.963495254516602 2500
val loss = 7.369961261749268
training loss = 5.9235920906066895 2600
val loss = 7.299505233764648
training loss = 5.8839240074157715 2700
val loss = 7.231320381164551
training loss = 5.841975212097168 2800
val loss = 7.160300254821777
training loss = 5.793758869171143 2900
val loss = 7.07866096496582
training loss = 5.732159614562988 3000
val loss = 6.972728729248047
training loss = 5.643601894378662 3100
val loss = 6.816225051879883
training loss = 5.503448486328125 3200
val loss = 6.564387798309326
training loss = 5.276739597320557 3300
val loss = 6.173336982727051
training loss = 4.920289993286133 3400
val loss = 5.631359100341797
training loss = 4.365605354309082 3500
val loss = 4.883056640625
training loss = 3.5567970275878906 3600
val loss = 3.8646669387817383
training loss = 2.638964891433716 3700
val loss = 2.905284881591797
training loss = 2.0754501819610596 3800
val loss = 2.7049708366394043
training loss = 1.9527199268341064 3900
val loss = 2.9508819580078125
training loss = 1.9413119554519653 4000
val loss = 3.0557830333709717
training loss = 1.93897545337677 4100
val loss = 3.07234525680542
training loss = 1.9373971223831177 4200
val loss = 3.07106614112854
training loss = 1.9360394477844238 4300
val loss = 3.067641019821167
training loss = 1.9348382949829102 4400
val loss = 3.0644474029541016
training loss = 1.9337689876556396 4500
val loss = 3.0617592334747314
training loss = 1.9328186511993408 4600
val loss = 3.0593316555023193
training loss = 1.9321513175964355 4700
val loss = 3.0298261642456055
training loss = 1.9307132959365845 4800
val loss = 3.0545573234558105
training loss = 1.9569473266601562 4900
val loss = 2.828064441680908
training loss = 1.9283722639083862 5000
val loss = 3.0480146408081055
training loss = 1.9299654960632324 5100
val loss = 3.1266980171203613
training loss = 1.925822138786316 5200
val loss = 3.036191463470459
training loss = 1.9245071411132812 5300
val loss = 3.040086269378662
training loss = 1.923110842704773 5400
val loss = 3.0502448081970215
training loss = 1.921526312828064 5500
val loss = 3.030228853225708
training loss = 1.9243961572647095 5600
val loss = 3.125478982925415
training loss = 1.9182308912277222 5700
val loss = 3.0213499069213867
training loss = 1.9165171384811401 5800
val loss = 3.0147180557250977
training loss = 1.9154391288757324 5900
val loss = 3.0527241230010986
training loss = 1.9124770164489746 6000
val loss = 3.002345323562622
training loss = 1.9915283918380737 6100
val loss = 2.6373116970062256
training loss = 1.9076249599456787 6200
val loss = 2.9822258949279785
training loss = 1.904850721359253 6300
val loss = 2.977975606918335
training loss = 1.9049243927001953 6400
val loss = 3.0518298149108887
training loss = 1.8984668254852295 6500
val loss = 2.9582479000091553
training loss = 1.9438285827636719 6600
val loss = 2.6610007286071777
training loss = 1.8912885189056396 6700
val loss = 2.9343881607055664
training loss = 1.88753342628479 6800
val loss = 2.9260005950927734
training loss = 1.8838615417480469 6900
val loss = 2.934504747390747
training loss = 1.8798606395721436 7000
val loss = 2.905639171600342
training loss = 1.9159822463989258 7100
val loss = 2.6323935985565186
training loss = 1.8719524145126343 7200
val loss = 2.8839364051818848
training loss = 1.8678406476974487 7300
val loss = 2.8709495067596436
training loss = 1.863681674003601 7400
val loss = 2.8718438148498535
training loss = 1.8592182397842407 7500
val loss = 2.8446381092071533
training loss = 2.180879831314087 7600
val loss = 3.9093031883239746
training loss = 1.8497618436813354 7700
val loss = 2.815769910812378
training loss = 1.8445733785629272 7800
val loss = 2.7969579696655273
training loss = 1.8397691249847412 7900
val loss = 2.818182945251465
training loss = 1.8329873085021973 8000
val loss = 2.757957935333252
training loss = 1.8570204973220825 8100
val loss = 3.0128660202026367
training loss = 1.8187954425811768 8200
val loss = 2.7012228965759277
training loss = 1.809906005859375 8300
val loss = 2.6802613735198975
training loss = 1.8006614446640015 8400
val loss = 2.64453125
training loss = 1.7893600463867188 8500
val loss = 2.6025772094726562
training loss = 1.778286337852478 8600
val loss = 2.534719467163086
training loss = 1.765138030052185 8700
val loss = 2.512427568435669
training loss = 1.782701015472412 8800
val loss = 2.74772047996521
training loss = 1.7399483919143677 8900
val loss = 2.411346197128296
training loss = 1.7274785041809082 9000
val loss = 2.3585848808288574
training loss = 1.718580961227417 9100
val loss = 2.345475912094116
training loss = 1.7095296382904053 9200
val loss = 2.276759147644043
training loss = 1.7013944387435913 9300
val loss = 2.2378547191619873
training loss = 1.715721845626831 9400
val loss = 2.431924343109131
training loss = 1.6894482374191284 9500
val loss = 2.1740994453430176
training loss = 1.6853914260864258 9600
val loss = 2.110506534576416
training loss = 1.6815671920776367 9700
val loss = 2.129812240600586
training loss = 1.678543210029602 9800
val loss = 2.1118075847625732
training loss = 1.6785334348678589 9900
val loss = 2.1630067825317383
training loss = 1.674800157546997 10000
val loss = 2.0864927768707275
training loss = 1.6743217706680298 10100
val loss = 2.0306270122528076
training loss = 1.6725871562957764 10200
val loss = 2.0698399543762207
training loss = 1.6717450618743896 10300
val loss = 2.0608763694763184
training loss = 1.6726689338684082 10400
val loss = 2.0074515342712402
training loss = 1.6709617376327515 10500
val loss = 2.051389694213867
training loss = 1.674386978149414 10600
val loss = 1.963410496711731
training loss = 1.6706993579864502 10700
val loss = 2.0439436435699463
training loss = 1.6705858707427979 10800
val loss = 2.043403148651123
training loss = 1.6709002256393433 10900
val loss = 2.0624396800994873
training loss = 1.6707314252853394 11000
val loss = 2.03975772857666
training loss = 1.7052624225616455 11100
val loss = 1.800445795059204
training loss = 1.6710526943206787 11200
val loss = 2.047106981277466
training loss = 1.6711130142211914 11300
val loss = 2.0380518436431885
training loss = 1.690521478652954 11400
val loss = 2.2559282779693604
training loss = 1.671465277671814 11500
val loss = 2.0385499000549316
training loss = 1.6716150045394897 11600
val loss = 2.0442981719970703
training loss = 1.671818733215332 11700
val loss = 2.041370391845703
training loss = 1.6719261407852173 11800
val loss = 2.0371503829956055
training loss = 1.6825863122940063 11900
val loss = 2.195892333984375
training loss = 1.6722490787506104 12000
val loss = 2.0374794006347656
training loss = 1.672346591949463 12100
val loss = 2.0366923809051514
training loss = 1.6748948097229004 12200
val loss = 2.111151933670044
training loss = 1.6726101636886597 12300
val loss = 2.0362205505371094
training loss = 1.6726871728897095 12400
val loss = 2.036344051361084
training loss = 1.6747573614120483 12500
val loss = 1.9753308296203613
training loss = 1.6728848218917847 12600
val loss = 2.0363969802856445
training loss = 1.6729384660720825 12700
val loss = 2.03867769241333
training loss = 1.6731698513031006 12800
val loss = 2.0538885593414307
training loss = 1.6730668544769287 12900
val loss = 2.03586745262146
training loss = 1.6839425563812256 13000
val loss = 1.8959720134735107
training loss = 1.673161268234253 13100
val loss = 2.0363049507141113
training loss = 1.6734626293182373 13200
val loss = 2.0610241889953613
training loss = 1.6732797622680664 13300
val loss = 2.0253021717071533
training loss = 1.6732044219970703 13400
val loss = 2.03543758392334
training loss = 1.6732628345489502 13500
val loss = 2.030435085296631
training loss = 1.6732094287872314 13600
val loss = 2.035416841506958
training loss = 2.094937324523926 13700
val loss = 3.3373982906341553
training loss = 1.6731971502304077 13800
val loss = 2.0330820083618164
training loss = 1.6731455326080322 13900
val loss = 2.0342214107513428
training loss = 1.680420160293579 14000
val loss = 1.9184226989746094
training loss = 1.6730942726135254 14100
val loss = 2.033566474914551
training loss = 1.6730291843414307 14200
val loss = 2.0330162048339844
training loss = 1.6734414100646973 14300
val loss = 2.0049076080322266
training loss = 1.6729414463043213 14400
val loss = 2.033480167388916
training loss = 1.6730585098266602 14500
val loss = 2.052478790283203
training loss = 1.6729161739349365 14600
val loss = 2.0206284523010254
training loss = 1.6727476119995117 14700
val loss = 2.03167462348938
training loss = 1.6916108131408691 14800
val loss = 1.853224277496338
training loss = 1.6726120710372925 14900
val loss = 2.033069610595703
training loss = 1.6728410720825195 15000
val loss = 2.004340648651123
training loss = 1.6725261211395264 15100
val loss = 2.0428929328918457
training loss = 1.6723461151123047 15200
val loss = 2.029050827026367
training loss = 1.6735153198242188 15300
val loss = 2.0815346240997314
training loss = 1.6721627712249756 15400
val loss = 2.0291688442230225
training loss = 1.6828221082687378 15500
val loss = 2.190110921859741
training loss = 1.671966552734375 15600
val loss = 2.026489019393921
training loss = 1.6732176542282104 15700
val loss = 1.9746644496917725
training loss = 1.6717904806137085 15800
val loss = 2.019155263900757
training loss = 1.671610951423645 15900
val loss = 2.026183605194092
training loss = 1.672532558441162 16000
val loss = 2.0738823413848877
training loss = 1.6713653802871704 16100
val loss = 2.0258915424346924
training loss = 1.833641767501831 16200
val loss = 1.583528757095337
training loss = 1.6711028814315796 16300
val loss = 2.0220580101013184
training loss = 1.670926570892334 16400
val loss = 2.0231409072875977
training loss = 1.6709696054458618 16500
val loss = 2.0066962242126465
training loss = 1.6706136465072632 16600
val loss = 2.0233185291290283
training loss = 1.6706147193908691 16700
val loss = 2.0126543045043945
training loss = 1.6702568531036377 16800
val loss = 2.0199077129364014
training loss = 1.6712150573730469 16900
val loss = 2.07246470451355
training loss = 1.669885277748108 17000
val loss = 2.031919479370117
training loss = 1.669562578201294 17100
val loss = 2.0202455520629883
training loss = 1.6703115701675415 17200
val loss = 1.975695252418518
training loss = 1.6690049171447754 17300
val loss = 2.019282817840576
training loss = 1.7419798374176025 17400
val loss = 1.6960265636444092
training loss = 1.6683213710784912 17500
val loss = 2.0170578956604004
training loss = 1.6678632497787476 17600
val loss = 2.0163140296936035
training loss = 1.668809413909912 17700
val loss = 1.9639630317687988
training loss = 1.666872262954712 17800
val loss = 2.0145926475524902
training loss = 2.1069018840789795 17900
val loss = 1.4333209991455078
training loss = 1.6656701564788818 18000
val loss = 2.0129995346069336
training loss = 1.664950966835022 18100
val loss = 2.010108470916748
training loss = 1.6643882989883423 18200
val loss = 2.019099235534668
training loss = 1.6636427640914917 18300
val loss = 2.0072102546691895
training loss = 1.70430588722229 18400
val loss = 2.338712215423584
training loss = 1.6625033617019653 18500
val loss = 2.0077595710754395
training loss = 1.661949872970581 18600
val loss = 2.0050771236419678
training loss = 1.6617404222488403 18700
val loss = 2.0223686695098877
training loss = 1.6611318588256836 18800
val loss = 2.002857208251953
training loss = 1.6674778461456299 18900
val loss = 2.127239942550659
reduced chi^2 level 2 = 1.660645604133606
Constrained alpha: 1.9342139959335327
Constrained beta: 2.2314796447753906
Constrained gamma: 14.838787078857422
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 900.3375,  866.3624,  951.8782,  984.0373, 1008.1235, 1052.1973,
        1018.1611, 1117.1522, 1136.6530, 1224.8992, 1223.9784, 1212.7731,
        1235.4092, 1288.1393, 1225.9102, 1429.2129, 1362.5573, 1429.0206,
        1497.0420, 1427.1157, 1544.3934, 1539.0514, 1660.3267, 1634.0308,
        1654.8147, 1711.9440, 1595.8157, 1733.4746, 1762.7107, 1698.4618,
        1667.4957, 1731.2826, 1679.3654, 1667.5155, 1688.6816, 1767.3683,
        1605.0767, 1570.0771, 1597.6764, 1530.4554, 1678.2262, 1576.1509,
        1411.2920, 1492.3135, 1362.9097, 1350.3019, 1278.2086, 1302.3281,
        1118.9913, 1161.2177, 1132.3804,  956.8235,  948.1379,  890.8142,
         883.7488,  849.8631,  823.3325,  699.2651,  630.4379,  566.2481,
         576.4818,  486.0165,  427.3270,  369.1332,  349.1584,  348.7145,
         270.6191,  271.3879,  193.5586,  156.9115,  151.4314,  136.8690,
         134.1730,   98.8547,  116.1369,   63.4415,   55.0431,   42.4627,
          31.5632,   44.5405,   22.1737,   39.8384,   35.7306])]
2881.7184573798195
1.6407997247912047 5.022435048587342 11.75866687894751
val isze = 8
idinces = [14 65  2 24 53 70 64 52 49 76 77 17  7 44 54 26  6 72 10 55 31 23 59 22
 25 40 62 34 75  0  3 21 42 63 58 60 33 81  9 82 38 66 48  1 30 27 11 35
  8 80 29 57 67  4 37 13  5 12 45 56 47 61 68 28 43 41 74 79 78 18 15 73
 71 20 16 36 39 50 51 69 19 46 32]
we are doing training validation split
training loss = 24.64461326599121 100
val loss = 18.985761642456055
training loss = 18.7578125 200
val loss = 18.87699317932129
training loss = 14.760516166687012 300
val loss = 15.482427597045898
training loss = 11.95725154876709 400
val loss = 12.807100296020508
training loss = 10.034552574157715 500
val loss = 10.76957893371582
training loss = 8.714917182922363 600
val loss = 9.243757247924805
training loss = 7.8030104637146 700
val loss = 8.109845161437988
training loss = 7.168217182159424 800
val loss = 7.270582675933838
training loss = 6.723560333251953 900
val loss = 6.651122093200684
training loss = 6.410498142242432 1000
val loss = 6.194540023803711
training loss = 6.18903112411499 1100
val loss = 5.858678340911865
training loss = 6.0314621925354 1200
val loss = 5.611428260803223
training loss = 5.91835880279541 1300
val loss = 5.429498672485352
training loss = 5.835937023162842 1400
val loss = 5.294975757598877
training loss = 5.774341583251953 1500
val loss = 5.194979190826416
training loss = 5.7264299392700195 1600
val loss = 5.1195220947265625
training loss = 5.686910152435303 1700
val loss = 5.06148624420166
training loss = 5.651695728302002 1800
val loss = 5.01529598236084
training loss = 5.617327690124512 1900
val loss = 4.97678804397583
training loss = 5.580464839935303 2000
val loss = 4.942163944244385
training loss = 5.537301540374756 2100
val loss = 4.908477306365967
training loss = 5.483067512512207 2200
val loss = 4.872256278991699
training loss = 5.412026405334473 2300
val loss = 4.829770088195801
training loss = 5.318784236907959 2400
val loss = 4.777022838592529
training loss = 5.200112342834473 2500
val loss = 4.709400177001953
training loss = 5.053344249725342 2600
val loss = 4.620962619781494
training loss = 4.871419429779053 2700
val loss = 4.50456428527832
training loss = 4.641984939575195 2800
val loss = 4.353631496429443
training loss = 4.350895404815674 2900
val loss = 4.162766456604004
training loss = 3.9879443645477295 3000
val loss = 3.928968906402588
training loss = 3.5573551654815674 3100
val loss = 3.6557939052581787
training loss = 3.092365026473999 3200
val loss = 3.3622312545776367
training loss = 2.660593271255493 3300
val loss = 3.0874884128570557
training loss = 2.336869955062866 3400
val loss = 2.880443572998047
training loss = 2.1504106521606445 3500
val loss = 2.7679080963134766
training loss = 2.0684468746185303 3600
val loss = 2.7329564094543457
training loss = 2.0390429496765137 3700
val loss = 2.7379446029663086
training loss = 2.0289347171783447 3800
val loss = 2.7558460235595703
training loss = 2.024792194366455 3900
val loss = 2.7758684158325195
training loss = 2.022521734237671 4000
val loss = 2.7947897911071777
training loss = 2.020963191986084 4100
val loss = 2.812136650085449
training loss = 2.0197412967681885 4200
val loss = 2.8278350830078125
training loss = 2.0186948776245117 4300
val loss = 2.841709613800049
training loss = 2.017735242843628 4400
val loss = 2.8539962768554688
training loss = 2.0168075561523438 4500
val loss = 2.864598512649536
training loss = 2.015867233276367 4600
val loss = 2.8735363483428955
training loss = 2.0148842334747314 4700
val loss = 2.880892753601074
training loss = 2.01383376121521 4800
val loss = 2.8868062496185303
training loss = 2.0127077102661133 4900
val loss = 2.891263961791992
training loss = 2.0114946365356445 5000
val loss = 2.894550323486328
training loss = 2.0101912021636963 5100
val loss = 2.896700859069824
training loss = 2.077904462814331 5200
val loss = 3.2697954177856445
training loss = 2.007312774658203 5300
val loss = 2.89642333984375
training loss = 2.141810655593872 5400
val loss = 3.447288990020752
training loss = 2.0044026374816895 5500
val loss = 2.8921384811401367
training loss = 2.0030181407928467 5600
val loss = 2.888153076171875
training loss = 2.0016651153564453 5700
val loss = 2.8788907527923584
training loss = 2.0002825260162354 5800
val loss = 2.8856348991394043
training loss = 1.9989818334579468 5900
val loss = 2.8769335746765137
training loss = 1.9977458715438843 6000
val loss = 2.879495620727539
training loss = 2.0007944107055664 6100
val loss = 2.956285238265991
training loss = 1.995399832725525 6200
val loss = 2.875579833984375
training loss = 1.994345784187317 6300
val loss = 2.8704123497009277
training loss = 1.998970627784729 6400
val loss = 2.9609522819519043
training loss = 1.9922847747802734 6500
val loss = 2.8648619651794434
training loss = 1.9931389093399048 6600
val loss = 2.8146612644195557
training loss = 1.9904098510742188 6700
val loss = 2.8657665252685547
training loss = 1.989526391029358 6800
val loss = 2.8595921993255615
training loss = 1.9887288808822632 6900
val loss = 2.8441648483276367
training loss = 1.987843632698059 7000
val loss = 2.8537535667419434
training loss = 1.987009882926941 7100
val loss = 2.8512582778930664
training loss = 1.986289143562317 7200
val loss = 2.8489928245544434
training loss = 2.0040667057037354 7300
val loss = 2.7046141624450684
training loss = 1.9848442077636719 7400
val loss = 2.844226360321045
training loss = 1.9841976165771484 7500
val loss = 2.844944715499878
training loss = 1.9838480949401855 7600
val loss = 2.864454507827759
training loss = 1.982913613319397 7700
val loss = 2.8407318592071533
training loss = 1.994800090789795 7800
val loss = 2.7193543910980225
training loss = 1.9817057847976685 7900
val loss = 2.8389835357666016
training loss = 1.981181263923645 8000
val loss = 2.8368711471557617
training loss = 1.9807084798812866 8100
val loss = 2.8228371143341064
training loss = 1.9801079034805298 8200
val loss = 2.8341381549835205
training loss = 1.987528920173645 8300
val loss = 2.737212896347046
training loss = 1.9791239500045776 8400
val loss = 2.8257246017456055
training loss = 1.9786646366119385 8500
val loss = 2.8309073448181152
training loss = 1.9887363910675049 8600
val loss = 2.9577441215515137
training loss = 1.9777638912200928 8700
val loss = 2.8299107551574707
training loss = 1.9775891304016113 8800
val loss = 2.846111297607422
training loss = 1.9769067764282227 8900
val loss = 2.829498291015625
training loss = 1.9765514135360718 9000
val loss = 2.826125144958496
training loss = 2.099663019180298 9100
val loss = 2.52018404006958
training loss = 1.97579026222229 9200
val loss = 2.8221147060394287
training loss = 1.9754623174667358 9300
val loss = 2.827134132385254
training loss = 1.9751977920532227 9400
val loss = 2.8103253841400146
training loss = 1.9747679233551025 9500
val loss = 2.821925640106201
training loss = 1.9750109910964966 9600
val loss = 2.8508620262145996
training loss = 1.9741158485412598 9700
val loss = 2.8210601806640625
training loss = 1.9758954048156738 9800
val loss = 2.7693943977355957
training loss = 1.9735031127929688 9900
val loss = 2.8200125694274902
training loss = 1.9738678932189941 10000
val loss = 2.8489999771118164
training loss = 1.9729114770889282 10100
val loss = 2.8200345039367676
training loss = 1.9726768732070923 10200
val loss = 2.817783832550049
training loss = 1.9795945882797241 10300
val loss = 2.9222469329833984
training loss = 1.972147822380066 10400
val loss = 2.8146097660064697
training loss = 1.9719300270080566 10500
val loss = 2.820068836212158
training loss = 1.971745252609253 10600
val loss = 2.827906608581543
training loss = 1.9714363813400269 10700
val loss = 2.8152899742126465
training loss = 1.9790337085723877 10800
val loss = 2.7187235355377197
training loss = 1.9709668159484863 10900
val loss = 2.813537120819092
training loss = 1.970778465270996 11000
val loss = 2.8122806549072266
training loss = 1.9706788063049316 11100
val loss = 2.8280694484710693
training loss = 1.970351219177246 11200
val loss = 2.8133201599121094
training loss = 1.9711052179336548 11300
val loss = 2.777224063873291
training loss = 1.9699487686157227 11400
val loss = 2.81204891204834
training loss = 1.9758282899856567 11500
val loss = 2.9070792198181152
training loss = 1.9695658683776855 11600
val loss = 2.810943126678467
training loss = 1.9696922302246094 11700
val loss = 2.791729211807251
training loss = 1.969198226928711 11800
val loss = 2.8121414184570312
training loss = 1.9690808057785034 11900
val loss = 2.8050594329833984
training loss = 1.9689494371414185 12000
val loss = 2.7986273765563965
training loss = 1.9687154293060303 12100
val loss = 2.810103416442871
training loss = 1.9693340063095093 12200
val loss = 2.8433284759521484
training loss = 1.9683927297592163 12300
val loss = 2.808849811553955
training loss = 1.977403163909912 12400
val loss = 2.928311824798584
training loss = 1.9680778980255127 12500
val loss = 2.8091769218444824
training loss = 1.9679630994796753 12600
val loss = 2.80824613571167
training loss = 1.970046043395996 12700
val loss = 2.8651211261749268
training loss = 1.9676532745361328 12800
val loss = 2.807508945465088
training loss = 1.96755051612854 12900
val loss = 2.8074052333831787
training loss = 1.9734058380126953 13000
val loss = 2.9017415046691895
training loss = 1.9672805070877075 13100
val loss = 2.806455135345459
training loss = 2.2686872482299805 13200
val loss = 2.4201900959014893
training loss = 1.9670205116271973 13300
val loss = 2.8042681217193604
training loss = 1.9669291973114014 13400
val loss = 2.8055124282836914
training loss = 1.967218041419983 13500
val loss = 2.830883741378784
training loss = 1.966689109802246 13600
val loss = 2.805941581726074
training loss = 1.967152714729309 13700
val loss = 2.776832103729248
training loss = 1.966448187828064 13800
val loss = 2.804466724395752
training loss = 1.966369867324829 13900
val loss = 2.8045883178710938
training loss = 1.9664902687072754 14000
val loss = 2.786198377609253
training loss = 1.9661505222320557 14100
val loss = 2.8043885231018066
training loss = 1.9838221073150635 14200
val loss = 2.9732322692871094
training loss = 1.9659408330917358 14300
val loss = 2.8045654296875
training loss = 1.966415286064148 14400
val loss = 2.7780380249023438
training loss = 1.965746283531189 14500
val loss = 2.8091535568237305
training loss = 1.9656685590744019 14600
val loss = 2.8033857345581055
training loss = 2.081817388534546 14700
val loss = 3.2939977645874023
training loss = 1.9654667377471924 14800
val loss = 2.8054399490356445
training loss = 1.9654090404510498 14900
val loss = 2.803018093109131
training loss = 1.9741300344467163 15000
val loss = 2.701648235321045
training loss = 1.9652305841445923 15100
val loss = 2.802549362182617
training loss = 1.9753376245498657 15200
val loss = 2.694634199142456
training loss = 1.9650601148605347 15300
val loss = 2.803959369659424
training loss = 1.9650330543518066 15400
val loss = 2.796851634979248
training loss = 1.9649574756622314 15500
val loss = 2.8119516372680664
training loss = 1.9648442268371582 15600
val loss = 2.8015542030334473
training loss = 1.9677376747131348 15700
val loss = 2.8676669597625732
training loss = 1.9646847248077393 15800
val loss = 2.801198959350586
training loss = 1.9678219556808472 15900
val loss = 2.739448308944702
training loss = 1.96454918384552 16000
val loss = 2.807401418685913
training loss = 1.9644855260849 16100
val loss = 2.801182746887207
training loss = 1.967292308807373 16200
val loss = 2.8658511638641357
training loss = 1.9643282890319824 16300
val loss = 2.799656867980957
training loss = 1.964290976524353 16400
val loss = 2.8006839752197266
training loss = 1.9643338918685913 16500
val loss = 2.8152239322662354
training loss = 1.9641467332839966 16600
val loss = 2.8000845909118652
training loss = 2.0535929203033447 16700
val loss = 2.528907299041748
training loss = 1.9640171527862549 16800
val loss = 2.798133373260498
training loss = 1.963982343673706 16900
val loss = 2.8017935752868652
training loss = 1.9638782739639282 17000
val loss = 2.8000290393829346
training loss = 1.963850498199463 17100
val loss = 2.799683094024658
training loss = 1.9763660430908203 17200
val loss = 2.9395434856414795
training loss = 1.9637194871902466 17300
val loss = 2.799177646636963
training loss = 1.9637089967727661 17400
val loss = 2.7948923110961914
training loss = 1.96365487575531 17500
val loss = 2.790332794189453
training loss = 1.9635696411132812 17600
val loss = 2.799039363861084
training loss = 1.9658410549163818 17700
val loss = 2.8570618629455566
training loss = 1.9634450674057007 17800
val loss = 2.7982935905456543
training loss = 1.9634766578674316 17900
val loss = 2.808070421218872
training loss = 1.9633333683013916 18000
val loss = 2.7944111824035645
training loss = 1.9632999897003174 18100
val loss = 2.798527717590332
training loss = 1.9741036891937256 18200
val loss = 2.927591562271118
training loss = 1.9631779193878174 18300
val loss = 2.798715353012085
training loss = 1.963155746459961 18400
val loss = 2.797877311706543
training loss = 1.9639134407043457 18500
val loss = 2.764841318130493
training loss = 1.9630379676818848 18600
val loss = 2.7974276542663574
training loss = 2.033879518508911 18700
val loss = 2.548957586288452
training loss = 1.9629236459732056 18800
val loss = 2.798673629760742
training loss = 1.9629034996032715 18900
val loss = 2.7975406646728516
training loss = 1.965476155281067 19000
val loss = 2.739698886871338
training loss = 1.9627877473831177 19100
val loss = 2.7971506118774414
training loss = 1.9630457162857056 19200
val loss = 2.7783448696136475
training loss = 1.9627124071121216 19300
val loss = 2.790788173675537
training loss = 1.9626656770706177 19400
val loss = 2.7978875637054443
training loss = 1.9625813961029053 19500
val loss = 2.7987754344940186
training loss = 1.9625614881515503 19600
val loss = 2.795957565307617
training loss = 1.963507890701294 19700
val loss = 2.834141254425049
training loss = 1.9624593257904053 19800
val loss = 2.7965102195739746
training loss = 1.9819930791854858 19900
val loss = 2.9740233421325684
training loss = 1.9623478651046753 20000
val loss = 2.79587459564209
training loss = 1.9623279571533203 20100
val loss = 2.7965047359466553
training loss = 1.9622795581817627 20200
val loss = 2.8030059337615967
training loss = 1.9622235298156738 20300
val loss = 2.7951173782348633
training loss = 1.9715898036956787 20400
val loss = 2.6906206607818604
training loss = 1.9621129035949707 20500
val loss = 2.79532527923584
training loss = 1.963125467300415 20600
val loss = 2.8332438468933105
training loss = 1.9620158672332764 20700
val loss = 2.7996506690979004
training loss = 1.9619799852371216 20800
val loss = 2.7942047119140625
training loss = 1.9627799987792969 20900
val loss = 2.7603354454040527
training loss = 1.961867094039917 21000
val loss = 2.794567584991455
training loss = 1.9883956909179688 21100
val loss = 2.6274170875549316
reduced chi^2 level 2 = 1.9618192911148071
Constrained alpha: 2.003204584121704
Constrained beta: 3.4047038555145264
Constrained gamma: 11.567570686340332
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 919.8023,  857.6165,  951.0093,  969.5179,  948.3431, 1095.6182,
        1116.7976, 1086.2692, 1148.5092, 1181.9413, 1213.4496, 1220.6290,
        1249.0551, 1266.1133, 1305.4630, 1470.7764, 1409.8115, 1396.3258,
        1577.7703, 1477.5156, 1642.0729, 1537.5706, 1597.4178, 1676.8477,
        1628.8621, 1686.0231, 1668.9176, 1689.9866, 1744.0192, 1710.1742,
        1661.1772, 1708.2603, 1714.7952, 1677.2764, 1741.2129, 1719.3876,
        1653.2363, 1613.9136, 1563.4346, 1636.0084, 1591.1348, 1551.6375,
        1568.0645, 1481.2963, 1335.0115, 1357.8431, 1229.0017, 1219.7677,
        1103.9729, 1197.4362, 1114.1357,  969.3137,  970.5441,  901.5859,
         851.0213,  815.8275,  782.7225,  689.8959,  628.2314,  543.0159,
         631.4944,  487.4741,  388.4474,  387.6281,  358.5984,  321.5369,
         301.5664,  265.1474,  190.8514,  157.9713,  170.2540,  151.1053,
         164.0910,   90.2491,   89.0985,   79.3372,   55.4826,   49.3533,
          37.4415,   55.1972,   17.3847,   45.3343,   43.3601])]
2558.6444811383044
1.7068091420111675 15.65434864371661 27.621977203549896
val isze = 8
idinces = [ 5 11 52 59  1 51 70 35 22 12 53 41 79 75 33  2 61 32 28 77  7 63  0 81
 71 65 29 69 39 40  8 25  3 76 56 38 68 36 48 10 21 78 45 74 55 80  9 50
 13  6 49 57 34 17 14 27 72 64 67 30 23 20 18 19 73 37 46 16 66 43 26 62
 15 44 31 82 42 58 54 47 60 24  4]
we are doing training validation split
training loss = 224.2932891845703 100
val loss = 307.2373046875
training loss = 11.928747177124023 200
val loss = 10.473359107971191
training loss = 8.537646293640137 300
val loss = 8.867918968200684
training loss = 8.370149612426758 400
val loss = 8.861888885498047
training loss = 8.198410034179688 500
val loss = 8.880993843078613
training loss = 8.031049728393555 600
val loss = 8.91874885559082
training loss = 7.874830722808838 700
val loss = 8.975738525390625
training loss = 7.734456539154053 800
val loss = 9.050413131713867
training loss = 7.612448692321777 900
val loss = 9.138885498046875
training loss = 7.509206771850586 1000
val loss = 9.235406875610352
training loss = 7.423323631286621 1100
val loss = 9.333142280578613
training loss = 7.352127552032471 1200
val loss = 9.42514419555664
training loss = 7.292316436767578 1300
val loss = 9.505609512329102
training loss = 7.2405781745910645 1400
val loss = 9.57009506225586
training loss = 7.194009780883789 1500
val loss = 9.61660099029541
training loss = 7.150362968444824 1600
val loss = 9.644726753234863
training loss = 7.108073711395264 1700
val loss = 9.655423164367676
training loss = 7.066160678863525 1800
val loss = 9.651106834411621
training loss = 7.024099349975586 1900
val loss = 9.634389877319336
training loss = 6.981680870056152 2000
val loss = 9.607914924621582
training loss = 6.93888521194458 2100
val loss = 9.574337005615234
training loss = 6.895812511444092 2200
val loss = 9.535776138305664
training loss = 6.85263204574585 2300
val loss = 9.494026184082031
training loss = 6.809550762176514 2400
val loss = 9.450654983520508
training loss = 6.766794681549072 2500
val loss = 9.406604766845703
training loss = 6.724571228027344 2600
val loss = 9.362945556640625
training loss = 6.683043956756592 2700
val loss = 9.320451736450195
training loss = 6.642268657684326 2800
val loss = 9.279165267944336
training loss = 6.602097511291504 2900
val loss = 9.239959716796875
training loss = 6.5620198249816895 3000
val loss = 9.202149391174316
training loss = 6.520809173583984 3100
val loss = 9.165163040161133
training loss = 6.475722789764404 3200
val loss = 9.127195358276367
training loss = 6.420281887054443 3300
val loss = 9.083780288696289
training loss = 6.338272571563721 3400
val loss = 9.024276733398438
training loss = 6.194052696228027 3500
val loss = 8.921485900878906
training loss = 5.94474458694458 3600
val loss = 8.707775115966797
training loss = 5.5293426513671875 3700
val loss = 8.242523193359375
training loss = 4.690272331237793 3800
val loss = 7.2687554359436035
training loss = 3.256983995437622 3900
val loss = 5.381796360015869
training loss = 2.42157244682312 4000
val loss = 3.59600830078125
training loss = 2.297740936279297 4100
val loss = 2.953667640686035
training loss = 2.2820847034454346 4200
val loss = 2.8002142906188965
training loss = 2.2719626426696777 4300
val loss = 2.731160879135132
training loss = 2.2633817195892334 4400
val loss = 2.6786417961120605
training loss = 2.255852460861206 4500
val loss = 2.6344082355499268
training loss = 2.2490956783294678 4600
val loss = 2.596710205078125
training loss = 2.2429091930389404 4700
val loss = 2.564344882965088
training loss = 2.239426851272583 4800
val loss = 2.4621129035949707
training loss = 2.231876850128174 4900
val loss = 2.5081787109375
training loss = 2.276709794998169 5000
val loss = 2.1853928565979004
training loss = 2.2222752571105957 5100
val loss = 2.4688267707824707
training loss = 2.227536678314209 5200
val loss = 2.3110499382019043
training loss = 2.2136948108673096 5300
val loss = 2.4399003982543945
training loss = 2.213737964630127 5400
val loss = 2.3320248126983643
training loss = 2.2058920860290527 5500
val loss = 2.416426658630371
training loss = 2.2075490951538086 5600
val loss = 2.2964890003204346
training loss = 2.198659896850586 5700
val loss = 2.3937506675720215
training loss = 2.1974191665649414 5800
val loss = 2.316267490386963
training loss = 2.191859245300293 5900
val loss = 2.378988742828369
training loss = 2.188533067703247 6000
val loss = 2.3634819984436035
training loss = 2.1854336261749268 6100
val loss = 2.350548267364502
training loss = 2.1821978092193604 6200
val loss = 2.356431484222412
training loss = 2.1803081035614014 6300
val loss = 2.298527240753174
training loss = 2.176182508468628 6400
val loss = 2.343686580657959
training loss = 2.2026467323303223 6500
val loss = 2.6292521953582764
training loss = 2.170323133468628 6600
val loss = 2.336318016052246
training loss = 2.1674129962921143 6700
val loss = 2.327639102935791
training loss = 2.166456699371338 6800
val loss = 2.3886823654174805
training loss = 2.1618258953094482 6900
val loss = 2.3173341751098633
training loss = 2.5550248622894287 7000
val loss = 3.6527328491210938
training loss = 2.1564202308654785 7100
val loss = 2.301697015762329
training loss = 2.1537206172943115 7200
val loss = 2.3029050827026367
training loss = 2.2420802116394043 7300
val loss = 1.9375189542770386
training loss = 2.148411989212036 7400
val loss = 2.294849395751953
training loss = 2.1457090377807617 7500
val loss = 2.288480281829834
training loss = 2.1435763835906982 7600
val loss = 2.25020694732666
training loss = 2.14048171043396 7700
val loss = 2.2782649993896484
training loss = 2.141845703125 7800
val loss = 2.3731722831726074
training loss = 2.1350862979888916 7900
val loss = 2.2676172256469727
training loss = 2.1322836875915527 8000
val loss = 2.262570858001709
training loss = 2.1303741931915283 8100
val loss = 2.2123641967773438
training loss = 2.126561164855957 8200
val loss = 2.249786376953125
training loss = 2.1420340538024902 8300
val loss = 2.0616462230682373
training loss = 2.1203956604003906 8400
val loss = 2.237574577331543
training loss = 2.116974353790283 8500
val loss = 2.230182647705078
training loss = 2.115018367767334 8600
val loss = 2.2813069820404053
training loss = 2.109658718109131 8700
val loss = 2.21547794342041
training loss = 2.1053850650787354 8800
val loss = 2.2098820209503174
training loss = 2.1011760234832764 8900
val loss = 2.2227327823638916
training loss = 2.096071481704712 9000
val loss = 2.193662166595459
training loss = 2.0981218814849854 9100
val loss = 2.3139452934265137
training loss = 2.0854177474975586 9200
val loss = 2.1800496578216553
training loss = 2.0799360275268555 9300
val loss = 2.1981863975524902
training loss = 2.0741846561431885 9400
val loss = 2.1692676544189453
training loss = 2.0686240196228027 9500
val loss = 2.161783218383789
training loss = 2.1126410961151123 9600
val loss = 2.503983974456787
training loss = 2.057889461517334 9700
val loss = 2.1458020210266113
training loss = 2.052675485610962 9800
val loss = 2.135547161102295
training loss = 2.047581195831299 9900
val loss = 2.1122732162475586
training loss = 2.0427682399749756 10000
val loss = 2.1129188537597656
training loss = 2.038012742996216 10100
val loss = 2.104593276977539
training loss = 2.033734083175659 10200
val loss = 2.113280773162842
training loss = 2.029219627380371 10300
val loss = 2.082904815673828
training loss = 2.025096893310547 10400
val loss = 2.0776562690734863
training loss = 2.0213022232055664 10500
val loss = 2.0639727115631104
training loss = 2.017592430114746 10600
val loss = 2.0531134605407715
training loss = 2.0143027305603027 10700
val loss = 2.0622920989990234
training loss = 2.010883092880249 10800
val loss = 2.0384159088134766
training loss = 2.0224697589874268 10900
val loss = 2.194732904434204
training loss = 2.0049898624420166 11000
val loss = 2.0253746509552
training loss = 2.0022690296173096 11100
val loss = 2.0179948806762695
training loss = 1.9997045993804932 11200
val loss = 2.009831666946411
training loss = 1.9973682165145874 11300
val loss = 2.0075583457946777
training loss = 1.996450662612915 11400
val loss = 2.0485920906066895
training loss = 1.9931038618087769 11500
val loss = 1.990135908126831
training loss = 1.9911227226257324 11600
val loss = 1.9938645362854004
training loss = 1.9967443943023682 11700
val loss = 1.8930332660675049
training loss = 1.9876223802566528 11800
val loss = 1.9862096309661865
training loss = 2.1973037719726562 11900
val loss = 1.639047622680664
training loss = 1.9845699071884155 12000
val loss = 1.9790691137313843
training loss = 1.9831758737564087 12100
val loss = 1.9794723987579346
training loss = 1.9828358888626099 12200
val loss = 1.9414339065551758
training loss = 1.9806900024414062 12300
val loss = 1.9752452373504639
training loss = 2.023893117904663 12400
val loss = 2.264251232147217
training loss = 1.9785043001174927 12500
val loss = 1.9733082056045532
training loss = 1.9774914979934692 12600
val loss = 1.9724923372268677
training loss = 1.9767320156097412 12700
val loss = 1.9852221012115479
training loss = 1.9756816625595093 12800
val loss = 1.968923807144165
training loss = 1.999078392982483 12900
val loss = 1.8069615364074707
training loss = 1.9740560054779053 13000
val loss = 1.9682621955871582
training loss = 1.9733048677444458 13100
val loss = 1.9621925354003906
training loss = 1.97267484664917 13200
val loss = 1.9771784543991089
training loss = 1.9719059467315674 13300
val loss = 1.965407133102417
training loss = 1.9995211362838745 13400
val loss = 1.7957079410552979
training loss = 1.9706333875656128 13500
val loss = 1.9632482528686523
training loss = 1.970021367073059 13600
val loss = 1.965030312538147
training loss = 1.969539999961853 13700
val loss = 1.9530398845672607
training loss = 1.96889328956604 13800
val loss = 1.9637401103973389
training loss = 2.0193116664886475 13900
val loss = 1.7475935220718384
training loss = 1.9678363800048828 14000
val loss = 1.964902400970459
training loss = 1.9673198461532593 14100
val loss = 1.9634275436401367
training loss = 1.9668926000595093 14200
val loss = 1.9726808071136475
training loss = 1.9663399457931519 14300
val loss = 1.9630670547485352
training loss = 2.000731945037842 14400
val loss = 1.7772674560546875
training loss = 1.9654090404510498 14500
val loss = 1.964479684829712
training loss = 1.9649441242218018 14600
val loss = 1.963138461112976
training loss = 1.969166874885559 14700
val loss = 2.046515464782715
training loss = 1.9640557765960693 14800
val loss = 1.9625518321990967
training loss = 1.9638421535491943 14900
val loss = 1.9454691410064697
training loss = 1.963200330734253 15000
val loss = 1.9684879779815674
training loss = 1.962738037109375 15100
val loss = 1.9632611274719238
training loss = 1.963322639465332 15200
val loss = 2.0012292861938477
training loss = 1.9618849754333496 15300
val loss = 1.9627329111099243
training loss = 1.9774082899093628 15400
val loss = 2.1230039596557617
training loss = 1.9610401391983032 15500
val loss = 1.9658236503601074
training loss = 1.960599422454834 15600
val loss = 1.963321328163147
training loss = 1.9608567953109741 15700
val loss = 1.9942731857299805
training loss = 1.9597619771957397 15800
val loss = 1.9635286331176758
training loss = 1.9593321084976196 15900
val loss = 1.9598710536956787
training loss = 1.9589604139328003 16000
val loss = 1.9724164009094238
training loss = 1.9584709405899048 16100
val loss = 1.96403169631958
training loss = 1.9580457210540771 16200
val loss = 1.9637720584869385
training loss = 1.9576172828674316 16300
val loss = 1.9635913372039795
training loss = 1.9571609497070312 16400
val loss = 1.9650506973266602
training loss = 1.9567564725875854 16500
val loss = 1.9614375829696655
training loss = 1.9562981128692627 16600
val loss = 1.9652431011199951
training loss = 2.1755478382110596 16700
val loss = 1.6343291997909546
training loss = 1.9554108381271362 16800
val loss = 1.9677953720092773
training loss = 1.9549390077590942 16900
val loss = 1.9666683673858643
training loss = 1.9545670747756958 17000
val loss = 1.9753000736236572
training loss = 1.9540390968322754 17100
val loss = 1.9669163227081299
training loss = 2.016663074493408 17200
val loss = 1.7351049184799194
training loss = 1.9531409740447998 17300
val loss = 1.9686148166656494
training loss = 1.9526574611663818 17400
val loss = 1.9677895307540894
training loss = 1.9600179195404053 17500
val loss = 2.0776562690734863
training loss = 1.9517680406570435 17600
val loss = 1.969261646270752
reduced chi^2 level 2 = 1.9516289234161377
Constrained alpha: 1.9166935682296753
Constrained beta: 2.039109945297241
Constrained gamma: 13.861356735229492
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 843.9489,  854.4362,  922.9959,  981.7823,  970.5400, 1055.5034,
        1108.6128, 1152.1331, 1116.6213, 1162.1896, 1212.7104, 1185.5565,
        1242.0328, 1231.2755, 1340.8085, 1366.5952, 1371.3934, 1386.9530,
        1562.4064, 1505.3605, 1623.8621, 1585.5712, 1628.7693, 1588.2534,
        1733.3990, 1756.1523, 1591.8849, 1800.0663, 1702.4789, 1708.9396,
        1675.1467, 1766.8477, 1776.9891, 1697.9547, 1697.3252, 1770.5127,
        1664.4465, 1576.1615, 1552.2269, 1676.3082, 1671.6918, 1531.9146,
        1525.2672, 1497.1156, 1314.2422, 1374.1609, 1273.3710, 1235.1499,
        1197.9855, 1153.5211, 1069.9585, 1031.3126,  921.8350,  893.6384,
         889.2318,  841.1006,  823.4788,  703.9681,  574.8118,  539.7579,
         539.3082,  448.2528,  418.5258,  387.1000,  355.3499,  300.1368,
         281.8251,  243.9608,  192.4960,  155.9708,  158.0988,  146.9129,
         130.1291,  103.2037,   89.9450,   70.7675,   57.3176,   40.2161,
          29.8153,   42.6059,   18.4701,   29.6595,   29.2543])]
2758.418284459161
1.3191036561102183 8.486221077642975 38.73069757433731
val isze = 8
idinces = [ 7 80 60 14 42 72 63 17 35 62 70 61 53 82 67 22 64 21 66 74 26  4 50 24
 29 18 65 59 68 16 27 33  1 81 19 25 48 47 49 79  2 12 73  5 11 77 23 43
  0 75 46 20 31  8 69 28 38 51 52 78 10 15 76 39 56 55 32 40 36  3 54 44
 30 71 45 34 13 41 58  6  9 37 57]
we are doing training validation split
training loss = 11.171980857849121 100
val loss = 5.0474748611450195
training loss = 8.873085021972656 200
val loss = 4.266849517822266
training loss = 8.334712028503418 300
val loss = 4.0169572830200195
training loss = 7.850703239440918 400
val loss = 3.8203887939453125
training loss = 7.445044994354248 500
val loss = 3.6834282875061035
training loss = 7.119207382202148 600
val loss = 3.6017799377441406
training loss = 6.864602088928223 700
val loss = 3.5653445720672607
training loss = 6.669256687164307 800
val loss = 3.5627779960632324
training loss = 6.52113151550293 900
val loss = 3.583524703979492
training loss = 6.409514904022217 1000
val loss = 3.6186952590942383
training loss = 6.32546329498291 1100
val loss = 3.6612966060638428
training loss = 6.26181697845459 1200
val loss = 3.7060508728027344
training loss = 6.212987422943115 1300
val loss = 3.7491958141326904
training loss = 6.174696445465088 1400
val loss = 3.788212299346924
training loss = 6.143753528594971 1500
val loss = 3.821549654006958
training loss = 6.117786884307861 1600
val loss = 3.84842586517334
training loss = 6.0950703620910645 1700
val loss = 3.8685829639434814
training loss = 6.074353218078613 1800
val loss = 3.8821372985839844
training loss = 6.054717540740967 1900
val loss = 3.889434814453125
training loss = 6.035473823547363 2000
val loss = 3.890953540802002
training loss = 6.01605749130249 2100
val loss = 3.887148857116699
training loss = 5.995944023132324 2200
val loss = 3.8783884048461914
training loss = 5.974552154541016 2300
val loss = 3.864845037460327
training loss = 5.951127052307129 2400
val loss = 3.8463706970214844
training loss = 5.924549102783203 2500
val loss = 3.8222522735595703
training loss = 5.893001556396484 2600
val loss = 3.790836811065674
training loss = 5.853275775909424 2700
val loss = 3.7486894130706787
training loss = 5.7992448806762695 2800
val loss = 3.6887283325195312
training loss = 5.718350410461426 2900
val loss = 3.595781087875366
training loss = 5.584015846252441 3000
val loss = 3.437025547027588
training loss = 5.347214698791504 3100
val loss = 3.1529314517974854
training loss = 4.967965602874756 3200
val loss = 2.714078903198242
training loss = 4.48519229888916 3300
val loss = 2.240063190460205
training loss = 3.9294254779815674 3400
val loss = 1.7932459115982056
training loss = 3.3150036334991455 3500
val loss = 1.3389875888824463
training loss = 2.7112016677856445 3600
val loss = 0.9412842988967896
training loss = 2.2486963272094727 3700
val loss = 0.7395697832107544
training loss = 2.0120596885681152 3800
val loss = 0.7713106870651245
training loss = 1.9343255758285522 3900
val loss = 0.8834617137908936
training loss = 1.9123200178146362 4000
val loss = 0.9555082321166992
training loss = 1.9026778936386108 4100
val loss = 0.9833663105964661
training loss = 1.8957971334457397 4200
val loss = 0.9905513525009155
training loss = 1.8899952173233032 4300
val loss = 0.9906263947486877
training loss = 1.8849105834960938 4400
val loss = 0.9887231588363647
training loss = 1.8804140090942383 4500
val loss = 0.9865347743034363
training loss = 1.8764302730560303 4600
val loss = 0.984471321105957
training loss = 1.872901201248169 4700
val loss = 0.9826755523681641
training loss = 1.8697764873504639 4800
val loss = 0.9811410903930664
training loss = 1.8670133352279663 4900
val loss = 0.9798265695571899
training loss = 1.864569902420044 5000
val loss = 0.9787482023239136
training loss = 1.8624143600463867 5100
val loss = 0.9778366088867188
training loss = 1.8605101108551025 5200
val loss = 0.9771493673324585
training loss = 1.8588330745697021 5300
val loss = 0.9766297340393066
training loss = 1.8573495149612427 5400
val loss = 0.9762073755264282
training loss = 1.8574333190917969 5500
val loss = 0.9972766637802124
training loss = 1.8544734716415405 5600
val loss = 0.9758968353271484
training loss = 1.8597601652145386 5700
val loss = 1.023292064666748
training loss = 1.8518506288528442 5800
val loss = 0.9748534560203552
training loss = 1.8506990671157837 5900
val loss = 0.9756292104721069
training loss = 1.8943616151809692 6000
val loss = 0.8980385661125183
training loss = 1.8484715223312378 6100
val loss = 0.9756511449813843
training loss = 1.8475159406661987 6200
val loss = 0.9760624766349792
training loss = 1.8467708826065063 6300
val loss = 0.9844740033149719
training loss = 1.8455806970596313 6400
val loss = 0.9758309125900269
training loss = 1.8542252779006958 6500
val loss = 1.0325474739074707
training loss = 1.843747615814209 6600
val loss = 0.975143551826477
training loss = 1.8429967164993286 6700
val loss = 0.9761378765106201
training loss = 1.8940575122833252 6800
val loss = 1.1359224319458008
training loss = 1.841448426246643 6900
val loss = 0.9754155874252319
training loss = 1.840804100036621 7000
val loss = 0.9764378070831299
training loss = 1.860690712928772 7100
val loss = 0.9151432514190674
training loss = 1.8394441604614258 7200
val loss = 0.9765416383743286
training loss = 1.8388928174972534 7300
val loss = 0.9767732620239258
training loss = 1.8393315076828003 7400
val loss = 0.9935956001281738
training loss = 1.8376843929290771 7500
val loss = 0.976722240447998
training loss = 1.837208867073059 7600
val loss = 0.9776409268379211
training loss = 1.836775779724121 7700
val loss = 0.983622133731842
training loss = 1.8361369371414185 7800
val loss = 0.9772239923477173
training loss = 1.8684875965118408 7900
val loss = 0.9084094762802124
training loss = 1.8351433277130127 8000
val loss = 0.9763141870498657
training loss = 1.8347492218017578 8100
val loss = 0.9775620698928833
training loss = 1.8354099988937378 8200
val loss = 0.9595329761505127
training loss = 1.8338357210159302 8300
val loss = 0.9775214195251465
training loss = 1.8371011018753052 8400
val loss = 0.9489245414733887
training loss = 1.8329895734786987 8500
val loss = 0.9786306023597717
training loss = 1.8326447010040283 8600
val loss = 0.9779338836669922
training loss = 1.8354289531707764 8700
val loss = 1.0091767311096191
reduced chi^2 level 2 = 1.832144021987915
Constrained alpha: 1.7937062978744507
Constrained beta: 4.055802345275879
Constrained gamma: 21.139507293701172
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 872.8195,  892.3002,  952.9175,  955.1372, 1013.3921, 1000.7072,
        1105.4874, 1147.3557, 1129.6365, 1160.6743, 1180.7788, 1161.4453,
        1197.3187, 1249.9304, 1314.9169, 1353.0471, 1412.7657, 1384.0211,
        1568.6237, 1482.8051, 1595.0842, 1605.2728, 1625.2904, 1579.6183,
        1668.3751, 1814.6091, 1680.8116, 1707.7366, 1757.5404, 1736.0972,
        1663.2552, 1713.2195, 1765.0399, 1745.8744, 1692.8252, 1768.0978,
        1759.5673, 1578.7357, 1650.1779, 1624.6830, 1645.9039, 1524.5095,
        1485.0352, 1534.4279, 1416.6879, 1328.0685, 1268.9242, 1232.2354,
        1223.4193, 1150.5923, 1114.2510, 1004.0304,  974.9005,  950.8023,
         891.5577,  853.4448,  821.2610,  702.2063,  621.2562,  554.3745,
         566.3931,  493.9156,  452.0598,  397.6088,  336.7220,  344.1658,
         301.3907,  269.5442,  198.1981,  172.9603,  161.0071,  139.9265,
         135.6011,  116.9993,   90.2378,   77.1045,   46.6222,   42.7033,
          35.1935,   45.5798,   15.6357,   50.8898,   24.9051])]
2449.1799249903906
2.5698946049114495 14.89341945047347 54.854532151504685
val isze = 8
idinces = [18 32 36 63 54 28 80 68  5 13 43 53 40 20 73 44 78 33  2 76 39  7 69  8
 52 58 79 23  1 42 82 62 12 81 22 77 48 24 72 49 57 29 71 16 70 45  4  6
  0 31 59 46 15 61 14 75 19 25 55  3 30 67 60 64 26 74 65 47 66 38 35 27
 41 37 17  9 50 11 21 56 51 10 34]
we are doing training validation split
training loss = 230.3168487548828 100
val loss = 216.11749267578125
training loss = 7.828428745269775 200
val loss = 5.284942150115967
training loss = 7.3259687423706055 300
val loss = 5.640799522399902
training loss = 7.2040815353393555 400
val loss = 5.739664077758789
training loss = 7.081639766693115 500
val loss = 5.857322692871094
training loss = 6.964822769165039 600
val loss = 5.995169639587402
training loss = 6.857931613922119 700
val loss = 6.148945331573486
training loss = 6.763393402099609 800
val loss = 6.312665939331055
training loss = 6.681886196136475 900
val loss = 6.478984355926514
training loss = 6.612636089324951 1000
val loss = 6.6402997970581055
training loss = 6.553871154785156 1100
val loss = 6.789521217346191
training loss = 6.503321170806885 1200
val loss = 6.921324253082275
training loss = 6.458670139312744 1300
val loss = 7.032217502593994
training loss = 6.417882442474365 1400
val loss = 7.120759963989258
training loss = 6.379361152648926 1500
val loss = 7.187468528747559
training loss = 6.341989040374756 1600
val loss = 7.233944416046143
training loss = 6.305051326751709 1700
val loss = 7.263286113739014
training loss = 6.268138408660889 1800
val loss = 7.278240203857422
training loss = 6.231061935424805 1900
val loss = 7.281847953796387
training loss = 6.193747043609619 2000
val loss = 7.276877403259277
training loss = 6.156210899353027 2100
val loss = 7.265517234802246
training loss = 6.118499279022217 2200
val loss = 7.249514102935791
training loss = 6.08069372177124 2300
val loss = 7.230330467224121
training loss = 6.042886257171631 2400
val loss = 7.209066867828369
training loss = 6.005189895629883 2500
val loss = 7.186501502990723
training loss = 5.967719078063965 2600
val loss = 7.163249969482422
training loss = 5.930574893951416 2700
val loss = 7.139875411987305
training loss = 5.8938093185424805 2800
val loss = 7.116585731506348
training loss = 5.857326030731201 2900
val loss = 7.093823432922363
training loss = 5.820703983306885 3000
val loss = 7.071698188781738
training loss = 5.782724857330322 3100
val loss = 7.0502519607543945
training loss = 5.740076541900635 3200
val loss = 7.028641700744629
training loss = 5.683170795440674 3300
val loss = 7.0049920082092285
training loss = 5.581967830657959 3400
val loss = 6.976109027862549
training loss = 5.367457866668701 3500
val loss = 6.926741600036621
training loss = 4.992361545562744 3600
val loss = 6.686567306518555
training loss = 4.347219467163086 3700
val loss = 6.069727897644043
training loss = 3.2527856826782227 3800
val loss = 5.006728172302246
training loss = 2.1272504329681396 3900
val loss = 3.7770309448242188
training loss = 1.860748291015625 4000
val loss = 3.3729350566864014
training loss = 1.839855432510376 4100
val loss = 3.310370683670044
training loss = 1.8304862976074219 4200
val loss = 3.2825746536254883
training loss = 1.822533369064331 4300
val loss = 3.2613134384155273
training loss = 1.8175138235092163 4400
val loss = 3.366361618041992
training loss = 1.8087819814682007 4500
val loss = 3.22822904586792
training loss = 1.8048979043960571 4600
val loss = 3.0893564224243164
training loss = 1.7967643737792969 4700
val loss = 3.2006828784942627
training loss = 1.7912895679473877 4800
val loss = 3.2175045013427734
training loss = 1.7857745885849 4900
val loss = 3.176441192626953
training loss = 1.7812199592590332 5000
val loss = 3.10042667388916
training loss = 1.775635838508606 5100
val loss = 3.1576895713806152
training loss = 1.7713452577590942 5200
val loss = 3.20184326171875
training loss = 1.7663918733596802 5300
val loss = 3.1378722190856934
training loss = 1.7625120878219604 5400
val loss = 3.077176094055176
training loss = 1.7580713033676147 5500
val loss = 3.122953414916992
training loss = 1.7691960334777832 5600
val loss = 2.8217740058898926
training loss = 1.7506123781204224 5700
val loss = 3.1110222339630127
training loss = 1.7472375631332397 5800
val loss = 3.1182024478912354
training loss = 1.7438814640045166 5900
val loss = 3.102463483810425
training loss = 1.7407792806625366 6000
val loss = 3.0926666259765625
training loss = 1.7402722835540771 6100
val loss = 2.9643430709838867
training loss = 1.734879970550537 6200
val loss = 3.0837790966033936
training loss = 1.763368010520935 6300
val loss = 3.5434272289276123
training loss = 1.7293798923492432 6400
val loss = 3.076084613800049
training loss = 1.7267823219299316 6500
val loss = 3.0713589191436768
training loss = 1.724790096282959 6600
val loss = 3.127429723739624
training loss = 1.7217427492141724 6700
val loss = 3.0653700828552246
training loss = 1.9769251346588135 6800
val loss = 4.545227527618408
training loss = 1.7169090509414673 6900
val loss = 3.065669059753418
training loss = 1.7145638465881348 7000
val loss = 3.056769371032715
training loss = 1.7128045558929443 7100
val loss = 3.1126160621643066
training loss = 1.7099858522415161 7200
val loss = 3.0528807640075684
training loss = 1.7871012687683105 7300
val loss = 3.80572509765625
training loss = 1.7055078744888306 7400
val loss = 3.052823066711426
training loss = 1.7033166885375977 7500
val loss = 3.045804738998413
training loss = 1.702217698097229 7600
val loss = 2.964012384414673
training loss = 1.6989926099777222 7700
val loss = 3.042417049407959
training loss = 1.9937846660614014 7800
val loss = 2.0235366821289062
training loss = 1.6947343349456787 7900
val loss = 3.0358591079711914
training loss = 1.6926358938217163 8000
val loss = 3.036816120147705
training loss = 1.6918176412582397 8100
val loss = 2.9503960609436035
training loss = 1.6884874105453491 8200
val loss = 3.0346591472625732
training loss = 1.7546613216400146 8300
val loss = 2.4756226539611816
training loss = 1.684396743774414 8400
val loss = 3.028247594833374
training loss = 1.682352900505066 8500
val loss = 3.0279927253723145
training loss = 1.680382251739502 8600
val loss = 3.0137245655059814
training loss = 1.6783809661865234 8700
val loss = 3.0269923210144043
training loss = 1.701492190361023 8800
val loss = 2.671565055847168
training loss = 1.674446940422058 8900
val loss = 3.0218193531036377
training loss = 1.6724873781204224 9000
val loss = 3.0131020545959473
training loss = 1.6706002950668335 9100
val loss = 3.0036768913269043
training loss = 1.6686145067214966 9200
val loss = 3.0208380222320557
training loss = 1.6669378280639648 9300
val loss = 2.981987476348877
training loss = 1.6648350954055786 9400
val loss = 3.0181198120117188
training loss = 1.6679526567459106 9500
val loss = 2.8550331592559814
training loss = 1.6610982418060303 9600
val loss = 3.0237131118774414
training loss = 1.6592462062835693 9700
val loss = 3.014674663543701
training loss = 1.8643791675567627 9800
val loss = 2.1533265113830566
training loss = 1.655542254447937 9900
val loss = 3.0121164321899414
training loss = 1.6537108421325684 10000
val loss = 3.0108797550201416
training loss = 1.9228808879852295 10100
val loss = 2.065054178237915
training loss = 1.6499487161636353 10200
val loss = 3.0049428939819336
training loss = 1.6480177640914917 10300
val loss = 3.0064523220062256
training loss = 1.64608895778656 10400
val loss = 3.0158371925354004
training loss = 1.644096851348877 10500
val loss = 3.003795623779297
training loss = 1.6622412204742432 10600
val loss = 3.3470757007598877
training loss = 1.6399178504943848 10700
val loss = 2.9995741844177246
training loss = 1.637648344039917 10800
val loss = 3.0003957748413086
training loss = 1.6354049444198608 10900
val loss = 3.0156679153442383
training loss = 1.6329033374786377 11000
val loss = 2.9950509071350098
training loss = 1.6709375381469727 11100
val loss = 3.4932541847229004
training loss = 1.6275490522384644 11200
val loss = 2.991992235183716
training loss = 1.6246954202651978 11300
val loss = 2.98624587059021
training loss = 1.6290993690490723 11400
val loss = 3.184232473373413
training loss = 1.618760347366333 11500
val loss = 2.979907989501953
training loss = 1.6282206773757935 11600
val loss = 3.239741086959839
training loss = 1.6127015352249146 11700
val loss = 2.964229106903076
training loss = 1.6096326112747192 11800
val loss = 2.9646520614624023
training loss = 1.607340693473816 11900
val loss = 2.8953161239624023
training loss = 1.603499174118042 12000
val loss = 2.953202247619629
training loss = 1.8318071365356445 12100
val loss = 4.251507759094238
training loss = 1.5971448421478271 12200
val loss = 2.943542242050171
training loss = 1.5938483476638794 12300
val loss = 2.936199188232422
training loss = 1.590553879737854 12400
val loss = 2.9351930618286133
training loss = 1.5872912406921387 12500
val loss = 2.9234447479248047
training loss = 1.5838255882263184 12600
val loss = 2.917794942855835
training loss = 1.581113338470459 12700
val loss = 2.8494343757629395
training loss = 1.5768319368362427 12800
val loss = 2.9035120010375977
training loss = 1.5899978876113892 12900
val loss = 3.1922426223754883
training loss = 1.569745659828186 13000
val loss = 2.8900537490844727
training loss = 1.5661197900772095 13100
val loss = 2.8836264610290527
training loss = 1.5626130104064941 13200
val loss = 2.861020565032959
training loss = 1.5590572357177734 13300
val loss = 2.8688457012176514
training loss = 1.5715662240982056 13400
val loss = 2.605774164199829
training loss = 1.5522065162658691 13500
val loss = 2.8569653034210205
training loss = 1.54887056350708 13600
val loss = 2.828402519226074
training loss = 1.5456783771514893 13700
val loss = 2.845705986022949
training loss = 1.5425642728805542 13800
val loss = 2.8339924812316895
training loss = 1.5397733449935913 13900
val loss = 2.8563671112060547
training loss = 1.5368084907531738 14000
val loss = 2.821131706237793
training loss = 1.6545072793960571 14100
val loss = 2.205134391784668
training loss = 1.5316283702850342 14200
val loss = 2.801405668258667
training loss = 1.5292190313339233 14300
val loss = 2.8036112785339355
training loss = 1.5270092487335205 14400
val loss = 2.8054471015930176
training loss = 1.5249625444412231 14500
val loss = 2.7937557697296143
training loss = 1.5561379194259644 14600
val loss = 3.196582794189453
training loss = 1.5212520360946655 14700
val loss = 2.7836756706237793
training loss = 1.5195817947387695 14800
val loss = 2.7789247035980225
training loss = 1.5183055400848389 14900
val loss = 2.8061771392822266
training loss = 1.516687035560608 15000
val loss = 2.771376848220825
training loss = 1.5213837623596191 15100
val loss = 2.9311881065368652
training loss = 1.5142107009887695 15200
val loss = 2.7689499855041504
training loss = 1.513090968132019 15300
val loss = 2.7597875595092773
training loss = 1.5130698680877686 15400
val loss = 2.820345163345337
training loss = 1.5111780166625977 15500
val loss = 2.754051923751831
training loss = 1.5473030805587769 15600
val loss = 2.398797035217285
training loss = 1.5095475912094116 15700
val loss = 2.750932216644287
training loss = 1.508801817893982 15800
val loss = 2.74995756149292
training loss = 1.5081663131713867 15900
val loss = 2.747926950454712
training loss = 1.5075491666793823 16000
val loss = 2.7409887313842773
training loss = 1.5088183879852295 16100
val loss = 2.8269784450531006
training loss = 1.5064382553100586 16200
val loss = 2.7418642044067383
training loss = 1.5059154033660889 16300
val loss = 2.7350668907165527
training loss = 1.50750732421875 16400
val loss = 2.642009735107422
training loss = 1.5050004720687866 16500
val loss = 2.7311089038848877
training loss = 1.508002758026123 16600
val loss = 2.613607168197632
training loss = 1.5041735172271729 16700
val loss = 2.728705406188965
training loss = 1.5037771463394165 16800
val loss = 2.7241759300231934
training loss = 1.5035160779953003 16900
val loss = 2.7017951011657715
training loss = 1.5030583143234253 17000
val loss = 2.7213287353515625
training loss = 1.5073018074035645 17100
val loss = 2.8582723140716553
training loss = 1.5023866891860962 17200
val loss = 2.720073699951172
training loss = 1.5060847997665405 17300
val loss = 2.8445894718170166
training loss = 1.501777172088623 17400
val loss = 2.7029528617858887
training loss = 1.5014400482177734 17500
val loss = 2.7116951942443848
training loss = 1.507449746131897 17600
val loss = 2.5588276386260986
training loss = 1.5008505582809448 17700
val loss = 2.7069029808044434
training loss = 1.5670605897903442 17800
val loss = 2.261897087097168
training loss = 1.5003001689910889 17900
val loss = 2.7128443717956543
training loss = 1.4999878406524658 18000
val loss = 2.7028632164001465
training loss = 1.5003619194030762 18100
val loss = 2.7507083415985107
training loss = 1.4994457960128784 18200
val loss = 2.6995697021484375
training loss = 1.5074775218963623 18300
val loss = 2.52557110786438
training loss = 1.4989136457443237 18400
val loss = 2.694627523422241
training loss = 1.4986380338668823 18500
val loss = 2.694002628326416
training loss = 1.5867118835449219 18600
val loss = 3.3566582202911377
training loss = 1.4981144666671753 18700
val loss = 2.6904828548431396
training loss = 1.4978359937667847 18800
val loss = 2.689424514770508
training loss = 1.4980441331863403 18900
val loss = 2.728806495666504
training loss = 1.4973199367523193 19000
val loss = 2.685850143432617
training loss = 1.5352379083633423 19100
val loss = 3.1026694774627686
training loss = 1.4968135356903076 19200
val loss = 2.6845459938049316
training loss = 1.4965338706970215 19300
val loss = 2.6792964935302734
training loss = 1.4963816404342651 19400
val loss = 2.695850133895874
training loss = 1.4960352182388306 19500
val loss = 2.6770219802856445
training loss = 1.7090997695922852 19600
val loss = 3.773071527481079
training loss = 1.4955472946166992 19700
val loss = 2.6735992431640625
training loss = 1.4952800273895264 19800
val loss = 2.672664165496826
training loss = 1.5350347757339478 19900
val loss = 2.326078176498413
training loss = 1.4947911500930786 20000
val loss = 2.670001268386841
training loss = 1.494528889656067 20100
val loss = 2.6728219985961914
training loss = 1.4943692684173584 20200
val loss = 2.68203067779541
training loss = 1.4940447807312012 20300
val loss = 2.6658387184143066
training loss = 1.5006699562072754 20400
val loss = 2.511486291885376
training loss = 1.493577480316162 20500
val loss = 2.664842128753662
training loss = 1.4933195114135742 20600
val loss = 2.66534161567688
training loss = 1.4931706190109253 20700
val loss = 2.6734352111816406
training loss = 1.4928770065307617 20800
val loss = 2.6598381996154785
training loss = 1.5486582517623901 20900
val loss = 2.261035919189453
training loss = 1.4924429655075073 21000
val loss = 2.659013509750366
training loss = 1.4922010898590088 21100
val loss = 2.656622886657715
training loss = 1.5007498264312744 21200
val loss = 2.84273624420166
training loss = 1.4917808771133423 21300
val loss = 2.655944347381592
training loss = 1.7843186855316162 21400
val loss = 1.9153650999069214
training loss = 1.4913794994354248 21500
val loss = 2.6500566005706787
training loss = 1.4911532402038574 21600
val loss = 2.65311861038208
training loss = 1.4916242361068726 21700
val loss = 2.700059413909912
training loss = 1.4907828569412231 21800
val loss = 2.6514203548431396
training loss = 1.5257740020751953 21900
val loss = 2.329845428466797
training loss = 1.490433931350708 22000
val loss = 2.65262770652771
training loss = 1.490231990814209 22100
val loss = 2.647280693054199
training loss = 1.4903146028518677 22200
val loss = 2.6761059761047363
training loss = 1.4899260997772217 22300
val loss = 2.649028778076172
training loss = 1.4941221475601196 22400
val loss = 2.527132987976074
training loss = 1.489655613899231 22500
val loss = 2.6538209915161133
training loss = 1.4894788265228271 22600
val loss = 2.6477646827697754
training loss = 1.4952887296676636 22700
val loss = 2.5070834159851074
training loss = 1.4892160892486572 22800
val loss = 2.6423251628875732
training loss = 1.4890550374984741 22900
val loss = 2.647041082382202
training loss = 1.4892301559448242 23000
val loss = 2.6771445274353027
training loss = 1.4888273477554321 23100
val loss = 2.6459996700286865
training loss = 1.489935278892517 23200
val loss = 2.580613613128662
training loss = 1.4886096715927124 23300
val loss = 2.644906520843506
training loss = 1.4884775876998901 23400
val loss = 2.6464662551879883
training loss = 1.4941169023513794 23500
val loss = 2.508840560913086
training loss = 1.4882893562316895 23600
val loss = 2.6469743251800537
training loss = 1.488176703453064 23700
val loss = 2.6385951042175293
training loss = 1.4882149696350098 23800
val loss = 2.6258535385131836
training loss = 1.4879868030548096 23900
val loss = 2.6458258628845215
training loss = 1.4879471063613892 24000
val loss = 2.652717351913452
training loss = 1.487831711769104 24100
val loss = 2.6466236114501953
training loss = 1.487719178199768 24200
val loss = 2.6467602252960205
training loss = 1.487679123878479 24300
val loss = 2.646256446838379
training loss = 1.4875764846801758 24400
val loss = 2.6457772254943848
training loss = 1.4875344038009644 24500
val loss = 2.6316075325012207
training loss = 1.4874398708343506 24600
val loss = 2.6413345336914062
training loss = 1.4873319864273071 24700
val loss = 2.6456570625305176
training loss = 1.489790439605713 24800
val loss = 2.554741144180298
training loss = 1.48720383644104 24900
val loss = 2.6461715698242188
training loss = 1.4946503639221191 25000
val loss = 2.4899089336395264
training loss = 1.4870729446411133 25100
val loss = 2.640606641769409
training loss = 1.4869680404663086 25200
val loss = 2.6459593772888184
training loss = 1.4869468212127686 25300
val loss = 2.641751289367676
training loss = 1.4868531227111816 25400
val loss = 2.6454315185546875
training loss = 1.4974302053451538 25500
val loss = 2.8489174842834473
training loss = 1.48674738407135 25600
val loss = 2.6378297805786133
training loss = 1.4866361618041992 25700
val loss = 2.645379066467285
training loss = 1.4948251247406006 25800
val loss = 2.4839847087860107
training loss = 1.4865388870239258 25900
val loss = 2.6443116664886475
training loss = 1.4864411354064941 26000
val loss = 2.645263195037842
training loss = 1.4949936866760254 26100
val loss = 2.8254971504211426
training loss = 1.4863319396972656 26200
val loss = 2.6464803218841553
training loss = 1.4862406253814697 26300
val loss = 2.639530658721924
training loss = 1.4863226413726807 26400
val loss = 2.6251156330108643
training loss = 1.4861176013946533 26500
val loss = 2.6444084644317627
training loss = 1.486365556716919 26600
val loss = 2.6147537231445312
training loss = 1.4860097169876099 26700
val loss = 2.6456871032714844
training loss = 1.5051344633102417 26800
val loss = 2.922950506210327
training loss = 1.4858949184417725 26900
val loss = 2.646540641784668
training loss = 1.4857977628707886 27000
val loss = 2.6446354389190674
training loss = 1.4863975048065186 27100
val loss = 2.598848819732666
training loss = 1.4857040643692017 27200
val loss = 2.6444814205169678
training loss = 1.4856114387512207 27300
val loss = 2.6483919620513916
training loss = 1.485724687576294 27400
val loss = 2.6232151985168457
reduced chi^2 level 2 = 1.4855934381484985
Constrained alpha: 1.8406795263290405
Constrained beta: 2.502119541168213
Constrained gamma: 13.099854469299316
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 920.7215,  830.4587,  912.4229,  921.6914, 1016.3423, 1054.1093,
        1125.1631, 1124.3204, 1136.8816, 1136.3508, 1208.6650, 1210.6057,
        1209.0757, 1284.3192, 1315.8317, 1389.6632, 1408.8528, 1390.0759,
        1522.2572, 1515.3596, 1618.1740, 1605.0403, 1623.2042, 1684.8710,
        1672.7910, 1689.8759, 1558.4229, 1810.0741, 1735.3650, 1652.1486,
        1594.0057, 1735.5573, 1710.6760, 1713.5363, 1747.6602, 1825.9622,
        1654.2104, 1541.8351, 1593.6228, 1592.2512, 1672.2850, 1514.8232,
        1483.4081, 1526.8619, 1381.9432, 1318.3070, 1239.3333, 1244.9301,
        1164.0895, 1168.8250, 1106.1432, 1003.9255,  952.5546,  912.9003,
         900.2275,  823.1264,  846.1736,  728.8145,  622.7375,  546.5267,
         562.2400,  483.6307,  396.4079,  407.3919,  348.6683,  316.1103,
         284.8198,  259.6282,  211.9362,  160.4238,  183.3684,  152.8100,
         143.0024,   92.4317,   87.2658,   73.4578,   48.3284,   38.6708,
          22.0574,   41.1309,   18.1724,   35.2004,   43.6667])]
2699.8315037670804
0.7915946937346546 14.37506402789075 59.50573504675567
val isze = 8
idinces = [62 23 24 52 18 63 48 41 28 45 15 49 30 36 77 10 67 74 69 19 34 40 13 54
 82 26  6 17  2 42 14 21 29 46  8  3 66 71 75 27 56 11 78 53 79 59 50 43
 22 51 16 55  1 32 81 68  0 57 39 25  7 20  5  9 33 64 37 31 38 61 35 72
 76 47 65 12 73 70  4 44 60 58 80]
we are doing training validation split
training loss = 89.05859375 100
val loss = 83.77726745605469
training loss = 8.335099220275879 200
val loss = 5.557964324951172
training loss = 8.042250633239746 300
val loss = 6.4923858642578125
training loss = 7.781980991363525 400
val loss = 6.446917533874512
training loss = 7.5135273933410645 500
val loss = 6.413153648376465
training loss = 7.251228332519531 600
val loss = 6.3978271484375
training loss = 7.007160663604736 700
val loss = 6.403377056121826
training loss = 6.790492534637451 800
val loss = 6.4295244216918945
training loss = 6.6068267822265625 900
val loss = 6.473288536071777
training loss = 6.457885265350342 1000
val loss = 6.5295329093933105
training loss = 6.341760635375977 1100
val loss = 6.5918097496032715
training loss = 6.253775596618652 1200
val loss = 6.653664588928223
training loss = 6.187807083129883 1300
val loss = 6.709652900695801
training loss = 6.137564659118652 1400
val loss = 6.756047248840332
training loss = 6.097597599029541 1500
val loss = 6.790986061096191
training loss = 6.063776969909668 1600
val loss = 6.814422607421875
training loss = 6.033337593078613 1700
val loss = 6.827374458312988
training loss = 6.0046186447143555 1800
val loss = 6.831671714782715
training loss = 5.976753234863281 1900
val loss = 6.829176902770996
training loss = 5.949349403381348 2000
val loss = 6.821745872497559
training loss = 5.922269344329834 2100
val loss = 6.810964584350586
training loss = 5.895488262176514 2200
val loss = 6.7980427742004395
training loss = 5.869018077850342 2300
val loss = 6.783802032470703
training loss = 5.842838764190674 2400
val loss = 6.768860340118408
training loss = 5.816889762878418 2500
val loss = 6.753558158874512
training loss = 5.791013240814209 2600
val loss = 6.737981796264648
training loss = 5.764908790588379 2700
val loss = 6.722088813781738
training loss = 5.738036632537842 2800
val loss = 6.70554256439209
training loss = 5.709397315979004 2900
val loss = 6.687489986419678
training loss = 5.677024841308594 3000
val loss = 6.666426658630371
training loss = 5.6365461349487305 3100
val loss = 6.6389384269714355
training loss = 5.576376438140869 3200
val loss = 6.596561908721924
training loss = 5.459647178649902 3300
val loss = 6.514401912689209
training loss = 5.177449703216553 3400
val loss = 6.316928386688232
training loss = 4.66318941116333 3500
val loss = 5.8618268966674805
training loss = 3.9793894290924072 3600
val loss = 5.119988918304443
training loss = 3.156411647796631 3700
val loss = 4.137331962585449
training loss = 2.530994176864624 3800
val loss = 3.262033224105835
training loss = 2.3379814624786377 3900
val loss = 2.8989479541778564
training loss = 2.309962511062622 4000
val loss = 2.8036065101623535
training loss = 2.300391912460327 4100
val loss = 2.762122392654419
training loss = 2.293212890625 4200
val loss = 2.7325079441070557
training loss = 2.287212371826172 4300
val loss = 2.707834243774414
training loss = 2.282099485397339 4400
val loss = 2.6866557598114014
training loss = 2.2777013778686523 4500
val loss = 2.66841983795166
training loss = 2.273883581161499 4600
val loss = 2.65278959274292
training loss = 2.2705376148223877 4700
val loss = 2.639420986175537
training loss = 2.267580986022949 4800
val loss = 2.628056764602661
training loss = 2.2649435997009277 4900
val loss = 2.6184520721435547
training loss = 2.2625625133514404 5000
val loss = 2.6103410720825195
training loss = 2.260218858718872 5100
val loss = 2.6014840602874756
training loss = 2.2578866481781006 5200
val loss = 2.594451427459717
training loss = 2.257622003555298 5300
val loss = 2.585132122039795
training loss = 2.2535030841827393 5400
val loss = 2.580065965652466
training loss = 2.2778522968292236 5500
val loss = 2.594633102416992
training loss = 2.2493691444396973 5600
val loss = 2.5670135021209717
training loss = 2.2473695278167725 5700
val loss = 2.561784505844116
training loss = 2.245462656021118 5800
val loss = 2.556258201599121
training loss = 2.2434980869293213 5900
val loss = 2.5504860877990723
training loss = 2.2422945499420166 6000
val loss = 2.543452739715576
training loss = 2.239816427230835 6100
val loss = 2.540463447570801
training loss = 2.2380716800689697 6200
val loss = 2.5359368324279785
training loss = 2.236353874206543 6300
val loss = 2.5326077938079834
training loss = 2.2345564365386963 6400
val loss = 2.5282392501831055
training loss = 2.233332872390747 6500
val loss = 2.5263266563415527
training loss = 2.231220245361328 6600
val loss = 2.5207622051239014
training loss = 2.229700803756714 6700
val loss = 2.5168378353118896
training loss = 2.227982759475708 6800
val loss = 2.5133731365203857
training loss = 2.2263264656066895 6900
val loss = 2.5114662647247314
training loss = 2.2259669303894043 7000
val loss = 2.512882947921753
training loss = 2.223139762878418 7100
val loss = 2.5057625770568848
training loss = 2.493485689163208 7200
val loss = 2.773054361343384
training loss = 2.2199971675872803 7300
val loss = 2.50068736076355
training loss = 2.2183964252471924 7400
val loss = 2.498256206512451
training loss = 2.2257978916168213 7500
val loss = 2.512532949447632
training loss = 2.2152693271636963 7600
val loss = 2.4933857917785645
training loss = 2.213653564453125 7700
val loss = 2.4915547370910645
training loss = 2.212200164794922 7800
val loss = 2.489651679992676
training loss = 2.2104909420013428 7900
val loss = 2.4869604110717773
training loss = 2.215876817703247 8000
val loss = 2.5005016326904297
training loss = 2.2072603702545166 8100
val loss = 2.4826581478118896
training loss = 2.2058157920837402 8200
val loss = 2.4800167083740234
training loss = 2.2040650844573975 8300
val loss = 2.4786667823791504
training loss = 2.2023704051971436 8400
val loss = 2.476356029510498
training loss = 2.2006635665893555 8500
val loss = 2.474217414855957
training loss = 2.1989550590515137 8600
val loss = 2.472144603729248
training loss = 2.1971614360809326 8700
val loss = 2.4702036380767822
training loss = 2.1984763145446777 8800
val loss = 2.474526882171631
training loss = 2.193582773208618 8900
val loss = 2.4654738903045654
training loss = 2.1916513442993164 9000
val loss = 2.463557243347168
training loss = 2.1898674964904785 9100
val loss = 2.4599123001098633
training loss = 2.1876723766326904 9200
val loss = 2.4584193229675293
training loss = 2.2071712017059326 9300
val loss = 2.489715814590454
training loss = 2.1833903789520264 9400
val loss = 2.452639102935791
training loss = 2.1810097694396973 9500
val loss = 2.450007200241089
training loss = 2.181431531906128 9600
val loss = 2.446272373199463
training loss = 2.1759066581726074 9700
val loss = 2.44327974319458
training loss = 2.174799680709839 9800
val loss = 2.443859338760376
training loss = 2.169991970062256 9900
val loss = 2.435612201690674
training loss = 2.166689157485962 10000
val loss = 2.43205189704895
training loss = 2.163980722427368 10100
val loss = 2.429175853729248
training loss = 2.1596102714538574 10200
val loss = 2.4232594966888428
training loss = 2.164588212966919 10300
val loss = 2.4333009719848633
training loss = 2.1519429683685303 10400
val loss = 2.4133248329162598
training loss = 2.147897958755493 10500
val loss = 2.4079031944274902
training loss = 2.1439034938812256 10600
val loss = 2.401155710220337
training loss = 2.139798641204834 10700
val loss = 2.3945727348327637
training loss = 2.198988914489746 10800
val loss = 2.455178737640381
training loss = 2.131356954574585 10900
val loss = 2.378592014312744
training loss = 2.126908779144287 11000
val loss = 2.3701186180114746
training loss = 2.125723123550415 11100
val loss = 2.363831043243408
training loss = 2.1177189350128174 11200
val loss = 2.3511974811553955
training loss = 2.112733840942383 11300
val loss = 2.341320037841797
training loss = 2.108182907104492 11400
val loss = 2.331181526184082
training loss = 2.102982521057129 11500
val loss = 2.3209495544433594
training loss = 2.097581386566162 11600
val loss = 2.3103742599487305
training loss = 2.0924062728881836 11700
val loss = 2.29960560798645
training loss = 2.086761951446533 11800
val loss = 2.288687229156494
training loss = 2.0888280868530273 11900
val loss = 2.2854819297790527
training loss = 2.075608253479004 12000
val loss = 2.2667481899261475
training loss = 2.0697288513183594 12100
val loss = 2.255819797515869
training loss = 2.0694804191589355 12200
val loss = 2.2519102096557617
training loss = 2.058505058288574 12300
val loss = 2.234461784362793
training loss = 2.0526654720306396 12400
val loss = 2.224022388458252
training loss = 2.06577730178833 12500
val loss = 2.2393462657928467
training loss = 2.041381597518921 12600
val loss = 2.20367431640625
training loss = 2.0357155799865723 12700
val loss = 2.1943752765655518
training loss = 2.030496120452881 12800
val loss = 2.183999538421631
training loss = 2.0250394344329834 12900
val loss = 2.175671339035034
training loss = 2.020507574081421 13000
val loss = 2.1686923503875732
training loss = 2.015223741531372 13100
val loss = 2.1593289375305176
training loss = 2.0160367488861084 13200
val loss = 2.1633875370025635
training loss = 2.0064661502838135 13300
val loss = 2.1451427936553955
training loss = 2.0608558654785156 13400
val loss = 2.2287611961364746
training loss = 1.9988477230072021 13500
val loss = 2.133049488067627
training loss = 1.9953572750091553 13600
val loss = 2.127756118774414
training loss = 1.992900013923645 13700
val loss = 2.126035690307617
training loss = 1.9894014596939087 13800
val loss = 2.1182150840759277
training loss = 1.9866573810577393 13900
val loss = 2.1144320964813232
training loss = 1.9844748973846436 14000
val loss = 2.111351490020752
training loss = 1.9821540117263794 14100
val loss = 2.106994867324829
training loss = 1.98456871509552 14200
val loss = 2.114406108856201
training loss = 1.9785388708114624 14300
val loss = 2.1006951332092285
training loss = 1.9768298864364624 14400
val loss = 2.098006010055542
training loss = 1.9787473678588867 14500
val loss = 2.1041476726531982
training loss = 1.9740058183670044 14600
val loss = 2.09271240234375
training loss = 1.9726343154907227 14700
val loss = 2.090282440185547
training loss = 1.9767383337020874 14800
val loss = 2.084865093231201
training loss = 1.9702703952789307 14900
val loss = 2.085463047027588
training loss = 2.006553888320923 15000
val loss = 2.1542840003967285
training loss = 1.9681177139282227 15100
val loss = 2.080056667327881
training loss = 1.9670130014419556 15200
val loss = 2.0783562660217285
training loss = 1.9684150218963623 15300
val loss = 2.0722174644470215
training loss = 1.9651050567626953 15400
val loss = 2.0737791061401367
training loss = 1.9640930891036987 15500
val loss = 2.0716638565063477
training loss = 1.963314414024353 15600
val loss = 2.0701818466186523
training loss = 1.9623357057571411 15700
val loss = 2.0670809745788574
training loss = 2.0966243743896484 15800
val loss = 2.278186321258545
training loss = 1.9606012105941772 15900
val loss = 2.0619688034057617
training loss = 1.9596666097640991 16000
val loss = 2.060420036315918
training loss = 1.9597023725509644 16100
val loss = 2.054628849029541
training loss = 1.9580425024032593 16200
val loss = 2.056089401245117
training loss = 1.9579052925109863 16300
val loss = 2.0507235527038574
training loss = 1.9565305709838867 16400
val loss = 2.052867889404297
training loss = 1.9556645154953003 16500
val loss = 2.0498838424682617
training loss = 1.9548472166061401 16600
val loss = 2.048593521118164
training loss = 1.9542192220687866 16700
val loss = 2.046959400177002
training loss = 1.9533624649047852 16800
val loss = 2.043835401535034
training loss = 1.9875203371047974 16900
val loss = 2.0551867485046387
training loss = 1.9519203901290894 17000
val loss = 2.0396251678466797
training loss = 1.9511418342590332 17100
val loss = 2.03798246383667
training loss = 1.9506067037582397 17200
val loss = 2.034169912338257
training loss = 1.9497976303100586 17300
val loss = 2.0343356132507324
training loss = 1.9490779638290405 17400
val loss = 2.0332345962524414
training loss = 1.9484647512435913 17500
val loss = 2.0307979583740234
training loss = 1.9477543830871582 17600
val loss = 2.028834342956543
training loss = 1.9762052297592163 17700
val loss = 2.0325465202331543
training loss = 1.946475625038147 17800
val loss = 2.025446891784668
training loss = 1.9458673000335693 17900
val loss = 2.0251576900482178
training loss = 1.9452908039093018 18000
val loss = 2.02311372756958
training loss = 1.9445687532424927 18100
val loss = 2.020211696624756
training loss = 1.9637231826782227 18200
val loss = 2.018850803375244
training loss = 1.9433842897415161 18300
val loss = 2.016880989074707
training loss = 1.9427348375320435 18400
val loss = 2.0156517028808594
training loss = 1.9423959255218506 18500
val loss = 2.011765480041504
training loss = 1.9415955543518066 18600
val loss = 2.0120716094970703
training loss = 2.0032055377960205 18700
val loss = 2.118997573852539
training loss = 1.9404932260513306 18800
val loss = 2.0088062286376953
training loss = 1.939874529838562 18900
val loss = 2.007352352142334
training loss = 1.9394551515579224 19000
val loss = 2.004359722137451
training loss = 1.9388222694396973 19100
val loss = 2.00447416305542
training loss = 1.9382210969924927 19200
val loss = 2.0027899742126465
training loss = 1.9378716945648193 19300
val loss = 1.9994962215423584
training loss = 1.9372036457061768 19400
val loss = 1.9998732805252075
training loss = 1.9366154670715332 19500
val loss = 1.9984002113342285
training loss = 1.9364248514175415 19600
val loss = 1.9999476671218872
training loss = 1.9356014728546143 19700
val loss = 1.9954421520233154
training loss = 2.4634969234466553 19800
val loss = 2.497101306915283
training loss = 1.9346232414245605 19900
val loss = 1.9931563138961792
training loss = 1.934061884880066 20000
val loss = 1.9911601543426514
training loss = 2.0960800647735596 20100
val loss = 2.250202178955078
training loss = 1.9330705404281616 20200
val loss = 1.9887094497680664
training loss = 1.9325082302093506 20300
val loss = 1.9867702722549438
training loss = 1.9447400569915771 20400
val loss = 2.0143215656280518
training loss = 1.9316641092300415 20500
val loss = 1.9843745231628418
training loss = 1.9311473369598389 20600
val loss = 1.9829442501068115
training loss = 1.9306124448776245 20700
val loss = 1.9814352989196777
training loss = 1.965752363204956 20800
val loss = 2.0493667125701904
training loss = 1.9296568632125854 20900
val loss = 1.97884202003479
training loss = 1.9291144609451294 21000
val loss = 1.977123498916626
training loss = 1.9294513463974 21100
val loss = 1.9728429317474365
training loss = 1.9282093048095703 21200
val loss = 1.9745577573776245
training loss = 1.9276729822158813 21300
val loss = 1.9728695154190063
training loss = 1.927586555480957 21400
val loss = 1.9749572277069092
training loss = 1.9267680644989014 21500
val loss = 1.970458745956421
training loss = 1.926409125328064 21600
val loss = 1.9669498205184937
training loss = 1.9259761571884155 21700
val loss = 1.9697383642196655
training loss = 1.9253795146942139 21800
val loss = 1.966465711593628
training loss = 1.9248535633087158 21900
val loss = 1.9648420810699463
training loss = 1.9249382019042969 22000
val loss = 1.9607940912246704
training loss = 1.9239689111709595 22100
val loss = 1.962402582168579
training loss = 1.9488803148269653 22200
val loss = 1.964914321899414
training loss = 1.9230841398239136 22300
val loss = 1.9606877565383911
training loss = 1.9225400686264038 22400
val loss = 1.9582027196884155
training loss = 1.9228253364562988 22500
val loss = 1.953526258468628
training loss = 1.9216426610946655 22600
val loss = 1.9556281566619873
training loss = 2.053114414215088 22700
val loss = 2.0503077507019043
training loss = 1.920786738395691 22800
val loss = 1.9524683952331543
training loss = 1.9202722311019897 22900
val loss = 1.9515876770019531
training loss = 2.0306966304779053 23000
val loss = 2.0265886783599854
training loss = 1.9193986654281616 23100
val loss = 1.9488842487335205
training loss = 1.918891191482544 23200
val loss = 1.9474908113479614
training loss = 1.9185174703598022 23300
val loss = 1.946317434310913
training loss = 1.9180164337158203 23400
val loss = 1.9448450803756714
training loss = 1.9177547693252563 23500
val loss = 1.9411253929138184
training loss = 1.9172612428665161 23600
val loss = 1.941316843032837
training loss = 1.9166768789291382 23700
val loss = 1.9409277439117432
training loss = 1.9235968589782715 23800
val loss = 1.9343023300170898
training loss = 1.9158655405044556 23900
val loss = 1.9374370574951172
training loss = 1.9153259992599487 24000
val loss = 1.9368246793746948
training loss = 1.944732666015625 24100
val loss = 1.99897301197052
training loss = 1.9144599437713623 24200
val loss = 1.9341201782226562
training loss = 1.9140779972076416 24300
val loss = 1.934621810913086
training loss = 1.9136536121368408 24400
val loss = 1.9331377744674683
training loss = 1.9131122827529907 24500
val loss = 1.9301668405532837
training loss = 1.9771928787231445 24600
val loss = 1.9607428312301636
training loss = 1.9122872352600098 24700
val loss = 1.9274333715438843
training loss = 1.9117923974990845 24800
val loss = 1.9260761737823486
training loss = 1.9167704582214355 24900
val loss = 1.9429069757461548
training loss = 1.9109389781951904 25000
val loss = 1.9235280752182007
training loss = 2.040306329727173 25100
val loss = 2.016993999481201
training loss = 1.9100735187530518 25200
val loss = 1.9210542440414429
training loss = 1.9350266456604004 25300
val loss = 1.9743506908416748
training loss = 1.9092344045639038 25400
val loss = 1.9190049171447754
training loss = 1.9087207317352295 25500
val loss = 1.9164478778839111
training loss = 1.9085547924041748 25600
val loss = 1.9174761772155762
training loss = 1.907891035079956 25700
val loss = 1.9139546155929565
training loss = 1.9147367477416992 25800
val loss = 1.903556227684021
training loss = 1.9070569276809692 25900
val loss = 1.911195158958435
training loss = 1.9065759181976318 26000
val loss = 1.9103772640228271
training loss = 1.9063777923583984 26100
val loss = 1.9113296270370483
training loss = 1.9057179689407349 26200
val loss = 1.907065749168396
training loss = 1.9053934812545776 26300
val loss = 1.9032835960388184
training loss = 1.904911756515503 26400
val loss = 1.904497742652893
training loss = 1.9044326543807983 26500
val loss = 1.9021978378295898
training loss = 1.9040707349777222 26600
val loss = 1.901756763458252
training loss = 1.9035834074020386 26700
val loss = 1.900193691253662
training loss = 1.9033700227737427 26800
val loss = 1.8969147205352783
training loss = 1.9027490615844727 26900
val loss = 1.8974988460540771
training loss = 2.146583318710327 27000
val loss = 2.260394811630249
training loss = 1.9019395112991333 27100
val loss = 1.894162654876709
training loss = 1.9014325141906738 27200
val loss = 1.8931032419204712
training loss = 1.9011880159378052 27300
val loss = 1.8920780420303345
training loss = 1.900598406791687 27400
val loss = 1.890399694442749
training loss = 1.9235185384750366 27500
val loss = 1.940344214439392
training loss = 1.8997539281845093 27600
val loss = 1.8883243799209595
training loss = 1.8992211818695068 27700
val loss = 1.8854727745056152
training loss = 1.8989754915237427 27800
val loss = 1.886472463607788
training loss = 1.8983441591262817 27900
val loss = 1.8827857971191406
training loss = 1.9032460451126099 28000
val loss = 1.895453691482544
training loss = 1.897468090057373 28100
val loss = 1.8798056840896606
training loss = 1.8969765901565552 28200
val loss = 1.8792169094085693
training loss = 1.8966072797775269 28300
val loss = 1.877733588218689
training loss = 1.8961169719696045 28400
val loss = 1.8744661808013916
training loss = 1.8959134817123413 28500
val loss = 1.874100923538208
training loss = 1.9045312404632568 28600
val loss = 1.8964712619781494
training loss = 1.8954293727874756 28700
val loss = 1.8731192350387573
training loss = 1.8951542377471924 28800
val loss = 1.872291088104248
training loss = 1.8948849439620972 28900
val loss = 1.871263027191162
training loss = 1.8945459127426147 29000
val loss = 1.870945930480957
training loss = 1.9566097259521484 29100
val loss = 1.9684454202651978
training loss = 1.893861174583435 29200
val loss = 1.8691798448562622
training loss = 1.8934861421585083 29300
val loss = 1.8682194948196411
training loss = 1.9022181034088135 29400
val loss = 1.8836982250213623
training loss = 1.8926849365234375 29500
val loss = 1.8662199974060059
training loss = 1.892256259918213 29600
val loss = 1.8649802207946777
training loss = 1.8927698135375977 29700
val loss = 1.8673744201660156
training loss = 1.8913886547088623 29800
val loss = 1.8626153469085693
training loss = 1.8909399509429932 29900
val loss = 1.8612499237060547
training loss = 1.8906879425048828 30000
val loss = 1.860951542854309
reduced chi^2 level 2 = 1.8913627862930298
Constrained alpha: 1.9456803798675537
Constrained beta: -0.0017871581949293613
Constrained gamma: 12.081424713134766
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 907.8326,  871.7430,  968.7775,  953.2635,  959.0348, 1011.2303,
        1114.6506, 1127.6517, 1168.4608, 1141.8649, 1171.7162, 1168.0720,
        1225.6405, 1312.4993, 1376.0298, 1406.4191, 1439.3954, 1403.8180,
        1536.2676, 1497.9651, 1567.4717, 1547.4137, 1660.3091, 1609.3539,
        1637.4725, 1694.9551, 1607.8514, 1737.1589, 1777.8490, 1630.0198,
        1715.1588, 1724.8735, 1652.9702, 1747.3717, 1723.2428, 1761.0176,
        1761.9536, 1657.5173, 1607.9360, 1628.3362, 1630.0332, 1627.4554,
        1504.6132, 1446.9344, 1361.7886, 1318.9379, 1252.5743, 1210.3018,
        1157.8710, 1172.3712, 1163.5127, 1012.0461,  919.2554,  953.5800,
         875.4584,  906.4150,  848.9042,  691.5300,  623.3411,  520.9567,
         532.8100,  456.9339,  394.3659,  363.2531,  335.4709,  339.9096,
         286.6958,  264.1786,  202.2626,  145.2725,  162.7117,  165.2445,
         150.4960,   99.6591,  106.1603,   60.5451,   58.6923,   40.1314,
          31.2306,   40.5375,   25.6587,   35.5759,   29.9750])]
2777.000087324196
1.5822655142648019 5.507066014559405 85.90970818677187
val isze = 8
idinces = [62 23 67 82 64 34 13 70  1 41 57  7 30 11 50 65 31 63 53 58 22 33 79 78
 32 73 59  5 71 29 80 48 36 77 35 69 10 38 43 51 76 52 44 27 25 47  9  3
 20 55 75 60 40 61 28 26 14  4 68 18 72 49 42 39 21 37  6 81  8 16 54 46
 19 17 12 56  0 66 24 15 45  2 74]
we are doing training validation split
training loss = 12.031622886657715 100
val loss = 14.88709545135498
training loss = 10.695895195007324 200
val loss = 12.511009216308594
training loss = 9.699560165405273 300
val loss = 11.227693557739258
training loss = 8.849838256835938 400
val loss = 10.057079315185547
training loss = 8.169238090515137 500
val loss = 9.05237865447998
training loss = 7.642410755157471 600
val loss = 8.223384857177734
training loss = 7.242651462554932 700
val loss = 7.5568461418151855
training loss = 6.942937850952148 800
val loss = 7.030807971954346
training loss = 6.719659805297852 900
val loss = 6.621245384216309
training loss = 6.553380489349365 1000
val loss = 6.305438041687012
training loss = 6.428483963012695 1100
val loss = 6.063136100769043
training loss = 6.332456111907959 1200
val loss = 5.876910209655762
training loss = 6.255161285400391 1300
val loss = 5.7320098876953125
training loss = 6.1881327629089355 1400
val loss = 5.616034507751465
training loss = 6.1239013671875 1500
val loss = 5.518362998962402
training loss = 6.055380344390869 1600
val loss = 5.429710388183594
training loss = 5.975319862365723 1700
val loss = 5.341399192810059
training loss = 5.875734329223633 1800
val loss = 5.244632720947266
training loss = 5.747004985809326 1900
val loss = 5.12900447845459
training loss = 5.576071739196777 2000
val loss = 4.980107307434082
training loss = 5.34351921081543 2100
val loss = 4.776096820831299
training loss = 5.0210652351379395 2200
val loss = 4.4846954345703125
training loss = 4.573888301849365 2300
val loss = 4.062047958374023
training loss = 3.98124361038208 2400
val loss = 3.461935520172119
training loss = 3.303119659423828 2500
val loss = 2.6882715225219727
training loss = 2.7341997623443604 2600
val loss = 1.8914597034454346
training loss = 2.4260330200195312 2700
val loss = 1.2978153228759766
training loss = 2.316767930984497 2800
val loss = 0.9682382345199585
training loss = 2.286501169204712 2900
val loss = 0.8123536109924316
training loss = 2.2770779132843018 3000
val loss = 0.7398082613945007
training loss = 2.2723546028137207 3100
val loss = 0.7027438879013062
training loss = 2.2688965797424316 3200
val loss = 0.6805567145347595
training loss = 2.265974521636963 3300
val loss = 0.6650845408439636
training loss = 2.2633843421936035 3400
val loss = 0.653239369392395
training loss = 2.26102614402771 3500
val loss = 0.6438535451889038
training loss = 2.2588422298431396 3600
val loss = 0.6364231109619141
training loss = 2.2567827701568604 3700
val loss = 0.6306414604187012
training loss = 2.254807233810425 3800
val loss = 0.6262936592102051
training loss = 2.2528843879699707 3900
val loss = 0.623172402381897
training loss = 2.250985860824585 4000
val loss = 0.6210945844650269
training loss = 2.2490978240966797 4100
val loss = 0.6198875904083252
training loss = 2.247201681137085 4200
val loss = 0.6194034814834595
training loss = 2.2453010082244873 4300
val loss = 0.6194922924041748
training loss = 2.2433903217315674 4400
val loss = 0.6200410723686218
training loss = 2.2416980266571045 4500
val loss = 0.622706413269043
training loss = 2.2398650646209717 4600
val loss = 0.6221405863761902
training loss = 2.2458090782165527 4700
val loss = 0.6391223073005676
training loss = 2.2367818355560303 4800
val loss = 0.6247978210449219
training loss = 2.321641206741333 4900
val loss = 0.6476280689239502
training loss = 2.2341768741607666 5000
val loss = 0.6274980306625366
training loss = 2.233381509780884 5100
val loss = 0.6310781240463257
training loss = 2.2320384979248047 5200
val loss = 0.6295023560523987
training loss = 2.2311153411865234 5300
val loss = 0.6311831474304199
training loss = 2.231503963470459 5400
val loss = 0.6293295621871948
training loss = 2.2294487953186035 5500
val loss = 0.6333351731300354
training loss = 2.235769510269165 5600
val loss = 0.6489744782447815
training loss = 2.2279646396636963 5700
val loss = 0.6348414421081543
training loss = 2.227313280105591 5800
val loss = 0.6361571550369263
training loss = 2.2271206378936768 5900
val loss = 0.6355863809585571
training loss = 2.2259747982025146 6000
val loss = 0.6375806331634521
training loss = 2.2254538536071777 6100
val loss = 0.637701690196991
training loss = 2.2247700691223145 6200
val loss = 0.6382753252983093
training loss = 2.224175453186035 6300
val loss = 0.6395454406738281
training loss = 2.225022792816162 6400
val loss = 0.6456789970397949
training loss = 2.222963809967041 6500
val loss = 0.6404454708099365
training loss = 2.222477436065674 6600
val loss = 0.6411678194999695
training loss = 2.222029447555542 6700
val loss = 0.6431118249893188
training loss = 2.2213523387908936 6800
val loss = 0.6418001055717468
training loss = 2.241219997406006 6900
val loss = 0.6430754661560059
training loss = 2.2203288078308105 7000
val loss = 0.6425222754478455
training loss = 2.2199065685272217 7100
val loss = 0.6429985165596008
training loss = 2.2227418422698975 7200
val loss = 0.6516493558883667
training loss = 2.2189412117004395 7300
val loss = 0.64357590675354
training loss = 2.218571901321411 7400
val loss = 0.6439834833145142
training loss = 2.2201766967773438 7500
val loss = 0.6414371728897095
training loss = 2.2177083492279053 7600
val loss = 0.6444705128669739
training loss = 2.2173869609832764 7700
val loss = 0.6449527144432068
training loss = 2.2170722484588623 7800
val loss = 0.6461941599845886
training loss = 2.2166616916656494 7900
val loss = 0.6454786062240601
training loss = 2.2183446884155273 8000
val loss = 0.6517940163612366
training loss = 2.2159881591796875 8100
val loss = 0.6460101008415222
training loss = 2.2157392501831055 8200
val loss = 0.646621823310852
training loss = 2.215437173843384 8300
val loss = 0.6475499272346497
training loss = 2.215101957321167 8400
val loss = 0.6469562649726868
training loss = 2.2183732986450195 8500
val loss = 0.6447141170501709
training loss = 2.2145562171936035 8600
val loss = 0.6475536823272705
training loss = 2.2143428325653076 8700
val loss = 0.6479169726371765
training loss = 2.2230875492095947 8800
val loss = 0.6620311141014099
training loss = 2.213829517364502 8900
val loss = 0.6484719514846802
training loss = 2.2136435508728027 9000
val loss = 0.6488332748413086
training loss = 2.215974807739258 9100
val loss = 0.6473063230514526
training loss = 2.2131643295288086 9200
val loss = 0.6494399309158325
training loss = 2.212994337081909 9300
val loss = 0.6497946977615356
training loss = 2.215458631515503 9400
val loss = 0.6480036973953247
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 922.5724,  831.6234,  883.2466,  942.5106,  983.4250, 1122.6259,
        1123.2198, 1137.7263, 1229.3982, 1170.6930, 1211.6548, 1240.1219,
        1282.9047, 1269.6770, 1322.3402, 1414.9623, 1476.1324, 1383.9615,
        1578.7224, 1438.5220, 1606.9247, 1570.5474, 1616.5385, 1650.5885,
        1649.7308, 1722.7964, 1682.7607, 1652.7214, 1749.8479, 1774.4381,
        1640.5156, 1690.9515, 1643.9353, 1775.4762, 1677.7048, 1836.4486,
        1612.7825, 1609.4595, 1666.7129, 1584.8431, 1593.3024, 1585.8080,
        1462.5594, 1525.3367, 1400.9061, 1311.5168, 1245.0457, 1289.2231,
        1184.0883, 1133.4647, 1047.0928,  989.5893,  977.6149,  925.2239,
         898.4562,  881.0049,  857.1542,  708.4851,  582.6638,  556.8571,
         580.3264,  483.4393,  454.1509,  398.5032,  343.9699,  328.0424,
         300.8815,  240.1774,  227.5252,  175.6708,  151.6182,  143.3404,
         138.0306,   88.5794,   92.6817,   65.1564,   40.7294,   43.3666,
          25.8492,   50.0158,   19.8761,   38.6578,   21.6941])]
2823.8750350632417
1.0810082190165398 8.893006697940143 72.25961538669796
val isze = 8
idinces = [60 58 75 18  5 51 65 71 69 28 23 73 62  8 13 39 61 25 44 81 49 41 21 78
 57 47 55 59 34 82 56 26 30 66 67 72 53 32 79 12  9 40 74 36  2 37 64 29
 15 35 70 17 11  0 80  7 48 77 20 33 22 38 19  6 50 63 24  1 43 46 54  4
 52 76 42  3 27 10 14 31 16 68 45]
we are doing training validation split
training loss = 7.835506439208984 100
val loss = 5.145157814025879
training loss = 5.71435546875 200
val loss = 6.550053596496582
training loss = 5.6524858474731445 300
val loss = 6.587657928466797
training loss = 5.591341018676758 400
val loss = 6.64642858505249
training loss = 5.535011291503906 500
val loss = 6.712247848510742
training loss = 5.485442638397217 600
val loss = 6.781999588012695
training loss = 5.443121910095215 700
val loss = 6.852674961090088
training loss = 5.407657146453857 800
val loss = 6.921420574188232
training loss = 5.378170490264893 900
val loss = 6.985899925231934
training loss = 5.353600025177002 1000
val loss = 7.04427433013916
training loss = 5.332862854003906 1100
val loss = 7.095217227935791
training loss = 5.314974308013916 1200
val loss = 7.137992858886719
training loss = 5.299091815948486 1300
val loss = 7.172260761260986
training loss = 5.2845330238342285 1400
val loss = 7.198153495788574
training loss = 5.270754337310791 1500
val loss = 7.216005802154541
training loss = 5.257342338562012 1600
val loss = 7.226412773132324
training loss = 5.243982791900635 1700
val loss = 7.23000955581665
training loss = 5.2304253578186035 1800
val loss = 7.227555751800537
training loss = 5.216463565826416 1900
val loss = 7.219671726226807
training loss = 5.201900005340576 2000
val loss = 7.206930160522461
training loss = 5.1865129470825195 2100
val loss = 7.18974494934082
training loss = 5.170043468475342 2200
val loss = 7.168237209320068
training loss = 5.152137756347656 2300
val loss = 7.142239570617676
training loss = 5.132315635681152 2400
val loss = 7.11124324798584
training loss = 5.109919548034668 2500
val loss = 7.07427453994751
training loss = 5.0840349197387695 2600
val loss = 7.029782772064209
training loss = 5.053437232971191 2700
val loss = 6.975628852844238
training loss = 5.016502380371094 2800
val loss = 6.90906286239624
training loss = 4.971059799194336 2900
val loss = 6.826700687408447
training loss = 4.914023399353027 3000
val loss = 6.724191665649414
training loss = 4.840541362762451 3100
val loss = 6.5950140953063965
training loss = 4.742313861846924 3200
val loss = 6.427810192108154
training loss = 4.60474967956543 3300
val loss = 6.201821804046631
training loss = 4.402759075164795 3400
val loss = 5.880617618560791
training loss = 4.096612930297852 3500
val loss = 5.407790660858154
training loss = 3.6391117572784424 3600
val loss = 4.727990627288818
training loss = 3.0425865650177 3700
val loss = 3.9102556705474854
training loss = 2.5215139389038086 3800
val loss = 3.318268299102783
training loss = 2.2874557971954346 3900
val loss = 3.1661484241485596
training loss = 2.232741355895996 4000
val loss = 3.1911377906799316
training loss = 2.221869945526123 4100
val loss = 3.2110204696655273
training loss = 2.217515468597412 4200
val loss = 3.212289810180664
training loss = 2.214280605316162 4300
val loss = 3.2069711685180664
training loss = 2.211451292037964 4400
val loss = 3.200502872467041
training loss = 2.208883762359619 4500
val loss = 3.194352626800537
training loss = 2.2065343856811523 4600
val loss = 3.188793897628784
training loss = 2.2043704986572266 4700
val loss = 3.1838698387145996
training loss = 2.202375888824463 4800
val loss = 3.1795430183410645
training loss = 2.200530767440796 4900
val loss = 3.1758131980895996
training loss = 2.1989896297454834 5000
val loss = 3.181736946105957
training loss = 2.1971991062164307 5100
val loss = 3.1669259071350098
training loss = 2.195723056793213 5200
val loss = 3.1674280166625977
training loss = 2.1942973136901855 5300
val loss = 3.1653590202331543
training loss = 2.192998170852661 5400
val loss = 3.162564277648926
training loss = 2.1925370693206787 5500
val loss = 3.1820411682128906
training loss = 2.190511465072632 5600
val loss = 3.1582789421081543
training loss = 2.2303924560546875 5700
val loss = 3.340703248977661
training loss = 2.1881706714630127 5800
val loss = 3.1540470123291016
training loss = 2.187117099761963 5900
val loss = 3.152566432952881
training loss = 2.186028242111206 6000
val loss = 3.145695447921753
training loss = 2.1849749088287354 6100
val loss = 3.1488380432128906
training loss = 2.217319965362549 6200
val loss = 3.025609016418457
training loss = 2.1829259395599365 6300
val loss = 3.1463189125061035
training loss = 2.1819865703582764 6400
val loss = 3.1458730697631836
training loss = 2.1810028553009033 6500
val loss = 3.1358096599578857
training loss = 2.1800174713134766 6600
val loss = 3.1409239768981934
training loss = 2.204873561859131 6700
val loss = 3.029081344604492
training loss = 2.1781442165374756 6800
val loss = 3.136780261993408
training loss = 2.1772749423980713 6900
val loss = 3.134305715560913
training loss = 2.1763436794281006 7000
val loss = 3.137925386428833
training loss = 2.17547607421875 7100
val loss = 3.1337509155273438
training loss = 2.1745970249176025 7200
val loss = 3.136648654937744
training loss = 2.173729419708252 7300
val loss = 3.13128662109375
training loss = 2.2847208976745605 7400
val loss = 2.9386110305786133
training loss = 2.1720354557037354 7500
val loss = 3.1300408840179443
training loss = 2.171236515045166 7600
val loss = 3.129394054412842
training loss = 2.170382022857666 7700
val loss = 3.128286361694336
training loss = 2.16959285736084 7800
val loss = 3.1251182556152344
training loss = 2.17000412940979 7900
val loss = 3.096703052520752
training loss = 2.1679935455322266 8000
val loss = 3.1228175163269043
training loss = 2.2779650688171387 8100
val loss = 2.932192325592041
training loss = 2.1664278507232666 8200
val loss = 3.122199535369873
training loss = 2.165682792663574 8300
val loss = 3.119304656982422
training loss = 2.1666722297668457 8400
val loss = 3.0877254009246826
training loss = 2.1641650199890137 8500
val loss = 3.1170473098754883
training loss = 2.1634414196014404 8600
val loss = 3.117156744003296
training loss = 2.1627511978149414 8700
val loss = 3.1075429916381836
training loss = 2.161968469619751 8800
val loss = 3.113823175430298
training loss = 2.2259950637817383 8900
val loss = 3.350106954574585
training loss = 2.160527467727661 9000
val loss = 3.114598274230957
training loss = 2.1598260402679443 9100
val loss = 3.110804319381714
training loss = 2.1609368324279785 9200
val loss = 3.1433987617492676
training loss = 2.1584079265594482 9300
val loss = 3.108475685119629
training loss = 2.298170566558838 9400
val loss = 3.4871976375579834
training loss = 2.1570043563842773 9500
val loss = 3.107280731201172
training loss = 2.1563358306884766 9600
val loss = 3.1060218811035156
training loss = 2.156147003173828 9700
val loss = 3.0871922969818115
training loss = 2.1549618244171143 9800
val loss = 3.103835105895996
training loss = 2.172905445098877 9900
val loss = 3.221452236175537
training loss = 2.153597116470337 10000
val loss = 3.1007423400878906
training loss = 2.152941942214966 10100
val loss = 3.100663900375366
training loss = 2.1533219814300537 10200
val loss = 3.125544786453247
training loss = 2.1515982151031494 10300
val loss = 3.098560094833374
training loss = 2.319028377532959 10400
val loss = 3.518056869506836
training loss = 2.15024995803833 10500
val loss = 3.096320152282715
training loss = 2.1496148109436035 10600
val loss = 3.0956244468688965
training loss = 2.2319300174713135 10700
val loss = 2.929748296737671
training loss = 2.148277521133423 10800
val loss = 3.093747615814209
training loss = 2.1476285457611084 10900
val loss = 3.0923078060150146
training loss = 2.148061990737915 11000
val loss = 3.1176841259002686
training loss = 2.146289825439453 11100
val loss = 3.09002685546875
training loss = 2.286623954772949 11200
val loss = 2.8979856967926025
training loss = 2.1449413299560547 11300
val loss = 3.090059757232666
training loss = 2.1442673206329346 11400
val loss = 3.086564779281616
training loss = 2.1435396671295166 11500
val loss = 3.0837998390197754
training loss = 2.1428780555725098 11600
val loss = 3.0839500427246094
training loss = 2.219205379486084 11700
val loss = 2.9199602603912354
training loss = 2.141448974609375 11800
val loss = 3.083510398864746
training loss = 2.140716314315796 11900
val loss = 3.0798826217651367
training loss = 2.141070604324341 12000
val loss = 3.104616641998291
training loss = 2.1391894817352295 12100
val loss = 3.076723575592041
training loss = 2.138411521911621 12200
val loss = 3.0751404762268066
training loss = 2.1584229469299316 12300
val loss = 3.198936939239502
training loss = 2.136672019958496 12400
val loss = 3.0713651180267334
training loss = 2.135725498199463 12500
val loss = 3.069875478744507
training loss = 2.1349871158599854 12600
val loss = 3.0802738666534424
training loss = 2.133671760559082 12700
val loss = 3.064998149871826
training loss = 2.132483720779419 12800
val loss = 3.0627384185791016
training loss = 2.1311347484588623 12900
val loss = 3.059246063232422
training loss = 2.1296937465667725 13000
val loss = 3.0571041107177734
training loss = 2.174283266067505 13100
val loss = 3.2502012252807617
training loss = 2.1261239051818848 13200
val loss = 3.052060127258301
training loss = 2.12394380569458 13300
val loss = 3.0478875637054443
training loss = 2.1214799880981445 13400
val loss = 3.043276786804199
training loss = 2.1188390254974365 13500
val loss = 3.0427143573760986
training loss = 2.1158981323242188 13600
val loss = 3.046884536743164
training loss = 2.112701892852783 13700
val loss = 3.040384292602539
training loss = 2.1094653606414795 13800
val loss = 3.0393831729888916
training loss = 2.1083836555480957 13900
val loss = 3.0746099948883057
training loss = 2.102971315383911 14000
val loss = 3.038088321685791
training loss = 2.0998337268829346 14100
val loss = 3.047579288482666
training loss = 2.09639048576355 14200
val loss = 3.029848575592041
training loss = 2.092947244644165 14300
val loss = 3.0331368446350098
training loss = 2.089723587036133 14400
val loss = 3.0417704582214355
training loss = 2.0859107971191406 14500
val loss = 3.026979923248291
training loss = 2.0907723903656006 14600
val loss = 2.9584851264953613
training loss = 2.0784003734588623 14700
val loss = 3.023383617401123
training loss = 2.074398994445801 14800
val loss = 3.0160858631134033
training loss = 2.07236385345459 14900
val loss = 3.0469441413879395
training loss = 2.066120147705078 15000
val loss = 3.0095596313476562
training loss = 2.090353012084961 15100
val loss = 2.9025046825408936
training loss = 2.0573198795318604 15200
val loss = 3.002023458480835
training loss = 2.052696704864502 15300
val loss = 2.9976818561553955
training loss = 2.0482614040374756 15400
val loss = 3.004298686981201
training loss = 2.0435168743133545 15500
val loss = 2.9948973655700684
training loss = 2.053218364715576 15600
val loss = 2.9103457927703857
training loss = 2.034213066101074 15700
val loss = 2.991156578063965
training loss = 2.0294442176818848 15800
val loss = 2.986398220062256
training loss = 2.0249760150909424 15900
val loss = 2.9846749305725098
training loss = 2.020455837249756 16000
val loss = 2.9866275787353516
training loss = 2.143190383911133 16100
val loss = 3.337419033050537
training loss = 2.0118348598480225 16200
val loss = 2.9839820861816406
training loss = 2.0076582431793213 16300
val loss = 2.9848999977111816
training loss = 2.0132639408111572 16400
val loss = 3.065683126449585
training loss = 1.999757170677185 16500
val loss = 2.984069585800171
training loss = 1.9965019226074219 16600
val loss = 3.0030343532562256
training loss = 1.9925432205200195 16700
val loss = 2.9905359745025635
training loss = 1.9890832901000977 16800
val loss = 2.9862208366394043
training loss = 1.9858968257904053 16900
val loss = 2.988645315170288
training loss = 1.98292076587677 17000
val loss = 2.9875056743621826
training loss = 1.980913519859314 17100
val loss = 2.966665744781494
training loss = 1.9774301052093506 17200
val loss = 2.9857263565063477
training loss = 1.9748785495758057 17300
val loss = 2.9911086559295654
training loss = 2.1207334995269775 17400
val loss = 3.3633642196655273
training loss = 1.9702372550964355 17500
val loss = 2.993408203125
training loss = 1.9680593013763428 17600
val loss = 2.9950270652770996
training loss = 1.9661141633987427 17700
val loss = 2.9994096755981445
training loss = 1.9642181396484375 17800
val loss = 2.9979310035705566
training loss = 1.977213978767395 17900
val loss = 2.9302098751068115
training loss = 1.9607540369033813 18000
val loss = 3.0015106201171875
training loss = 1.9591162204742432 18100
val loss = 3.0029184818267822
training loss = 1.9578896760940552 18200
val loss = 3.016474723815918
training loss = 1.9561622142791748 18300
val loss = 3.0063982009887695
training loss = 2.046823024749756 18400
val loss = 2.855400323867798
training loss = 1.9534554481506348 18500
val loss = 3.0077333450317383
training loss = 1.9521383047103882 18600
val loss = 3.011589288711548
training loss = 1.9512982368469238 18700
val loss = 3.000237464904785
training loss = 1.9497605562210083 18800
val loss = 3.0149571895599365
training loss = 2.263989210128784 18900
val loss = 3.605246067047119
training loss = 1.9475220441818237 19000
val loss = 3.0197949409484863
training loss = 1.9464223384857178 19100
val loss = 3.0205256938934326
training loss = 1.9549809694290161 19200
val loss = 3.096269369125366
training loss = 1.9443520307540894 19300
val loss = 3.024949073791504
training loss = 1.9433038234710693 19400
val loss = 3.0262041091918945
training loss = 1.9423418045043945 19500
val loss = 3.029155731201172
training loss = 1.94135582447052 19600
val loss = 3.0302746295928955
training loss = 2.1013689041137695 19700
val loss = 2.8676810264587402
training loss = 1.9394495487213135 19800
val loss = 3.0342118740081787
training loss = 1.9384799003601074 19900
val loss = 3.036775588989258
training loss = 1.9398938417434692 20000
val loss = 3.0720112323760986
training loss = 1.9366943836212158 20100
val loss = 3.0407509803771973
training loss = 1.9357552528381348 20200
val loss = 3.0431740283966064
training loss = 1.9348840713500977 20300
val loss = 3.044342517852783
training loss = 1.9339815378189087 20400
val loss = 3.0476632118225098
training loss = 2.075862169265747 20500
val loss = 3.3992011547088623
training loss = 1.9322673082351685 20600
val loss = 3.0505166053771973
training loss = 1.9313814640045166 20700
val loss = 3.054832696914673
training loss = 1.9387931823730469 20800
val loss = 3.1191463470458984
training loss = 1.9296802282333374 20900
val loss = 3.059709310531616
training loss = 2.3582205772399902 21000
val loss = 3.7726316452026367
reduced chi^2 level 2 = 1.928189754486084
Constrained alpha: 1.8480284214019775
Constrained beta: 2.5231845378875732
Constrained gamma: 14.553315162658691
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 866.7248,  831.9734,  898.1335,  962.1204,  978.3112, 1022.2104,
        1065.3433, 1085.6986, 1169.7144, 1222.6062, 1198.9227, 1158.8024,
        1273.0396, 1234.7430, 1377.7267, 1383.1565, 1432.1826, 1471.5154,
        1521.9932, 1530.0413, 1603.5466, 1603.7433, 1498.7218, 1614.0032,
        1625.9180, 1725.7952, 1590.8914, 1722.8357, 1736.1420, 1683.6803,
        1709.0603, 1708.7915, 1690.0450, 1761.0624, 1729.3248, 1719.0500,
        1692.0001, 1573.3978, 1701.8888, 1654.9323, 1681.6157, 1594.9758,
        1493.9534, 1489.0704, 1337.0941, 1310.3522, 1287.6996, 1276.4244,
        1156.0258, 1199.6416, 1101.9738,  965.8031,  917.9169,  968.7405,
         902.0654,  867.6683,  803.0811,  708.4709,  591.4794,  554.8902,
         568.1667,  444.0283,  463.9778,  377.8765,  355.2406,  367.4089,
         276.9479,  252.2455,  220.2157,  190.7190,  198.5302,  122.9914,
         144.5715,  119.8129,   96.0948,   67.6246,   48.9644,   42.8190,
          37.6853,   48.4866,   29.5431,   39.5756,   32.9471])]
2709.163273638669
1.32033877553692 13.325271028138442 22.870148558469605
val isze = 8
idinces = [36 67 28 37 56 26 33  9 78 77 81 68 18 79 70 15 64 54 75 24 65  7 20 30
 17 69 32  4  5 63 76 52 57 66 21 27 59 71 35 16  1 45 61 29 22 25 46 13
 38 50 34 10 72 53 31 14 82  2 41 55  8 42 62 47 43 48 12  0 23 44 74 40
 80 58 60 11 51  3 19 73 39 49  6]
we are doing training validation split
training loss = 113.9948959350586 100
val loss = 103.26264953613281
training loss = 8.00438404083252 200
val loss = 3.0393528938293457
training loss = 7.965357303619385 300
val loss = 3.0422935485839844
training loss = 7.930809020996094 400
val loss = 3.0740034580230713
training loss = 7.8943328857421875 500
val loss = 3.1097912788391113
training loss = 7.857019424438477 600
val loss = 3.1484508514404297
training loss = 7.819540977478027 700
val loss = 3.1885600090026855
training loss = 7.782238006591797 800
val loss = 3.228485107421875
training loss = 7.745176792144775 900
val loss = 3.266822576522827
training loss = 7.708245754241943 1000
val loss = 3.3020315170288086
training loss = 7.671219348907471 1100
val loss = 3.3330674171447754
training loss = 7.6338348388671875 1200
val loss = 3.3590195178985596
training loss = 7.595848083496094 1300
val loss = 3.379512071609497
training loss = 7.557072162628174 1400
val loss = 3.3943464756011963
training loss = 7.5173845291137695 1500
val loss = 3.40370512008667
training loss = 7.476749897003174 1600
val loss = 3.4082376956939697
training loss = 7.435206890106201 1700
val loss = 3.4082372188568115
training loss = 7.392856597900391 1800
val loss = 3.4044764041900635
training loss = 7.349865913391113 1900
val loss = 3.3977210521698
training loss = 7.306453704833984 2000
val loss = 3.3885834217071533
training loss = 7.262872695922852 2100
val loss = 3.3776676654815674
training loss = 7.219414234161377 2200
val loss = 3.3656442165374756
training loss = 7.1763739585876465 2300
val loss = 3.3529393672943115
training loss = 7.134040832519531 2400
val loss = 3.340045690536499
training loss = 7.092639923095703 2500
val loss = 3.3274221420288086
training loss = 7.052306652069092 2600
val loss = 3.3154525756835938
training loss = 7.0129852294921875 2700
val loss = 3.304100513458252
training loss = 6.974339485168457 2800
val loss = 3.293999195098877
training loss = 6.935548782348633 2900
val loss = 3.2849552631378174
training loss = 6.894846439361572 3000
val loss = 3.2767555713653564
training loss = 6.84823751449585 3100
val loss = 3.269550323486328
training loss = 6.785383701324463 3200
val loss = 3.2629711627960205
training loss = 6.676146984100342 3300
val loss = 3.2583117485046387
training loss = 6.4499335289001465 3400
val loss = 3.258039951324463
training loss = 6.05305814743042 3500
val loss = 3.2048325538635254
training loss = 5.360710620880127 3600
val loss = 2.9630894660949707
training loss = 3.999647617340088 3700
val loss = 2.493072509765625
training loss = 2.6770107746124268 3800
val loss = 1.983582854270935
training loss = 2.2234091758728027 3900
val loss = 1.9789925813674927
training loss = 2.125319004058838 4000
val loss = 2.0662879943847656
training loss = 2.0670619010925293 4100
val loss = 2.112757682800293
training loss = 2.022221565246582 4200
val loss = 2.151520252227783
training loss = 1.9868077039718628 4300
val loss = 2.189239978790283
training loss = 1.9583920240402222 4400
val loss = 2.2255735397338867
training loss = 1.93525230884552 4500
val loss = 2.259845733642578
training loss = 1.91614830493927 4600
val loss = 2.291659355163574
training loss = 1.9001656770706177 4700
val loss = 2.3208348751068115
training loss = 1.8866251707077026 4800
val loss = 2.3472957611083984
training loss = 1.8750131130218506 4900
val loss = 2.371119976043701
training loss = 1.864939570426941 5000
val loss = 2.3923745155334473
training loss = 1.8560986518859863 5100
val loss = 2.411153554916382
training loss = 1.8561755418777466 5200
val loss = 2.401686191558838
training loss = 1.8419106006622314 5300
val loss = 2.440214157104492
training loss = 1.8359898328781128 5400
val loss = 2.449864387512207
training loss = 1.830916404724121 5500
val loss = 2.4587600231170654
training loss = 1.8260990381240845 5600
val loss = 2.4691410064697266
training loss = 1.8254814147949219 5700
val loss = 2.460967540740967
training loss = 1.8176692724227905 5800
val loss = 2.48260498046875
training loss = 1.8197147846221924 5900
val loss = 2.5205025672912598
training loss = 1.810259222984314 6000
val loss = 2.493316173553467
training loss = 1.8068009614944458 6100
val loss = 2.4973669052124023
training loss = 1.8071222305297852 6200
val loss = 2.4892635345458984
training loss = 1.8004339933395386 6300
val loss = 2.5063066482543945
training loss = 1.7972869873046875 6400
val loss = 2.5100297927856445
training loss = 1.7949070930480957 6500
val loss = 2.5224971771240234
training loss = 1.7912256717681885 6600
val loss = 2.5183420181274414
training loss = 1.8307759761810303 6700
val loss = 2.6362128257751465
training loss = 1.7850199937820435 6800
val loss = 2.5272626876831055
training loss = 1.7816628217697144 6900
val loss = 2.5304458141326904
training loss = 1.7784020900726318 7000
val loss = 2.5322039127349854
training loss = 1.7745755910873413 7100
val loss = 2.537926197052002
training loss = 1.7983630895614624 7200
val loss = 2.6158089637756348
training loss = 1.7665165662765503 7300
val loss = 2.54081392288208
training loss = 1.7620793581008911 7400
val loss = 2.53946852684021
training loss = 1.7589499950408936 7500
val loss = 2.5314645767211914
training loss = 1.7532694339752197 7600
val loss = 2.532280921936035
training loss = 1.7489033937454224 7700
val loss = 2.5263373851776123
training loss = 1.7450124025344849 7800
val loss = 2.524690628051758
training loss = 1.7410812377929688 7900
val loss = 2.5190582275390625
training loss = 1.76217782497406 8000
val loss = 2.574756145477295
training loss = 1.7341679334640503 8100
val loss = 2.517148494720459
training loss = 1.7309544086456299 8200
val loss = 2.5176634788513184
training loss = 1.7279809713363647 8300
val loss = 2.5188581943511963
training loss = 1.7251209020614624 8400
val loss = 2.520991802215576
training loss = 1.7360845804214478 8500
val loss = 2.531240463256836
training loss = 1.7198292016983032 8600
val loss = 2.524916172027588
training loss = 1.7177927494049072 8700
val loss = 2.5254178047180176
training loss = 1.7150667905807495 8800
val loss = 2.5286355018615723
reduced chi^2 level 2 = 1.7140597105026245
Constrained alpha: 1.7639154195785522
Constrained beta: 2.9705114364624023
Constrained gamma: 17.365327835083008
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 876.2003,  854.6644,  937.8289, 1012.8146, 1032.8959, 1065.9348,
        1050.8812, 1122.4519, 1176.5802, 1135.2684, 1219.0454, 1188.2070,
        1233.0392, 1230.8638, 1354.3313, 1419.5778, 1358.4476, 1407.5171,
        1530.3521, 1522.0565, 1582.6975, 1509.2135, 1573.4379, 1568.8340,
        1710.8409, 1665.7429, 1626.9747, 1791.4391, 1793.3358, 1609.4681,
        1686.3732, 1707.9624, 1641.9766, 1682.2765, 1699.6143, 1799.1072,
        1652.4020, 1575.7816, 1661.8547, 1621.5349, 1630.2357, 1503.0902,
        1508.1206, 1553.9585, 1294.3197, 1337.5671, 1238.4757, 1203.7755,
        1199.6215, 1155.1188, 1061.6071,  982.7109,  930.0417,  938.2859,
         921.9996,  901.0870,  820.6832,  733.9071,  578.8082,  549.8791,
         531.9944,  461.9647,  421.4913,  419.3793,  387.9219,  365.6435,
         267.7453,  260.8872,  195.8749,  137.5281,  176.7064,  153.4922,
         136.2565,  100.4838,  100.6076,   67.3285,   53.8909,   44.0419,
          33.0934,   41.7299,   12.3285,   26.6914,   31.7364])]
3155.8816137757112
4.091312118009033 16.115186546843617 38.61171624800007
val isze = 8
idinces = [29 21 63 28 54  3 52 18 82 81 11 71 38 77  9 33 57 44 15 53 45 23 12 64
 35 49 59 32 72  2  8 39  5 42 47 78 25 48 46 51 50 17 65 13 27  6 30 40
 68  0 58 55 36 16 60  7 80 62 37 70 61 24 14 74 41 66 73 19 76 69 31 34
 10 26 79  1 22 56  4 75 67 43 20]
we are doing training validation split
training loss = 406.519287109375 100
val loss = 423.06689453125
training loss = 8.419000625610352 200
val loss = 4.855368614196777
training loss = 7.821336269378662 300
val loss = 5.544300079345703
training loss = 7.542442321777344 400
val loss = 5.476794242858887
training loss = 7.2845940589904785 500
val loss = 5.419425964355469
training loss = 7.061419486999512 600
val loss = 5.384322166442871
training loss = 6.877529621124268 700
val loss = 5.367682456970215
training loss = 6.730433940887451 800
val loss = 5.362892150878906
training loss = 6.613641738891602 900
val loss = 5.362978935241699
training loss = 6.519730091094971 1000
val loss = 5.362709045410156
training loss = 6.4422926902771 1100
val loss = 5.359216690063477
training loss = 6.3766398429870605 1200
val loss = 5.351842403411865
training loss = 6.3196516036987305 1300
val loss = 5.34089994430542
training loss = 6.269326686859131 1400
val loss = 5.327528953552246
training loss = 6.22437858581543 1500
val loss = 5.312749862670898
training loss = 6.183899402618408 1600
val loss = 5.2973785400390625
training loss = 6.147211074829102 1700
val loss = 5.282084941864014
training loss = 6.113775253295898 1800
val loss = 5.267212867736816
training loss = 6.083128929138184 1900
val loss = 5.252956867218018
training loss = 6.05489444732666 2000
val loss = 5.239413261413574
training loss = 6.028738975524902 2100
val loss = 5.2265143394470215
training loss = 6.004388809204102 2200
val loss = 5.214386463165283
training loss = 5.98159646987915 2300
val loss = 5.202885150909424
training loss = 5.960153579711914 2400
val loss = 5.191883563995361
training loss = 5.939881324768066 2500
val loss = 5.181509017944336
training loss = 5.92061710357666 2600
val loss = 5.171555995941162
training loss = 5.902219295501709 2700
val loss = 5.162010669708252
training loss = 5.884571075439453 2800
val loss = 5.152900695800781
training loss = 5.867550373077393 2900
val loss = 5.144057273864746
training loss = 5.8510565757751465 3000
val loss = 5.135495662689209
training loss = 5.834996223449707 3100
val loss = 5.127224445343018
training loss = 5.819276332855225 3200
val loss = 5.11914587020874
training loss = 5.803821086883545 3300
val loss = 5.111207485198975
training loss = 5.788548946380615 3400
val loss = 5.1033124923706055
training loss = 5.7733964920043945 3500
val loss = 5.095636367797852
training loss = 5.758378028869629 3600
val loss = 5.079455375671387
training loss = 5.743322849273682 3700
val loss = 5.079827308654785
training loss = 5.728736400604248 3800
val loss = 5.088833332061768
training loss = 5.7138671875 3900
val loss = 5.066347122192383
training loss = 5.699854373931885 4000
val loss = 5.06926155090332
training loss = 5.6862874031066895 4100
val loss = 5.052917957305908
training loss = 5.673848628997803 4200
val loss = 5.038699150085449
training loss = 5.662520408630371 4300
val loss = 5.041411399841309
training loss = 5.6526336669921875 4400
val loss = 5.047849178314209
training loss = 5.644101619720459 4500
val loss = 5.0338826179504395
training loss = 5.638740539550781 4600
val loss = 5.074617862701416
training loss = 5.6314377784729 4700
val loss = 5.030725002288818
training loss = 5.627013683319092 4800
val loss = 5.04036808013916
training loss = 5.623191833496094 4900
val loss = 5.025718688964844
training loss = 5.620041847229004 5000
val loss = 5.027565956115723
training loss = 5.619171619415283 5100
val loss = 4.982973098754883
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 886.3454,  893.6088,  936.6624,  954.3676,  986.3892, 1050.4310,
        1079.9825, 1104.5519, 1120.9689, 1158.4904, 1247.4457, 1135.7681,
        1207.1600, 1277.6548, 1342.4429, 1416.9780, 1380.2847, 1468.9802,
        1585.0918, 1488.0110, 1567.6455, 1581.2574, 1610.8490, 1681.8619,
        1635.5858, 1735.1648, 1617.4926, 1691.9880, 1746.7732, 1700.0879,
        1645.6664, 1744.6970, 1682.9824, 1659.7273, 1686.2769, 1808.5474,
        1676.3115, 1653.7269, 1706.8719, 1628.3009, 1551.1451, 1547.2117,
        1540.9574, 1476.2111, 1346.9114, 1354.1127, 1220.1455, 1328.7300,
        1197.4387, 1156.5500, 1038.6039,  972.7195,  930.1739,  920.8542,
         965.7253,  844.9572,  843.1254,  714.3129,  565.7352,  545.5693,
         541.5816,  469.4177,  448.1992,  413.5637,  375.9919,  319.2220,
         313.1885,  211.6581,  210.4497,  185.4866,  150.7401,  155.4200,
         152.8650,  111.8477,  103.8194,   74.8907,   59.4030,   45.2051,
          35.6569,   35.4069,   18.8386,   36.8230,   33.0530])]
2534.905621993151
3.136623959346985 4.313830306492845 48.26544891945005
val isze = 8
idinces = [42  0 51 30 67 49 10 34 66 70 27 72 16 32  5 55 21 38  1 33 36 39 68 71
 50  9 46 56 58  3 77 26 12 31 14 62 41  6 35 74 54 17 29 81 40 13 52  4
 75 82 22 24 78 57 18 15 65 47 64 63  2 48  8 73 44 53 37 28 69  7 79 61
 80 60 76 23 59 20 25 11 43 45 19]
we are doing training validation split
training loss = 47.332183837890625 100
val loss = 71.99031066894531
training loss = 28.276042938232422 200
val loss = 42.84268569946289
training loss = 18.54231834411621 300
val loss = 27.747146606445312
training loss = 13.484256744384766 400
val loss = 19.391626358032227
training loss = 10.694894790649414 500
val loss = 14.433039665222168
training loss = 9.077493667602539 600
val loss = 11.320070266723633
training loss = 8.104839324951172 700
val loss = 9.282308578491211
training loss = 7.504312515258789 800
val loss = 7.906062126159668
training loss = 7.125730991363525 900
val loss = 6.954133033752441
training loss = 6.882145404815674 1000
val loss = 6.283249855041504
training loss = 6.721331596374512 1100
val loss = 5.803362846374512
training loss = 6.611042022705078 1200
val loss = 5.455958366394043
training loss = 6.530866622924805 1300
val loss = 5.202103614807129
training loss = 6.467451095581055 1400
val loss = 5.015293121337891
training loss = 6.411333084106445 1500
val loss = 4.87712287902832
training loss = 6.354390621185303 1600
val loss = 4.774695873260498
training loss = 6.286828517913818 1700
val loss = 4.699278354644775
training loss = 6.192422866821289 1800
val loss = 4.646364212036133
training loss = 6.044121265411377 1900
val loss = 4.6165876388549805
training loss = 5.818840503692627 2000
val loss = 4.603233814239502
training loss = 5.521379470825195 2100
val loss = 4.548476696014404
training loss = 5.136613368988037 2200
val loss = 4.389022350311279
training loss = 4.62507963180542 2300
val loss = 4.137190341949463
training loss = 3.970904588699341 2400
val loss = 3.8110640048980713
training loss = 3.2343969345092773 2500
val loss = 3.4170663356781006
training loss = 2.590768814086914 2600
val loss = 3.0063893795013428
training loss = 2.2167141437530518 2700
val loss = 2.6884970664978027
training loss = 2.0834226608276367 2800
val loss = 2.523937463760376
training loss = 2.0461058616638184 2900
val loss = 2.4646899700164795
training loss = 2.030421018600464 3000
val loss = 2.4498767852783203
training loss = 2.0192677974700928 3100
val loss = 2.450061798095703
training loss = 2.0098252296447754 3200
val loss = 2.4546549320220947
training loss = 2.001530408859253 3300
val loss = 2.4602363109588623
training loss = 1.9941627979278564 3400
val loss = 2.4657838344573975
training loss = 1.9875890016555786 3500
val loss = 2.4708800315856934
training loss = 1.9816951751708984 3600
val loss = 2.4754765033721924
training loss = 1.9763914346694946 3700
val loss = 2.4795145988464355
training loss = 1.9715944528579712 3800
val loss = 2.4829487800598145
training loss = 1.9672335386276245 3900
val loss = 2.485795259475708
training loss = 1.963249921798706 4000
val loss = 2.4881091117858887
training loss = 1.9595915079116821 4100
val loss = 2.4898509979248047
training loss = 1.9562106132507324 4200
val loss = 2.491034507751465
training loss = 1.9530729055404663 4300
val loss = 2.491750717163086
training loss = 1.95014488697052 4400
val loss = 2.492051839828491
training loss = 1.9473979473114014 4500
val loss = 2.491961717605591
training loss = 1.9448094367980957 4600
val loss = 2.49151611328125
training loss = 1.942360281944275 4700
val loss = 2.4908416271209717
training loss = 1.9400323629379272 4800
val loss = 2.4900548458099365
training loss = 1.9385895729064941 4900
val loss = 2.5190253257751465
training loss = 1.935585379600525 5000
val loss = 2.488495349884033
training loss = 1.9334745407104492 5100
val loss = 2.4877851009368896
training loss = 1.9315011501312256 5200
val loss = 2.486896514892578
training loss = 1.9297239780426025 5300
val loss = 2.5005316734313965
training loss = 1.927731990814209 5400
val loss = 2.486591100692749
training loss = 1.9269920587539673 5500
val loss = 2.456164598464966
training loss = 1.9242067337036133 5600
val loss = 2.486178398132324
training loss = 1.922610878944397 5700
val loss = 2.4884917736053467
training loss = 1.9209774732589722 5800
val loss = 2.4927115440368652
training loss = 1.9194141626358032 5900
val loss = 2.486924171447754
training loss = 1.919028401374817 6000
val loss = 2.4558732509613037
training loss = 1.9164067506790161 6100
val loss = 2.4875681400299072
training loss = 1.9208089113235474 6200
val loss = 2.5676188468933105
training loss = 1.9135819673538208 6300
val loss = 2.483908176422119
training loss = 1.9122437238693237 6400
val loss = 2.488107442855835
training loss = 2.162609815597534 6500
val loss = 2.310404062271118
training loss = 1.9096765518188477 6600
val loss = 2.485854387283325
training loss = 1.908469319343567 6700
val loss = 2.4885988235473633
training loss = 1.9075719118118286 6800
val loss = 2.472238063812256
training loss = 1.9060869216918945 6900
val loss = 2.4887828826904297
training loss = 1.9064528942108154 7000
val loss = 2.527378559112549
training loss = 1.9038655757904053 7100
val loss = 2.4941911697387695
training loss = 1.9027889966964722 7200
val loss = 2.489121913909912
training loss = 1.9030253887176514 7300
val loss = 2.4569520950317383
training loss = 1.9006950855255127 7400
val loss = 2.489429473876953
training loss = 1.901639461517334 7500
val loss = 2.5350229740142822
training loss = 1.8986945152282715 7600
val loss = 2.4879150390625
training loss = 1.897739291191101 7700
val loss = 2.4895222187042236
training loss = 1.8969550132751465 7800
val loss = 2.503171682357788
training loss = 1.8958638906478882 7900
val loss = 2.490206718444824
training loss = 1.8949642181396484 8000
val loss = 2.4881482124328613
training loss = 1.8941779136657715 8100
val loss = 2.5002193450927734
training loss = 1.89319908618927 8200
val loss = 2.489769458770752
training loss = 1.915244221687317 8300
val loss = 2.3751823902130127
training loss = 1.89150071144104 8400
val loss = 2.4899837970733643
training loss = 1.8906885385513306 8500
val loss = 2.494420051574707
training loss = 1.8898885250091553 8600
val loss = 2.4848859310150146
training loss = 1.8890615701675415 8700
val loss = 2.4896466732025146
training loss = 1.9123458862304688 8800
val loss = 2.3732714653015137
training loss = 1.8875035047531128 8900
val loss = 2.4905903339385986
training loss = 1.8867353200912476 9000
val loss = 2.489771604537964
training loss = 1.888668417930603 9100
val loss = 2.5419483184814453
training loss = 1.8852524757385254 9200
val loss = 2.4894819259643555
training loss = 1.884512186050415 9300
val loss = 2.4896419048309326
training loss = 1.8840913772583008 9400
val loss = 2.506592273712158
training loss = 1.883087158203125 9500
val loss = 2.4896936416625977
training loss = 1.9086873531341553 9600
val loss = 2.368241310119629
training loss = 1.8816956281661987 9700
val loss = 2.4896676540374756
training loss = 1.8810112476348877 9800
val loss = 2.4898524284362793
training loss = 1.8808579444885254 9900
val loss = 2.4690489768981934
training loss = 1.8796930313110352 10000
val loss = 2.4901795387268066
training loss = 1.8868032693862915 10100
val loss = 2.416224956512451
training loss = 1.8784204721450806 10200
val loss = 2.4875121116638184
training loss = 1.8777862787246704 10300
val loss = 2.489859104156494
training loss = 1.8787754774093628 10400
val loss = 2.454580545425415
reduced chi^2 level 2 = 1.8767733573913574
Constrained alpha: 1.8422443866729736
Constrained beta: 3.8167004585266113
Constrained gamma: 18.663589477539062
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 870.3167,  870.6970,  917.3504,  893.0531,  970.3023, 1084.2898,
        1056.5157, 1175.1229, 1174.0925, 1206.8661, 1186.9501, 1146.7404,
        1239.4717, 1310.6552, 1328.1416, 1432.9270, 1351.1232, 1447.9259,
        1462.4448, 1479.4915, 1597.8932, 1566.5289, 1646.9893, 1585.6555,
        1668.8755, 1686.6261, 1640.9600, 1653.3174, 1767.1785, 1693.5911,
        1676.8605, 1730.3503, 1690.9768, 1795.3203, 1694.9933, 1776.2247,
        1686.2151, 1663.3892, 1639.4823, 1583.7584, 1557.6581, 1556.2942,
        1498.3456, 1492.6185, 1360.8026, 1284.0370, 1232.9606, 1251.6846,
        1162.9498, 1134.7773, 1102.4725,  983.5363,  915.7537,  935.3818,
         852.2748,  889.6515,  816.1087,  735.3651,  586.0980,  539.4260,
         521.4546,  501.6270,  439.8251,  394.1431,  380.0958,  346.1884,
         297.3944,  248.8370,  209.7205,  155.9778,  165.4120,  148.2618,
         156.1972,  104.8431,   98.6601,   76.9045,   52.8969,   43.4081,
          27.8691,   45.2110,   20.1989,   38.7070,   38.7950])]
2501.7514806875106
0.43596456333954925 0.21702902942382307 75.33186656819014
val isze = 8
idinces = [11 70 44 58 66 62 41 35 67 17 28 61 76 10  4 77 43 38 82 21 19 13 59 30
 54 72 39 46 33  3 50 23 22 20 56 81 69 63 15 31  5 18 37  7 57 32 48 25
 34 65 27 60 53 68 64 40 36  8  2 51 16  0 75 78 12  9 55 79  6 29 73  1
 42 49 24 80 14 26 71 74 45 52 47]
we are doing training validation split
training loss = 109.53703308105469 100
val loss = 83.16972351074219
training loss = 16.186132431030273 200
val loss = 20.24279022216797
training loss = 11.04794692993164 300
val loss = 16.79014778137207
training loss = 7.760461807250977 400
val loss = 11.34353256225586
training loss = 6.288998126983643 500
val loss = 9.010106086730957
training loss = 5.190046310424805 600
val loss = 7.325016498565674
training loss = 4.27437162399292 700
val loss = 6.120511531829834
training loss = 3.4982380867004395 800
val loss = 4.886411666870117
training loss = 3.017651081085205 900
val loss = 4.141933441162109
training loss = 2.732684850692749 1000
val loss = 3.6043663024902344
training loss = 2.566193103790283 1100
val loss = 3.278289794921875
training loss = 2.458965301513672 1200
val loss = 3.0513579845428467
training loss = 2.383319616317749 1300
val loss = 2.9132981300354004
training loss = 2.3237011432647705 1400
val loss = 2.8173484802246094
training loss = 2.2732508182525635 1500
val loss = 2.737853765487671
training loss = 2.226243495941162 1600
val loss = 2.7033073902130127
training loss = 2.1820127964019775 1700
val loss = 2.7217533588409424
training loss = 2.1339542865753174 1800
val loss = 2.6973299980163574
training loss = 2.0943655967712402 1900
val loss = 2.8395729064941406
training loss = 2.030766248703003 2000
val loss = 2.7779157161712646
training loss = 2.0610620975494385 2100
val loss = 2.6391091346740723
training loss = 1.92036771774292 2200
val loss = 2.8438515663146973
training loss = 1.973480224609375 2300
val loss = 3.2039523124694824
training loss = 1.8327521085739136 2400
val loss = 2.8107781410217285
training loss = 1.8366408348083496 2500
val loss = 2.6512012481689453
training loss = 1.7732086181640625 2600
val loss = 2.7412986755371094
training loss = 1.8279016017913818 2700
val loss = 2.5581836700439453
training loss = 1.7329590320587158 2800
val loss = 2.6694154739379883
training loss = 1.7210278511047363 2900
val loss = 2.701059579849243
training loss = 1.7039648294448853 3000
val loss = 2.6290485858917236
training loss = 1.6919749975204468 3100
val loss = 2.5971078872680664
training loss = 1.6814029216766357 3200
val loss = 2.588155746459961
training loss = 1.6713955402374268 3300
val loss = 2.5769903659820557
training loss = 1.6624253988265991 3400
val loss = 2.563107967376709
training loss = 1.6536697149276733 3500
val loss = 2.555896759033203
training loss = 1.6635699272155762 3600
val loss = 2.666698455810547
training loss = 1.6377769708633423 3700
val loss = 2.5430312156677246
training loss = 1.631502389907837 3800
val loss = 2.512706756591797
training loss = 1.623300313949585 3900
val loss = 2.534419536590576
training loss = 1.6166818141937256 4000
val loss = 2.522688388824463
training loss = 1.6099824905395508 4100
val loss = 2.526648998260498
training loss = 1.6038553714752197 4200
val loss = 2.527977466583252
training loss = 1.597671627998352 4300
val loss = 2.5241353511810303
training loss = 1.5966992378234863 4400
val loss = 2.4759538173675537
training loss = 1.5863094329833984 4500
val loss = 2.523158550262451
training loss = 1.5936373472213745 4600
val loss = 2.614293336868286
training loss = 1.5758119821548462 4700
val loss = 2.5225071907043457
training loss = 1.590218186378479 4800
val loss = 2.4391984939575195
training loss = 1.5661462545394897 4900
val loss = 2.52355694770813
training loss = 1.601536750793457 5000
val loss = 2.707118272781372
training loss = 1.557254672050476 5100
val loss = 2.526224374771118
training loss = 1.7033292055130005 5200
val loss = 2.9265518188476562
training loss = 1.5491178035736084 5300
val loss = 2.531200647354126
training loss = 1.5633498430252075 5400
val loss = 2.644885540008545
training loss = 1.5417166948318481 5500
val loss = 2.530947685241699
training loss = 1.5380749702453613 5600
val loss = 2.5361931324005127
training loss = 1.5406562089920044 5700
val loss = 2.4896535873413086
training loss = 1.5316624641418457 5800
val loss = 2.540889024734497
training loss = 1.8854196071624756 5900
val loss = 2.448901653289795
training loss = 1.5258252620697021 6000
val loss = 2.5474181175231934
training loss = 1.5259877443313599 6100
val loss = 2.513929843902588
training loss = 1.5206013917922974 6200
val loss = 2.543607234954834
training loss = 1.5227525234222412 6300
val loss = 2.606675624847412
training loss = 1.5157523155212402 6400
val loss = 2.5607454776763916
training loss = 1.7277785539627075 6500
val loss = 3.0748236179351807
training loss = 1.5114339590072632 6600
val loss = 2.565436840057373
training loss = 1.5188132524490356 6700
val loss = 2.6278576850891113
training loss = 1.5075714588165283 6800
val loss = 2.569932699203491
training loss = 1.5066105127334595 6900
val loss = 2.5875604152679443
training loss = 1.504141092300415 7000
val loss = 2.5737671852111816
training loss = 1.504564881324768 7100
val loss = 2.546801805496216
training loss = 1.5011428594589233 7200
val loss = 2.5798354148864746
training loss = 1.5020073652267456 7300
val loss = 2.6119532585144043
training loss = 1.4984920024871826 7400
val loss = 2.5847105979919434
training loss = 1.4988110065460205 7500
val loss = 2.560603618621826
training loss = 1.4961769580841064 7600
val loss = 2.589486598968506
training loss = 1.5008169412612915 7700
val loss = 2.6469831466674805
training loss = 1.4941328763961792 7800
val loss = 2.5939176082611084
training loss = 1.5073262453079224 7900
val loss = 2.5292391777038574
training loss = 1.4923696517944336 8000
val loss = 2.5983269214630127
training loss = 1.6031136512756348 8100
val loss = 2.487314462661743
training loss = 1.4908313751220703 8200
val loss = 2.603529930114746
training loss = 1.4900189638137817 8300
val loss = 2.60540771484375
training loss = 1.4901793003082275 8400
val loss = 2.6231884956359863
training loss = 1.488748550415039 8500
val loss = 2.6092286109924316
training loss = 1.4892840385437012 8600
val loss = 2.6284565925598145
training loss = 1.4876540899276733 8700
val loss = 2.6128082275390625
training loss = 1.4873560667037964 8800
val loss = 2.6268274784088135
training loss = 1.4869004487991333 8900
val loss = 2.606259822845459
training loss = 1.4862134456634521 9000
val loss = 2.6176810264587402
training loss = 1.5286027193069458 9100
val loss = 2.781397581100464
training loss = 1.4854243993759155 9200
val loss = 2.6183407306671143
training loss = 1.4974849224090576 9300
val loss = 2.5628068447113037
reduced chi^2 level 2 = 1.4852795600891113
Constrained alpha: 1.954155445098877
Constrained beta: -2.2046178855816834e-05
Constrained gamma: 26.472896575927734
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 838.9357,  907.6446,  920.0400,  962.5780,  990.0315, 1077.5454,
        1071.0964, 1138.2859, 1176.6515, 1191.9189, 1214.6191, 1203.7404,
        1209.5172, 1212.0165, 1321.5342, 1387.2775, 1418.9652, 1449.8000,
        1543.1044, 1500.7971, 1532.4515, 1542.1807, 1569.1073, 1638.2228,
        1654.4717, 1635.2666, 1578.0511, 1779.9926, 1681.7073, 1738.8301,
        1643.6447, 1728.8232, 1717.5082, 1765.5936, 1735.2766, 1751.9666,
        1693.3936, 1634.7977, 1660.2476, 1614.6880, 1618.5253, 1471.0396,
        1478.3655, 1519.4586, 1338.3033, 1311.4479, 1276.4126, 1237.7321,
        1158.0961, 1210.8132, 1091.7083,  941.9280,  925.0018,  932.3075,
         907.4687,  835.3500,  840.2500,  707.2048,  584.3949,  524.7748,
         553.4650,  475.9588,  479.3228,  367.6819,  348.5969,  323.9689,
         298.8221,  269.2802,  227.7296,  155.8627,  149.8048,  156.5376,
         141.8040,   97.8184,   78.1064,   75.5330,   38.5317,   34.2145,
          41.2405,   47.4361,   33.3558,   40.5951,   34.6804])]
2540.535007928744
3.8041946700013076 18.987835477337242 56.45926314080859
val isze = 8
idinces = [80 31 11 35 50 40 18 16 71 76 56 78  6 74 43 15  5 64 45 79 21 38 26 73
 70  8  7 23  3 47 29  2 75 13 48 62 25 69 55 17 82 41 51 42 65 77 61 10
  0 14 12  9 60 28 72 63 19  4 67 66  1 32 57 27 53 46 58 20 37 30 33 44
 68 36 34 49 59 22 54 39 81 24 52]
we are doing training validation split
training loss = 481.1445007324219 100
val loss = 550.4547119140625
training loss = 62.31229782104492 200
val loss = 35.59123611450195
training loss = 10.92478084564209 300
val loss = 7.82844352722168
training loss = 10.166187286376953 400
val loss = 7.730384349822998
training loss = 9.40553092956543 500
val loss = 7.724518775939941
training loss = 8.693674087524414 600
val loss = 7.763868808746338
training loss = 8.07172966003418 700
val loss = 7.851019382476807
training loss = 7.565216541290283 800
val loss = 7.9799089431762695
training loss = 7.1805419921875 900
val loss = 8.135761260986328
training loss = 6.906134605407715 1000
val loss = 8.298440933227539
training loss = 6.718720436096191 1100
val loss = 8.448043823242188
training loss = 6.591897487640381 1200
val loss = 8.570404052734375
training loss = 6.503045082092285 1300
val loss = 8.65938949584961
training loss = 6.436272144317627 1400
val loss = 8.716215133666992
training loss = 6.381981372833252 1500
val loss = 8.746200561523438
training loss = 6.334998607635498 1600
val loss = 8.756165504455566
training loss = 6.292710304260254 1700
val loss = 8.7525634765625
training loss = 6.253804683685303 1800
val loss = 8.740327835083008
training loss = 6.217570781707764 1900
val loss = 8.722885131835938
training loss = 6.183568477630615 2000
val loss = 8.702714920043945
training loss = 6.1514763832092285 2100
val loss = 8.681036949157715
training loss = 6.121035575866699 2200
val loss = 8.658851623535156
training loss = 6.092032432556152 2300
val loss = 8.636582374572754
training loss = 6.064270973205566 2400
val loss = 8.614509582519531
training loss = 6.037593841552734 2500
val loss = 8.592479705810547
training loss = 6.011850833892822 2600
val loss = 8.570730209350586
training loss = 5.986926555633545 2700
val loss = 8.549222946166992
training loss = 5.962714195251465 2800
val loss = 8.527859687805176
training loss = 5.939128875732422 2900
val loss = 8.506820678710938
training loss = 5.916109085083008 3000
val loss = 8.485674858093262
training loss = 5.893620491027832 3100
val loss = 8.464849472045898
training loss = 5.871652126312256 3200
val loss = 8.444232940673828
training loss = 5.850235939025879 3300
val loss = 8.423832893371582
training loss = 5.8294501304626465 3400
val loss = 8.40353775024414
training loss = 5.8094305992126465 3500
val loss = 8.384112358093262
training loss = 5.79035758972168 3600
val loss = 8.365326881408691
training loss = 5.772480487823486 3700
val loss = 8.347664833068848
training loss = 5.7560529708862305 3800
val loss = 8.331598281860352
training loss = 5.741302967071533 3900
val loss = 8.316766738891602
training loss = 5.729037284851074 4000
val loss = 8.376436233520508
training loss = 5.717240333557129 4100
val loss = 8.293432235717773
training loss = 5.7154459953308105 4200
val loss = 8.563837051391602
training loss = 5.695040702819824 4300
val loss = 8.267772674560547
training loss = 5.679169178009033 4400
val loss = 8.4268798828125
training loss = 5.630612850189209 4500
val loss = 8.193793296813965
training loss = 5.483175754547119 4600
val loss = 7.725040435791016
training loss = 4.978369235992432 4700
val loss = 7.458101749420166
training loss = 3.6478402614593506 4800
val loss = 5.850669860839844
training loss = 2.3118677139282227 4900
val loss = 2.868574619293213
training loss = 2.196657419204712 5000
val loss = 2.4217920303344727
training loss = 2.176502227783203 5100
val loss = 2.3635034561157227
training loss = 2.1648921966552734 5200
val loss = 2.4174416065216064
training loss = 2.152733087539673 5300
val loss = 2.2919061183929443
training loss = 2.1833341121673584 5400
val loss = 1.9526358842849731
training loss = 2.138578414916992 5500
val loss = 2.2509145736694336
training loss = 2.1329843997955322 5600
val loss = 2.2389538288116455
training loss = 2.1281304359436035 5700
val loss = 2.2036163806915283
training loss = 2.123504877090454 5800
val loss = 2.2158493995666504
training loss = 2.1193830966949463 5900
val loss = 2.17536997795105
training loss = 2.1148929595947266 6000
val loss = 2.2010722160339355
training loss = 2.1107802391052246 6100
val loss = 2.189774990081787
training loss = 2.1067705154418945 6200
val loss = 2.2065422534942627
training loss = 2.102508544921875 6300
val loss = 2.1734085083007812
training loss = 2.2162418365478516 6400
val loss = 2.958854913711548
training loss = 2.0937764644622803 6500
val loss = 2.1602697372436523
training loss = 2.089045524597168 6600
val loss = 2.151122808456421
training loss = 2.084308385848999 6700
val loss = 2.1093053817749023
training loss = 2.078643798828125 6800
val loss = 2.133148670196533
training loss = 2.1566224098205566 6900
val loss = 1.6952579021453857
training loss = 2.0663516521453857 7000
val loss = 2.117161989212036
training loss = 2.0594520568847656 7100
val loss = 2.1031503677368164
training loss = 2.052987575531006 7200
val loss = 2.153015613555908
training loss = 2.0445146560668945 7300
val loss = 2.081181526184082
training loss = 2.036571979522705 7400
val loss = 2.0825226306915283
training loss = 2.028852701187134 7500
val loss = 2.052178382873535
training loss = 2.0211005210876465 7600
val loss = 2.0514254570007324
training loss = 2.024233102798462 7700
val loss = 1.862667441368103
training loss = 2.006157159805298 7800
val loss = 2.03231143951416
training loss = 1.9988102912902832 7900
val loss = 2.0170516967773438
training loss = 1.991933822631836 8000
val loss = 2.000680685043335
training loss = 1.9850295782089233 8100
val loss = 2.0066843032836914
training loss = 1.9793974161148071 8200
val loss = 1.942112922668457
training loss = 1.9722298383712769 8300
val loss = 1.9889943599700928
training loss = 1.9796069860458374 8400
val loss = 1.7824749946594238
training loss = 1.960005283355713 8500
val loss = 1.9779231548309326
training loss = 1.954006552696228 8600
val loss = 1.9635831117630005
training loss = 1.9496667385101318 8700
val loss = 2.0273020267486572
training loss = 1.942460536956787 8800
val loss = 1.9468698501586914
training loss = 1.936480164527893 8900
val loss = 1.936587929725647
training loss = 1.9310537576675415 9000
val loss = 1.9665212631225586
training loss = 1.9247090816497803 9100
val loss = 1.9193171262741089
training loss = 1.9525036811828613 9200
val loss = 1.616942048072815
training loss = 1.9129846096038818 9300
val loss = 1.9024382829666138
training loss = 1.9077820777893066 9400
val loss = 1.8501919507980347
training loss = 1.9019546508789062 9500
val loss = 1.8930907249450684
training loss = 1.8965977430343628 9600
val loss = 1.8737800121307373
training loss = 1.8919250965118408 9700
val loss = 1.9022245407104492
training loss = 1.8864612579345703 9800
val loss = 1.855234980583191
training loss = 1.9004789590835571 9900
val loss = 1.6189258098602295
training loss = 1.8765678405761719 10000
val loss = 1.8391444683074951
training loss = 1.8715840578079224 10100
val loss = 1.8233542442321777
training loss = 1.8668957948684692 10200
val loss = 1.8247727155685425
training loss = 1.8621586561203003 10300
val loss = 1.8103368282318115
training loss = 1.8624695539474487 10400
val loss = 1.9347889423370361
training loss = 1.8529138565063477 10500
val loss = 1.7952866554260254
training loss = 1.8513425588607788 10600
val loss = 1.6897087097167969
training loss = 1.8442240953445435 10700
val loss = 1.7866737842559814
training loss = 1.840050458908081 10800
val loss = 1.768941879272461
training loss = 1.8362305164337158 10900
val loss = 1.779928207397461
training loss = 1.8325012922286987 11000
val loss = 1.75471830368042
training loss = 1.8289384841918945 11100
val loss = 1.752589225769043
training loss = 1.8259294033050537 11200
val loss = 1.760277509689331
training loss = 1.82284677028656 11300
val loss = 1.7357078790664673
training loss = 1.8952423334121704 11400
val loss = 2.3165040016174316
training loss = 1.8175712823867798 11500
val loss = 1.720369577407837
training loss = 1.8152306079864502 11600
val loss = 1.720463752746582
training loss = 1.8220692873001099 11700
val loss = 1.8962658643722534
training loss = 1.8111381530761719 11800
val loss = 1.710306167602539
training loss = 1.8092964887619019 11900
val loss = 1.7027156352996826
training loss = 1.807706594467163 12000
val loss = 1.6985443830490112
training loss = 1.8061695098876953 12100
val loss = 1.702799677848816
training loss = 1.8084322214126587 12200
val loss = 1.8119471073150635
training loss = 1.803552508354187 12300
val loss = 1.698277235031128
training loss = 1.8023419380187988 12400
val loss = 1.6970734596252441
training loss = 1.8015568256378174 12500
val loss = 1.7221496105194092
training loss = 1.8003032207489014 12600
val loss = 1.6922423839569092
training loss = 2.1790034770965576 12700
val loss = 1.0827665328979492
training loss = 1.7985649108886719 12800
val loss = 1.695325493812561
training loss = 1.7977441549301147 12900
val loss = 1.688246250152588
training loss = 1.895509123802185 13000
val loss = 2.3727364540100098
training loss = 1.796316146850586 13100
val loss = 1.6896770000457764
training loss = 1.795628547668457 13200
val loss = 1.6825735569000244
training loss = 1.7952146530151367 13300
val loss = 1.7066175937652588
training loss = 1.7944568395614624 13400
val loss = 1.6829723119735718
training loss = 1.799500823020935 13500
val loss = 1.5561549663543701
training loss = 1.7934359312057495 13600
val loss = 1.6821634769439697
training loss = 1.792929768562317 13700
val loss = 1.6777920722961426
training loss = 1.7926266193389893 13800
val loss = 1.6980969905853271
training loss = 1.7920808792114258 13900
val loss = 1.6796737909317017
training loss = 1.8326380252838135 14000
val loss = 1.369963526725769
training loss = 1.7913302183151245 14100
val loss = 1.6773829460144043
training loss = 1.7909495830535889 14200
val loss = 1.6778883934020996
training loss = 1.7966245412826538 14300
val loss = 1.8229267597198486
training loss = 1.7903072834014893 14400
val loss = 1.6776751279830933
training loss = 1.8302018642425537 14500
val loss = 1.3695578575134277
training loss = 1.7897520065307617 14600
val loss = 1.683929204940796
training loss = 1.7894320487976074 14700
val loss = 1.6758735179901123
training loss = 1.7893059253692627 14800
val loss = 1.6580040454864502
training loss = 1.7889539003372192 14900
val loss = 1.6749083995819092
training loss = 1.9628732204437256 15000
val loss = 1.1514649391174316
training loss = 1.7885172367095947 15100
val loss = 1.6746407747268677
training loss = 1.788282871246338 15200
val loss = 1.673741340637207
training loss = 1.7888784408569336 15300
val loss = 1.7224019765853882
training loss = 1.7879319190979004 15400
val loss = 1.6733334064483643
training loss = 1.7877283096313477 15500
val loss = 1.6699802875518799
training loss = 1.7877072095870972 15600
val loss = 1.6904046535491943
training loss = 1.7874135971069336 15700
val loss = 1.6718908548355103
training loss = 1.7933651208877563 15800
val loss = 1.81827974319458
training loss = 1.787146806716919 15900
val loss = 1.672446608543396
training loss = 1.7869817018508911 16000
val loss = 1.6713433265686035
training loss = 1.787358045578003 16100
val loss = 1.6322369575500488
training loss = 1.786750078201294 16200
val loss = 1.6704562902450562
training loss = 2.0890679359436035 16300
val loss = 1.069183111190796
training loss = 1.786557674407959 16400
val loss = 1.6641110181808472
training loss = 1.7864162921905518 16500
val loss = 1.6689566373825073
training loss = 1.8102405071258545 16600
val loss = 1.4216859340667725
training loss = 1.7862427234649658 16700
val loss = 1.6669062376022339
training loss = 1.7861210107803345 16800
val loss = 1.668344259262085
training loss = 1.7864307165145874 16900
val loss = 1.634399652481079
training loss = 1.785969853401184 17000
val loss = 1.6672307252883911
training loss = 1.9595876932144165 17100
val loss = 2.6345467567443848
training loss = 1.785831332206726 17200
val loss = 1.6710364818572998
training loss = 1.7857187986373901 17300
val loss = 1.6670308113098145
training loss = 1.7863595485687256 17400
val loss = 1.619941234588623
training loss = 1.7855806350708008 17500
val loss = 1.6655904054641724
training loss = 1.7884666919708252 17600
val loss = 1.765628457069397
training loss = 1.7854474782943726 17700
val loss = 1.6668951511383057
training loss = 1.836856722831726 17800
val loss = 2.1366071701049805
training loss = 1.7853217124938965 17900
val loss = 1.6629259586334229
training loss = 1.7852197885513306 18000
val loss = 1.6642171144485474
training loss = 1.7854061126708984 18100
val loss = 1.638248324394226
training loss = 1.785102128982544 18200
val loss = 1.6630642414093018
training loss = 1.874379277229309 18300
val loss = 2.3117446899414062
training loss = 1.7849990129470825 18400
val loss = 1.6611582040786743
training loss = 1.7848942279815674 18500
val loss = 1.6624081134796143
training loss = 1.7929680347442627 18600
val loss = 1.5112335681915283
training loss = 1.7847939729690552 18700
val loss = 1.6615718603134155
training loss = 1.7846989631652832 18800
val loss = 1.6672890186309814
training loss = 1.7848089933395386 18900
val loss = 1.678786039352417
training loss = 1.7845971584320068 19000
val loss = 1.6608588695526123
training loss = 1.8436660766601562 19100
val loss = 2.1627964973449707
training loss = 1.7845054864883423 19200
val loss = 1.6634787321090698
training loss = 1.7843809127807617 19300
val loss = 1.6577328443527222
training loss = 1.7844574451446533 19400
val loss = 1.670802116394043
training loss = 1.7842869758605957 19500
val loss = 1.659354567527771
training loss = 1.7990617752075195 19600
val loss = 1.4527592658996582
training loss = 1.7842046022415161 19700
val loss = 1.6572816371917725
training loss = 1.7840700149536133 19800
val loss = 1.6580026149749756
training loss = 1.784204125404358 19900
val loss = 1.6722012758255005
training loss = 1.783990502357483 20000
val loss = 1.6571310758590698
training loss = 1.7854679822921753 20100
val loss = 1.7310678958892822
training loss = 1.783909559249878 20200
val loss = 1.6504510641098022
training loss = 1.78375244140625 20300
val loss = 1.656336784362793
training loss = 1.7839787006378174 20400
val loss = 1.633225917816162
training loss = 1.7836567163467407 20500
val loss = 1.6552181243896484
training loss = 2.118725538253784 20600
val loss = 1.013326644897461
training loss = 1.7836015224456787 20700
val loss = 1.6478044986724854
training loss = 1.7834326028823853 20800
val loss = 1.6539316177368164
training loss = 1.790867805480957 20900
val loss = 1.8196470737457275
training loss = 1.7833691835403442 21000
val loss = 1.653738260269165
training loss = 1.7833040952682495 21100
val loss = 1.670365571975708
training loss = 1.783375859260559 21200
val loss = 1.63772714138031
training loss = 1.7831419706344604 21300
val loss = 1.6517317295074463
training loss = 1.7938156127929688 21400
val loss = 1.4802217483520508
training loss = 1.783081293106079 21500
val loss = 1.6495680809020996
training loss = 1.7830194234848022 21600
val loss = 1.6307282447814941
training loss = 1.7830798625946045 21700
val loss = 1.653869867324829
training loss = 1.7828937768936157 21800
val loss = 1.649292230606079
training loss = 1.9020540714263916 21900
val loss = 2.395801544189453
training loss = 1.7828540802001953 22000
val loss = 1.6514804363250732
training loss = 1.7826589345932007 22100
val loss = 1.6476927995681763
training loss = 1.7829128503799438 22200
val loss = 1.6302727460861206
training loss = 1.7826329469680786 22300
val loss = 1.6465370655059814
training loss = 1.7937630414962769 22400
val loss = 1.4662797451019287
training loss = 1.7826310396194458 22500
val loss = 1.655792236328125
training loss = 1.7823885679244995 22600
val loss = 1.6445598602294922
training loss = 1.7831710577011108 22700
val loss = 1.6918878555297852
training loss = 1.782373070716858 22800
val loss = 1.6428871154785156
training loss = 1.7821530103683472 22900
val loss = 1.6437909603118896
training loss = 1.7823312282562256 23000
val loss = 1.6455549001693726
training loss = 1.782096028327942 23100
val loss = 1.6418137550354004
training loss = 1.8153374195098877 23200
val loss = 2.0010719299316406
training loss = 1.7820255756378174 23300
val loss = 1.6404964923858643
training loss = 1.7818372249603271 23400
val loss = 1.6262274980545044
training loss = 1.7820186614990234 23500
val loss = 1.6323153972625732
training loss = 1.781762719154358 23600
val loss = 1.6381351947784424
training loss = 1.8333889245986938 23700
val loss = 1.2710888385772705
training loss = 1.7817517518997192 23800
val loss = 1.6385884284973145
training loss = 1.7815003395080566 23900
val loss = 1.636014461517334
training loss = 1.7869137525558472 24000
val loss = 1.7748650312423706
training loss = 1.7814970016479492 24100
val loss = 1.6354262828826904
training loss = 1.7812423706054688 24200
val loss = 1.6314918994903564
training loss = 1.7816219329833984 24300
val loss = 1.6071110963821411
training loss = 1.7811503410339355 24400
val loss = 1.632250189781189
training loss = 1.7820111513137817 24500
val loss = 1.589134693145752
training loss = 1.7811373472213745 24600
val loss = 1.6324646472930908
training loss = 1.7808724641799927 24700
val loss = 1.6300621032714844
training loss = 1.7811459302902222 24800
val loss = 1.6336725950241089
training loss = 1.7808704376220703 24900
val loss = 1.6295604705810547
training loss = 1.8441903591156006 25000
val loss = 2.1496682167053223
training loss = 1.7808045148849487 25100
val loss = 1.6277388334274292
training loss = 1.780535101890564 25200
val loss = 1.6274418830871582
training loss = 1.7827982902526855 25300
val loss = 1.7110551595687866
training loss = 1.7805265188217163 25400
val loss = 1.6266489028930664
training loss = 1.7802983522415161 25500
val loss = 1.6362841129302979
training loss = 1.7805843353271484 25600
val loss = 1.632360577583313
training loss = 1.7802960872650146 25700
val loss = 1.624653697013855
training loss = 1.8656163215637207 25800
val loss = 2.238567352294922
training loss = 1.780311942100525 25900
val loss = 1.6318097114562988
training loss = 1.7800194025039673 26000
val loss = 1.622507929801941
training loss = 1.8203498125076294 26100
val loss = 1.3014185428619385
training loss = 1.7801034450531006 26200
val loss = 1.6199147701263428
training loss = 1.779829978942871 26300
val loss = 1.6208994388580322
training loss = 1.7942521572113037 26400
val loss = 1.4204485416412354
training loss = 1.7798038721084595 26500
val loss = 1.6212934255599976
training loss = 1.7795817852020264 26600
val loss = 1.6304244995117188
training loss = 1.779930830001831 26700
val loss = 1.5994552373886108
training loss = 1.7795380353927612 26800
val loss = 1.6178890466690063
training loss = 1.8226591348648071 26900
val loss = 1.2788722515106201
training loss = 1.7795809507369995 27000
val loss = 1.6205884218215942
training loss = 1.7793114185333252 27100
val loss = 1.6160476207733154
training loss = 1.7797330617904663 27200
val loss = 1.6403415203094482
training loss = 1.7793530225753784 27300
val loss = 1.6159608364105225
training loss = 1.7790969610214233 27400
val loss = 1.617464542388916
training loss = 1.779402256011963 27500
val loss = 1.6250345706939697
training loss = 1.779104471206665 27600
val loss = 1.6136080026626587
training loss = 1.8609257936477661 27700
val loss = 1.174939751625061
training loss = 1.779147982597351 27800
val loss = 1.6114338636398315
training loss = 1.7788925170898438 27900
val loss = 1.6122148036956787
training loss = 1.781490683555603 28000
val loss = 1.7007527351379395
training loss = 1.7789400815963745 28100
val loss = 1.6122138500213623
training loss = 1.7802857160568237 28200
val loss = 1.5402629375457764
training loss = 1.7790035009384155 28300
val loss = 1.6082886457443237
training loss = 1.7787492275238037 28400
val loss = 1.610001802444458
training loss = 1.8194676637649536 28500
val loss = 2.0045814514160156
training loss = 1.7788454294204712 28600
val loss = 1.6093494892120361
training loss = 1.7786035537719727 28700
val loss = 1.6082355976104736
training loss = 1.779592752456665 28800
val loss = 1.5652258396148682
training loss = 1.778666377067566 28900
val loss = 1.6082326173782349
training loss = 1.7785592079162598 29000
val loss = 1.5871888399124146
training loss = 1.7788684368133545 29100
val loss = 1.5923877954483032
training loss = 1.7785453796386719 29200
val loss = 1.6071758270263672
training loss = 2.0806214809417725 29300
val loss = 2.8888449668884277
training loss = 1.7787630558013916 29400
val loss = 1.6154992580413818
training loss = 1.7785050868988037 29500
val loss = 1.6062703132629395
training loss = 1.7815968990325928 29600
val loss = 1.5052460432052612
training loss = 1.7786600589752197 29700
val loss = 1.6173698902130127
training loss = 1.7783933877944946 29800
val loss = 1.6049503087997437
training loss = 1.8173127174377441 29900
val loss = 1.9880402088165283
training loss = 1.7784693241119385 30000
val loss = 1.6035799980163574
reduced chi^2 level 2 = 1.7784676551818848
Constrained alpha: 1.9353148937225342
Constrained beta: 0.6946333646774292
Constrained gamma: 11.897025108337402
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 892.2159,  896.2596,  939.5981,  904.0991,  998.7616, 1039.8197,
        1055.3552, 1115.0270, 1149.2047, 1177.9465, 1171.5797, 1230.8832,
        1237.8274, 1298.8424, 1331.8766, 1441.2078, 1432.6007, 1472.4946,
        1568.8638, 1504.3049, 1601.1080, 1604.4773, 1635.3771, 1629.7858,
        1661.1792, 1662.9159, 1634.8027, 1710.6256, 1651.8066, 1709.2361,
        1715.2795, 1773.5548, 1653.3702, 1650.3289, 1669.8745, 1790.6024,
        1633.0785, 1598.2246, 1616.0967, 1554.9159, 1641.5693, 1553.2679,
        1510.2173, 1487.0371, 1389.3602, 1325.1541, 1322.8856, 1283.4979,
        1139.3822, 1207.8551, 1131.4604,  972.3846,  979.7159,  965.5002,
         871.6181,  905.5046,  827.0518,  715.9191,  627.4865,  492.8243,
         537.7161,  487.7084,  460.0396,  403.2111,  369.7527,  357.8171,
         284.2725,  265.8266,  211.7311,  181.6357,  153.5871,  137.3875,
         144.6335,   97.8592,   89.2029,   67.9871,   47.2048,   38.0023,
          22.6099,   41.3695,   25.5369,   43.4792,   29.1507])]
2318.080080961394
3.3039289112680326 1.0180794494636225 12.548799687836997
val isze = 8
idinces = [78 65 46 48 22 28 38 29 60 63 23 33 47 80 56 54 50 39 72 69 55  3 62 27
 59 74 15 77  5 81  2 82 13 64 35 66 21 53  9  4 26  6 25 73 18 36 17 31
 20 45  1 52 57 19 58 68 41 44 42 34 76  8 12 24 40 61 51 11 71 32 70 79
 10 67 43  0  7 30 14 37 16 75 49]
we are doing training validation split
training loss = 121.5995101928711 100
val loss = 137.30514526367188
training loss = 46.88309860229492 200
val loss = 47.19654083251953
training loss = 22.785308837890625 300
val loss = 19.935976028442383
training loss = 13.825883865356445 400
val loss = 10.397092819213867
training loss = 9.58620834350586 500
val loss = 6.229313373565674
training loss = 7.340162754058838 600
val loss = 4.23700475692749
training loss = 6.085494041442871 700
val loss = 3.2733802795410156
training loss = 5.360752105712891 800
val loss = 2.8276679515838623
training loss = 4.919133186340332 900
val loss = 2.639033317565918
training loss = 4.614775657653809 1000
val loss = 2.5615034103393555
training loss = 4.3577375411987305 1100
val loss = 2.5105581283569336
training loss = 4.0976881980896 1200
val loss = 2.4435408115386963
training loss = 3.8144962787628174 1300
val loss = 2.3546698093414307
training loss = 3.502652645111084 1400
val loss = 2.2611021995544434
training loss = 3.1639394760131836 1500
val loss = 2.183961868286133
training loss = 2.8127782344818115 1600
val loss = 2.144956588745117
training loss = 2.478245496749878 1700
val loss = 2.1677756309509277
training loss = 2.195902109146118 1800
val loss = 2.2675940990448
training loss = 1.9912606477737427 1900
val loss = 2.4356017112731934
training loss = 1.866255283355713 2000
val loss = 2.6366617679595947
training loss = 1.8018107414245605 2100
val loss = 2.827704429626465
training loss = 1.7731236219406128 2200
val loss = 2.980499267578125
training loss = 1.7616993188858032 2300
val loss = 3.0887441635131836
training loss = 1.7575219869613647 2400
val loss = 3.1601812839508057
training loss = 1.7561694383621216 2500
val loss = 3.2063214778900146
training loss = 1.7558858394622803 2600
val loss = 3.2369000911712646
training loss = 1.7559894323349 2700
val loss = 3.2584309577941895
training loss = 1.7561938762664795 2800
val loss = 3.2746996879577637
training loss = 1.7563683986663818 2900
val loss = 3.2876715660095215
training loss = 1.7564489841461182 3000
val loss = 3.2983901500701904
training loss = 1.7563985586166382 3100
val loss = 3.3072452545166016
training loss = 1.7561959028244019 3200
val loss = 3.3144121170043945
training loss = 1.7558317184448242 3300
val loss = 3.319981336593628
training loss = 1.755301594734192 3400
val loss = 3.3240151405334473
training loss = 1.754602074623108 3500
val loss = 3.3265953063964844
training loss = 1.753738284111023 3600
val loss = 3.32776141166687
training loss = 1.7527145147323608 3700
val loss = 3.327646255493164
training loss = 1.751541018486023 3800
val loss = 3.3264074325561523
training loss = 1.750227451324463 3900
val loss = 3.3240303993225098
training loss = 1.7489242553710938 4000
val loss = 3.330688953399658
training loss = 1.7469795942306519 4100
val loss = 3.3163630962371826
training loss = 1.7454534769058228 4200
val loss = 3.321225166320801
training loss = 1.7433549165725708 4300
val loss = 3.3046653270721436
training loss = 1.7417060136795044 4400
val loss = 3.3080649375915527
training loss = 1.7396128177642822 4500
val loss = 3.2923059463500977
training loss = 1.7377535104751587 4600
val loss = 3.2827975749969482
training loss = 1.7358911037445068 4700
val loss = 3.2804951667785645
training loss = 1.7351986169815063 4800
val loss = 3.2961645126342773
training loss = 1.7322595119476318 4900
val loss = 3.267518997192383
training loss = 1.7304952144622803 5000
val loss = 3.2638907432556152
training loss = 1.7351430654525757 5100
val loss = 3.3116278648376465
training loss = 1.7270797491073608 5200
val loss = 3.2549588680267334
training loss = 1.7254399061203003 5300
val loss = 3.250084400177002
training loss = 1.724668264389038 5400
val loss = 3.263289451599121
training loss = 1.7222622632980347 5500
val loss = 3.242487907409668
training loss = 1.7207238674163818 5600
val loss = 3.237044334411621
training loss = 1.7192989587783813 5700
val loss = 3.2399325370788574
training loss = 1.7177600860595703 5800
val loss = 3.2310848236083984
training loss = 1.7165206670761108 5900
val loss = 3.220130205154419
training loss = 1.714955449104309 6000
val loss = 3.2240426540374756
training loss = 1.727067470550537 6100
val loss = 3.3020424842834473
training loss = 1.7123126983642578 6200
val loss = 3.2171530723571777
training loss = 1.7110213041305542 6300
val loss = 3.2141919136047363
training loss = 1.7097872495651245 6400
val loss = 3.2083797454833984
training loss = 1.7085527181625366 6500
val loss = 3.207496166229248
training loss = 1.7199479341506958 6600
val loss = 3.156486988067627
reduced chi^2 level 2 = 1.7093755006790161
Constrained alpha: 1.9547544717788696
Constrained beta: 3.461453914642334
Constrained gamma: 13.51769733428955
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 864.8820,  877.5286,  963.0952,  956.8280,  985.1772, 1042.4547,
        1100.6412, 1181.0388, 1182.3055, 1218.6967, 1220.2524, 1144.5610,
        1184.9297, 1283.8633, 1394.9424, 1429.4011, 1386.6997, 1458.0706,
        1522.8322, 1470.3960, 1563.7552, 1560.3040, 1638.1763, 1604.0492,
        1700.0347, 1767.0936, 1575.3245, 1731.7926, 1752.9913, 1805.4203,
        1691.4423, 1712.4055, 1706.7800, 1811.9099, 1683.4402, 1765.9899,
        1654.3983, 1539.9797, 1704.8950, 1595.0217, 1564.1685, 1553.2156,
        1464.0181, 1483.6555, 1377.0874, 1356.6005, 1244.3574, 1246.7858,
        1173.0923, 1193.7036, 1054.4032,  966.7200,  989.9335,  934.0074,
         907.1919,  897.8044,  782.2651,  678.5621,  614.9465,  512.9565,
         554.2291,  497.6003,  462.3359,  389.2878,  372.5193,  375.4200,
         306.9463,  237.1336,  188.0227,  168.7867,  146.5036,  165.7572,
         147.8018,  106.4488,  101.8567,   64.3375,   55.1568,   40.9719,
          29.8918,   42.1218,   20.2827,   45.0587,   41.6143])]
2717.1308424452236
3.066556592036973 11.20427339387554 82.38197677724715
val isze = 8
idinces = [76 24 25 53 50 30 59 48 49 52 12 80  0 60 16 47 36 56  3 22  9 18 63 26
 74 19  1 11 44 78 64 37 10  2 40 17 68 14 45 27 58 13 34  5 35 71 82 81
 73  4 20 21 72 51  7 28 55 54  6 70 39 61 33 57 31 15 79  8 66 38 77 65
 41 67 23 46 43 32 42 75 69 62 29]
we are doing training validation split
training loss = 150.0997772216797 100
val loss = 146.79550170898438
training loss = 7.76851749420166 200
val loss = 12.476612091064453
training loss = 7.438295364379883 300
val loss = 12.069124221801758
training loss = 7.164575576782227 400
val loss = 12.003122329711914
training loss = 6.951078414916992 500
val loss = 11.95168399810791
training loss = 6.791672229766846 600
val loss = 11.91222095489502
training loss = 6.67542028427124 700
val loss = 11.880428314208984
training loss = 6.591207981109619 800
val loss = 11.852884292602539
training loss = 6.529583930969238 900
val loss = 11.827038764953613
training loss = 6.483227729797363 1000
val loss = 11.80128002166748
training loss = 6.4468231201171875 1100
val loss = 11.774775505065918
training loss = 6.416682243347168 1200
val loss = 11.746938705444336
training loss = 6.390342712402344 1300
val loss = 11.7176513671875
training loss = 6.366207599639893 1400
val loss = 11.686824798583984
training loss = 6.343261241912842 1500
val loss = 11.654582977294922
training loss = 6.320872783660889 1600
val loss = 11.62090015411377
training loss = 6.298638343811035 1700
val loss = 11.585981369018555
training loss = 6.276307582855225 1800
val loss = 11.54983139038086
training loss = 6.253697395324707 1900
val loss = 11.512504577636719
training loss = 6.230659008026123 2000
val loss = 11.474008560180664
training loss = 6.207015037536621 2100
val loss = 11.434172630310059
training loss = 6.182522773742676 2200
val loss = 11.392610549926758
training loss = 6.156750202178955 2300
val loss = 11.348579406738281
training loss = 6.1288557052612305 2400
val loss = 11.3001708984375
training loss = 6.096925258636475 2500
val loss = 11.243239402770996
training loss = 6.055718898773193 2600
val loss = 11.165836334228516
training loss = 5.986906051635742 2700
val loss = 11.025704383850098
training loss = 5.806583404541016 2800
val loss = 10.634807586669922
training loss = 5.163105487823486 2900
val loss = 9.295125961303711
training loss = 3.7850189208984375 3000
val loss = 6.5525031089782715
training loss = 2.7828166484832764 3100
val loss = 4.394204139709473
training loss = 2.5288407802581787 3200
val loss = 3.8009424209594727
training loss = 2.469681978225708 3300
val loss = 3.8063483238220215
training loss = 2.437934398651123 3400
val loss = 3.905684471130371
training loss = 2.4144723415374756 3500
val loss = 3.9534058570861816
training loss = 2.3994359970092773 3600
val loss = 4.018413066864014
training loss = 2.3870882987976074 3700
val loss = 4.063233852386475
training loss = 2.3772501945495605 3800
val loss = 4.1006760597229
training loss = 2.3686654567718506 3900
val loss = 4.119714736938477
training loss = 2.361034870147705 4000
val loss = 4.139896869659424
training loss = 2.353959321975708 4100
val loss = 4.148996829986572
training loss = 2.347468137741089 4200
val loss = 4.153157711029053
training loss = 2.3412625789642334 4300
val loss = 4.15092134475708
training loss = 2.335557699203491 4400
val loss = 4.148684024810791
training loss = 2.330326795578003 4500
val loss = 4.145889759063721
training loss = 2.324965000152588 4600
val loss = 4.135618686676025
training loss = 2.3212344646453857 4700
val loss = 4.121201515197754
training loss = 2.3154985904693604 4800
val loss = 4.11652946472168
training loss = 2.3223235607147217 4900
val loss = 4.141788482666016
training loss = 2.3070101737976074 5000
val loss = 4.096023082733154
training loss = 2.3369979858398438 5100
val loss = 4.159753799438477
training loss = 2.2992489337921143 5200
val loss = 4.073307037353516
training loss = 2.2957141399383545 5300
val loss = 4.06329345703125
training loss = 2.2923104763031006 5400
val loss = 4.05048942565918
training loss = 2.288943290710449 5500
val loss = 4.041287899017334
training loss = 2.2859747409820557 5600
val loss = 4.033007621765137
training loss = 2.2827022075653076 5700
val loss = 4.020664691925049
training loss = 2.366821765899658 5800
val loss = 4.148200988769531
training loss = 2.2769222259521484 5900
val loss = 4.001687049865723
training loss = 2.274202823638916 6000
val loss = 3.994408369064331
training loss = 2.271810293197632 6100
val loss = 3.9845128059387207
training loss = 2.2689149379730225 6200
val loss = 3.9776716232299805
training loss = 2.2664363384246826 6300
val loss = 3.972318649291992
training loss = 2.2639315128326416 6400
val loss = 3.964235782623291
training loss = 2.2614505290985107 6500
val loss = 3.9569339752197266
training loss = 2.308535099029541 6600
val loss = 4.03324556350708
training loss = 2.2565932273864746 6700
val loss = 3.9444384574890137
training loss = 2.2542004585266113 6800
val loss = 3.939065933227539
training loss = 2.252079963684082 6900
val loss = 3.935579776763916
training loss = 2.249297857284546 7000
val loss = 3.9272003173828125
training loss = 2.2633979320526123 7100
val loss = 3.9564590454101562
training loss = 2.2442026138305664 7200
val loss = 3.9156031608581543
training loss = 2.241532325744629 7300
val loss = 3.910445213317871
training loss = 2.2388031482696533 7400
val loss = 3.905613899230957
training loss = 2.235731601715088 7500
val loss = 3.898496389389038
training loss = 2.234847068786621 7600
val loss = 3.8934340476989746
training loss = 2.228938579559326 7700
val loss = 3.885932445526123
training loss = 2.224970817565918 7800
val loss = 3.880681037902832
training loss = 2.2219595909118652 7900
val loss = 3.8720943927764893
training loss = 2.215824842453003 8000
val loss = 3.8697311878204346
training loss = 2.210888624191284 8100
val loss = 3.863982915878296
training loss = 2.2055623531341553 8200
val loss = 3.8600172996520996
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 876.7779,  821.8191,  998.4251,  956.7531,  947.7194, 1106.7469,
        1098.4319, 1165.0287, 1159.9468, 1167.5472, 1159.3676, 1167.0295,
        1290.5258, 1255.1293, 1293.3580, 1345.3907, 1344.9135, 1400.3690,
        1612.2500, 1408.5900, 1579.0486, 1523.9626, 1646.1644, 1617.0114,
        1665.6180, 1687.7673, 1571.6044, 1785.2426, 1737.6848, 1733.8005,
        1664.6749, 1712.7574, 1745.2083, 1767.2194, 1678.5110, 1793.3534,
        1719.0360, 1586.2253, 1629.1083, 1614.9585, 1607.7681, 1492.4165,
        1456.2313, 1500.0592, 1351.5726, 1313.8542, 1295.6564, 1176.8717,
        1219.0693, 1232.5823, 1087.3882,  960.8486,  942.7828,  948.0727,
         872.4531,  863.5571,  847.0506,  717.4566,  631.5937,  557.1512,
         566.6669,  449.9731,  476.2364,  402.0690,  321.6629,  361.9846,
         269.3752,  251.0094,  223.9418,  141.8452,  145.6357,  150.3965,
         150.8857,  104.7068,  105.2371,   54.6517,   56.5178,   43.5934,
          26.0202,   40.7120,   20.6280,   29.0017,   39.0608])]
3070.276158504501
2.0432994268108535 3.829002104024939 14.982370638988884
val isze = 8
idinces = [19 47 14 15 32 36 20 72 42 75 48 12 29 66 64 78  0 23 68  1 38 65 37 11
 35 76 17 39 34 81 28 41  2 67 80 82 10 61 69 53 26 62 46 59  5 16 58 63
 40 56 71 21 45  6 27 50 44 52  3 30 33  9 60 70 43  8 79 73 22 25 55 49
 18  7 54 57 74 77  4 13 24 51 31]
we are doing training validation split
training loss = 36.275489807128906 100
val loss = 23.981170654296875
training loss = 25.630325317382812 200
val loss = 18.459823608398438
training loss = 18.691295623779297 300
val loss = 13.666522979736328
training loss = 14.27624225616455 400
val loss = 10.763093948364258
training loss = 11.442394256591797 500
val loss = 9.056684494018555
training loss = 9.587896347045898 600
val loss = 8.077031135559082
training loss = 8.352928161621094 700
val loss = 7.536944389343262
training loss = 7.519725322723389 800
val loss = 7.263384819030762
training loss = 6.952673435211182 900
val loss = 7.150527477264404
training loss = 6.5648322105407715 1000
val loss = 7.13308048248291
training loss = 6.2990593910217285 1100
val loss = 7.170041084289551
training loss = 6.116994857788086 1200
val loss = 7.23557186126709
training loss = 5.992502212524414 1300
val loss = 7.313423156738281
training loss = 5.907557964324951 1400
val loss = 7.393429756164551
training loss = 5.849668025970459 1500
val loss = 7.469514846801758
training loss = 5.810133457183838 1600
val loss = 7.538223743438721
training loss = 5.782923221588135 1700
val loss = 7.597814559936523
training loss = 5.763876914978027 1800
val loss = 7.647769927978516
training loss = 5.750147342681885 1900
val loss = 7.688314437866211
training loss = 5.739825248718262 2000
val loss = 7.72010612487793
training loss = 5.7316365242004395 2100
val loss = 7.744061470031738
training loss = 5.724755764007568 2200
val loss = 7.761261940002441
training loss = 5.718663692474365 2300
val loss = 7.7727766036987305
training loss = 5.713033199310303 2400
val loss = 7.779645919799805
training loss = 5.707665920257568 2500
val loss = 7.78280782699585
training loss = 5.702446460723877 2600
val loss = 7.78306770324707
training loss = 5.697296619415283 2700
val loss = 7.781129837036133
training loss = 5.692166328430176 2800
val loss = 7.777540683746338
training loss = 5.687013149261475 2900
val loss = 7.772702217102051
training loss = 5.681781768798828 3000
val loss = 7.766912460327148
training loss = 5.67639684677124 3100
val loss = 7.760348320007324
training loss = 5.670742034912109 3200
val loss = 7.7530646324157715
training loss = 5.664629936218262 3300
val loss = 7.744927406311035
training loss = 5.65772008895874 3400
val loss = 7.735617637634277
training loss = 5.649415016174316 3500
val loss = 7.7244110107421875
training loss = 5.638572692871094 3600
val loss = 7.709853649139404
training loss = 5.622830390930176 3700
val loss = 7.688745021820068
training loss = 5.596990585327148 3800
val loss = 7.6538519859313965
training loss = 5.548799991607666 3900
val loss = 7.586944580078125
training loss = 5.451107025146484 4000
val loss = 7.443777084350586
training loss = 5.27032470703125 4100
val loss = 7.161404609680176
training loss = 5.023458957672119 4200
val loss = 6.787199974060059
training loss = 4.726273059844971 4300
val loss = 6.40711784362793
training loss = 4.3328328132629395 4400
val loss = 5.938712120056152
training loss = 3.790938377380371 4500
val loss = 5.293242454528809
training loss = 3.140857696533203 4600
val loss = 4.49722146987915
training loss = 2.701005458831787 4700
val loss = 3.852006196975708
training loss = 2.60186505317688 4800
val loss = 3.6001434326171875
training loss = 2.588031530380249 4900
val loss = 3.5325205326080322
training loss = 2.5811493396759033 5000
val loss = 3.50811767578125
training loss = 2.575080156326294 5100
val loss = 3.4927234649658203
training loss = 2.5693938732147217 5200
val loss = 3.480175018310547
training loss = 2.5640268325805664 5300
val loss = 3.4693005084991455
training loss = 2.558947801589966 5400
val loss = 3.460145950317383
training loss = 2.55460262298584 5500
val loss = 3.4413747787475586
training loss = 2.5502662658691406 5600
val loss = 3.4446334838867188
training loss = 2.5574448108673096 5700
val loss = 3.3774631023406982
training loss = 2.542900562286377 5800
val loss = 3.4333908557891846
training loss = 2.5395805835723877 5900
val loss = 3.429647207260132
training loss = 2.5377657413482666 6000
val loss = 3.4554443359375
training loss = 2.533576726913452 6100
val loss = 3.4227421283721924
training loss = 2.5307629108428955 6200
val loss = 3.4173812866210938
training loss = 2.5281760692596436 6300
val loss = 3.4115521907806396
training loss = 2.525566577911377 6400
val loss = 3.4163758754730225
training loss = 2.544843912124634 6500
val loss = 3.5606658458709717
training loss = 2.520693302154541 6600
val loss = 3.412754774093628
training loss = 2.5183610916137695 6700
val loss = 3.4124040603637695
training loss = 2.5177316665649414 6800
val loss = 3.4447884559631348
training loss = 2.5138676166534424 6900
val loss = 3.410329580307007
training loss = 2.5574018955230713 7000
val loss = 3.3092236518859863
training loss = 2.509554862976074 7100
val loss = 3.406604766845703
training loss = 2.5074710845947266 7200
val loss = 3.4085640907287598
training loss = 2.5632927417755127 7300
val loss = 3.67080020904541
training loss = 2.503326177597046 7400
val loss = 3.405972480773926
training loss = 2.5012688636779785 7500
val loss = 3.4076895713806152
training loss = 2.5005974769592285 7600
val loss = 3.4375417232513428
training loss = 2.4972758293151855 7700
val loss = 3.4071757793426514
training loss = 2.495302200317383 7800
val loss = 3.413682222366333
training loss = 2.4932854175567627 7900
val loss = 3.4044384956359863
training loss = 2.491284132003784 8000
val loss = 3.4076240062713623
training loss = 2.4908838272094727 8100
val loss = 3.4395651817321777
training loss = 2.4874184131622314 8200
val loss = 3.4073526859283447
training loss = 2.4854650497436523 8300
val loss = 3.408137321472168
training loss = 2.483452081680298 8400
val loss = 3.4077138900756836
training loss = 2.4815776348114014 8500
val loss = 3.408513307571411
training loss = 2.4796042442321777 8600
val loss = 3.409541130065918
training loss = 2.477796792984009 8700
val loss = 3.400270938873291
training loss = 2.4757096767425537 8800
val loss = 3.410335063934326
training loss = 2.5255799293518066 8900
val loss = 3.3062684535980225
training loss = 2.471788167953491 9000
val loss = 3.412189245223999
training loss = 2.469789981842041 9100
val loss = 3.414551019668579
training loss = 2.467971086502075 9200
val loss = 3.4046473503112793
training loss = 2.4659030437469482 9300
val loss = 3.413680076599121
training loss = 2.4642207622528076 9400
val loss = 3.400614023208618
training loss = 2.4619650840759277 9500
val loss = 3.415128231048584
training loss = 2.493821144104004 9600
val loss = 3.6054484844207764
training loss = 2.458038806915283 9700
val loss = 3.4160823822021484
training loss = 2.4560744762420654 9800
val loss = 3.418280601501465
training loss = 2.462252140045166 9900
val loss = 3.5006136894226074
training loss = 2.4521377086639404 10000
val loss = 3.420078754425049
training loss = 2.450845241546631 10100
val loss = 3.4016785621643066
training loss = 2.4482219219207764 10200
val loss = 3.4264321327209473
training loss = 2.4462292194366455 10300
val loss = 3.4236156940460205
training loss = 2.44978666305542 10400
val loss = 3.491353750228882
training loss = 2.4423129558563232 10500
val loss = 3.4253621101379395
training loss = 2.4403111934661865 10600
val loss = 3.4270806312561035
training loss = 2.4383544921875 10700
val loss = 3.4306726455688477
training loss = 2.436413288116455 10800
val loss = 3.4289650917053223
training loss = 2.4346537590026855 10900
val loss = 3.4442403316497803
training loss = 2.432487964630127 11000
val loss = 3.4263710975646973
training loss = 2.4304652214050293 11100
val loss = 3.432378053665161
training loss = 2.4461448192596436 11200
val loss = 3.5607571601867676
training loss = 2.426454782485962 11300
val loss = 3.4336438179016113
training loss = 2.425384283065796 11400
val loss = 3.462569236755371
training loss = 2.422358751296997 11500
val loss = 3.438715934753418
training loss = 2.420238971710205 11600
val loss = 3.437713623046875
training loss = 2.4187111854553223 11700
val loss = 3.457770347595215
training loss = 2.415973663330078 11800
val loss = 3.438894510269165
training loss = 2.413673162460327 11900
val loss = 3.4399466514587402
training loss = 2.411956787109375 12000
val loss = 3.462207317352295
training loss = 2.4087066650390625 12100
val loss = 3.4410390853881836
training loss = 2.420300006866455 12200
val loss = 3.367772340774536
training loss = 2.4029316902160645 12300
val loss = 3.4410557746887207
training loss = 2.3994991779327393 12400
val loss = 3.4445362091064453
training loss = 2.3959248065948486 12500
val loss = 3.4341869354248047
training loss = 2.391488790512085 12600
val loss = 3.4465432167053223
training loss = 2.3883614540100098 12700
val loss = 3.481910228729248
training loss = 2.3819050788879395 12800
val loss = 3.451570987701416
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 886.8088,  880.9949,  935.7694,  952.8605,  991.1746, 1046.3958,
        1067.1914, 1082.1776, 1135.9810, 1168.7284, 1201.7177, 1281.6357,
        1237.6112, 1248.8628, 1316.4655, 1381.8561, 1349.2974, 1413.2469,
        1627.3370, 1516.4901, 1575.9985, 1548.5653, 1559.6252, 1571.4371,
        1612.2797, 1773.5449, 1606.5543, 1711.5143, 1715.6105, 1680.0577,
        1663.1986, 1748.2131, 1759.8324, 1724.8451, 1691.5138, 1660.2363,
        1621.6451, 1581.5894, 1601.0258, 1560.2064, 1659.7004, 1570.9819,
        1490.1536, 1473.0850, 1395.1212, 1327.8317, 1269.0975, 1262.1765,
        1131.8779, 1136.1157, 1070.3752,  997.4059,  952.4589,  939.1667,
         861.9110,  821.7633,  848.6589,  673.9692,  603.2957,  574.3691,
         551.2000,  430.9000,  423.5876,  357.7857,  356.1802,  340.6633,
         267.5558,  254.3772,  219.1286,  185.4505,  162.9660,  132.0132,
         146.3918,   80.1238,  108.4412,   59.0993,   59.4775,   29.3986,
          30.4394,   42.7031,   23.1420,   43.5902,   32.4409])]
2472.021445855359
1.5661848706157289 16.924778151554886 88.31279646491801
val isze = 8
idinces = [45 68 50 23 17 43 61 24 19 49  3  8 15  5 21 41 34 53 10 39 31  0 28 78
 26 54 65 36 29 12 33 38  6 18 74 27 62 20 46 52  2 40 60 75  4 51 47 67
 59  9 66 44  7 73 64 69 30 16 81 37 76 72 55 13 35  1 48 82 79 57 42 71
 25 14 80 63 77 11 32 22 70 56 58]
we are doing training validation split
training loss = 173.1238555908203 100
val loss = 189.55807495117188
training loss = 15.527965545654297 200
val loss = 9.349900245666504
training loss = 10.67077922821045 300
val loss = 6.631593704223633
training loss = 10.229208946228027 400
val loss = 6.219527244567871
training loss = 9.748566627502441 500
val loss = 5.787927627563477
training loss = 9.24843692779541 600
val loss = 5.345938682556152
training loss = 8.748788833618164 700
val loss = 4.913986682891846
training loss = 8.269562721252441 800
val loss = 4.511612892150879
training loss = 7.829380035400391 900
val loss = 4.156367778778076
training loss = 7.4435224533081055 1000
val loss = 3.861417293548584
training loss = 7.121785640716553 1100
val loss = 3.6335372924804688
training loss = 6.866933822631836 1200
val loss = 3.471785068511963
training loss = 6.6745219230651855 1300
val loss = 3.3682408332824707
training loss = 6.534476280212402 1400
val loss = 3.31002140045166
training loss = 6.433902263641357 1500
val loss = 3.282559871673584
training loss = 6.360076427459717 1600
val loss = 3.2727479934692383
training loss = 6.302572727203369 1700
val loss = 3.270742893218994
training loss = 6.254067420959473 1800
val loss = 3.270371198654175
training loss = 6.210103988647461 1900
val loss = 3.268596887588501
training loss = 6.168335914611816 2000
val loss = 3.2643795013427734
training loss = 6.127734661102295 2100
val loss = 3.257819175720215
training loss = 6.087968826293945 2200
val loss = 3.249502658843994
training loss = 6.0490217208862305 2300
val loss = 3.2400238513946533
training loss = 6.01096773147583 2400
val loss = 3.229985475540161
training loss = 5.973845958709717 2500
val loss = 3.219712018966675
training loss = 5.9375901222229 2600
val loss = 3.2094168663024902
training loss = 5.901956558227539 2700
val loss = 3.199176549911499
training loss = 5.866426944732666 2800
val loss = 3.188845157623291
training loss = 5.830030918121338 2900
val loss = 3.178046703338623
training loss = 5.790884017944336 3000
val loss = 3.1661438941955566
training loss = 5.744974613189697 3100
val loss = 3.1516714096069336
training loss = 5.682464599609375 3200
val loss = 3.1313540935516357
training loss = 5.576809406280518 3300
val loss = 3.097419023513794
training loss = 5.369243144989014 3400
val loss = 3.032627582550049
training loss = 5.004660129547119 3500
val loss = 2.903311014175415
training loss = 4.391168117523193 3600
val loss = 2.6358797550201416
training loss = 3.355760335922241 3700
val loss = 2.11019229888916
training loss = 2.3370158672332764 3800
val loss = 1.3777194023132324
training loss = 2.018543243408203 3900
val loss = 0.9852142930030823
training loss = 1.9850924015045166 4000
val loss = 0.9041399359703064
training loss = 1.9738017320632935 4100
val loss = 0.8889847993850708
training loss = 1.9650546312332153 4200
val loss = 0.8836137652397156
training loss = 1.9577527046203613 4300
val loss = 0.8804031014442444
training loss = 1.9515429735183716 4400
val loss = 0.8781949281692505
training loss = 1.9461926221847534 4500
val loss = 0.8766897320747375
training loss = 1.941519856452942 4600
val loss = 0.8756897449493408
training loss = 1.9373905658721924 4700
val loss = 0.8751572966575623
training loss = 1.9337186813354492 4800
val loss = 0.8813459277153015
training loss = 1.9301046133041382 4900
val loss = 0.8681408762931824
training loss = 1.926762342453003 5000
val loss = 0.8711268901824951
training loss = 1.923454999923706 5100
val loss = 0.8839272260665894
training loss = 1.9202710390090942 5200
val loss = 0.8747329711914062
training loss = 1.917209506034851 5300
val loss = 0.8871804475784302
training loss = 1.914106845855713 5400
val loss = 0.8755387663841248
training loss = 1.9154932498931885 5500
val loss = 0.9608986377716064
training loss = 1.9083188772201538 5600
val loss = 0.8774199485778809
training loss = 1.9055612087249756 5700
val loss = 0.8784757256507874
training loss = 1.9030567407608032 5800
val loss = 0.8949081897735596
training loss = 1.9002500772476196 5900
val loss = 0.8786919713020325
training loss = 1.8977022171020508 6000
val loss = 0.8839925527572632
training loss = 1.895216464996338 6100
val loss = 0.8806456923484802
training loss = 1.8929952383041382 6200
val loss = 0.863486111164093
training loss = 1.8904211521148682 6300
val loss = 0.8821441531181335
training loss = 1.8881034851074219 6400
val loss = 0.8828821182250977
training loss = 1.9112838506698608 6500
val loss = 1.106767177581787
training loss = 1.8836216926574707 6600
val loss = 0.8861704468727112
training loss = 1.8814109563827515 6700
val loss = 0.8857283592224121
training loss = 1.879642367362976 6800
val loss = 0.8651025891304016
training loss = 1.8772023916244507 6900
val loss = 0.8883744478225708
training loss = 1.998979091644287 7000
val loss = 1.4609901905059814
training loss = 1.87311851978302 7100
val loss = 0.8868938088417053
training loss = 1.8711020946502686 7200
val loss = 0.8921059370040894
training loss = 1.9115194082260132 7300
val loss = 1.1921298503875732
training loss = 1.867195725440979 7400
val loss = 0.8931736946105957
training loss = 1.8652466535568237 7500
val loss = 0.895869255065918
training loss = 1.865666389465332 7600
val loss = 0.8409230709075928
training loss = 1.8615361452102661 7700
val loss = 0.8982676267623901
training loss = 1.8596765995025635 7800
val loss = 0.8995286822319031
training loss = 1.8596538305282593 7900
val loss = 0.8513266444206238
training loss = 1.855985164642334 8000
val loss = 0.9009443521499634
training loss = 1.8541138172149658 8100
val loss = 0.9022221565246582
training loss = 1.8524848222732544 8200
val loss = 0.8887892365455627
training loss = 1.8504520654678345 8300
val loss = 0.9064121246337891
training loss = 1.866479754447937 8400
val loss = 1.0926498174667358
training loss = 1.8467161655426025 8500
val loss = 0.9084057211875916
training loss = 1.8450837135314941 8600
val loss = 0.8894296884536743
training loss = 1.8428717851638794 8700
val loss = 0.904168963432312
training loss = 1.8407680988311768 8800
val loss = 0.9143753051757812
training loss = 1.8397549390792847 8900
val loss = 0.8754660487174988
training loss = 1.836460828781128 9000
val loss = 0.9177422523498535
training loss = 1.83405601978302 9100
val loss = 0.920911431312561
training loss = 1.8440654277801514 9200
val loss = 1.0757254362106323
training loss = 1.828718662261963 9300
val loss = 0.9254173636436462
training loss = 1.8256170749664307 9400
val loss = 0.9298697113990784
training loss = 1.8229032754898071 9500
val loss = 0.9665185213088989
training loss = 1.8186097145080566 9600
val loss = 0.9383120536804199
training loss = 1.9355156421661377 9700
val loss = 1.507673978805542
training loss = 1.8105595111846924 9800
val loss = 0.9521417617797852
training loss = 1.8062965869903564 9900
val loss = 0.952852725982666
training loss = 1.804946780204773 10000
val loss = 0.8953555226325989
training loss = 1.7978918552398682 10100
val loss = 0.9616490006446838
training loss = 1.8015239238739014 10200
val loss = 1.083378553390503
training loss = 1.7897282838821411 10300
val loss = 0.9714475870132446
training loss = 1.7857308387756348 10400
val loss = 0.9741135239601135
reduced chi^2 level 2 = 1.7829419374465942
Constrained alpha: 1.8676096200942993
Constrained beta: 2.92669677734375
Constrained gamma: 21.062517166137695
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 831.2139,  873.2148,  980.8333,  900.9595, 1004.7984, 1071.8774,
        1082.7808, 1129.2780, 1136.5872, 1121.0929, 1204.6801, 1152.2928,
        1265.5024, 1293.0398, 1306.4779, 1425.9342, 1395.8171, 1451.7567,
        1586.8385, 1484.3176, 1573.0115, 1585.2878, 1530.1396, 1580.5178,
        1677.5774, 1722.1418, 1648.9221, 1612.0970, 1780.4941, 1732.2396,
        1708.0068, 1756.0074, 1696.3468, 1791.0652, 1683.1196, 1746.0543,
        1728.9384, 1640.0166, 1597.7327, 1659.1876, 1687.2396, 1608.6782,
        1486.0304, 1520.1967, 1353.2330, 1246.3329, 1214.7539, 1231.4589,
        1165.0938, 1183.5380, 1089.4766,  997.2288,  926.2925,  913.3132,
         906.4782,  874.8146,  846.0569,  713.7725,  649.8605,  521.2037,
         513.5862,  477.3379,  422.9385,  406.1613,  368.5565,  360.0224,
         271.5386,  291.9576,  219.4138,  156.6940,  167.3519,  138.4243,
         155.0640,  108.0093,  102.6594,   66.6053,   38.2937,   51.3768,
          43.9524,   41.8155,   22.1238,   29.7164,   25.1802])]
2923.79454459122
0.04293594643561993 12.948248317594102 78.82961660037614
val isze = 8
idinces = [60 41  2 30  1 59 70 56 24 12  3 64 13 16 17 82 25 10 71 20 63 40 69 53
 76 19 77 18 22 34 68  0  4 49 50 35 32 21 43 29  6 15 45  9  7 57 28 26
 23 42 81 36 52 78  8 55 47 48 39 65 11 54 61 14  5 31 37 38 44 73 75 72
 27 33 62 67 46 79 80 74 66 58 51]
we are doing training validation split
training loss = 20.247068405151367 100
val loss = 21.742202758789062
training loss = 11.291731834411621 200
val loss = 6.8744306564331055
training loss = 10.909337997436523 300
val loss = 6.601747035980225
training loss = 10.482218742370605 400
val loss = 6.376959323883057
training loss = 10.014949798583984 500
val loss = 6.168057441711426
training loss = 9.525735855102539 600
val loss = 5.998074054718018
training loss = 9.033369064331055 700
val loss = 5.888314247131348
training loss = 8.557092666625977 800
val loss = 5.856505393981934
training loss = 8.115483283996582 900
val loss = 5.914257049560547
training loss = 7.724661827087402 1000
val loss = 6.064141273498535
training loss = 7.396029472351074 1100
val loss = 6.2974853515625
training loss = 7.134373664855957 1200
val loss = 6.594208717346191
training loss = 6.937132358551025 1300
val loss = 6.925453186035156
training loss = 6.795454025268555 1400
val loss = 7.259125709533691
training loss = 6.696774482727051 1500
val loss = 7.566329002380371
training loss = 6.627995014190674 1600
val loss = 7.826910972595215
training loss = 6.577938556671143 1700
val loss = 8.031498908996582
training loss = 6.53849458694458 1800
val loss = 8.180384635925293
training loss = 6.504542827606201 1900
val loss = 8.280412673950195
training loss = 6.473177433013916 2000
val loss = 8.341377258300781
training loss = 6.442859172821045 2100
val loss = 8.373313903808594
training loss = 6.412724018096924 2200
val loss = 8.384772300720215
training loss = 6.382134437561035 2300
val loss = 8.382211685180664
training loss = 6.350407600402832 2400
val loss = 8.370170593261719
training loss = 6.316586971282959 2500
val loss = 8.35126781463623
training loss = 6.279180526733398 2600
val loss = 8.326513290405273
training loss = 6.235626220703125 2700
val loss = 8.295636177062988
training loss = 6.18110466003418 2800
val loss = 8.256497383117676
training loss = 6.105565071105957 2900
val loss = 8.204920768737793
training loss = 5.986449718475342 3000
val loss = 8.132772445678711
training loss = 5.7763519287109375 3100
val loss = 8.025341987609863
training loss = 5.4135847091674805 3200
val loss = 7.834652900695801
training loss = 4.873262882232666 3300
val loss = 7.385397911071777
training loss = 4.134393692016602 3400
val loss = 6.478726387023926
training loss = 3.2699875831604004 3500
val loss = 5.203463554382324
training loss = 2.6707420349121094 3600
val loss = 4.087547302246094
training loss = 2.477334976196289 3700
val loss = 3.6179144382476807
training loss = 2.4037508964538574 3800
val loss = 3.5049827098846436
training loss = 2.3511807918548584 3900
val loss = 3.481747627258301
training loss = 2.3090457916259766 4000
val loss = 3.4795384407043457
training loss = 2.274653911590576 4100
val loss = 3.4848079681396484
training loss = 2.2462668418884277 4200
val loss = 3.4940552711486816
training loss = 2.2226083278656006 4300
val loss = 3.505640983581543
training loss = 2.2027151584625244 4400
val loss = 3.518564462661743
training loss = 2.185842275619507 4500
val loss = 3.53206729888916
training loss = 2.17140793800354 4600
val loss = 3.545720100402832
training loss = 2.1589505672454834 4700
val loss = 3.559147834777832
training loss = 2.1481006145477295 4800
val loss = 3.5722286701202393
training loss = 2.138554573059082 4900
val loss = 3.5847787857055664
training loss = 2.1300668716430664 5000
val loss = 3.5968167781829834
training loss = 2.127485513687134 5100
val loss = 3.6582000255584717
training loss = 2.116117000579834 5200
val loss = 3.6186656951904297
training loss = 2.110344886779785 5300
val loss = 3.6330909729003906
training loss = 2.1050405502319336 5400
val loss = 3.6318607330322266
training loss = 2.1001718044281006 5500
val loss = 3.6447606086730957
training loss = 2.097982406616211 5600
val loss = 3.6225016117095947
training loss = 2.0913286209106445 5700
val loss = 3.6609060764312744
training loss = 2.0873019695281982 5800
val loss = 3.6759181022644043
training loss = 2.083289384841919 5900
val loss = 3.6714377403259277
training loss = 2.0794105529785156 6000
val loss = 3.6840572357177734
training loss = 2.0755209922790527 6100
val loss = 3.6911392211914062
training loss = 2.071958541870117 6200
val loss = 3.711977481842041
training loss = 2.067608594894409 6300
val loss = 3.7062759399414062
training loss = 2.069798707962036 6400
val loss = 3.7709593772888184
training loss = 2.0591022968292236 6500
val loss = 3.7192342281341553
training loss = 2.0545196533203125 6600
val loss = 3.7285099029541016
training loss = 2.049807548522949 6700
val loss = 3.7280964851379395
training loss = 2.044884443283081 6800
val loss = 3.7364368438720703
training loss = 2.040107011795044 6900
val loss = 3.7525277137756348
training loss = 2.0347819328308105 7000
val loss = 3.7440121173858643
training loss = 2.1134653091430664 7100
val loss = 3.631324052810669
training loss = 2.024456739425659 7200
val loss = 3.7526562213897705
training loss = 2.0191922187805176 7300
val loss = 3.7553014755249023
training loss = 2.0139760971069336 7400
val loss = 3.765796184539795
training loss = 2.0085647106170654 7500
val loss = 3.7663729190826416
training loss = 2.004338026046753 7600
val loss = 3.750789165496826
training loss = 1.9976445436477661 7700
val loss = 3.7779903411865234
training loss = 1.9951905012130737 7800
val loss = 3.822380542755127
training loss = 1.9863682985305786 7900
val loss = 3.7926619052886963
training loss = 1.9805591106414795 8000
val loss = 3.795738935470581
training loss = 1.975969910621643 8100
val loss = 3.8242712020874023
training loss = 1.9690361022949219 8200
val loss = 3.808229446411133
training loss = 2.1823949813842773 8300
val loss = 3.717789888381958
training loss = 1.9575505256652832 8400
val loss = 3.819735288619995
training loss = 1.9518929719924927 8500
val loss = 3.826842784881592
training loss = 1.9649876356124878 8600
val loss = 3.929319381713867
training loss = 1.9407886266708374 8700
val loss = 3.8390157222747803
training loss = 1.9353352785110474 8800
val loss = 3.845994234085083
training loss = 1.9305310249328613 8900
val loss = 3.8629324436187744
reduced chi^2 level 2 = 1.9268461465835571
Constrained alpha: 1.708250641822815
Constrained beta: 3.573117256164551
Constrained gamma: 25.634342193603516
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 875.5960,  853.4444,  938.6713,  957.1531,  986.8898, 1050.6095,
        1090.4565, 1141.0741, 1189.9624, 1099.5197, 1226.9692, 1240.8116,
        1280.8496, 1191.4146, 1238.6959, 1361.9301, 1398.9644, 1510.9038,
        1588.2909, 1425.9883, 1580.4662, 1582.7396, 1598.0900, 1606.2887,
        1658.7745, 1636.3954, 1626.6177, 1630.8445, 1659.4327, 1728.3575,
        1602.0953, 1705.6406, 1714.5624, 1734.5132, 1699.2693, 1736.1771,
        1675.9264, 1603.4521, 1604.5613, 1575.7848, 1632.1071, 1549.2211,
        1597.1149, 1546.7086, 1354.1696, 1368.3320, 1248.7717, 1204.6664,
        1199.0276, 1199.8546, 1095.3147,  999.0607,  965.9274,  886.2399,
         919.7935,  911.7724,  842.5679,  754.2753,  596.7046,  569.7230,
         574.7111,  498.3891,  438.9099,  362.1914,  349.4403,  340.2984,
         322.3895,  269.4146,  213.7638,  184.8065,  174.3253,  148.7432,
         136.9584,  111.6181,  112.0717,   65.7598,   46.3125,   42.1993,
          32.2028,   42.1514,   21.4196,   41.3845,   36.1358])]
2838.6144516542663
2.6543525725287904 5.71197058550085 39.78689605224503
val isze = 8
idinces = [76 12 69  0 56 81 80 18  5 48 49 62 67 21 23  2  6 59 66 39 63 47 82 51
 10 45 31 24  9 42 28 74 57 71 41 38  3 15 20 64 79 14 25 50 30 40 37 22
 33 52 29 54 26 73 32 77 27 43 36 44 16 58 78 17 34 35 65  1  7 46 60 13
 68 61  8 72 19 11 75 55 53  4 70]
we are doing training validation split
training loss = 24.176305770874023 100
val loss = 24.952747344970703
training loss = 17.975181579589844 200
val loss = 17.621315002441406
training loss = 13.62973690032959 300
val loss = 13.193939208984375
training loss = 10.746902465820312 400
val loss = 10.688257217407227
training loss = 8.883564949035645 500
val loss = 9.341578483581543
training loss = 7.68335485458374 600
val loss = 8.653693199157715
training loss = 6.908017635345459 700
val loss = 8.333763122558594
training loss = 6.405158996582031 800
val loss = 8.216381072998047
training loss = 6.077728271484375 900
val loss = 8.206880569458008
training loss = 5.863541126251221 1000
val loss = 8.250611305236816
training loss = 5.722346305847168 1100
val loss = 8.315908432006836
training loss = 5.62795352935791 1200
val loss = 8.384687423706055
training loss = 5.563284873962402 1300
val loss = 8.447056770324707
training loss = 5.5172295570373535 1400
val loss = 8.498085021972656
training loss = 5.482612609863281 1500
val loss = 8.535931587219238
training loss = 5.454875469207764 1600
val loss = 8.560484886169434
training loss = 5.431173801422119 1700
val loss = 8.572772979736328
training loss = 5.4097700119018555 1800
val loss = 8.574275016784668
training loss = 5.389653205871582 1900
val loss = 8.566730499267578
training loss = 5.370241641998291 2000
val loss = 8.55180549621582
training loss = 5.351219177246094 2100
val loss = 8.530989646911621
training loss = 5.3324151039123535 2200
val loss = 8.505648612976074
training loss = 5.3136982917785645 2300
val loss = 8.476653099060059
training loss = 5.294918060302734 2400
val loss = 8.444628715515137
training loss = 5.275840759277344 2500
val loss = 8.409725189208984
training loss = 5.255993843078613 2600
val loss = 8.371310234069824
training loss = 5.234452247619629 2700
val loss = 8.327672004699707
training loss = 5.20924186706543 2800
val loss = 8.274654388427734
training loss = 5.1758527755737305 2900
val loss = 8.202301025390625
training loss = 5.123216152191162 3000
val loss = 8.086215019226074
training loss = 5.025415897369385 3100
val loss = 7.871485710144043
training loss = 4.848027229309082 3200
val loss = 7.503899097442627
training loss = 4.597461700439453 3300
val loss = 7.065579891204834
training loss = 4.267448425292969 3400
val loss = 6.588477611541748
training loss = 3.7979331016540527 3500
val loss = 5.934353828430176
training loss = 3.1415867805480957 3600
val loss = 5.034670829772949
training loss = 2.4167704582214355 3700
val loss = 4.185360908508301
training loss = 2.0257441997528076 3800
val loss = 4.047775745391846
training loss = 1.9504748582839966 3900
val loss = 4.223184108734131
training loss = 1.9370321035385132 4000
val loss = 4.306113243103027
training loss = 1.9298871755599976 4100
val loss = 4.337096214294434
training loss = 1.924208641052246 4200
val loss = 4.354309558868408
training loss = 1.9194289445877075 4300
val loss = 4.367108345031738
training loss = 1.9153493642807007 4400
val loss = 4.377353668212891
training loss = 1.911842942237854 4500
val loss = 4.385359287261963
training loss = 1.9088029861450195 4600
val loss = 4.391331195831299
training loss = 1.90614652633667 4700
val loss = 4.395403861999512
training loss = 1.9038071632385254 4800
val loss = 4.397782802581787
training loss = 1.9017252922058105 4900
val loss = 4.3984527587890625
training loss = 1.8998596668243408 5000
val loss = 4.397882461547852
training loss = 1.9286526441574097 5100
val loss = 4.1487531661987305
training loss = 1.8966028690338135 5200
val loss = 4.3981733322143555
training loss = 1.8952144384384155 5300
val loss = 4.3929219245910645
training loss = 1.8941669464111328 5400
val loss = 4.364786148071289
training loss = 1.8927018642425537 5500
val loss = 4.387290954589844
training loss = 1.8936798572540283 5600
val loss = 4.4541521072387695
training loss = 1.8904508352279663 5700
val loss = 4.38359260559082
training loss = 1.8894716501235962 5800
val loss = 4.380529880523682
training loss = 1.8952842950820923 5900
val loss = 4.258124351501465
training loss = 1.887560248374939 6000
val loss = 4.375087738037109
training loss = 1.890785813331604 6100
val loss = 4.2792229652404785
training loss = 1.885810136795044 6200
val loss = 4.36867618560791
training loss = 1.885023832321167 6300
val loss = 4.369340419769287
training loss = 1.8844971656799316 6400
val loss = 4.391990661621094
training loss = 1.8834573030471802 6500
val loss = 4.363888740539551
training loss = 1.8860846757888794 6600
val loss = 4.450502872467041
training loss = 1.8820003271102905 6700
val loss = 4.359303951263428
training loss = 1.8856934309005737 6800
val loss = 4.259138584136963
training loss = 1.8806451559066772 6900
val loss = 4.354785919189453
training loss = 1.8800480365753174 7000
val loss = 4.35207986831665
training loss = 1.8794761896133423 7100
val loss = 4.33738899230957
training loss = 1.8788423538208008 7200
val loss = 4.348474502563477
training loss = 1.8827592134475708 7300
val loss = 4.44990348815918
training loss = 1.8777153491973877 7400
val loss = 4.345340728759766
training loss = 2.1489028930664062 7500
val loss = 3.7244718074798584
training loss = 1.8766674995422363 7600
val loss = 4.341159343719482
training loss = 1.8762142658233643 7700
val loss = 4.339693069458008
training loss = 1.8759562969207764 7800
val loss = 4.360219955444336
training loss = 1.8752763271331787 7900
val loss = 4.334409713745117
training loss = 1.8760743141174316 8000
val loss = 4.280120849609375
training loss = 1.874407172203064 8100
val loss = 4.331275939941406
training loss = 1.8941980600357056 8200
val loss = 4.549983978271484
training loss = 1.873592734336853 8300
val loss = 4.329719066619873
training loss = 1.8732454776763916 8400
val loss = 4.324906349182129
training loss = 1.8730591535568237 8500
val loss = 4.300178527832031
training loss = 1.872511625289917 8600
val loss = 4.3209333419799805
training loss = 1.9676388502120972 8700
val loss = 3.914337635040283
training loss = 1.8718445301055908 8800
val loss = 4.31210994720459
training loss = 1.87155282497406 8900
val loss = 4.315704345703125
training loss = 1.8720390796661377 9000
val loss = 4.2718353271484375
training loss = 1.8709416389465332 9100
val loss = 4.311904430389404
training loss = 1.870701551437378 9200
val loss = 4.307744979858398
training loss = 1.8704674243927002 9300
val loss = 4.295441627502441
training loss = 1.870161533355713 9400
val loss = 4.307785511016846
training loss = 1.9012736082077026 9500
val loss = 4.587477207183838
training loss = 1.8696407079696655 9600
val loss = 4.304842472076416
training loss = 1.8694466352462769 9700
val loss = 4.303371906280518
training loss = 1.8696190118789673 9800
val loss = 4.3311004638671875
training loss = 1.8689982891082764 9900
val loss = 4.299095630645752
training loss = 1.8688281774520874 10000
val loss = 4.300009727478027
training loss = 1.8687154054641724 10100
val loss = 4.3146562576293945
training loss = 1.8684265613555908 10200
val loss = 4.296374320983887
training loss = 1.9159997701644897 10300
val loss = 3.994882106781006
training loss = 1.8680652379989624 10400
val loss = 4.295958995819092
training loss = 1.8679883480072021 10500
val loss = 4.280062675476074
training loss = 1.8677181005477905 10600
val loss = 4.288337230682373
training loss = 1.867594838142395 10700
val loss = 4.290127754211426
training loss = 1.867419719696045 10800
val loss = 4.293705940246582
training loss = 1.8672893047332764 10900
val loss = 4.289010047912598
training loss = 1.8706868886947632 11000
val loss = 4.1999311447143555
training loss = 1.86701238155365 11100
val loss = 4.28234338760376
training loss = 1.86691415309906 11200
val loss = 4.284388542175293
training loss = 1.8668298721313477 11300
val loss = 4.298501014709473
training loss = 1.8666455745697021 11400
val loss = 4.282843589782715
training loss = 1.866564154624939 11500
val loss = 4.280998706817627
training loss = 1.8667253255844116 11600
val loss = 4.3072052001953125
training loss = 1.8663326501846313 11700
val loss = 4.2796783447265625
training loss = 2.2293901443481445 11800
val loss = 5.391427040100098
training loss = 1.8661208152770996 11900
val loss = 4.283090114593506
training loss = 1.866047978401184 12000
val loss = 4.277310371398926
training loss = 1.875143051147461 12100
val loss = 4.422411918640137
training loss = 1.865848422050476 12200
val loss = 4.273979187011719
training loss = 1.8657963275909424 12300
val loss = 4.274751663208008
training loss = 1.8765625953674316 12400
val loss = 4.435741424560547
training loss = 1.8656039237976074 12500
val loss = 4.2737040519714355
training loss = 1.8655637502670288 12600
val loss = 4.272661209106445
training loss = 1.8884950876235962 12700
val loss = 4.510549068450928
training loss = 1.8654053211212158 12800
val loss = 4.27205228805542
training loss = 1.8653699159622192 12900
val loss = 4.269145965576172
training loss = 1.865268588066101 13000
val loss = 4.273930072784424
training loss = 1.8652299642562866 13100
val loss = 4.269494533538818
training loss = 1.8662365674972534 13200
val loss = 4.317592144012451
training loss = 1.8651093244552612 13300
val loss = 4.266848564147949
training loss = 2.1777184009552 13400
val loss = 5.284623146057129
training loss = 1.864998459815979 13500
val loss = 4.271763801574707
training loss = 1.8649691343307495 13600
val loss = 4.266290187835693
training loss = 1.8660202026367188 13700
val loss = 4.317028999328613
training loss = 1.8648505210876465 13800
val loss = 4.265165328979492
training loss = 1.8648971319198608 13900
val loss = 4.276003837585449
training loss = 1.8647758960723877 14000
val loss = 4.256237983703613
training loss = 1.8647377490997314 14100
val loss = 4.263896942138672
training loss = 1.866916298866272 14200
val loss = 4.335263252258301
training loss = 1.8646504878997803 14300
val loss = 4.264036178588867
training loss = 1.9421334266662598 14400
val loss = 3.893932342529297
training loss = 1.8645626306533813 14500
val loss = 4.258935928344727
training loss = 1.8645557165145874 14600
val loss = 4.26187801361084
training loss = 1.8700834512710571 14700
val loss = 4.375667572021484
training loss = 1.8644771575927734 14800
val loss = 4.260188579559326
training loss = 1.8646256923675537 14900
val loss = 4.278335094451904
training loss = 1.8644102811813354 15000
val loss = 4.263607025146484
training loss = 1.8644063472747803 15100
val loss = 4.2599639892578125
training loss = 1.86739182472229 15200
val loss = 4.343603134155273
training loss = 1.8643451929092407 15300
val loss = 4.258578777313232
training loss = 1.8656657934188843 15400
val loss = 4.203681945800781
training loss = 1.8642873764038086 15500
val loss = 4.260447025299072
training loss = 1.864292860031128 15600
val loss = 4.257731914520264
training loss = 1.8643559217453003 15700
val loss = 4.241001129150391
training loss = 1.864237666130066 15800
val loss = 4.257812023162842
training loss = 1.8772335052490234 15900
val loss = 4.092945098876953
training loss = 1.8641785383224487 16000
val loss = 4.258422374725342
training loss = 1.8641892671585083 16100
val loss = 4.257285118103027
training loss = 1.865586757659912 16200
val loss = 4.314181804656982
training loss = 1.8641443252563477 16300
val loss = 4.256860256195068
training loss = 1.8665733337402344 16400
val loss = 4.181858062744141
training loss = 1.8641040325164795 16500
val loss = 4.254281044006348
training loss = 1.8643251657485962 16600
val loss = 4.276882648468018
training loss = 1.8640629053115845 16700
val loss = 4.250804901123047
training loss = 1.8640666007995605 16800
val loss = 4.255398750305176
training loss = 1.8873647451400757 16900
val loss = 4.03867769241333
training loss = 1.8640202283859253 17000
val loss = 4.2578630447387695
training loss = 1.8640323877334595 17100
val loss = 4.254831314086914
training loss = 1.86452054977417 17200
val loss = 4.289445400238037
training loss = 1.8639848232269287 17300
val loss = 4.255846977233887
training loss = 1.8639986515045166 17400
val loss = 4.254776477813721
training loss = 1.86439049243927 17500
val loss = 4.285495758056641
training loss = 1.8639620542526245 17600
val loss = 4.254518508911133
training loss = 1.8640170097351074 17700
val loss = 4.238245964050293
training loss = 1.8639205694198608 17800
val loss = 4.255619049072266
training loss = 1.8639347553253174 17900
val loss = 4.253425598144531
training loss = 1.864549160003662 18000
val loss = 4.215094566345215
training loss = 1.8638916015625 18100
val loss = 4.25388240814209
training loss = 1.8898568153381348 18200
val loss = 4.027870178222656
training loss = 1.8638505935668945 18300
val loss = 4.2531609535217285
training loss = 1.8638627529144287 18400
val loss = 4.252981185913086
training loss = 1.8639065027236938 18500
val loss = 4.268222332000732
training loss = 1.8638120889663696 18600
val loss = 4.252995014190674
training loss = 1.8640971183776855 18700
val loss = 4.277162551879883
training loss = 1.8637951612472534 18800
val loss = 4.2610554695129395
training loss = 1.8637712001800537 18900
val loss = 4.252734184265137
training loss = 1.866377830505371 19000
val loss = 4.330842018127441
training loss = 1.8637113571166992 19100
val loss = 4.252089500427246
training loss = 1.8637288808822632 19200
val loss = 4.246463298797607
training loss = 1.8637303113937378 19300
val loss = 4.239150524139404
training loss = 1.8636528253555298 19400
val loss = 4.252377986907959
training loss = 1.8635878562927246 19500
val loss = 4.2501068115234375
training loss = 1.8635783195495605 19600
val loss = 4.252894401550293
training loss = 1.8672398328781128 19700
val loss = 4.162292957305908
training loss = 1.8634968996047974 19800
val loss = 4.253360748291016
training loss = 1.86347496509552 19900
val loss = 4.251760959625244
training loss = 1.8633838891983032 20000
val loss = 4.24731969833374
training loss = 1.8633508682250977 20100
val loss = 4.252013683319092
training loss = 1.8633171319961548 20200
val loss = 4.251367092132568
training loss = 1.863206386566162 20300
val loss = 4.251749515533447
training loss = 1.8631606101989746 20400
val loss = 4.251445293426514
training loss = 1.8771638870239258 20500
val loss = 4.082842826843262
training loss = 1.8629838228225708 20600
val loss = 4.254365921020508
training loss = 1.862901210784912 20700
val loss = 4.251108169555664
training loss = 1.8629783391952515 20800
val loss = 4.228123664855957
training loss = 1.8626391887664795 20900
val loss = 4.250674247741699
training loss = 1.862526535987854 21000
val loss = 4.250189304351807
training loss = 1.8650552034378052 21100
val loss = 4.173360824584961
training loss = 1.8621875047683716 21200
val loss = 4.250100612640381
training loss = 1.8620343208312988 21300
val loss = 4.24137020111084
training loss = 1.861775517463684 21400
val loss = 4.252691268920898
training loss = 1.8615601062774658 21500
val loss = 4.246869087219238
training loss = 1.8626823425292969 21600
val loss = 4.300805568695068
training loss = 1.8610153198242188 21700
val loss = 4.248575210571289
training loss = 1.8607378005981445 21800
val loss = 4.243549346923828
training loss = 2.1323328018188477 21900
val loss = 3.6266748905181885
training loss = 1.860070824623108 22000
val loss = 4.24412727355957
training loss = 1.8597168922424316 22100
val loss = 4.238006591796875
training loss = 1.8595024347305298 22200
val loss = 4.257965087890625
training loss = 1.8589221239089966 22300
val loss = 4.235547065734863
training loss = 1.9233976602554321 22400
val loss = 3.8906545639038086
training loss = 1.8580546379089355 22500
val loss = 4.2320756912231445
training loss = 1.8576198816299438 22600
val loss = 4.232412338256836
training loss = 1.8572015762329102 22700
val loss = 4.241403579711914
training loss = 1.8566652536392212 22800
val loss = 4.227194786071777
training loss = 1.8768516778945923 22900
val loss = 4.450784683227539
training loss = 1.85565984249115 23000
val loss = 4.224889755249023
training loss = 1.855156660079956 23100
val loss = 4.222745895385742
training loss = 1.854601263999939 23200
val loss = 4.218984127044678
training loss = 1.8540821075439453 23300
val loss = 4.219540596008301
training loss = 2.012450933456421 23400
val loss = 3.7175612449645996
training loss = 1.8529579639434814 23500
val loss = 4.211874485015869
training loss = 1.8523852825164795 23600
val loss = 4.215012073516846
training loss = 1.8537437915802002 23700
val loss = 4.147289276123047
training loss = 1.8511818647384644 23800
val loss = 4.210901737213135
training loss = 2.066298007965088 23900
val loss = 3.6435554027557373
training loss = 1.8499300479888916 24000
val loss = 4.203557968139648
training loss = 1.8492883443832397 24100
val loss = 4.205144882202148
training loss = 1.8492016792297363 24200
val loss = 4.166817665100098
training loss = 1.8479706048965454 24300
val loss = 4.201052665710449
training loss = 2.000385284423828 24400
val loss = 4.8702802658081055
training loss = 1.8466134071350098 24500
val loss = 4.199663162231445
training loss = 1.8459261655807495 24600
val loss = 4.1935296058654785
training loss = 1.8461918830871582 24700
val loss = 4.2353692054748535
training loss = 1.8445278406143188 24800
val loss = 4.186613082885742
training loss = 1.8438305854797363 24900
val loss = 4.185057163238525
training loss = 1.8840886354446411 25000
val loss = 4.506321907043457
training loss = 1.8423916101455688 25100
val loss = 4.180428504943848
training loss = 1.841667890548706 25200
val loss = 4.171069145202637
training loss = 1.8409345149993896 25300
val loss = 4.170591354370117
training loss = 1.8402165174484253 25400
val loss = 4.169214248657227
training loss = 1.8406339883804321 25500
val loss = 4.2161431312561035
training loss = 1.8387928009033203 25600
val loss = 4.162204265594482
training loss = 1.8396530151367188 25700
val loss = 4.21809196472168
training loss = 1.837409257888794 25800
val loss = 4.163721561431885
training loss = 1.8366750478744507 25900
val loss = 4.151036262512207
training loss = 1.8372406959533691 26000
val loss = 4.200387954711914
training loss = 1.8353055715560913 26100
val loss = 4.143857002258301
training loss = 1.8355954885482788 26200
val loss = 4.184465408325195
training loss = 1.8339860439300537 26300
val loss = 4.134410381317139
training loss = 1.8333388566970825 26400
val loss = 4.131977081298828
training loss = 1.8342430591583252 26500
val loss = 4.069817543029785
training loss = 1.8320846557617188 26600
val loss = 4.124963760375977
training loss = 1.8331897258758545 26700
val loss = 4.057537078857422
training loss = 1.830878496170044 26800
val loss = 4.116573810577393
training loss = 1.830374836921692 26900
val loss = 4.126451015472412
training loss = 1.8297882080078125 27000
val loss = 4.097165107727051
training loss = 1.8291757106781006 27100
val loss = 4.1053924560546875
training loss = 1.8313597440719604 27200
val loss = 4.180880069732666
training loss = 1.8281217813491821 27300
val loss = 4.098724365234375
training loss = 2.1697428226470947 27400
val loss = 5.168570518493652
training loss = 1.8271265029907227 27500
val loss = 4.093927383422852
training loss = 1.826640248298645 27600
val loss = 4.086337089538574
training loss = 1.826892375946045 27700
val loss = 4.1235761642456055
training loss = 1.825729250907898 27800
val loss = 4.079789161682129
training loss = 1.8258652687072754 27900
val loss = 4.040323734283447
training loss = 1.8248791694641113 28000
val loss = 4.073849678039551
training loss = 1.8246701955795288 28100
val loss = 4.047340393066406
training loss = 1.8241125345230103 28200
val loss = 4.075372695922852
training loss = 1.8236935138702393 28300
val loss = 4.063105583190918
training loss = 1.8254581689834595 28400
val loss = 4.129600524902344
training loss = 1.8229773044586182 28500
val loss = 4.056876182556152
training loss = 1.8867570161819458 28600
val loss = 3.7129337787628174
training loss = 1.8223090171813965 28700
val loss = 4.046765327453613
training loss = 1.8219773769378662 28800
val loss = 4.045834064483643
training loss = 1.8219152688980103 28900
val loss = 4.067953109741211
training loss = 1.8213770389556885 29000
val loss = 4.041533946990967
training loss = 1.851414442062378 29100
val loss = 4.316244125366211
training loss = 1.8208214044570923 29200
val loss = 4.0359883308410645
training loss = 1.820549726486206 29300
val loss = 4.03342866897583
training loss = 1.8206754922866821 29400
val loss = 4.060299873352051
training loss = 1.8200472593307495 29500
val loss = 4.028704643249512
training loss = 1.8228719234466553 29600
val loss = 4.109194755554199
training loss = 1.819594383239746 29700
val loss = 4.030486106872559
training loss = 1.8193416595458984 29800
val loss = 4.0205583572387695
training loss = 1.8206030130386353 29900
val loss = 3.9622273445129395
training loss = 1.8189172744750977 30000
val loss = 4.01687479019165
reduced chi^2 level 2 = 1.8189152479171753
Constrained alpha: 1.917203426361084
Constrained beta: 3.2995119094848633
Constrained gamma: 11.941521644592285
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 867.0947,  895.6241,  888.1275,  927.8796, 1017.2000, 1077.3522,
        1120.4562, 1148.7542, 1178.7781, 1100.1221, 1188.2472, 1222.1019,
        1235.9260, 1287.4822, 1295.6207, 1388.2123, 1370.2396, 1386.6138,
        1556.9846, 1524.5509, 1553.2904, 1530.4589, 1675.8365, 1617.5186,
        1661.4550, 1697.5779, 1575.2787, 1713.7349, 1656.5208, 1698.6306,
        1667.2303, 1699.9232, 1642.1692, 1748.4713, 1706.5697, 1834.2347,
        1665.9380, 1628.9396, 1631.7266, 1589.7356, 1525.5719, 1552.5212,
        1518.6085, 1506.3014, 1385.4048, 1302.4093, 1306.7513, 1209.5394,
        1128.3671, 1175.6656, 1086.2100,  939.8769,  909.5415,  932.1682,
         870.3531,  867.4771,  824.9064,  734.0803,  595.6237,  534.7921,
         556.0815,  455.8552,  459.6845,  384.3332,  322.3942,  343.7972,
         305.3293,  259.0013,  208.2708,  173.9508,  148.6589,  126.7706,
         145.3719,  123.5406,   91.9636,   65.8343,   43.4980,   46.7869,
          41.3977,   34.4560,   23.7894,   42.9569,   25.2479])]
2759.386675423046
3.2397312812094436 10.013346645602365 15.69851213324256
val isze = 8
idinces = [56 35 20 19 71 42 50 81 59 38 10 23 66 33 16 68 18 54 55 75 73 43 65 11
 62  5  9 44 34 47 77 45  3 72 80 15 13 27 40 26 37 41 63 12 36  6 79 58
 29 61 76  4 31 30 78 64 51  7  0 53  8 48 60 69  2 49 46 22 52 67 74 17
 82  1 28 24 70 21 32 14 25 57 39]
we are doing training validation split
training loss = 189.93634033203125 100
val loss = 220.59083557128906
training loss = 12.601888656616211 200
val loss = 18.431766510009766
training loss = 10.131564140319824 300
val loss = 16.013086318969727
training loss = 8.49285888671875 400
val loss = 14.437360763549805
training loss = 7.4406538009643555 500
val loss = 13.407504081726074
training loss = 6.775416374206543 600
val loss = 12.73410415649414
training loss = 6.3580193519592285 700
val loss = 12.290019989013672
training loss = 6.097108840942383 800
val loss = 11.993476867675781
training loss = 5.933929920196533 900
val loss = 11.792309761047363
training loss = 5.831040859222412 1000
val loss = 11.653196334838867
training loss = 5.764735221862793 1100
val loss = 11.55449104309082
training loss = 5.720162868499756 1200
val loss = 11.482476234436035
training loss = 5.688154220581055 1300
val loss = 11.428054809570312
training loss = 5.663156986236572 1400
val loss = 11.385343551635742
training loss = 5.641890525817871 1500
val loss = 11.35022258758545
training loss = 5.622481822967529 1600
val loss = 11.32043743133545
training loss = 5.603897571563721 1700
val loss = 11.293846130371094
training loss = 5.585591793060303 1800
val loss = 11.269490242004395
training loss = 5.567293643951416 1900
val loss = 11.246532440185547
training loss = 5.5488762855529785 2000
val loss = 11.22421932220459
training loss = 5.5302815437316895 2100
val loss = 11.20236587524414
training loss = 5.511503219604492 2200
val loss = 11.180465698242188
training loss = 5.492547035217285 2300
val loss = 11.158724784851074
training loss = 5.473426818847656 2400
val loss = 11.136460304260254
training loss = 5.454179286956787 2500
val loss = 11.114202499389648
training loss = 5.434831142425537 2600
val loss = 11.091580390930176
training loss = 5.415429592132568 2700
val loss = 11.068765640258789
training loss = 5.396030426025391 2800
val loss = 11.045772552490234
training loss = 5.376697063446045 2900
val loss = 11.022523880004883
training loss = 5.357522010803223 3000
val loss = 10.999173164367676
training loss = 5.338608264923096 3100
val loss = 10.97600269317627
training loss = 5.320083141326904 3200
val loss = 10.952960014343262
training loss = 5.302095890045166 3300
val loss = 10.93033504486084
training loss = 5.284822940826416 3400
val loss = 10.908042907714844
training loss = 5.26844596862793 3500
val loss = 10.88690185546875
training loss = 5.253140449523926 3600
val loss = 10.86683464050293
training loss = 5.239055633544922 3700
val loss = 10.847892761230469
training loss = 5.226253509521484 3800
val loss = 10.83036994934082
training loss = 5.214658260345459 3900
val loss = 10.81421947479248
training loss = 5.2039313316345215 4000
val loss = 10.799221992492676
training loss = 5.193243980407715 4100
val loss = 10.784244537353516
training loss = 5.180583477020264 4200
val loss = 10.76650619506836
training loss = 5.160455226898193 4300
val loss = 10.739749908447266
training loss = 5.115610599517822 4400
val loss = 10.683271408081055
training loss = 5.004298210144043 4500
val loss = 10.555012702941895
training loss = 4.852303981781006 4600
val loss = 9.605822563171387
training loss = 4.443991184234619 4700
val loss = 9.899068832397461
training loss = 3.7417941093444824 4800
val loss = 10.022650718688965
training loss = 2.42387056350708 4900
val loss = 7.043137073516846
training loss = 1.8386338949203491 5000
val loss = 5.6176605224609375
training loss = 1.769715666770935 5100
val loss = 5.529571533203125
training loss = 1.7314172983169556 5200
val loss = 5.592120170593262
training loss = 1.706893801689148 5300
val loss = 5.743544578552246
training loss = 1.6888914108276367 5400
val loss = 5.686498641967773
training loss = 1.6864891052246094 5500
val loss = 6.0279693603515625
training loss = 1.667650818824768 5600
val loss = 5.734426498413086
training loss = 1.7536377906799316 5700
val loss = 6.762171745300293
training loss = 1.6556329727172852 5800
val loss = 5.760953903198242
training loss = 1.6515096426010132 5900
val loss = 5.753006935119629
training loss = 1.6481560468673706 6000
val loss = 5.760004997253418
training loss = 1.6453735828399658 6100
val loss = 5.786795616149902
training loss = 1.643331527709961 6200
val loss = 5.736083984375
training loss = 1.6411197185516357 6300
val loss = 5.789607048034668
training loss = 1.6395387649536133 6400
val loss = 5.754313945770264
training loss = 1.6379871368408203 6500
val loss = 5.790136337280273
training loss = 1.6371158361434937 6600
val loss = 5.846859455108643
training loss = 1.6355605125427246 6700
val loss = 5.779376029968262
training loss = 1.6345683336257935 6800
val loss = 5.784152984619141
training loss = 1.63434898853302 6900
val loss = 5.701546669006348
training loss = 1.6328039169311523 7000
val loss = 5.7807512283325195
training loss = 1.6320626735687256 7100
val loss = 5.7743635177612305
training loss = 1.63130521774292 7200
val loss = 5.774066925048828
training loss = 1.630689263343811 7300
val loss = 5.771283149719238
training loss = 1.7061058282852173 7400
val loss = 5.016598701477051
training loss = 1.6294788122177124 7500
val loss = 5.760248184204102
training loss = 1.6289782524108887 7600
val loss = 5.763045310974121
training loss = 1.6999750137329102 7700
val loss = 5.0252461433410645
training loss = 1.6279687881469727 7800
val loss = 5.759268760681152
training loss = 1.6275460720062256 7900
val loss = 5.754117965698242
training loss = 1.628889560699463 8000
val loss = 5.882267475128174
training loss = 1.6266793012619019 8100
val loss = 5.7499799728393555
training loss = 1.6263149976730347 8200
val loss = 5.739670753479004
training loss = 1.625884771347046 8300
val loss = 5.740797996520996
training loss = 1.6255565881729126 8400
val loss = 5.740439414978027
training loss = 1.6257082223892212 8500
val loss = 5.805969715118408
training loss = 1.6248539686203003 8600
val loss = 5.737280368804932
training loss = 1.62456476688385 8700
val loss = 5.727434158325195
training loss = 1.6243315935134888 8800
val loss = 5.763182640075684
training loss = 1.6239454746246338 8900
val loss = 5.728081703186035
training loss = 1.6252779960632324 9000
val loss = 5.605691909790039
training loss = 1.6233630180358887 9100
val loss = 5.724743843078613
training loss = 1.793014407157898 9200
val loss = 7.117142677307129
training loss = 1.6228102445602417 9300
val loss = 5.720437526702881
training loss = 1.6225959062576294 9400
val loss = 5.71762752532959
training loss = 1.6254777908325195 9500
val loss = 5.888120651245117
training loss = 1.6221007108688354 9600
val loss = 5.712289810180664
training loss = 1.6219427585601807 9700
val loss = 5.728562355041504
training loss = 1.6216930150985718 9800
val loss = 5.687114715576172
training loss = 1.6214618682861328 9900
val loss = 5.708298683166504
training loss = 1.622220516204834 10000
val loss = 5.80184268951416
training loss = 1.6210390329360962 10100
val loss = 5.703847885131836
training loss = 1.6208821535110474 10200
val loss = 5.702279090881348
training loss = 1.6206895112991333 10300
val loss = 5.723537445068359
training loss = 1.6204928159713745 10400
val loss = 5.700124740600586
training loss = 1.6203606128692627 10500
val loss = 5.706160068511963
training loss = 1.6201616525650024 10600
val loss = 5.680593967437744
training loss = 1.6200083494186401 10700
val loss = 5.695587158203125
training loss = 1.6252763271331787 10800
val loss = 5.913390159606934
training loss = 1.619666337966919 10900
val loss = 5.689047813415527
training loss = 1.6195584535598755 11000
val loss = 5.691106796264648
training loss = 1.620559573173523 11100
val loss = 5.789249420166016
training loss = 1.6192704439163208 11200
val loss = 5.685230731964111
training loss = 1.6204458475112915 11300
val loss = 5.793930530548096
training loss = 1.6190162897109985 11400
val loss = 5.673801898956299
training loss = 1.6189117431640625 11500
val loss = 5.68410587310791
training loss = 1.6190664768218994 11600
val loss = 5.629362106323242
training loss = 1.6186596155166626 11700
val loss = 5.681834697723389
training loss = 1.6270173788070679 11800
val loss = 5.41316032409668
training loss = 1.6184500455856323 11900
val loss = 5.694058418273926
training loss = 1.6183569431304932 12000
val loss = 5.678881645202637
training loss = 1.6218420267105103 12100
val loss = 5.502057075500488
training loss = 1.6181432008743286 12200
val loss = 5.677618503570557
training loss = 1.6191998720169067 12300
val loss = 5.575634002685547
training loss = 1.6179771423339844 12400
val loss = 5.657871246337891
training loss = 1.6178942918777466 12500
val loss = 5.673703193664551
training loss = 1.6205729246139526 12600
val loss = 5.835693359375
training loss = 1.6177077293395996 12700
val loss = 5.67218017578125
training loss = 1.6176643371582031 12800
val loss = 5.674424648284912
training loss = 1.6176601648330688 12900
val loss = 5.635914325714111
training loss = 1.6174935102462769 13000
val loss = 5.669710159301758
training loss = 1.7972897291183472 13100
val loss = 7.113409042358398
training loss = 1.6173382997512817 13200
val loss = 5.659811019897461
training loss = 1.6173018217086792 13300
val loss = 5.667210578918457
training loss = 1.6280382871627808 13400
val loss = 5.36621618270874
training loss = 1.617163062095642 13500
val loss = 5.667835235595703
training loss = 1.6171350479125977 13600
val loss = 5.664936065673828
training loss = 1.6170021295547485 13700
val loss = 5.674595355987549
training loss = 1.616972804069519 13800
val loss = 5.664546966552734
training loss = 1.616956353187561 13900
val loss = 5.663081169128418
training loss = 1.8937687873840332 14000
val loss = 7.507599830627441
training loss = 1.6168372631072998 14100
val loss = 5.653116226196289
training loss = 1.6168156862258911 14200
val loss = 5.661294937133789
training loss = 1.620283603668213 14300
val loss = 5.484737873077393
training loss = 1.6167051792144775 14400
val loss = 5.658514976501465
training loss = 1.6166985034942627 14500
val loss = 5.6636552810668945
training loss = 1.6167610883712769 14600
val loss = 5.621791839599609
training loss = 1.616605281829834 14700
val loss = 5.657794952392578
training loss = 1.617563247680664 14800
val loss = 5.756235122680664
training loss = 1.616513967514038 14900
val loss = 5.65880823135376
training loss = 1.6355150938034058 15000
val loss = 6.086849212646484
training loss = 1.6164323091506958 15100
val loss = 5.65244197845459
training loss = 1.616431713104248 15200
val loss = 5.655746936798096
training loss = 1.6302142143249512 15300
val loss = 6.023412704467773
training loss = 1.6163157224655151 15400
val loss = 5.654741287231445
training loss = 1.6163231134414673 15500
val loss = 5.654786109924316
training loss = 1.6163930892944336 15600
val loss = 5.628417015075684
training loss = 1.616323709487915 15700
val loss = 5.67838191986084
training loss = 1.616259217262268 15800
val loss = 5.653444766998291
training loss = 1.6165082454681396 15900
val loss = 5.600519180297852
training loss = 1.6161936521530151 16000
val loss = 5.652246475219727
training loss = 1.6190197467803955 16100
val loss = 5.494757652282715
training loss = 1.6161409616470337 16200
val loss = 5.638190269470215
training loss = 1.6161326169967651 16300
val loss = 5.651555061340332
training loss = 1.6249970197677612 16400
val loss = 5.373682498931885
training loss = 1.616076111793518 16500
val loss = 5.6472883224487305
training loss = 1.6160879135131836 16600
val loss = 5.652410507202148
training loss = 1.6162573099136353 16700
val loss = 5.603924751281738
training loss = 1.616034746170044 16800
val loss = 5.649550437927246
training loss = 1.6589043140411377 16900
val loss = 5.06705904006958
training loss = 1.615966558456421 17000
val loss = 5.649205207824707
training loss = 1.6159842014312744 17100
val loss = 5.6490325927734375
training loss = 1.6487162113189697 17200
val loss = 6.218317985534668
training loss = 1.6159400939941406 17300
val loss = 5.646262168884277
training loss = 1.6159666776657104 17400
val loss = 5.637081623077393
training loss = 1.615976333618164 17500
val loss = 5.674666404724121
training loss = 1.6159106492996216 17600
val loss = 5.647780418395996
training loss = 1.6382734775543213 17700
val loss = 6.115806579589844
training loss = 1.615865707397461 17800
val loss = 5.650238513946533
training loss = 1.6158814430236816 17900
val loss = 5.646338939666748
training loss = 1.6163520812988281 18000
val loss = 5.71624755859375
training loss = 1.615831732749939 18100
val loss = 5.647070407867432
training loss = 1.616856575012207 18200
val loss = 5.551082611083984
training loss = 1.6158229112625122 18300
val loss = 5.664880752563477
training loss = 1.6158007383346558 18400
val loss = 5.646064758300781
training loss = 1.6236329078674316 18500
val loss = 5.917251110076904
training loss = 1.6157481670379639 18600
val loss = 5.64306116104126
reduced chi^2 level 2 = 1.615754246711731
Constrained alpha: 1.8735771179199219
Constrained beta: 4.058669090270996
Constrained gamma: 12.622196197509766
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 887.9111,  910.3022,  957.3271,  920.9831, 1025.5166, 1097.8978,
        1129.3855, 1163.3446, 1107.5132, 1169.5154, 1216.2006, 1214.1327,
        1246.5901, 1262.1202, 1358.6920, 1389.1213, 1403.3986, 1425.4764,
        1535.3162, 1456.6494, 1596.5861, 1526.8513, 1577.9379, 1627.9391,
        1699.1427, 1717.1479, 1580.8384, 1727.4150, 1703.7078, 1703.8096,
        1675.4989, 1745.3163, 1727.5441, 1696.9454, 1680.0367, 1750.1514,
        1661.8147, 1533.8907, 1661.2460, 1635.5697, 1607.8687, 1556.4025,
        1490.4211, 1464.6506, 1362.2649, 1341.4738, 1357.4506, 1164.5217,
        1076.0685, 1207.6652, 1107.2758,  993.7100,  970.5677,  920.1501,
         861.1356,  843.4013,  804.8500,  718.1588,  560.8608,  556.1594,
         543.7830,  515.8114,  396.0531,  401.0381,  334.0165,  321.7434,
         299.4889,  246.3245,  221.1550,  164.1671,  193.1572,  155.8157,
         146.1324,  113.6319,   90.1483,   73.8533,   55.0547,   33.8224,
          39.6870,   42.3495,   22.8925,   38.7549,   33.8858])]
2470.7759745079966
1.0147108160783351 19.075322212090178 16.97913556287497
val isze = 8
idinces = [50 51 33  1 25 22 65 72  7 61 44 70 40 12 14 73 28 68 23  2 49 64 24 36
 31 34  3 19 82 76 10 32 42 47 43 71 27 79 48 20 74  6 62 53 52 16 77 39
 66 13  5 21  4  8 57 56 11 38 46 59 58 55 30 35 78 41 26  0 37 81 75  9
 54 15 80 18 45 63 60 69 67 29 17]
we are doing training validation split
training loss = 310.77508544921875 100
val loss = 352.67645263671875
training loss = 52.025150299072266 200
val loss = 59.99080276489258
training loss = 9.940007209777832 300
val loss = 13.703213691711426
training loss = 9.598769187927246 400
val loss = 13.186600685119629
training loss = 9.226943016052246 500
val loss = 12.619223594665527
training loss = 8.838274955749512 600
val loss = 12.01322078704834
training loss = 8.44668197631836 700
val loss = 11.386870384216309
training loss = 8.065939903259277 800
val loss = 10.759462356567383
training loss = 7.709003448486328 900
val loss = 10.150508880615234
training loss = 7.387007236480713 1000
val loss = 9.578508377075195
training loss = 7.108079433441162 1100
val loss = 9.059097290039062
training loss = 6.876351833343506 1200
val loss = 8.603407859802246
training loss = 6.691459655761719 1300
val loss = 8.216617584228516
training loss = 6.5489301681518555 1400
val loss = 7.8977460861206055
training loss = 6.44136905670166 1500
val loss = 7.6406121253967285
training loss = 6.3601250648498535 1600
val loss = 7.435710906982422
training loss = 6.296899318695068 1700
val loss = 7.272298812866211
training loss = 6.244826793670654 1800
val loss = 7.140183925628662
training loss = 6.198922157287598 1900
val loss = 7.030770778656006
training loss = 6.155965328216553 2000
val loss = 6.9373650550842285
training loss = 6.1141157150268555 2100
val loss = 6.855072021484375
training loss = 6.072447299957275 2200
val loss = 6.780475616455078
training loss = 6.030604839324951 2300
val loss = 6.711238861083984
training loss = 5.988543510437012 2400
val loss = 6.645872116088867
training loss = 5.946381568908691 2500
val loss = 6.583471775054932
training loss = 5.904341697692871 2600
val loss = 6.523569583892822
training loss = 5.862698078155518 2700
val loss = 6.465989112854004
training loss = 5.8217549324035645 2800
val loss = 6.410778522491455
training loss = 5.781837463378906 2900
val loss = 6.358093738555908
training loss = 5.743231773376465 3000
val loss = 6.308103084564209
training loss = 5.706133842468262 3100
val loss = 6.260887145996094
training loss = 5.670603275299072 3200
val loss = 6.2163920402526855
training loss = 5.6364426612854 3300
val loss = 6.174295902252197
training loss = 5.603061676025391 3400
val loss = 6.133925914764404
training loss = 5.569197654724121 3500
val loss = 6.093860149383545
training loss = 5.532116889953613 3600
val loss = 6.051220893859863
training loss = 5.484848499298096 3700
val loss = 5.998476028442383
training loss = 5.403802394866943 3800
val loss = 5.908024311065674
training loss = 5.194573879241943 3900
val loss = 5.653414726257324
training loss = 4.705623149871826 4000
val loss = 5.022538661956787
training loss = 3.729562997817993 4100
val loss = 3.92216157913208
training loss = 2.4642741680145264 4200
val loss = 2.561062812805176
training loss = 1.8687196969985962 4300
val loss = 1.8500280380249023
training loss = 1.7894092798233032 4400
val loss = 1.6594290733337402
training loss = 1.7646408081054688 4500
val loss = 1.5641043186187744
training loss = 1.7473844289779663 4600
val loss = 1.4945590496063232
training loss = 1.7345845699310303 4700
val loss = 1.4408974647521973
training loss = 1.724845051765442 4800
val loss = 1.3989641666412354
training loss = 1.7172698974609375 4900
val loss = 1.365373134613037
training loss = 1.7115881443023682 5000
val loss = 1.3399207592010498
training loss = 1.707007646560669 5100
val loss = 1.322326421737671
training loss = 1.7041690349578857 5200
val loss = 1.3010342121124268
training loss = 1.7003568410873413 5300
val loss = 1.295053482055664
training loss = 1.7072877883911133 5400
val loss = 1.2738633155822754
training loss = 1.6956673860549927 5500
val loss = 1.2763148546218872
training loss = 1.6938096284866333 5600
val loss = 1.270358920097351
training loss = 1.6923905611038208 5700
val loss = 1.260503888130188
training loss = 1.6907768249511719 5800
val loss = 1.2592432498931885
training loss = 1.6928085088729858 5900
val loss = 1.2439523935317993
training loss = 1.6884145736694336 6000
val loss = 1.250779151916504
training loss = 1.6874289512634277 6100
val loss = 1.247554063796997
training loss = 1.6866339445114136 6200
val loss = 1.2418453693389893
training loss = 1.6857420206069946 6300
val loss = 1.242165446281433
training loss = 1.696811556816101 6400
val loss = 1.2259762287139893
training loss = 1.684367299079895 6500
val loss = 1.2374573945999146
training loss = 1.683796763420105 6600
val loss = 1.2359018325805664
training loss = 1.6891539096832275 6700
val loss = 1.257155179977417
training loss = 1.682780146598816 6800
val loss = 1.232140302658081
training loss = 1.6823676824569702 6900
val loss = 1.2311432361602783
training loss = 1.694059133529663 7000
val loss = 1.2684922218322754
training loss = 1.6816025972366333 7100
val loss = 1.2284804582595825
training loss = 1.6812981367111206 7200
val loss = 1.2276818752288818
training loss = 1.682793140411377 7300
val loss = 1.2381410598754883
training loss = 1.6807326078414917 7400
val loss = 1.2255420684814453
training loss = 1.7578927278518677 7500
val loss = 1.3735673427581787
training loss = 1.6802608966827393 7600
val loss = 1.224109411239624
training loss = 1.6800862550735474 7700
val loss = 1.2234472036361694
training loss = 1.6903557777404785 7800
val loss = 1.2578125
training loss = 1.6797401905059814 7900
val loss = 1.2219070196151733
training loss = 1.6798920631408691 8000
val loss = 1.218079924583435
training loss = 1.679452657699585 8100
val loss = 1.2207560539245605
training loss = 1.6793638467788696 8200
val loss = 1.2208139896392822
training loss = 1.6901503801345825 8300
val loss = 1.2070873975753784
training loss = 1.679139494895935 8400
val loss = 1.2196500301361084
training loss = 1.679085373878479 8500
val loss = 1.2197191715240479
training loss = 1.952558994293213 8600
val loss = 1.6328613758087158
training loss = 1.6789377927780151 8700
val loss = 1.2191259860992432
training loss = 1.6789048910140991 8800
val loss = 1.2194335460662842
training loss = 1.678818941116333 8900
val loss = 1.2180044651031494
training loss = 1.678799033164978 9000
val loss = 1.2186756134033203
training loss = 1.6864842176437378 9100
val loss = 1.247597098350525
training loss = 1.6787196397781372 9200
val loss = 1.2182632684707642
training loss = 2.0048670768737793 9300
val loss = 1.6996135711669922
training loss = 1.6786515712738037 9400
val loss = 1.2182735204696655
training loss = 1.6786519289016724 9500
val loss = 1.2180033922195435
training loss = 1.6857666969299316 9600
val loss = 1.246253490447998
training loss = 1.678607702255249 9700
val loss = 1.2175352573394775
training loss = 1.6786717176437378 9800
val loss = 1.219740867614746
training loss = 1.6785887479782104 9900
val loss = 1.2184218168258667
training loss = 1.6785888671875 10000
val loss = 1.2177038192749023
training loss = 1.6909401416778564 10100
val loss = 1.2576333284378052
training loss = 1.6785658597946167 10200
val loss = 1.2175695896148682
training loss = 1.6785894632339478 10300
val loss = 1.2173988819122314
training loss = 1.6787018775939941 10400
val loss = 1.2204787731170654
training loss = 1.678568959236145 10500
val loss = 1.2175697088241577
training loss = 1.7652918100357056 10600
val loss = 1.2373474836349487
training loss = 1.6785576343536377 10700
val loss = 1.2179306745529175
training loss = 1.6785798072814941 10800
val loss = 1.2177283763885498
training loss = 1.6795161962509155 10900
val loss = 1.2101869583129883
training loss = 1.678568720817566 11000
val loss = 1.2175307273864746
training loss = 1.7280949354171753 11100
val loss = 1.2139346599578857
training loss = 1.678558588027954 11200
val loss = 1.2167152166366577
training loss = 1.6785835027694702 11300
val loss = 1.2177661657333374
training loss = 1.6830153465270996 11400
val loss = 1.238506555557251
training loss = 1.6785740852355957 11500
val loss = 1.2174856662750244
training loss = 1.6787828207015991 11600
val loss = 1.221477746963501
training loss = 1.6785796880722046 11700
val loss = 1.218569040298462
training loss = 1.6785850524902344 11800
val loss = 1.2178267240524292
training loss = 1.6846119165420532 11900
val loss = 1.2431488037109375
training loss = 1.6785752773284912 12000
val loss = 1.2179290056228638
training loss = 1.6972191333770752 12100
val loss = 1.2717409133911133
training loss = 1.6785680055618286 12200
val loss = 1.2185478210449219
training loss = 1.6785744428634644 12300
val loss = 1.2178828716278076
training loss = 1.7009564638137817 12400
val loss = 1.2050468921661377
training loss = 1.6785333156585693 12500
val loss = 1.2174973487854004
training loss = 1.6785540580749512 12600
val loss = 1.217846155166626
training loss = 1.7968811988830566 12700
val loss = 1.2567503452301025
training loss = 1.6785047054290771 12800
val loss = 1.2179102897644043
training loss = 1.6785165071487427 12900
val loss = 1.217740774154663
training loss = 1.6985626220703125 13000
val loss = 1.203093409538269
training loss = 1.6784660816192627 13100
val loss = 1.216707706451416
training loss = 1.6784677505493164 13200
val loss = 1.217667818069458
training loss = 1.6912225484848022 13300
val loss = 1.2023277282714844
training loss = 1.6784042119979858 13400
val loss = 1.2175970077514648
training loss = 1.678398847579956 13500
val loss = 1.2177979946136475
training loss = 1.6784001588821411 13600
val loss = 1.2146048545837402
training loss = 1.6783044338226318 13700
val loss = 1.2171849012374878
training loss = 1.9400383234024048 13800
val loss = 1.361114501953125
training loss = 1.6781972646713257 13900
val loss = 1.2161657810211182
training loss = 1.678160309791565 14000
val loss = 1.216870903968811
training loss = 1.6790622472763062 14100
val loss = 1.225193977355957
training loss = 1.6779998540878296 14200
val loss = 1.2162880897521973
training loss = 1.677963137626648 14300
val loss = 1.2176282405853271
training loss = 1.6778500080108643 14400
val loss = 1.2137768268585205
training loss = 1.677714228630066 14500
val loss = 1.215705394744873
training loss = 1.6775999069213867 14600
val loss = 1.212712049484253
training loss = 1.6774221658706665 14700
val loss = 1.214733600616455
training loss = 1.677274465560913 14800
val loss = 1.2150496244430542
training loss = 1.6771371364593506 14900
val loss = 1.2165727615356445
training loss = 1.676843285560608 15000
val loss = 1.2138954401016235
training loss = 1.780881643295288 15100
val loss = 1.2421722412109375
training loss = 1.6762782335281372 15200
val loss = 1.213098168373108
training loss = 1.675972819328308 15300
val loss = 1.2138991355895996
training loss = 1.6756432056427002 15400
val loss = 1.2147644758224487
training loss = 1.6751036643981934 15500
val loss = 1.2119141817092896
training loss = 1.6766663789749146 15600
val loss = 1.2005486488342285
training loss = 1.6739990711212158 15700
val loss = 1.21073317527771
training loss = 1.673366904258728 15800
val loss = 1.210942029953003
training loss = 1.67359459400177 15900
val loss = 1.219498634338379
training loss = 1.6718146800994873 16000
val loss = 1.2106359004974365
training loss = 1.672157883644104 16100
val loss = 1.2037394046783447
training loss = 1.6699742078781128 16200
val loss = 1.2107696533203125
training loss = 1.668973684310913 16300
val loss = 1.211173415184021
training loss = 1.6679139137268066 16400
val loss = 1.2090175151824951
training loss = 1.666846752166748 16500
val loss = 1.2103180885314941
training loss = 1.6722373962402344 16600
val loss = 1.2396855354309082
training loss = 1.6646333932876587 16700
val loss = 1.2089145183563232
training loss = 1.6635311841964722 16800
val loss = 1.2087196111679077
training loss = 1.6630781888961792 16900
val loss = 1.2149736881256104
training loss = 1.6613261699676514 17000
val loss = 1.2060370445251465
training loss = 1.710579514503479 17100
val loss = 1.320112943649292
training loss = 1.6591511964797974 17200
val loss = 1.2024785280227661
training loss = 1.6581013202667236 17300
val loss = 1.2012680768966675
training loss = 1.665569543838501 17400
val loss = 1.1821653842926025
training loss = 1.6559951305389404 17500
val loss = 1.1972657442092896
training loss = 1.6549534797668457 17600
val loss = 1.1960304975509644
training loss = 1.654155969619751 17700
val loss = 1.1982245445251465
training loss = 1.6529158353805542 17800
val loss = 1.1923370361328125
training loss = 1.952223300933838 17900
val loss = 1.350067138671875
training loss = 1.6508864164352417 18000
val loss = 1.1882097721099854
training loss = 1.6498781442642212 18100
val loss = 1.1876230239868164
training loss = 1.6488770246505737 18200
val loss = 1.1868326663970947
training loss = 1.6479004621505737 18300
val loss = 1.184660792350769
training loss = 1.7769067287445068 18400
val loss = 1.2157089710235596
training loss = 1.645943284034729 18500
val loss = 1.1810778379440308
training loss = 1.644975185394287 18600
val loss = 1.1809159517288208
training loss = 1.648261547088623 18700
val loss = 1.1646231412887573
training loss = 1.6430786848068237 18800
val loss = 1.178719401359558
training loss = 1.6532540321350098 18900
val loss = 1.1583889722824097
training loss = 1.6412131786346436 19000
val loss = 1.1760286092758179
training loss = 1.6403000354766846 19100
val loss = 1.1759512424468994
training loss = 1.6397943496704102 19200
val loss = 1.1810519695281982
training loss = 1.6385388374328613 19300
val loss = 1.174078106880188
training loss = 1.703749656677246 19400
val loss = 1.1662662029266357
training loss = 1.636832356452942 19500
val loss = 1.1729457378387451
training loss = 1.6360379457473755 19600
val loss = 1.174182415008545
training loss = 1.6352564096450806 19700
val loss = 1.168630838394165
training loss = 1.634403944015503 19800
val loss = 1.1708176136016846
training loss = 1.6375775337219238 19900
val loss = 1.1930086612701416
training loss = 1.632891058921814 20000
val loss = 1.1699135303497314
training loss = 1.886400580406189 20100
val loss = 1.2781808376312256
training loss = 1.6314572095870972 20200
val loss = 1.1676521301269531
training loss = 1.630751609802246 20300
val loss = 1.168652892112732
training loss = 1.6319624185562134 20400
val loss = 1.1560918092727661
training loss = 1.6294450759887695 20500
val loss = 1.1678903102874756
training loss = 1.633144736289978 20600
val loss = 1.193092942237854
training loss = 1.6281895637512207 20700
val loss = 1.1665639877319336
training loss = 1.6275824308395386 20800
val loss = 1.16746187210083
training loss = 1.6270701885223389 20900
val loss = 1.1637777090072632
training loss = 1.6264501810073853 21000
val loss = 1.1666064262390137
training loss = 1.6566461324691772 21100
val loss = 1.2545586824417114
training loss = 1.6253869533538818 21200
val loss = 1.1658153533935547
training loss = 1.624872088432312 21300
val loss = 1.1661354303359985
training loss = 1.624930500984192 21400
val loss = 1.1739670038223267
training loss = 1.6238973140716553 21500
val loss = 1.165966510772705
training loss = 1.6241836547851562 21600
val loss = 1.1744545698165894
training loss = 1.6229829788208008 21700
val loss = 1.1658940315246582
training loss = 1.6263610124588013 21800
val loss = 1.1489841938018799
training loss = 1.6221232414245605 21900
val loss = 1.1658285856246948
training loss = 1.6217118501663208 22000
val loss = 1.1655659675598145
training loss = 1.6324152946472168 22100
val loss = 1.2111496925354004
training loss = 1.6209279298782349 22200
val loss = 1.1652216911315918
training loss = 1.6268516778945923 22300
val loss = 1.1449685096740723
training loss = 1.620206594467163 22400
val loss = 1.166572093963623
training loss = 1.61983323097229 22500
val loss = 1.165348768234253
training loss = 1.6206488609313965 22600
val loss = 1.1542621850967407
training loss = 1.6191589832305908 22700
val loss = 1.1652634143829346
training loss = 2.0922014713287354 22800
val loss = 1.4201624393463135
training loss = 1.6185396909713745 22900
val loss = 1.1636613607406616
training loss = 1.6182371377944946 23000
val loss = 1.1653289794921875
training loss = 1.6188884973526 23100
val loss = 1.1556053161621094
training loss = 1.6176576614379883 23200
val loss = 1.1661672592163086
training loss = 1.617365837097168 23300
val loss = 1.1653175354003906
training loss = 1.6170892715454102 23400
val loss = 1.164329171180725
training loss = 1.6168298721313477 23500
val loss = 1.165406584739685
training loss = 1.6172972917556763 23600
val loss = 1.1746124029159546
training loss = 1.6163229942321777 23700
val loss = 1.1651010513305664
training loss = 1.6160746812820435 23800
val loss = 1.1654052734375
training loss = 1.616025686264038 23900
val loss = 1.1599161624908447
training loss = 1.6156132221221924 24000
val loss = 1.1654998064041138
training loss = 1.6161017417907715 24100
val loss = 1.1775248050689697
training loss = 1.6151739358901978 24200
val loss = 1.1661242246627808
training loss = 1.6149523258209229 24300
val loss = 1.1655399799346924
training loss = 1.6156020164489746 24400
val loss = 1.1764397621154785
training loss = 1.614540696144104 24500
val loss = 1.1655490398406982
training loss = 1.615133285522461 24600
val loss = 1.176789402961731
training loss = 1.6141842603683472 24700
val loss = 1.1668388843536377
training loss = 1.6139788627624512 24800
val loss = 1.1653857231140137
training loss = 1.6137971878051758 24900
val loss = 1.1657094955444336
training loss = 1.6139047145843506 25000
val loss = 1.1738359928131104
training loss = 1.6134381294250488 25100
val loss = 1.1658451557159424
training loss = 1.6132631301879883 25200
val loss = 1.1656219959259033
training loss = 1.6137363910675049 25300
val loss = 1.156191110610962
training loss = 1.6129308938980103 25400
val loss = 1.1656181812286377
training loss = 1.6448429822921753 25500
val loss = 1.1336684226989746
training loss = 1.6126141548156738 25600
val loss = 1.1660820245742798
training loss = 1.6124560832977295 25700
val loss = 1.1650111675262451
training loss = 1.6123379468917847 25800
val loss = 1.1672306060791016
training loss = 1.6121574640274048 25900
val loss = 1.165631890296936
training loss = 1.6494886875152588 26000
val loss = 1.1313589811325073
training loss = 1.611870527267456 26100
val loss = 1.165520429611206
training loss = 1.6117357015609741 26200
val loss = 1.1645777225494385
training loss = 1.61167573928833 26300
val loss = 1.1615638732910156
training loss = 1.611461877822876 26400
val loss = 1.165551781654358
training loss = 1.6143089532852173 26500
val loss = 1.187577724456787
training loss = 1.6112020015716553 26600
val loss = 1.1654125452041626
training loss = 1.6114015579223633 26700
val loss = 1.1726913452148438
training loss = 1.6109529733657837 26800
val loss = 1.1641252040863037
training loss = 1.6108341217041016 26900
val loss = 1.1654326915740967
training loss = 1.6108478307724 27000
val loss = 1.1688807010650635
training loss = 1.6105977296829224 27100
val loss = 1.1648199558258057
training loss = 1.6104825735092163 27200
val loss = 1.165398359298706
training loss = 1.6107134819030762 27300
val loss = 1.1576220989227295
training loss = 1.6102644205093384 27400
val loss = 1.1651678085327148
training loss = 1.6140987873077393 27500
val loss = 1.1933164596557617
training loss = 1.6100715398788452 27600
val loss = 1.1666773557662964
training loss = 1.6099443435668945 27700
val loss = 1.1652917861938477
training loss = 1.6132999658584595 27800
val loss = 1.1453012228012085
training loss = 1.6097424030303955 27900
val loss = 1.1647841930389404
training loss = 1.6096446514129639 28000
val loss = 1.1656107902526855
training loss = 1.6095811128616333 28100
val loss = 1.1617995500564575
training loss = 1.6094517707824707 28200
val loss = 1.164963960647583
training loss = 1.6149168014526367 28300
val loss = 1.1409648656845093
training loss = 1.6092631816864014 28400
val loss = 1.1645536422729492
training loss = 1.609208106994629 28500
val loss = 1.1673388481140137
training loss = 1.609080195426941 28600
val loss = 1.164473295211792
training loss = 1.6089982986450195 28700
val loss = 1.1648280620574951
training loss = 1.7756602764129639 28800
val loss = 1.4955646991729736
training loss = 1.608830451965332 28900
val loss = 1.1632728576660156
training loss = 1.608742356300354 29000
val loss = 1.1648035049438477
training loss = 1.610345721244812 29100
val loss = 1.1500247716903687
training loss = 1.6085784435272217 29200
val loss = 1.164431095123291
training loss = 1.920693039894104 29300
val loss = 1.70542573928833
training loss = 1.6084263324737549 29400
val loss = 1.1651335954666138
training loss = 1.6083439588546753 29500
val loss = 1.1647120714187622
training loss = 1.610129475593567 29600
val loss = 1.1825287342071533
training loss = 1.6081905364990234 29700
val loss = 1.1642136573791504
training loss = 1.6431975364685059 29800
val loss = 1.2759416103363037
training loss = 1.6080455780029297 29900
val loss = 1.164434552192688
training loss = 1.6079739332199097 30000
val loss = 1.1643428802490234
reduced chi^2 level 2 = 1.6079730987548828
Constrained alpha: 1.9330835342407227
Constrained beta: 2.8457090854644775
Constrained gamma: 11.719914436340332
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 827.1240,  891.9241,  978.7256,  953.2057,  958.5901, 1039.2021,
        1084.1189, 1167.8280, 1122.9121, 1192.1833, 1226.5349, 1232.0861,
        1259.4288, 1229.6338, 1338.6705, 1415.8270, 1436.3053, 1436.4404,
        1502.7806, 1445.5930, 1659.9281, 1510.1372, 1568.3713, 1659.1927,
        1718.5637, 1774.5172, 1584.3008, 1695.8075, 1724.9706, 1681.7661,
        1659.2186, 1696.4679, 1683.4309, 1635.8696, 1674.6279, 1772.9585,
        1677.3907, 1597.5892, 1705.7683, 1639.4664, 1592.6177, 1577.2761,
        1492.3619, 1551.1932, 1335.4691, 1407.6228, 1328.5143, 1213.8182,
        1147.0103, 1163.9698, 1054.5994, 1068.3492,  889.3146,  957.5331,
         922.3560,  873.7380,  819.9145,  718.2305,  598.6487,  565.2594,
         575.8226,  478.9611,  456.3893,  384.6539,  367.6396,  340.7649,
         282.9875,  234.5678,  229.1703,  176.6819,  149.9301,  146.0106,
         141.2352,  113.9861,   91.9051,   69.6482,   55.7082,   40.2258,
          35.0014,   44.5203,   12.4374,   27.0823,   33.4412])]
2854.1749926259076
4.8342371357516765 17.30332659931228 60.87977045392322
val isze = 8
idinces = [19 56 68 32 73 60 52 78 36 30 55  7 47 74 28 35 11 21 65 43 17 81 15  0
 77 50 20 70 37 24 12 22 59 51 58 26 57  6 44 67 34  1 39 66 82 27 64  3
 29 10 41 61  4 45 49 18 48 23 13 63 71 16  5 80 42 69  9 79 38 72 76 75
 14  2 31 40 25  8 62 46 54 53 33]
we are doing training validation split
training loss = 694.40625 100
val loss = 575.8848266601562
training loss = 667.4807739257812 200
val loss = 555.8948974609375
training loss = 570.208251953125 300
val loss = 477.7364501953125
training loss = 35.417476654052734 400
val loss = 66.94542694091797
training loss = 18.19487953186035 500
val loss = 33.98831558227539
training loss = 8.392380714416504 600
val loss = 16.877866744995117
training loss = 4.4963788986206055 700
val loss = 9.033964157104492
training loss = 3.646331548690796 800
val loss = 6.673527717590332
training loss = 3.3793551921844482 900
val loss = 6.080550193786621
training loss = 3.208702564239502 1000
val loss = 5.794512748718262
training loss = 2.9883711338043213 1100
val loss = 5.280882358551025
training loss = 2.8333544731140137 1200
val loss = 4.912354946136475
training loss = 2.666217088699341 1300
val loss = 4.463239669799805
training loss = 2.5447912216186523 1400
val loss = 4.13407564163208
training loss = 2.42996883392334 1500
val loss = 3.8249411582946777
training loss = 2.3447396755218506 1600
val loss = 3.597048759460449
training loss = 3.5507373809814453 1700
val loss = 2.810821771621704
training loss = 2.19699764251709 1800
val loss = 3.2816567420959473
training loss = 2.224092483520508 1900
val loss = 3.342137098312378
training loss = 2.089830160140991 2000
val loss = 3.076209783554077
training loss = 2.067970037460327 2100
val loss = 3.0873236656188965
training loss = 2.01267671585083 2200
val loss = 2.946352481842041
training loss = 1.991688847541809 2300
val loss = 2.8453361988067627
training loss = 1.9551249742507935 2400
val loss = 2.8695006370544434
training loss = 1.9435276985168457 2500
val loss = 2.804793119430542
training loss = 1.9165010452270508 2600
val loss = 2.8293097019195557
training loss = 2.1393980979919434 2700
val loss = 3.0680441856384277
training loss = 1.8855074644088745 2800
val loss = 2.8056583404541016
training loss = 1.8670982122421265 2900
val loss = 2.7939600944519043
training loss = 1.861464500427246 3000
val loss = 2.762410879135132
training loss = 1.8452706336975098 3100
val loss = 2.796746253967285
training loss = 1.8399943113327026 3200
val loss = 2.7677738666534424
training loss = 1.8274072408676147 3300
val loss = 2.7991232872009277
training loss = 1.828018069267273 3400
val loss = 2.7343907356262207
training loss = 1.813032627105713 3500
val loss = 2.8073298931121826
training loss = 1.8093782663345337 3600
val loss = 2.795849084854126
training loss = 1.8039559125900269 3700
val loss = 2.783841848373413
training loss = 1.798680067062378 3800
val loss = 2.8169503211975098
training loss = 2.0246481895446777 3900
val loss = 3.2670741081237793
training loss = 1.790306806564331 4000
val loss = 2.8538575172424316
training loss = 1.8227273225784302 4100
val loss = 3.0054566860198975
training loss = 1.7833797931671143 4200
val loss = 2.8471627235412598
training loss = 2.030592679977417 4300
val loss = 2.604496479034424
training loss = 1.7771501541137695 4400
val loss = 2.870086669921875
training loss = 2.2947592735290527 4500
val loss = 2.4059886932373047
training loss = 1.7718585729599 4600
val loss = 2.8875651359558105
training loss = 1.7961384057998657 4700
val loss = 2.7754898071289062
training loss = 1.7674874067306519 4800
val loss = 2.9050354957580566
training loss = 1.7667173147201538 4900
val loss = 2.8915653228759766
training loss = 1.7752941846847534 5000
val loss = 2.7374138832092285
training loss = 1.7627286911010742 5100
val loss = 2.908020496368408
training loss = 1.768528699874878 5200
val loss = 2.849912166595459
training loss = 1.7719368934631348 5300
val loss = 2.756976366043091
training loss = 1.760915994644165 5400
val loss = 2.896243095397949
training loss = 1.7578725814819336 5500
val loss = 2.954440116882324
training loss = 1.75764000415802 5600
val loss = 2.9023571014404297
training loss = 1.8086670637130737 5700
val loss = 2.7860541343688965
training loss = 1.9175572395324707 5800
val loss = 3.159271478652954
training loss = 1.7527649402618408 5900
val loss = 2.949044704437256
training loss = 1.755324125289917 6000
val loss = 2.901606559753418
training loss = 1.7528047561645508 6100
val loss = 2.929372787475586
training loss = 1.751041293144226 6200
val loss = 2.955282688140869
training loss = 1.748953938484192 6300
val loss = 2.965357780456543
training loss = 1.7621606588363647 6400
val loss = 2.8773293495178223
training loss = 1.7472705841064453 6500
val loss = 2.9732842445373535
training loss = 1.7826180458068848 6600
val loss = 2.849773645401001
training loss = 1.7458935976028442 6700
val loss = 2.9753103256225586
training loss = 1.9192705154418945 6800
val loss = 3.216127872467041
training loss = 1.7444710731506348 6900
val loss = 2.9853668212890625
training loss = 1.7528438568115234 7000
val loss = 2.927506446838379
training loss = 1.7427889108657837 7100
val loss = 2.987600803375244
training loss = 1.7469838857650757 7200
val loss = 2.931396007537842
training loss = 1.7825757265090942 7300
val loss = 2.876335382461548
training loss = 1.741217851638794 7400
val loss = 2.98862624168396
reduced chi^2 level 2 = 1.740265965461731
Constrained alpha: 3.8486428260803223
Constrained beta: 3.6427698135375977
Constrained gamma: 42.89965057373047
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 869.5345,  858.5184,  944.6039,  940.3753, 1041.1957, 1071.3684,
        1013.4455, 1126.3792, 1099.6675, 1218.3156, 1182.2327, 1198.1757,
        1231.7830, 1170.4604, 1319.1532, 1392.4949, 1412.7982, 1492.5511,
        1531.7004, 1543.0482, 1560.1681, 1571.1877, 1619.5537, 1552.2419,
        1714.5541, 1782.6827, 1592.9264, 1656.6047, 1646.8848, 1710.6310,
        1624.2462, 1774.5221, 1756.5195, 1733.7314, 1736.9414, 1742.9926,
        1641.6993, 1606.0337, 1644.5775, 1642.1201, 1622.7987, 1538.2058,
        1475.8915, 1493.5927, 1289.0427, 1362.0125, 1276.3322, 1185.1932,
        1176.2769, 1215.9591, 1081.9528,  930.9553,  980.0276,  941.4911,
         941.2800,  864.0936,  861.4442,  723.4493,  636.2824,  572.4479,
         566.8433,  472.8570,  421.4063,  369.3748,  335.0625,  290.0397,
         340.8335,  263.4579,  172.7450,  166.8170,  167.6647,  140.9341,
         140.6801,  105.3245,  105.0963,   63.3105,   49.1070,   40.8144,
          40.8159,   43.0601,   13.0323,   39.7354,   41.3563])]
2956.769924859259
4.284897049069467 18.97791845344479 20.325253420282408
val isze = 8
idinces = [66 15 72 53 32 20 13 35 18 21 34 57 12  5  9 80 23 58 62 51 37  6 64 50
 33 11 67 68 46 81 60 19 28 16 56 59 24 74 39  7 54  8 45 14 26 77  4 42
 71 78 48 76 69 75 22 49 10 55 65 44 27  0 79 29 70 47 43  1 25 82 73 40
 38  2 36 31 52 41  3 61 30 17 63]
we are doing training validation split
training loss = 603.3267822265625 100
val loss = 611.8274536132812
training loss = 143.4603729248047 200
val loss = 143.36569213867188
training loss = 7.557085037231445 300
val loss = 12.946355819702148
training loss = 7.389767646789551 400
val loss = 12.945073127746582
training loss = 7.237462520599365 500
val loss = 12.546148300170898
training loss = 7.094882011413574 600
val loss = 12.144755363464355
training loss = 6.968157768249512 700
val loss = 11.758432388305664
training loss = 6.860753059387207 800
val loss = 11.399378776550293
training loss = 6.7735161781311035 900
val loss = 11.075968742370605
training loss = 6.705069065093994 1000
val loss = 10.792797088623047
training loss = 6.652499198913574 1100
val loss = 10.5508451461792
training loss = 6.612193584442139 1200
val loss = 10.348180770874023
training loss = 6.58056640625 1300
val loss = 10.180895805358887
training loss = 6.554595470428467 1400
val loss = 10.044086456298828
training loss = 6.532015323638916 1500
val loss = 9.932353973388672
training loss = 6.511319160461426 1600
val loss = 9.840788841247559
training loss = 6.491608142852783 1700
val loss = 9.764863967895508
training loss = 6.472407341003418 1800
val loss = 9.700778007507324
training loss = 6.453482627868652 1900
val loss = 9.64542293548584
training loss = 6.434751987457275 2000
val loss = 9.596559524536133
training loss = 6.4161906242370605 2100
val loss = 9.552239418029785
training loss = 6.397796154022217 2200
val loss = 9.511253356933594
training loss = 6.379570484161377 2300
val loss = 9.472554206848145
training loss = 6.3615193367004395 2400
val loss = 9.4357328414917
training loss = 6.343630313873291 2500
val loss = 9.40021800994873
training loss = 6.325893402099609 2600
val loss = 9.365743637084961
training loss = 6.308284759521484 2700
val loss = 9.332223892211914
training loss = 6.290787696838379 2800
val loss = 9.299342155456543
training loss = 6.273370742797852 2900
val loss = 9.267181396484375
training loss = 6.256006717681885 3000
val loss = 9.235448837280273
training loss = 6.238663673400879 3100
val loss = 9.204301834106445
training loss = 6.221310138702393 3200
val loss = 9.17335319519043
training loss = 6.203912258148193 3300
val loss = 9.142786026000977
training loss = 6.1864333152771 3400
val loss = 9.112382888793945
training loss = 6.168848514556885 3500
val loss = 9.082240104675293
training loss = 6.151116847991943 3600
val loss = 9.05196762084961
training loss = 6.133200645446777 3700
val loss = 9.021862030029297
training loss = 6.115042686462402 3800
val loss = 8.991695404052734
training loss = 6.096521377563477 3900
val loss = 8.961052894592285
training loss = 6.077333927154541 4000
val loss = 8.929598808288574
training loss = 6.056604862213135 4100
val loss = 8.89583969116211
training loss = 6.031120777130127 4200
val loss = 8.840068817138672
training loss = 5.988747596740723 4300
val loss = 8.788341522216797
training loss = 5.847286701202393 4400
val loss = 8.669149398803711
training loss = 5.552145957946777 4500
val loss = 8.041189193725586
training loss = 5.0778703689575195 4600
val loss = 7.417559623718262
training loss = 4.269254684448242 4700
val loss = 6.388764381408691
training loss = 3.1442253589630127 4800
val loss = 5.203795433044434
training loss = 2.6326141357421875 4900
val loss = 4.92301082611084
training loss = 2.5978591442108154 5000
val loss = 5.0659613609313965
training loss = 2.611224412918091 5100
val loss = 4.973518371582031
training loss = 2.5906896591186523 5200
val loss = 5.176488876342773
training loss = 2.58772611618042 5300
val loss = 5.1806559562683105
training loss = 2.5840020179748535 5400
val loss = 5.227231025695801
training loss = 2.5803961753845215 5500
val loss = 5.2232279777526855
training loss = 2.5767440795898438 5600
val loss = 5.221855640411377
training loss = 2.573018789291382 5700
val loss = 5.229529857635498
training loss = 2.5694804191589355 5800
val loss = 5.208737850189209
training loss = 2.5656864643096924 5900
val loss = 5.2238664627075195
training loss = 2.5628578662872314 6000
val loss = 5.1774582862854
training loss = 2.5586442947387695 6100
val loss = 5.213326930999756
training loss = 2.555222988128662 6200
val loss = 5.209622383117676
training loss = 2.551877737045288 6300
val loss = 5.210501670837402
training loss = 2.5485827922821045 6400
val loss = 5.198071002960205
training loss = 2.554781436920166 6500
val loss = 5.3413238525390625
training loss = 2.542271614074707 6600
val loss = 5.188233852386475
training loss = 2.539231061935425 6700
val loss = 5.1910400390625
training loss = 2.5362462997436523 6800
val loss = 5.175467014312744
training loss = 2.533374547958374 6900
val loss = 5.174069881439209
training loss = 2.540090322494507 7000
val loss = 5.039456844329834
training loss = 2.5278067588806152 7100
val loss = 5.164878845214844
training loss = 2.525146722793579 7200
val loss = 5.170701026916504
training loss = 2.5225300788879395 7300
val loss = 5.165564060211182
training loss = 2.519937515258789 7400
val loss = 5.153578758239746
training loss = 2.517932891845703 7500
val loss = 5.181795597076416
training loss = 2.515010356903076 7600
val loss = 5.146900653839111
training loss = 2.5147480964660645 7700
val loss = 5.209404468536377
training loss = 2.510348320007324 7800
val loss = 5.140038967132568
training loss = 2.551635503768921 7900
val loss = 5.486638069152832
training loss = 2.5059242248535156 8000
val loss = 5.135784149169922
training loss = 2.503793478012085 8100
val loss = 5.131463527679443
training loss = 2.501734972000122 8200
val loss = 5.122192859649658
training loss = 2.4997732639312744 8300
val loss = 5.125124931335449
training loss = 2.497817277908325 8400
val loss = 5.123592853546143
training loss = 2.496016263961792 8500
val loss = 5.133334159851074
training loss = 2.4941020011901855 8600
val loss = 5.117885589599609
training loss = 2.519259452819824 8700
val loss = 5.379748344421387
training loss = 2.490553855895996 8800
val loss = 5.107631206512451
training loss = 2.4888508319854736 8900
val loss = 5.111063480377197
training loss = 2.4876577854156494 9000
val loss = 5.076403617858887
training loss = 2.485584020614624 9100
val loss = 5.106274604797363
training loss = 2.4982590675354004 9200
val loss = 5.289163112640381
training loss = 2.482496500015259 9300
val loss = 5.103707790374756
training loss = 2.481015682220459 9400
val loss = 5.102363586425781
training loss = 2.4795925617218018 9500
val loss = 5.104193210601807
training loss = 2.4781999588012695 9600
val loss = 5.096843242645264
training loss = 2.5125010013580322 9700
val loss = 4.864250183105469
training loss = 2.4755160808563232 9800
val loss = 5.093420028686523
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 917.9702,  892.0778,  957.7019,  957.0180, 1023.2584, 1095.0190,
        1083.2219, 1077.4425, 1139.1093, 1226.7292, 1215.3752, 1195.3267,
        1264.8024, 1244.2821, 1383.4041, 1381.8958, 1402.6805, 1443.3967,
        1561.3789, 1513.1991, 1593.7719, 1560.9828, 1627.5812, 1535.1348,
        1656.2026, 1725.6324, 1570.5575, 1740.5901, 1694.9043, 1693.4626,
        1704.3715, 1744.0311, 1700.7991, 1702.1552, 1684.8990, 1696.3633,
        1597.4449, 1602.2124, 1680.6920, 1611.4598, 1570.6135, 1561.6580,
        1422.4067, 1503.7124, 1380.0182, 1338.3336, 1228.8427, 1212.7385,
        1191.1550, 1165.4911, 1095.4060, 1010.5120, 1013.1674,  856.3299,
         945.1461,  854.8796,  773.4171,  698.9593,  612.7267,  531.8437,
         546.1636,  450.9754,  466.8380,  387.7584,  362.0278,  331.6852,
         291.5415,  254.0874,  204.9860,  171.4164,  177.8527,  135.0673,
         140.1091,   97.3770,  109.1637,   70.4331,   42.3011,   41.8885,
          23.8522,   34.5104,   20.8835,   37.8524,   48.9999])]
2272.603285822508
3.7262337380140282 10.063788053124354 33.39505069544133
val isze = 8
idinces = [18 43  8 24  9 65 71 58 67 63 48 44 25 72  3 46 27 68 39 30 42 47 31 73
 19 10 52  2 16  6 78 33 75  7 76 37 38 28 79 40 77 81 34 50 69  4 32 61
  5 53 60 26 55 56 12  1 36 82 17 14 20 35 80 62  0 70 29 41 64 45 21 22
 57 59 11 13 49 15 23 66 74 54 51]
we are doing training validation split
training loss = 211.70651245117188 100
val loss = 171.17990112304688
training loss = 11.55650806427002 200
val loss = 11.194721221923828
training loss = 9.2369384765625 300
val loss = 8.76948070526123
training loss = 7.79258394241333 400
val loss = 7.027604103088379
training loss = 6.915177345275879 500
val loss = 5.901923179626465
training loss = 6.3889079093933105 600
val loss = 5.187952041625977
training loss = 6.075040340423584 700
val loss = 4.739631652832031
training loss = 5.8875627517700195 800
val loss = 4.458985328674316
training loss = 5.774029731750488 900
val loss = 4.282530784606934
training loss = 5.702871322631836 1000
val loss = 4.17008113861084
training loss = 5.655411243438721 1100
val loss = 4.0966596603393555
training loss = 5.620832920074463 1200
val loss = 4.046884059906006
training loss = 5.593046188354492 1300
val loss = 4.011481761932373
training loss = 5.568747520446777 1400
val loss = 3.9850635528564453
training loss = 5.546198844909668 1500
val loss = 3.9641857147216797
training loss = 5.524537086486816 1600
val loss = 3.946843385696411
training loss = 5.5033440589904785 1700
val loss = 3.9318666458129883
training loss = 5.482424259185791 1800
val loss = 3.918379545211792
training loss = 5.461681365966797 1900
val loss = 3.9058663845062256
training loss = 5.441065788269043 2000
val loss = 3.893970012664795
training loss = 5.4205427169799805 2100
val loss = 3.882481575012207
training loss = 5.400089740753174 2200
val loss = 3.8712244033813477
training loss = 5.379677772521973 2300
val loss = 3.8602120876312256
training loss = 5.35927677154541 2400
val loss = 3.8492953777313232
training loss = 5.338854789733887 2500
val loss = 3.83840012550354
training loss = 5.318378448486328 2600
val loss = 3.827545642852783
training loss = 5.297804355621338 2700
val loss = 3.8167529106140137
training loss = 5.277096748352051 2800
val loss = 3.8059744834899902
training loss = 5.2562150955200195 2900
val loss = 3.7951176166534424
training loss = 5.235130310058594 3000
val loss = 3.78432559967041
training loss = 5.213832378387451 3100
val loss = 3.7734665870666504
training loss = 5.192347049713135 3200
val loss = 3.7627077102661133
training loss = 5.170775890350342 3300
val loss = 3.752033233642578
training loss = 5.149340629577637 3400
val loss = 3.741379737854004
training loss = 5.1290998458862305 3500
val loss = 3.760727882385254
training loss = 5.10900354385376 3600
val loss = 3.718822479248047
training loss = 5.091787815093994 3700
val loss = 3.718017816543579
training loss = 5.077673435211182 3800
val loss = 3.7234740257263184
training loss = 5.066812515258789 3900
val loss = 3.7090444564819336
training loss = 5.0594563484191895 4000
val loss = 3.7001471519470215
training loss = 5.054717063903809 4100
val loss = 3.6997714042663574
training loss = 5.051815032958984 4200
val loss = 3.7070794105529785
training loss = 5.050156593322754 4300
val loss = 3.695467472076416
training loss = 5.048938751220703 4400
val loss = 3.7088747024536133
training loss = 5.048397541046143 4500
val loss = 3.6841351985931396
training loss = 5.033158302307129 4600
val loss = 3.703695297241211
training loss = 4.292498588562012 4700
val loss = 3.1688380241394043
training loss = 2.8282411098480225 4800
val loss = 2.628448247909546
training loss = 2.479192018508911 4900
val loss = 2.3650009632110596
training loss = 2.3356237411499023 5000
val loss = 2.2924890518188477
training loss = 2.2522189617156982 5100
val loss = 2.3553647994995117
training loss = 2.1916420459747314 5200
val loss = 2.2534847259521484
training loss = 2.142120122909546 5300
val loss = 2.3253791332244873
training loss = 2.0750770568847656 5400
val loss = 2.0508899688720703
training loss = 2.061774253845215 5500
val loss = 1.7280423641204834
training loss = 1.9657361507415771 5600
val loss = 1.9432426691055298
training loss = 1.9046226739883423 5700
val loss = 1.905065655708313
training loss = 1.8433417081832886 5800
val loss = 1.8584812879562378
training loss = 1.7788324356079102 5900
val loss = 1.804614543914795
training loss = 1.7181525230407715 6000
val loss = 1.7591357231140137
training loss = 1.656386375427246 6100
val loss = 1.7574138641357422
training loss = 1.6014230251312256 6200
val loss = 1.6061605215072632
training loss = 1.541184902191162 6300
val loss = 1.7220314741134644
training loss = 1.527482032775879 6400
val loss = 1.5675805807113647
training loss = 1.5057787895202637 6500
val loss = 1.7003366947174072
training loss = 1.4951813220977783 6600
val loss = 1.6575429439544678
training loss = 1.4845080375671387 6700
val loss = 1.717147707939148
training loss = 1.4758650064468384 6800
val loss = 1.7127012014389038
training loss = 1.4717620611190796 6900
val loss = 1.8056563138961792
training loss = 1.4622448682785034 7000
val loss = 1.7197918891906738
training loss = 1.4569212198257446 7100
val loss = 1.7386894226074219
training loss = 1.452213168144226 7200
val loss = 1.7277923822402954
training loss = 1.451940894126892 7300
val loss = 1.6398608684539795
training loss = 1.4445003271102905 7400
val loss = 1.73280668258667
training loss = 1.4412400722503662 7500
val loss = 1.735949993133545
training loss = 1.4385114908218384 7600
val loss = 1.760067105293274
training loss = 1.4356410503387451 7700
val loss = 1.7417471408843994
training loss = 1.4458810091018677 7800
val loss = 1.926326870918274
training loss = 1.4308788776397705 7900
val loss = 1.746997356414795
training loss = 1.4287246465682983 8000
val loss = 1.7546794414520264
training loss = 1.4267818927764893 8100
val loss = 1.7349607944488525
training loss = 1.424744963645935 8200
val loss = 1.7512742280960083
training loss = 1.4232982397079468 8300
val loss = 1.7225532531738281
training loss = 1.4210741519927979 8400
val loss = 1.754410743713379
training loss = 1.4193768501281738 8500
val loss = 1.7431092262268066
training loss = 1.4175715446472168 8600
val loss = 1.7533884048461914
training loss = 1.4157824516296387 8700
val loss = 1.7561461925506592
training loss = 1.4153103828430176 8800
val loss = 1.8116143941879272
training loss = 1.4121463298797607 8900
val loss = 1.7569983005523682
training loss = 1.4113422632217407 9000
val loss = 1.8132727146148682
training loss = 1.4077706336975098 9100
val loss = 1.7550259828567505
training loss = 1.404944896697998 9200
val loss = 1.7403961420059204
training loss = 1.4022639989852905 9300
val loss = 1.767714262008667
training loss = 1.39987313747406 9400
val loss = 1.7561330795288086
training loss = 1.397782564163208 9500
val loss = 1.759132742881775
training loss = 1.3956409692764282 9600
val loss = 1.7544031143188477
training loss = 1.4075322151184082 9700
val loss = 1.587646245956421
training loss = 1.3913198709487915 9800
val loss = 1.7565815448760986
training loss = 1.3888108730316162 9900
val loss = 1.7527730464935303
training loss = 1.4087258577346802 10000
val loss = 1.5404670238494873
training loss = 1.38264000415802 10100
val loss = 1.7500910758972168
training loss = 1.380553126335144 10200
val loss = 1.7510101795196533
training loss = 1.379238486289978 10300
val loss = 1.7169744968414307
training loss = 1.3769971132278442 10400
val loss = 1.748291254043579
training loss = 1.3788660764694214 10500
val loss = 1.6602516174316406
training loss = 1.3736369609832764 10600
val loss = 1.7451815605163574
training loss = 1.3718702793121338 10700
val loss = 1.7548972368240356
training loss = 1.3703206777572632 10800
val loss = 1.7585651874542236
training loss = 1.368429183959961 10900
val loss = 1.7437870502471924
training loss = 1.368655800819397 11000
val loss = 1.67582106590271
training loss = 1.3648489713668823 11100
val loss = 1.7419675588607788
training loss = 1.5505529642105103 11200
val loss = 1.2734415531158447
training loss = 1.3611518144607544 11300
val loss = 1.7426347732543945
training loss = 1.359132170677185 11400
val loss = 1.7367342710494995
training loss = 1.3572651147842407 11500
val loss = 1.7325396537780762
training loss = 1.3552477359771729 11600
val loss = 1.7337615489959717
reduced chi^2 level 2 = 1.3549937009811401
Constrained alpha: 1.9236078262329102
Constrained beta: 3.076519727706909
Constrained gamma: 16.463306427001953
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 844.6740,  890.9457,  947.5431,  935.1413, 1022.6243, 1061.0680,
        1030.7557, 1128.2361, 1184.8390, 1172.3153, 1212.4504, 1205.9167,
        1236.3253, 1246.3304, 1373.5892, 1406.7878, 1381.1276, 1440.4360,
        1527.0428, 1569.9469, 1576.7111, 1563.0648, 1594.6527, 1607.4515,
        1672.1824, 1687.8656, 1631.0574, 1715.4149, 1670.4769, 1687.6409,
        1708.4640, 1685.7484, 1705.7379, 1789.2621, 1680.5212, 1722.8767,
        1688.5355, 1549.2902, 1687.9631, 1647.5901, 1626.1888, 1593.9115,
        1523.5745, 1519.9946, 1332.9432, 1297.4606, 1287.9064, 1301.8403,
        1158.2998, 1193.9880, 1006.1253,  988.6989,  972.1440,  940.1952,
         890.5136,  897.5522,  796.0537,  732.8707,  591.0089,  509.9178,
         593.4288,  482.1150,  398.1468,  386.5592,  327.5299,  334.8471,
         267.9371,  247.1423,  196.0128,  178.5518,  175.7121,  172.2538,
         137.8488,  102.4070,   91.6511,   60.6493,   56.9788,   50.7939,
          40.3502,   43.6952,   28.6970,   42.4380,   33.3745])]
2643.670608279742
0.03830596782333717 3.7161864955249024 54.68015735761759
val isze = 8
idinces = [57 49 53  9 47 32 52 15 40  8 17  3 23 72 14 56 43  1  2 68 67 55  5 76
 75 54  0 39 50 73 77 31 79  4 27 38 13 28 20 16 71  6 58 11 81 36 82 26
 22 74 24 44 78 62 35 80 19 61  7 60 37 12 10 21 30 64 65 46 69 59 45 63
 48 51 18 41 29 66 25 42 70 33 34]
we are doing training validation split
training loss = 20.221254348754883 100
val loss = 14.666764259338379
training loss = 14.15381908416748 200
val loss = 5.387270927429199
training loss = 11.844776153564453 300
val loss = 4.199950218200684
training loss = 10.052994728088379 400
val loss = 3.31072998046875
training loss = 8.752985000610352 500
val loss = 2.6853208541870117
training loss = 7.853899955749512 600
val loss = 2.2555136680603027
training loss = 7.254796504974365 700
val loss = 1.963570475578308
training loss = 6.865019798278809 800
val loss = 1.7657533884048462
training loss = 6.610309600830078 900
val loss = 1.629919171333313
training loss = 6.43197774887085 1000
val loss = 1.5323104858398438
training loss = 6.283896446228027 1100
val loss = 1.455082893371582
training loss = 6.1308979988098145 1200
val loss = 1.3850858211517334
training loss = 5.9512434005737305 1300
val loss = 1.3131890296936035
training loss = 5.736888408660889 1400
val loss = 1.232391119003296
training loss = 5.480518817901611 1500
val loss = 1.1367498636245728
training loss = 5.16622257232666 1600
val loss = 1.0231857299804688
training loss = 4.770808219909668 1700
val loss = 0.8905389308929443
training loss = 4.263448715209961 1800
val loss = 0.740544855594635
training loss = 3.620129108428955 1900
val loss = 0.5912281274795532
training loss = 2.906853199005127 2000
val loss = 0.5176414847373962
training loss = 2.330521583557129 2100
val loss = 0.6375218033790588
training loss = 2.048347234725952 2200
val loss = 0.9584712386131287
training loss = 1.954125165939331 2300
val loss = 1.1023308038711548
training loss = 1.9270976781845093 2400
val loss = 1.1938695907592773
training loss = 1.916251301765442 2500
val loss = 1.2124727964401245
training loss = 1.9095526933670044 2600
val loss = 1.1937260627746582
training loss = 2.039076566696167 2700
val loss = 0.8963416814804077
training loss = 1.900479793548584 2800
val loss = 1.1431200504302979
training loss = 1.896752119064331 2900
val loss = 1.1209759712219238
training loss = 1.8930001258850098 3000
val loss = 1.1092475652694702
training loss = 1.8882882595062256 3100
val loss = 1.0763400793075562
training loss = 1.8826758861541748 3200
val loss = 1.0460762977600098
training loss = 1.8756498098373413 3300
val loss = 1.0357935428619385
training loss = 1.8681968450546265 3400
val loss = 1.0336464643478394
training loss = 1.8600900173187256 3500
val loss = 1.0053437948226929
training loss = 1.8550636768341064 3600
val loss = 0.9461168646812439
training loss = 1.8455110788345337 3700
val loss = 0.9877331256866455
training loss = 1.9285014867782593 3800
val loss = 0.7777003049850464
training loss = 1.8318760395050049 3900
val loss = 0.9811912178993225
training loss = 1.8249174356460571 4000
val loss = 0.988329291343689
training loss = 1.8179000616073608 4100
val loss = 0.9759286046028137
training loss = 1.8104679584503174 4200
val loss = 0.9808074235916138
training loss = 1.821122646331787 4300
val loss = 1.1513762474060059
training loss = 1.79481041431427 4400
val loss = 0.9875189661979675
training loss = 1.9380770921707153 4500
val loss = 1.5843455791473389
training loss = 1.7783573865890503 4600
val loss = 0.9950660467147827
training loss = 1.7698018550872803 4700
val loss = 1.00349760055542
training loss = 1.7615560293197632 4800
val loss = 1.0005464553833008
training loss = 1.7530648708343506 4900
val loss = 1.019802212715149
training loss = 1.781220555305481 5000
val loss = 0.8461225032806396
training loss = 1.7369165420532227 5100
val loss = 1.0380278825759888
training loss = 1.7290350198745728 5200
val loss = 1.0502396821975708
training loss = 1.7220849990844727 5300
val loss = 1.0755908489227295
training loss = 1.7150797843933105 5400
val loss = 1.0733085870742798
training loss = 1.7422196865081787 5500
val loss = 0.8947595953941345
training loss = 1.7026431560516357 5600
val loss = 1.092070460319519
training loss = 1.6969083547592163 5700
val loss = 1.1088014841079712
training loss = 1.699296474456787 5800
val loss = 1.0234464406967163
training loss = 1.6873339414596558 5900
val loss = 1.1328186988830566
training loss = 1.6829274892807007 6000
val loss = 1.1418647766113281
training loss = 1.6792747974395752 6100
val loss = 1.1360526084899902
training loss = 1.6754120588302612 6200
val loss = 1.1633963584899902
training loss = 1.9633073806762695 6300
val loss = 2.2227652072906494
training loss = 1.6691795587539673 6400
val loss = 1.177450180053711
training loss = 1.6663447618484497 6500
val loss = 1.188574194908142
training loss = 1.670233130455017 6600
val loss = 1.0987756252288818
training loss = 1.6615989208221436 6700
val loss = 1.203416109085083
training loss = 1.6593809127807617 6800
val loss = 1.2085986137390137
training loss = 1.6581825017929077 6900
val loss = 1.2539334297180176
reduced chi^2 level 2 = 1.6556825637817383
Constrained alpha: 1.7973965406417847
Constrained beta: 3.4654102325439453
Constrained gamma: 23.251911163330078
(1, 75)
(1, 8)
