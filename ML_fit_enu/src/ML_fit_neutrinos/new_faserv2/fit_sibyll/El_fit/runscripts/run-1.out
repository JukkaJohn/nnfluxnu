84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 863.2592,  821.6299,  940.8856, 1000.9749, 1023.4708, 1107.2926,
        1090.5360, 1160.3536, 1144.8396, 1184.5428, 1229.2905, 1200.9799,
        1248.5707, 1208.5840, 1279.3098, 1432.6742, 1428.6599, 1424.5413,
        1562.9811, 1520.3108, 1572.7183, 1551.5349, 1551.5583, 1586.1261,
        1585.9637, 1725.1521, 1594.2562, 1763.2428, 1718.3518, 1675.6577,
        1649.9503, 1788.3794, 1748.2889, 1749.5629, 1720.7435, 1760.5156,
        1630.2380, 1554.2723, 1709.4689, 1597.8621, 1617.2040, 1529.9512,
        1504.2435, 1456.7443, 1387.7073, 1354.1622, 1205.5225, 1234.3779,
        1189.6680, 1223.0800, 1099.5918, 1012.2378,  985.0381,  955.1047,
         891.6837,  879.8274,  774.4901,  685.2071,  619.9376,  549.0165,
         534.8109,  420.6974,  449.1267,  372.2865,  348.3240,  337.3945,
         307.4216,  277.8264,  203.1987,  172.1771,  153.8300,  144.1158,
         150.2659,   97.9962,  115.5973,   74.0199,   47.1787,   42.5979,
          26.4175,   39.4306,   18.0552,   51.5235,   33.6703])]
2717.8005173264314
0.2893775571115337 9.314809694969505 31.566055640172586
val isze = 8
idinces = [61 51 80  5 23 55 24 35 33 11 69 81 25 10 82 63 20 68 48 56  3 16 41  2
 36 19  4 34 13 50 75 59 44 71 72 18 37 40 77 29 79 30 28 42 74 67 14 70
 58 22 12 73 53 26  0 52 43 15 66 49 47 21 62 31  8 39  9 76 32 78 65  1
 54 38 46 17  7 45 27  6 60 64 57]
we are doing training validation split
training loss = 8.55368709564209 100
val loss = 9.216371536254883
training loss = 6.8154215812683105 200
val loss = 5.799705505371094
training loss = 6.754960060119629 300
val loss = 5.855967998504639
training loss = 6.689861297607422 400
val loss = 5.920741558074951
training loss = 6.6244378089904785 500
val loss = 5.992488861083984
training loss = 6.561947822570801 600
val loss = 6.068740367889404
training loss = 6.504676342010498 700
val loss = 6.147040367126465
training loss = 6.4539794921875 800
val loss = 6.224924564361572
training loss = 6.410348892211914 900
val loss = 6.300007343292236
training loss = 6.373559474945068 1000
val loss = 6.370150566101074
training loss = 6.342828750610352 1100
val loss = 6.433538913726807
training loss = 6.317060470581055 1200
val loss = 6.488833427429199
training loss = 6.295035362243652 1300
val loss = 6.535238265991211
training loss = 6.275592803955078 1400
val loss = 6.572518348693848
training loss = 6.257748126983643 1500
val loss = 6.60093879699707
training loss = 6.240742206573486 1600
val loss = 6.621148109436035
training loss = 6.224039077758789 1700
val loss = 6.634074687957764
training loss = 6.207305431365967 1800
val loss = 6.640783309936523
training loss = 6.190366268157959 1900
val loss = 6.642431259155273
training loss = 6.173156261444092 2000
val loss = 6.640053749084473
training loss = 6.155665874481201 2100
val loss = 6.634609222412109
training loss = 6.137942790985107 2200
val loss = 6.62691593170166
training loss = 6.120042324066162 2300
val loss = 6.617656707763672
training loss = 6.102027416229248 2400
val loss = 6.6073527336120605
training loss = 6.083954811096191 2500
val loss = 6.5963945388793945
training loss = 6.065860271453857 2600
val loss = 6.585089683532715
training loss = 6.04774808883667 2700
val loss = 6.573570728302002
training loss = 6.029571056365967 2800
val loss = 6.561933994293213
training loss = 6.011197090148926 2900
val loss = 6.550149917602539
training loss = 5.992349147796631 3000
val loss = 6.538012504577637
training loss = 5.9724650382995605 3100
val loss = 6.525147914886475
training loss = 5.950403690338135 3200
val loss = 6.510622024536133
training loss = 5.923606872558594 3300
val loss = 6.49262809753418
training loss = 5.885679721832275 3400
val loss = 6.467024803161621
training loss = 5.818761825561523 3500
val loss = 6.423548221588135
training loss = 5.6768107414245605 3600
val loss = 6.338625431060791
training loss = 5.409863471984863 3700
val loss = 6.171505451202393
training loss = 5.026373863220215 3800
val loss = 5.8520660400390625
training loss = 4.435541152954102 3900
val loss = 5.287392616271973
training loss = 3.469655752182007 4000
val loss = 4.348887920379639
training loss = 2.4139955043792725 4100
val loss = 3.3304553031921387
training loss = 2.111766815185547 4200
val loss = 3.06034517288208
training loss = 2.0762341022491455 4300
val loss = 3.0362792015075684
training loss = 2.056793451309204 4400
val loss = 3.023406505584717
training loss = 2.0419039726257324 4500
val loss = 3.0154285430908203
training loss = 2.0301880836486816 4600
val loss = 3.011190891265869
training loss = 2.020803689956665 4700
val loss = 3.0096330642700195
training loss = 2.0131540298461914 4800
val loss = 3.0099332332611084
training loss = 2.006796360015869 4900
val loss = 3.011563539505005
training loss = 2.0056312084198 5000
val loss = 3.038766860961914
training loss = 1.9969502687454224 5100
val loss = 3.017024517059326
training loss = 1.993018388748169 5200
val loss = 3.020145893096924
training loss = 1.9896026849746704 5300
val loss = 3.0224430561065674
training loss = 1.9865434169769287 5400
val loss = 3.026515483856201
training loss = 1.9847967624664307 5500
val loss = 3.0192322731018066
training loss = 1.9811406135559082 5600
val loss = 3.0328774452209473
training loss = 1.9786995649337769 5700
val loss = 3.0362744331359863
training loss = 1.97652006149292 5800
val loss = 3.037571430206299
training loss = 1.9742778539657593 5900
val loss = 3.0438179969787598
training loss = 1.9786609411239624 6000
val loss = 3.078296184539795
training loss = 1.9702562093734741 6100
val loss = 3.050285816192627
training loss = 1.9683126211166382 6200
val loss = 3.0538041591644287
training loss = 1.9665777683258057 6300
val loss = 3.0594005584716797
training loss = 1.9647831916809082 6400
val loss = 3.060241222381592
training loss = 1.9630286693572998 6500
val loss = 3.0636684894561768
training loss = 1.9620097875595093 6600
val loss = 3.0747427940368652
training loss = 1.959707260131836 6700
val loss = 3.0699462890625
training loss = 2.085909366607666 6800
val loss = 3.099699020385742
training loss = 1.9565097093582153 6900
val loss = 3.0762453079223633
training loss = 1.9549390077590942 7000
val loss = 3.0794458389282227
training loss = 1.9534786939620972 7100
val loss = 3.0808353424072266
training loss = 1.95198655128479 7200
val loss = 3.085618019104004
training loss = 1.9533445835113525 7300
val loss = 3.1068644523620605
training loss = 1.949111819267273 7400
val loss = 3.0921945571899414
training loss = 1.947694182395935 7500
val loss = 3.0947916507720947
training loss = 1.9470386505126953 7600
val loss = 3.105522632598877
training loss = 1.9450398683547974 7700
val loss = 3.1005382537841797
training loss = 1.943709373474121 7800
val loss = 3.1038565635681152
training loss = 1.9426476955413818 7900
val loss = 3.1104073524475098
training loss = 1.9411810636520386 8000
val loss = 3.1090874671936035
training loss = 1.9773411750793457 8100
val loss = 3.2013587951660156
training loss = 1.9387626647949219 8200
val loss = 3.1148924827575684
training loss = 1.937581181526184 8300
val loss = 3.11727237701416
training loss = 1.9631714820861816 8400
val loss = 3.191279172897339
training loss = 1.9353020191192627 8500
val loss = 3.1222026348114014
training loss = 1.9341830015182495 8600
val loss = 3.125143527984619
training loss = 1.9347193241119385 8700
val loss = 3.118709087371826
training loss = 1.9320608377456665 8800
val loss = 3.130009651184082
training loss = 1.9310046434402466 8900
val loss = 3.1322221755981445
training loss = 1.9301567077636719 9000
val loss = 3.131736993789673
training loss = 1.9289990663528442 9100
val loss = 3.1374893188476562
training loss = 1.9280585050582886 9200
val loss = 3.1423516273498535
training loss = 1.9270811080932617 9300
val loss = 3.141855239868164
training loss = 1.926141381263733 9400
val loss = 3.144469738006592
training loss = 1.9262467622756958 9500
val loss = 3.13940691947937
training loss = 1.924355149269104 9600
val loss = 3.1489105224609375
training loss = 1.924480676651001 9700
val loss = 3.144181966781616
training loss = 1.9226479530334473 9800
val loss = 3.1518046855926514
training loss = 1.9217841625213623 9900
val loss = 3.155458450317383
training loss = 1.9264310598373413 10000
val loss = 3.144813299179077
reduced chi^2 level 2 = 1.9216814041137695
Constrained alpha: 1.9064903259277344
Constrained beta: 3.0838868618011475
Constrained gamma: 18.412912368774414
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 854.0952,  842.5512,  944.4949,  884.6213,  950.0901, 1050.6470,
        1103.2175, 1154.6750, 1168.2014, 1205.3596, 1240.4084, 1181.6287,
        1255.7527, 1236.9114, 1374.3256, 1461.5746, 1340.3115, 1491.6830,
        1569.1600, 1541.5022, 1595.2386, 1594.2670, 1675.7168, 1545.6548,
        1588.3926, 1751.6711, 1633.6985, 1714.8810, 1768.2649, 1730.2988,
        1706.0638, 1780.3688, 1691.8525, 1741.9089, 1692.2521, 1735.4371,
        1568.6545, 1551.3269, 1719.6875, 1637.8822, 1630.8729, 1506.8391,
        1533.4821, 1509.4244, 1292.7866, 1374.8973, 1251.9347, 1292.1892,
        1197.6392, 1146.0040, 1087.7756, 1010.8553,  999.6115,  899.8564,
         916.6202,  851.4649,  833.5113,  667.5878,  595.4938,  559.2170,
         542.1381,  485.3518,  442.6982,  395.8521,  340.4227,  320.2328,
         313.8120,  253.3483,  191.5020,  152.0456,  156.9341,  138.0878,
         148.6267,  104.8443,   96.7235,   66.5227,   58.1394,   40.3999,
          36.5847,   43.4580,   17.9370,   47.0983,   30.2050])]
2895.7655226466095
2.130692467862838 5.29452678936615 5.7741148789312575
val isze = 8
idinces = [68 30 40 16 46 56 77 11 75  1 80 34  8 60 14 82 22 78 79 38 17 23 20 51
 19 67 72 33 49 31  3 71  5 39 25 10 28 81 12 21 29 48  6 27 44 52 58 42
 53 26 47 55  4 65 37 63 45 13 64 74  0  7 24 62 32 57 35 70 59 36 66 15
 43  9 18  2 69 50 61 73 76 54 41]
we are doing training validation split
training loss = 38.87679672241211 100
val loss = 39.869205474853516
training loss = 27.042226791381836 200
val loss = 27.23977279663086
training loss = 19.69890594482422 300
val loss = 19.172710418701172
training loss = 15.186635971069336 400
val loss = 14.071645736694336
training loss = 12.356621742248535 500
val loss = 10.809807777404785
training loss = 10.535554885864258 600
val loss = 8.678454399108887
training loss = 9.339303970336914 700
val loss = 7.257796287536621
training loss = 8.541731834411621 800
val loss = 6.295610427856445
training loss = 8.00450611114502 900
val loss = 5.635891914367676
training loss = 7.640058994293213 1000
val loss = 5.179224967956543
training loss = 7.391459941864014 1100
val loss = 4.860691070556641
training loss = 7.220877647399902 1200
val loss = 4.636919021606445
training loss = 7.102860450744629 1300
val loss = 4.478675365447998
training loss = 7.020103454589844 1400
val loss = 4.36586332321167
training loss = 6.960799217224121 1500
val loss = 4.284621715545654
training loss = 6.916894912719727 1600
val loss = 4.225344181060791
training loss = 6.882942199707031 1700
val loss = 4.181359767913818
training loss = 6.855288028717041 1800
val loss = 4.1480913162231445
training loss = 6.831536769866943 1900
val loss = 4.122337341308594
training loss = 6.8101396560668945 2000
val loss = 4.1018218994140625
training loss = 6.7901434898376465 2100
val loss = 4.085051536560059
training loss = 6.77100133895874 2200
val loss = 4.070952415466309
training loss = 6.752424716949463 2300
val loss = 4.0588178634643555
training loss = 6.734302520751953 2400
val loss = 4.048116683959961
training loss = 6.716628551483154 2500
val loss = 4.038529396057129
training loss = 6.699479103088379 2600
val loss = 4.029787063598633
training loss = 6.682954788208008 2700
val loss = 4.021782875061035
training loss = 6.66717529296875 2800
val loss = 4.014397621154785
training loss = 6.652258396148682 2900
val loss = 4.007576942443848
training loss = 6.638296127319336 3000
val loss = 4.001310348510742
training loss = 6.625326633453369 3100
val loss = 3.9956307411193848
training loss = 6.613342761993408 3200
val loss = 3.990421772003174
training loss = 6.602254390716553 3300
val loss = 3.9856784343719482
training loss = 6.591824531555176 3400
val loss = 3.9813618659973145
training loss = 6.581608772277832 3500
val loss = 3.977271318435669
training loss = 6.570667266845703 3600
val loss = 3.973064422607422
training loss = 6.556831359863281 3700
val loss = 3.967966318130493
training loss = 6.534168243408203 3800
val loss = 3.9600462913513184
training loss = 6.4829583168029785 3900
val loss = 3.943160057067871
training loss = 6.330290794372559 4000
val loss = 3.899838924407959
training loss = 5.9590229988098145 4100
val loss = 3.842353105545044
training loss = 5.485771656036377 4200
val loss = 3.7641851902008057
training loss = 4.829911708831787 4300
val loss = 3.609801769256592
training loss = 3.7974812984466553 4400
val loss = 3.4786925315856934
training loss = 2.6316895484924316 4500
val loss = 3.669950008392334
training loss = 2.3221538066864014 4600
val loss = 4.05013370513916
training loss = 2.282742500305176 4700
val loss = 4.149709224700928
training loss = 2.2576115131378174 4800
val loss = 4.189760684967041
training loss = 2.238788604736328 4900
val loss = 4.2181596755981445
training loss = 2.224010705947876 5000
val loss = 4.238690376281738
training loss = 2.2118921279907227 5100
val loss = 4.252870559692383
training loss = 2.201554775238037 5200
val loss = 4.261911392211914
training loss = 2.1924338340759277 5300
val loss = 4.266846656799316
training loss = 2.1841585636138916 5400
val loss = 4.268437385559082
training loss = 2.177220344543457 5500
val loss = 4.250087261199951
training loss = 2.1704742908477783 5600
val loss = 4.266101837158203
training loss = 2.167795419692993 5700
val loss = 4.333880424499512
training loss = 2.1587588787078857 5800
val loss = 4.259205341339111
training loss = 2.165198802947998 5900
val loss = 4.389051914215088
training loss = 2.1483678817749023 6000
val loss = 4.249807357788086
training loss = 2.143434762954712 6100
val loss = 4.246048450469971
training loss = 2.1387791633605957 6200
val loss = 4.246877670288086
training loss = 2.134242296218872 6300
val loss = 4.236560821533203
training loss = 2.129796028137207 6400
val loss = 4.230291843414307
training loss = 2.125579357147217 6500
val loss = 4.226533889770508
training loss = 2.1213576793670654 6600
val loss = 4.217780113220215
training loss = 2.1174581050872803 6700
val loss = 4.215396881103516
training loss = 2.113492965698242 6800
val loss = 4.207633018493652
training loss = 2.1098201274871826 6900
val loss = 4.196307182312012
training loss = 2.1061036586761475 7000
val loss = 4.198471546173096
training loss = 2.107316017150879 7100
val loss = 4.278892517089844
training loss = 2.099127769470215 7200
val loss = 4.1868438720703125
training loss = 2.0972466468811035 7300
val loss = 4.2285308837890625
training loss = 2.0925838947296143 7400
val loss = 4.176647186279297
training loss = 2.089479684829712 7500
val loss = 4.17155647277832
training loss = 2.0963447093963623 7600
val loss = 4.291750431060791
training loss = 2.0836002826690674 7700
val loss = 4.161846160888672
training loss = 2.1592955589294434 7800
val loss = 3.9184508323669434
training loss = 2.0781283378601074 7900
val loss = 4.154778957366943
training loss = 2.0755131244659424 8000
val loss = 4.149481773376465
training loss = 2.0730814933776855 8100
val loss = 4.148498058319092
training loss = 2.0707006454467773 8200
val loss = 4.139702796936035
training loss = 2.072983741760254 8300
val loss = 4.065369606018066
training loss = 2.0662853717803955 8400
val loss = 4.132941246032715
training loss = 2.064985990524292 8500
val loss = 4.097222328186035
training loss = 2.062244176864624 8600
val loss = 4.119000434875488
training loss = 2.060291290283203 8700
val loss = 4.1223039627075195
training loss = 2.060678482055664 8800
val loss = 4.0708794593811035
training loss = 2.0568172931671143 8900
val loss = 4.117114067077637
training loss = 2.0551888942718506 9000
val loss = 4.114286422729492
training loss = 2.0948193073272705 9100
val loss = 4.3825578689575195
training loss = 2.0520801544189453 9200
val loss = 4.110246658325195
training loss = 2.0506398677825928 9300
val loss = 4.107143878936768
training loss = 2.0583760738372803 9400
val loss = 4.008251667022705
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 828.6782,  849.8884,  956.9372,  946.7507, 1046.9680, 1066.7462,
        1124.8469, 1154.4790, 1121.9338, 1160.1743, 1168.0127, 1197.7841,
        1264.0474, 1252.5443, 1315.2023, 1446.0100, 1379.2131, 1441.0824,
        1498.6187, 1476.6831, 1588.3473, 1514.2961, 1539.2413, 1570.4209,
        1645.1112, 1713.4370, 1645.0834, 1808.3115, 1652.2561, 1778.0718,
        1673.9590, 1734.9330, 1712.4143, 1781.9841, 1714.9048, 1692.0884,
        1612.5892, 1688.9384, 1708.2094, 1662.9141, 1612.3380, 1599.7190,
        1534.1559, 1546.9894, 1346.5338, 1332.3523, 1313.8680, 1224.4645,
        1131.9473, 1153.3359, 1085.6349, 1050.1980,  969.4885,  969.1348,
         871.3544,  830.4430,  829.6474,  658.1512,  627.9603,  490.9517,
         537.6336,  485.0417,  410.0647,  413.3830,  363.6019,  346.8624,
         306.4301,  235.1742,  237.7407,  149.6485,  144.7570,  128.0045,
         171.1711,  126.2892,  104.1459,   79.0282,   57.9272,   45.2240,
          32.4548,   43.3408,   17.5917,   33.3178,   30.6762])]
2853.991466500855
2.6274736610989247 16.276994487438472 10.92620237875045
val isze = 8
idinces = [53 21 79 68 10 20 60 36 14 54 27 73  7 15 75  8 43 57 55 42 24 33 49 32
 41 19 38 56 64 69 13  1 78 39 50 40 37 66 81 74  4 67 76 61 77 23 52 63
 11 51 48 22 46 29 18 47 44 70  2 59 58 30 17 72 82 65  0  3 45  6 25 34
 31 16 80 28 12  5 71 26 35 62  9]
we are doing training validation split
training loss = 393.7387390136719 100
val loss = 311.5043640136719
training loss = 19.749364852905273 200
val loss = 21.376832962036133
training loss = 7.6470136642456055 300
val loss = 7.941693305969238
training loss = 7.627218723297119 400
val loss = 7.895928382873535
training loss = 7.605601787567139 500
val loss = 7.848570823669434
training loss = 7.582613468170166 600
val loss = 7.799229621887207
training loss = 7.558572769165039 700
val loss = 7.749054908752441
training loss = 7.533620834350586 800
val loss = 7.69891357421875
training loss = 7.5079731941223145 900
val loss = 7.649542808532715
training loss = 7.481706619262695 1000
val loss = 7.601334571838379
training loss = 7.4548139572143555 1100
val loss = 7.554619789123535
training loss = 7.427299976348877 1200
val loss = 7.509537696838379
training loss = 7.39917516708374 1300
val loss = 7.4661149978637695
training loss = 7.370433330535889 1400
val loss = 7.424271106719971
training loss = 7.341102123260498 1500
val loss = 7.383882522583008
training loss = 7.311192989349365 1600
val loss = 7.344736576080322
training loss = 7.28073263168335 1700
val loss = 7.306639671325684
training loss = 7.249752998352051 1800
val loss = 7.269364356994629
training loss = 7.218286514282227 1900
val loss = 7.2326979637146
training loss = 7.186356067657471 2000
val loss = 7.196444511413574
training loss = 7.153999328613281 2100
val loss = 7.1604228019714355
training loss = 7.121238708496094 2200
val loss = 7.124499320983887
training loss = 7.088103294372559 2300
val loss = 7.088534355163574
training loss = 7.054620742797852 2400
val loss = 7.052453994750977
training loss = 7.020823955535889 2500
val loss = 7.016188621520996
training loss = 6.986751556396484 2600
val loss = 6.9796929359436035
training loss = 6.952449798583984 2700
val loss = 6.942973613739014
training loss = 6.917986869812012 2800
val loss = 6.90605354309082
training loss = 6.883455753326416 2900
val loss = 6.869019031524658
training loss = 6.848956108093262 3000
val loss = 6.832481384277344
training loss = 6.814650058746338 3100
val loss = 6.7960076332092285
training loss = 6.780738353729248 3200
val loss = 6.760170936584473
training loss = 6.74746036529541 3300
val loss = 6.7255401611328125
training loss = 6.715132713317871 3400
val loss = 6.692211151123047
training loss = 6.68421745300293 3500
val loss = 6.658418655395508
training loss = 6.655055046081543 3600
val loss = 6.625856399536133
training loss = 6.6279754638671875 3700
val loss = 6.595588684082031
training loss = 6.603381156921387 3800
val loss = 6.566412925720215
training loss = 6.581387042999268 3900
val loss = 6.5385332107543945
training loss = 6.562842845916748 4000
val loss = 6.519060134887695
training loss = 6.543618202209473 4100
val loss = 6.48743200302124
training loss = 6.525132656097412 4200
val loss = 6.459660530090332
training loss = 6.501148223876953 4300
val loss = 6.421202659606934
training loss = 6.443472385406494 4400
val loss = 6.349129676818848
training loss = 6.232823848724365 4500
val loss = 6.059704303741455
training loss = 5.738227844238281 4600
val loss = 5.471901893615723
training loss = 4.848677158355713 4700
val loss = 4.6041178703308105
training loss = 3.1535987854003906 4800
val loss = 2.8268661499023438
training loss = 2.22190260887146 4900
val loss = 1.9222866296768188
training loss = 2.138669013977051 5000
val loss = 1.9955151081085205
training loss = 2.0968453884124756 5100
val loss = 2.081148624420166
training loss = 2.0758512020111084 5200
val loss = 2.1608734130859375
training loss = 2.054450273513794 5300
val loss = 2.210728168487549
training loss = 2.0432722568511963 5400
val loss = 2.254295825958252
training loss = 2.0353593826293945 5500
val loss = 2.286273956298828
training loss = 2.028949022293091 5600
val loss = 2.308650493621826
training loss = 2.0267021656036377 5700
val loss = 2.3269295692443848
training loss = 2.018549919128418 5800
val loss = 2.3334925174713135
training loss = 2.017346143722534 5900
val loss = 2.3423919677734375
training loss = 2.0091354846954346 6000
val loss = 2.338461399078369
training loss = 2.004457712173462 6100
val loss = 2.335200071334839
training loss = 2.0007224082946777 6200
val loss = 2.330780029296875
training loss = 1.995423674583435 6300
val loss = 2.3236634731292725
training loss = 1.9920440912246704 6400
val loss = 2.3180718421936035
training loss = 1.986801028251648 6500
val loss = 2.3090596199035645
training loss = 1.982819676399231 6600
val loss = 2.301731824874878
training loss = 1.985171914100647 6700
val loss = 2.2983851432800293
training loss = 1.975630283355713 6800
val loss = 2.29158616065979
training loss = 1.9755245447158813 6900
val loss = 2.2943921089172363
training loss = 1.9695606231689453 7000
val loss = 2.2868714332580566
training loss = 1.9669009447097778 7100
val loss = 2.2854974269866943
training loss = 2.0834743976593018 7200
val loss = 2.3718442916870117
training loss = 1.9622148275375366 7300
val loss = 2.2855136394500732
training loss = 1.960150957107544 7400
val loss = 2.2857351303100586
training loss = 1.959923505783081 7500
val loss = 2.292538642883301
training loss = 1.9566019773483276 7600
val loss = 2.287493944168091
training loss = 1.9588590860366821 7700
val loss = 2.286564350128174
training loss = 1.9536446332931519 7800
val loss = 2.2896175384521484
training loss = 1.952345371246338 7900
val loss = 2.289809465408325
training loss = 1.951236605644226 8000
val loss = 2.2910046577453613
training loss = 1.950101375579834 8100
val loss = 2.2918295860290527
training loss = 1.958622932434082 8200
val loss = 2.2926881313323975
training loss = 1.948223352432251 8300
val loss = 2.293891191482544
training loss = 1.9473893642425537 8400
val loss = 2.2940216064453125
training loss = 1.95004403591156 8500
val loss = 2.304062843322754
training loss = 1.945899248123169 8600
val loss = 2.295705556869507
training loss = 1.9756892919540405 8700
val loss = 2.308830499649048
training loss = 1.9446361064910889 8800
val loss = 2.297215461730957
training loss = 1.944061279296875 8900
val loss = 2.2972211837768555
training loss = 1.9437148571014404 9000
val loss = 2.298027992248535
training loss = 1.9430478811264038 9100
val loss = 2.2987165451049805
training loss = 1.9568661451339722 9200
val loss = 2.325402021408081
training loss = 1.9421539306640625 9300
val loss = 2.299957275390625
training loss = 2.3176071643829346 9400
val loss = 2.7110109329223633
training loss = 1.941379427909851 9500
val loss = 2.3019113540649414
training loss = 1.9410209655761719 9600
val loss = 2.3012876510620117
training loss = 1.9598356485366821 9700
val loss = 2.3354196548461914
training loss = 1.9403654336929321 9800
val loss = 2.3028712272644043
training loss = 1.940057396888733 9900
val loss = 2.3025317192077637
reduced chi^2 level 2 = 1.9400423765182495
Constrained alpha: 1.8074918985366821
Constrained beta: 4.049136161804199
Constrained gamma: 13.402742385864258
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 858.0175,  857.9679,  925.7203,  990.5774, 1045.9023, 1127.9821,
        1046.6018, 1178.7653, 1189.4019, 1148.5312, 1236.8186, 1221.1768,
        1322.0743, 1250.3893, 1290.1732, 1409.9288, 1446.3220, 1463.0236,
        1472.8210, 1585.9752, 1560.3053, 1569.2640, 1580.6827, 1639.0597,
        1657.4745, 1738.5994, 1609.7810, 1674.0989, 1738.3962, 1639.5535,
        1721.1727, 1786.7791, 1682.1945, 1668.5895, 1747.3965, 1743.0629,
        1695.6421, 1574.5847, 1657.7985, 1703.9557, 1574.5408, 1542.3708,
        1457.6570, 1493.7279, 1335.4404, 1294.7638, 1306.8578, 1189.1543,
        1203.0748, 1199.3682, 1053.4144, 1003.9872,  897.9794,  938.3052,
         955.9181,  850.3273,  829.8582,  695.1152,  590.9694,  504.2233,
         592.2841,  463.5110,  469.0211,  402.7446,  339.7162,  342.0190,
         297.3914,  273.2250,  190.3486,  156.5540,  172.3800,  138.4893,
         161.4452,  117.9888,   99.5005,   63.5253,   37.6875,   55.6032,
          41.7057,   41.6261,   22.8227,   37.1912,   27.1639])]
2856.876716317399
4.0681050415639115 0.19994028881256343 22.456793367315097
val isze = 8
idinces = [76 11 81 37 18 52 46 32 25 29 47 80 59 40  4  1 72 17 57 79 10 15 45 34
 54 22 31  6 13 42 39  9 63 64 27 49 51 69 55 38 43 73 23 62 21  8 41 58
 77 60 19 75 56 26 20 36 35 78 16  2 14 65  3 82 70  7 44 33 74 24 48  5
 50 61  0 66 71 68 30 53 67 12 28]
we are doing training validation split
training loss = 211.04754638671875 100
val loss = 273.4886169433594
training loss = 77.79705810546875 200
val loss = 83.3318862915039
training loss = 16.87616539001465 300
val loss = 7.856475830078125
training loss = 11.938037872314453 400
val loss = 5.57615852355957
training loss = 9.657136917114258 500
val loss = 4.596228122711182
training loss = 8.26823616027832 600
val loss = 4.246816158294678
training loss = 7.243130683898926 700
val loss = 3.971527338027954
training loss = 6.361295223236084 800
val loss = 3.7388978004455566
training loss = 5.483698844909668 900
val loss = 3.5547986030578613
training loss = 4.5935750007629395 1000
val loss = 3.537215232849121
training loss = 3.720271110534668 1100
val loss = 3.851074695587158
training loss = 3.184732675552368 1200
val loss = 4.350183010101318
training loss = 2.985628843307495 1300
val loss = 4.614674091339111
training loss = 2.8874824047088623 1400
val loss = 4.7447004318237305
training loss = 2.821390151977539 1500
val loss = 4.807966232299805
training loss = 2.772045373916626 1600
val loss = 4.8651275634765625
training loss = 2.737133264541626 1700
val loss = 4.8923492431640625
training loss = 2.7082977294921875 1800
val loss = 4.884890079498291
training loss = 2.6816182136535645 1900
val loss = 4.849119186401367
training loss = 2.6533195972442627 2000
val loss = 4.788280963897705
training loss = 2.6147212982177734 2100
val loss = 4.694559097290039
training loss = 2.559941053390503 2200
val loss = 4.562830924987793
training loss = 2.5116207599639893 2300
val loss = 4.4952874183654785
training loss = 2.4766483306884766 2400
val loss = 4.465007305145264
training loss = 2.4505956172943115 2500
val loss = 4.418877124786377
training loss = 2.4309840202331543 2600
val loss = 4.391080856323242
training loss = 2.4143683910369873 2700
val loss = 4.335570335388184
training loss = 2.399852991104126 2800
val loss = 4.368039131164551
training loss = 2.386765241622925 2900
val loss = 4.31469202041626
training loss = 2.3748581409454346 3000
val loss = 4.3293561935424805
training loss = 2.3633339405059814 3100
val loss = 4.329947471618652
training loss = 2.3529534339904785 3200
val loss = 4.306247234344482
training loss = 2.342543601989746 3300
val loss = 4.30759859085083
training loss = 2.3352787494659424 3400
val loss = 4.168139934539795
training loss = 2.323467969894409 3500
val loss = 4.287176132202148
training loss = 2.3165223598480225 3600
val loss = 4.143019676208496
training loss = 2.3058693408966064 3700
val loss = 4.270470142364502
training loss = 2.297154426574707 3800
val loss = 4.279321670532227
training loss = 2.2893736362457275 3900
val loss = 4.246738910675049
training loss = 2.2811458110809326 4000
val loss = 4.244473457336426
training loss = 2.2751777172088623 4100
val loss = 4.143150329589844
training loss = 2.266117811203003 4200
val loss = 4.229554176330566
training loss = 2.27028226852417 4300
val loss = 3.929097890853882
training loss = 2.252332925796509 4400
val loss = 4.2189412117004395
training loss = 2.2452571392059326 4500
val loss = 4.2069172859191895
training loss = 2.239210367202759 4600
val loss = 4.19173526763916
training loss = 2.2325351238250732 4700
val loss = 4.197314739227295
training loss = 2.2598769664764404 4800
val loss = 4.6668596267700195
training loss = 2.2207860946655273 4900
val loss = 4.188337326049805
training loss = 2.214707374572754 5000
val loss = 4.181870460510254
training loss = 2.209665060043335 5100
val loss = 4.213688373565674
training loss = 2.20379638671875 5200
val loss = 4.173610210418701
training loss = 2.206374168395996 5300
val loss = 4.391252517700195
training loss = 2.193617343902588 5400
val loss = 4.166203022003174
training loss = 2.262331962585449 5500
val loss = 4.974434852600098
training loss = 2.1840004920959473 5600
val loss = 4.158490180969238
training loss = 2.2169480323791504 5700
val loss = 3.6810731887817383
training loss = 2.1752071380615234 5800
val loss = 4.183748245239258
training loss = 2.170586347579956 5900
val loss = 4.156669616699219
training loss = 2.1765670776367188 6000
val loss = 4.41448450088501
training loss = 2.162525177001953 6100
val loss = 4.160320281982422
training loss = 2.420527935028076 6200
val loss = 3.081946611404419
training loss = 2.1550650596618652 6300
val loss = 4.16086483001709
training loss = 2.151179552078247 6400
val loss = 4.1579365730285645
training loss = 2.1487326622009277 6500
val loss = 4.231602668762207
training loss = 2.144578456878662 6600
val loss = 4.166858196258545
training loss = 2.1471407413482666 6700
val loss = 3.965148687362671
training loss = 2.1384241580963135 6800
val loss = 4.185181140899658
training loss = 2.1351473331451416 6900
val loss = 4.172152996063232
training loss = 2.149444341659546 7000
val loss = 4.5174970626831055
training loss = 2.129624843597412 7100
val loss = 4.183603286743164
training loss = 2.1266732215881348 7200
val loss = 4.183002471923828
training loss = 2.124444007873535 7300
val loss = 4.206371307373047
training loss = 2.121652603149414 7400
val loss = 4.189689636230469
training loss = 2.1362152099609375 7500
val loss = 3.862558603286743
training loss = 2.1170036792755127 7600
val loss = 4.200719833374023
training loss = 2.1338789463043213 7700
val loss = 3.8540525436401367
training loss = 2.1128296852111816 7800
val loss = 4.2078752517700195
training loss = 2.110507011413574 7900
val loss = 4.209174156188965
training loss = 2.158827781677246 8000
val loss = 3.6897406578063965
training loss = 2.1066324710845947 8100
val loss = 4.218340873718262
training loss = 2.1049416065216064 8200
val loss = 4.271233081817627
training loss = 2.1032493114471436 8300
val loss = 4.197212219238281
training loss = 2.1011452674865723 8400
val loss = 4.227004051208496
training loss = 2.101250171661377 8500
val loss = 4.315206050872803
training loss = 2.0979630947113037 8600
val loss = 4.233797550201416
training loss = 2.0969173908233643 8700
val loss = 4.165591239929199
training loss = 2.095308542251587 8800
val loss = 4.272967338562012
training loss = 2.0935027599334717 8900
val loss = 4.244880676269531
training loss = 2.372234344482422 9000
val loss = 5.884335994720459
training loss = 2.090960741043091 9100
val loss = 4.236628532409668
training loss = 2.0893921852111816 9200
val loss = 4.251932144165039
training loss = 2.0926315784454346 9300
val loss = 4.093963623046875
training loss = 2.087050437927246 9400
val loss = 4.257807731628418
training loss = 2.373100519180298 9500
val loss = 3.154440402984619
training loss = 2.084890842437744 9600
val loss = 4.268655776977539
training loss = 2.083571434020996 9700
val loss = 4.265475749969482
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 875.5991,  851.7449,  925.7134,  992.6431, 1009.0112, 1132.8230,
        1064.3073, 1106.5051, 1167.6315, 1189.2542, 1142.5743, 1162.4498,
        1218.0139, 1248.5391, 1287.6433, 1445.5184, 1341.0250, 1450.8688,
        1580.2709, 1538.5402, 1659.6937, 1522.1898, 1576.7527, 1559.1008,
        1600.8138, 1706.6196, 1577.2747, 1635.3093, 1786.3945, 1633.8207,
        1679.9408, 1698.5220, 1697.0219, 1765.8792, 1707.8413, 1771.7622,
        1718.2994, 1697.9253, 1692.7435, 1609.6307, 1654.7808, 1602.8163,
        1464.6085, 1536.0927, 1357.9973, 1314.4192, 1316.3806, 1264.2656,
        1115.3409, 1282.2819, 1084.4821,  963.7300,  940.6270,  905.1006,
         887.5629,  837.2651,  871.6471,  722.3027,  569.4125,  540.0066,
         563.2507,  447.2818,  459.0414,  412.4061,  384.8522,  344.5686,
         274.2018,  270.5096,  195.4082,  193.6493,  135.6996,  133.7170,
         148.9374,  111.5569,  109.2160,   71.8714,   36.9903,   35.4973,
          35.7674,   44.9641,   16.8220,   27.7485,   32.5598])]
3392.8621004437196
4.0726493360349 6.6407406712734325 3.53093219070012
val isze = 8
idinces = [11 54 42 24 41 70 76 51 58 50 57 80 33 21 17  0  9 26  7 15 25 40 69 44
 66 38 19 39 71 30 68 65 32 22 43 73 28 82 13 10  3 75  5  6 18 12 63 20
 67 47 59 45 60 36  1 49 62 14  8 46 79 56 55 35 74 61 64 37 31 27 34 23
  2 16 48 29 72 81 78 77  4 52 53]
we are doing training validation split
training loss = 145.23171997070312 100
val loss = 144.49644470214844
training loss = 38.41117858886719 200
val loss = 43.4561767578125
training loss = 23.497106552124023 300
val loss = 25.523784637451172
training loss = 16.510791778564453 400
val loss = 16.787742614746094
training loss = 12.763224601745605 500
val loss = 12.071077346801758
training loss = 10.58234691619873 600
val loss = 9.341069221496582
training loss = 9.24707317352295 700
val loss = 7.684173107147217
training loss = 8.402482032775879 800
val loss = 6.644835948944092
training loss = 7.8569111824035645 900
val loss = 5.977508544921875
training loss = 7.499736309051514 1000
val loss = 5.541925430297852
training loss = 7.263904571533203 1100
val loss = 5.25418758392334
training loss = 7.1072869300842285 1200
val loss = 5.062536239624023
training loss = 7.002722263336182 1300
val loss = 4.933812141418457
training loss = 6.932375431060791 1400
val loss = 4.846793174743652
training loss = 6.884417533874512 1500
val loss = 4.787363529205322
training loss = 6.850963592529297 1600
val loss = 4.74616813659668
training loss = 6.826754093170166 1700
val loss = 4.717111587524414
training loss = 6.808315277099609 1800
val loss = 4.696053504943848
training loss = 6.79336404800415 1900
val loss = 4.680257320404053
training loss = 6.780429840087891 2000
val loss = 4.667885780334473
training loss = 6.768579483032227 2100
val loss = 4.65779972076416
training loss = 6.75723123550415 2200
val loss = 4.649196147918701
training loss = 6.746032238006592 2300
val loss = 4.641603469848633
training loss = 6.73477029800415 2400
val loss = 4.634608268737793
training loss = 6.723320007324219 2500
val loss = 4.628024101257324
training loss = 6.711618900299072 2600
val loss = 4.6216888427734375
training loss = 6.699621677398682 2700
val loss = 4.615538597106934
training loss = 6.687316417694092 2800
val loss = 4.609367370605469
training loss = 6.674688816070557 2900
val loss = 4.603288173675537
training loss = 6.66174840927124 3000
val loss = 4.597158432006836
training loss = 6.64849853515625 3100
val loss = 4.590998649597168
training loss = 6.634958267211914 3200
val loss = 4.584851264953613
training loss = 6.621143341064453 3300
val loss = 4.578651428222656
training loss = 6.607090473175049 3400
val loss = 4.572440147399902
training loss = 6.592835903167725 3500
val loss = 4.566242218017578
training loss = 6.578432559967041 3600
val loss = 4.5600409507751465
training loss = 6.563941478729248 3700
val loss = 4.553899765014648
training loss = 6.549444198608398 3800
val loss = 4.547946453094482
training loss = 6.535035133361816 3900
val loss = 4.542054176330566
training loss = 6.5208420753479 4000
val loss = 4.536446571350098
training loss = 6.507007598876953 4100
val loss = 4.531080722808838
training loss = 6.493694305419922 4200
val loss = 4.526047706604004
training loss = 6.481090068817139 4300
val loss = 4.521450042724609
training loss = 6.469388961791992 4400
val loss = 4.517399311065674
training loss = 6.458772659301758 4500
val loss = 4.5138397216796875
training loss = 6.449402809143066 4600
val loss = 4.510961055755615
training loss = 6.441375732421875 4700
val loss = 4.508755683898926
training loss = 6.434710502624512 4800
val loss = 4.507034778594971
training loss = 6.429338455200195 4900
val loss = 4.505990028381348
training loss = 6.425108432769775 5000
val loss = 4.505333423614502
training loss = 6.421804904937744 5100
val loss = 4.505165100097656
training loss = 6.4191670417785645 5200
val loss = 4.505074501037598
training loss = 6.417719841003418 5300
val loss = 4.546829700469971
training loss = 6.415505886077881 5400
val loss = 4.505064010620117
training loss = 6.481143951416016 5500
val loss = 4.138684272766113
training loss = 6.412417411804199 5600
val loss = 4.504565715789795
training loss = 6.413854122161865 5700
val loss = 4.605207443237305
training loss = 6.40846586227417 5800
val loss = 4.5052385330200195
training loss = 6.40535831451416 5900
val loss = 4.498080253601074
training loss = 6.400827884674072 6000
val loss = 4.501176834106445
training loss = 6.392755031585693 6100
val loss = 4.500340938568115
training loss = 6.388944625854492 6200
val loss = 4.717904090881348
training loss = 6.305191516876221 6300
val loss = 4.477749824523926
training loss = 5.701414585113525 6400
val loss = 4.874233245849609
training loss = 4.4612135887146 6500
val loss = 4.253206253051758
training loss = 3.1736395359039307 6600
val loss = 4.487462043762207
training loss = 2.7532007694244385 6700
val loss = 4.893003463745117
training loss = 2.685333013534546 6800
val loss = 4.951146125793457
training loss = 2.645108222961426 6900
val loss = 4.920017242431641
training loss = 2.6429338455200195 7000
val loss = 5.220847129821777
training loss = 2.5993266105651855 7100
val loss = 4.847923278808594
training loss = 2.6026899814605713 7200
val loss = 5.087946891784668
training loss = 2.5762176513671875 7300
val loss = 4.793448448181152
training loss = 2.573751211166382 7400
val loss = 4.633284091949463
training loss = 2.561933755874634 7500
val loss = 4.748005390167236
training loss = 2.5564379692077637 7600
val loss = 4.7419023513793945
training loss = 2.5519356727600098 7700
val loss = 4.716887474060059
training loss = 2.547924280166626 7800
val loss = 4.711477279663086
training loss = 2.5495975017547607 7900
val loss = 4.563469409942627
training loss = 2.5413107872009277 8000
val loss = 4.685263633728027
training loss = 2.5384581089019775 8100
val loss = 4.656795024871826
training loss = 2.5358617305755615 8200
val loss = 4.66872501373291
training loss = 2.5334699153900146 8300
val loss = 4.650793075561523
training loss = 2.5329997539520264 8400
val loss = 4.560588836669922
training loss = 2.529277801513672 8500
val loss = 4.629830360412598
training loss = 2.528526544570923 8600
val loss = 4.553247451782227
training loss = 2.525623083114624 8700
val loss = 4.600374221801758
training loss = 2.5239555835723877 8800
val loss = 4.598269462585449
training loss = 2.538987159729004 8900
val loss = 4.8448262214660645
training loss = 2.5209126472473145 9000
val loss = 4.576835632324219
training loss = 2.5194954872131348 9100
val loss = 4.569363117218018
training loss = 2.518930196762085 9200
val loss = 4.507648944854736
training loss = 2.5169060230255127 9300
val loss = 4.5506391525268555
training loss = 2.547569513320923 9400
val loss = 4.905555248260498
training loss = 2.514566421508789 9500
val loss = 4.534308433532715
training loss = 2.5134658813476562 9600
val loss = 4.5211286544799805
training loss = 2.5126168727874756 9700
val loss = 4.489851951599121
training loss = 2.5114693641662598 9800
val loss = 4.505800724029541
training loss = 2.5199413299560547 9900
val loss = 4.690834045410156
training loss = 2.5096487998962402 10000
val loss = 4.4913787841796875
training loss = 2.5087969303131104 10100
val loss = 4.481117248535156
training loss = 2.5081868171691895 10200
val loss = 4.500365257263184
training loss = 2.5072619915008545 10300
val loss = 4.4658203125
training loss = 2.5065290927886963 10400
val loss = 4.457127094268799
training loss = 2.5061256885528564 10500
val loss = 4.483956336975098
training loss = 2.5051915645599365 10600
val loss = 4.44338321685791
training loss = 2.5054171085357666 10700
val loss = 4.383009910583496
training loss = 2.503978729248047 10800
val loss = 4.430580139160156
training loss = 2.518847703933716 10900
val loss = 4.6686930656433105
training loss = 2.502889394760132 11000
val loss = 4.415280342102051
training loss = 2.502394676208496 11100
val loss = 4.407519340515137
training loss = 2.5022687911987305 11200
val loss = 4.367977142333984
training loss = 2.5014851093292236 11300
val loss = 4.395175933837891
training loss = 2.502100706100464 11400
val loss = 4.33125638961792
training loss = 2.5006706714630127 11500
val loss = 4.3817901611328125
training loss = 2.5839264392852783 11600
val loss = 4.9872846603393555
training loss = 2.4999537467956543 11700
val loss = 4.368062973022461
training loss = 2.4999778270721436 11800
val loss = 4.4014058113098145
training loss = 2.499359607696533 11900
val loss = 4.348091125488281
training loss = 2.499030828475952 12000
val loss = 4.3548688888549805
training loss = 2.4987504482269287 12100
val loss = 4.351487636566162
training loss = 2.49851655960083 12200
val loss = 4.345122337341309
training loss = 2.4982922077178955 12300
val loss = 4.334476947784424
training loss = 2.4981396198272705 12400
val loss = 4.319295883178711
training loss = 2.49786376953125 12500
val loss = 4.330305099487305
training loss = 2.5002477169036865 12600
val loss = 4.4232988357543945
training loss = 2.4975013732910156 12700
val loss = 4.321678638458252
training loss = 2.5123353004455566 12800
val loss = 4.107255935668945
training loss = 2.497205972671509 12900
val loss = 4.305789947509766
training loss = 2.4970574378967285 13000
val loss = 4.308551788330078
training loss = 2.4989523887634277 13100
val loss = 4.2264533042907715
training loss = 2.496814489364624 13200
val loss = 4.2998528480529785
training loss = 2.496706962585449 13300
val loss = 4.297574043273926
training loss = 2.4966838359832764 13400
val loss = 4.278409004211426
training loss = 2.4965267181396484 13500
val loss = 4.290124893188477
training loss = 2.496488571166992 13600
val loss = 4.275252342224121
training loss = 2.496373414993286 13700
val loss = 4.286619186401367
training loss = 2.4963107109069824 13800
val loss = 4.279894828796387
training loss = 2.496486186981201 13900
val loss = 4.306093215942383
training loss = 2.496204376220703 14000
val loss = 4.273196697235107
training loss = 2.4961960315704346 14100
val loss = 4.274028778076172
training loss = 2.496126413345337 14200
val loss = 4.264157295227051
training loss = 2.496089220046997 14300
val loss = 4.26534366607666
training loss = 2.4967644214630127 14400
val loss = 4.215939998626709
training loss = 2.4960386753082275 14500
val loss = 4.260207176208496
training loss = 2.5966567993164062 14600
val loss = 3.778336763381958
training loss = 2.496011972427368 14700
val loss = 4.25877046585083
training loss = 2.4959983825683594 14800
val loss = 4.251870155334473
training loss = 2.496668577194214 14900
val loss = 4.299066066741943
training loss = 2.495990514755249 15000
val loss = 4.247690200805664
training loss = 2.501904010772705 15100
val loss = 4.11798620223999
training loss = 2.4959988594055176 15200
val loss = 4.24542236328125
training loss = 2.496001720428467 15300
val loss = 4.241971015930176
training loss = 2.49655818939209 15400
val loss = 4.28327751159668
training loss = 2.496026039123535 15500
val loss = 4.237691879272461
training loss = 2.5155789852142334 15600
val loss = 4.0056939125061035
training loss = 2.496058464050293 15700
val loss = 4.234125137329102
training loss = 2.573198080062866 15800
val loss = 4.807764530181885
training loss = 2.496105432510376 15900
val loss = 4.22774600982666
training loss = 2.496119499206543 16000
val loss = 4.230857849121094
training loss = 2.4962105751037598 16100
val loss = 4.241494178771973
training loss = 2.496175765991211 16200
val loss = 4.226125717163086
training loss = 2.496265411376953 16300
val loss = 4.239833831787109
training loss = 2.496277332305908 16400
val loss = 4.212672233581543
training loss = 2.49625563621521 16500
val loss = 4.221134185791016
training loss = 2.4964492321014404 16600
val loss = 4.243186950683594
training loss = 2.4963250160217285 16700
val loss = 4.218599319458008
training loss = 2.7613871097564697 16800
val loss = 5.391238212585449
training loss = 2.4964001178741455 16900
val loss = 4.220972537994385
training loss = 2.4964170455932617 17000
val loss = 4.2155375480651855
training loss = 2.4965617656707764 17100
val loss = 4.198890686035156
training loss = 2.4964964389801025 17200
val loss = 4.213106155395508
training loss = 2.7155070304870605 17300
val loss = 5.250080585479736
training loss = 2.4965832233428955 17400
val loss = 4.209000587463379
training loss = 2.4965968132019043 17500
val loss = 4.209871292114258
training loss = 2.49666690826416 17600
val loss = 4.2149434089660645
training loss = 2.4966788291931152 17700
val loss = 4.209504127502441
training loss = 2.49763560295105 17800
val loss = 4.154648780822754
training loss = 2.496786117553711 17900
val loss = 4.217063903808594
training loss = 2.4967782497406006 18000
val loss = 4.205489158630371
training loss = 2.5087435245513916 18100
val loss = 4.028700828552246
training loss = 2.4968574047088623 18200
val loss = 4.205791473388672
training loss = 2.4968934059143066 18300
val loss = 4.195162296295166
training loss = 2.4969675540924072 18400
val loss = 4.194985389709473
training loss = 2.4969534873962402 18500
val loss = 4.2019853591918945
training loss = 2.680696487426758 18600
val loss = 5.130539894104004
training loss = 2.4970502853393555 18700
val loss = 4.208460807800293
training loss = 2.4970507621765137 18800
val loss = 4.199892997741699
training loss = 2.500613212585449 18900
val loss = 4.096705913543701
training loss = 2.4971327781677246 19000
val loss = 4.197596549987793
training loss = 2.4971461296081543 19100
val loss = 4.19748067855835
training loss = 2.519340753555298 19200
val loss = 4.487876892089844
training loss = 2.497225522994995 19300
val loss = 4.195522308349609
training loss = 2.4972405433654785 19400
val loss = 4.195290565490723
training loss = 2.4979350566864014 19500
val loss = 4.242099285125732
training loss = 2.4973156452178955 19600
val loss = 4.195367813110352
training loss = 2.7691593170166016 19700
val loss = 5.375351905822754
training loss = 2.497385263442993 19800
val loss = 4.192841053009033
training loss = 2.497404098510742 19900
val loss = 4.194357395172119
training loss = 2.4976913928985596 20000
val loss = 4.167272090911865
training loss = 2.497476816177368 20100
val loss = 4.1919846534729
training loss = 2.4975244998931885 20200
val loss = 4.199619293212891
training loss = 2.4975533485412598 20300
val loss = 4.190035820007324
training loss = 2.4975688457489014 20400
val loss = 4.190832138061523
training loss = 2.498084783554077 20500
val loss = 4.229714393615723
training loss = 2.4976398944854736 20600
val loss = 4.1897382736206055
training loss = 2.5238754749298096 20700
val loss = 4.504297256469727
training loss = 2.4977147579193115 20800
val loss = 4.187312602996826
training loss = 2.4977316856384277 20900
val loss = 4.187718868255615
training loss = 2.4986283779144287 21000
val loss = 4.2409987449646
training loss = 2.497805595397949 21100
val loss = 4.187311172485352
training loss = 2.5026113986968994 21200
val loss = 4.0699310302734375
training loss = 2.4978935718536377 21300
val loss = 4.179727554321289
training loss = 2.4978902339935303 21400
val loss = 4.186482906341553
training loss = 2.4983761310577393 21500
val loss = 4.2242960929870605
training loss = 2.497957706451416 21600
val loss = 4.1856303215026855
training loss = 2.4991977214813232 21700
val loss = 4.251194000244141
training loss = 2.4980244636535645 21800
val loss = 4.186396598815918
training loss = 2.638728380203247 21900
val loss = 4.981118679046631
training loss = 2.4981088638305664 22000
val loss = 4.17941427230835
training loss = 2.498112916946411 22100
val loss = 4.183712005615234
training loss = 2.506989002227783 22200
val loss = 4.362030506134033
training loss = 2.4981799125671387 22300
val loss = 4.181845664978027
training loss = 2.4982283115386963 22400
val loss = 4.193045616149902
training loss = 2.4982547760009766 22500
val loss = 4.177392959594727
training loss = 2.4982588291168213 22600
val loss = 4.1818647384643555
training loss = 2.5002334117889404 22700
val loss = 4.262760162353516
training loss = 2.4983203411102295 22800
val loss = 4.181366920471191
training loss = 2.7693772315979004 22900
val loss = 3.5291171073913574
training loss = 2.498394012451172 23000
val loss = 4.177891254425049
training loss = 2.4984004497528076 23100
val loss = 4.180957317352295
training loss = 2.5093843936920166 23200
val loss = 4.009231090545654
training loss = 2.498467445373535 23300
val loss = 4.180356502532959
training loss = 2.498479127883911 23400
val loss = 4.18009090423584
training loss = 2.5052640438079834 23500
val loss = 4.045150279998779
training loss = 2.4985382556915283 23600
val loss = 4.180109977722168
training loss = 2.498748540878296 23700
val loss = 4.204538345336914
training loss = 2.4985904693603516 23800
val loss = 4.182621955871582
training loss = 2.498602867126465 23900
val loss = 4.179477691650391
training loss = 2.499140977859497 24000
val loss = 4.220639228820801
training loss = 2.4986565113067627 24100
val loss = 4.179120063781738
training loss = 2.5010368824005127 24200
val loss = 4.265772342681885
training loss = 2.4987127780914307 24300
val loss = 4.181975841522217
training loss = 2.4987213611602783 24400
val loss = 4.177743434906006
training loss = 2.4988653659820557 24500
val loss = 4.162022590637207
training loss = 2.4987707138061523 24600
val loss = 4.178567409515381
training loss = 2.498835563659668 24700
val loss = 4.164546966552734
training loss = 2.498814821243286 24800
val loss = 4.178297519683838
training loss = 2.498823404312134 24900
val loss = 4.177387237548828
training loss = 2.5072598457336426 25000
val loss = 4.026533126831055
training loss = 2.4988632202148438 25100
val loss = 4.176940441131592
training loss = 2.6222853660583496 25200
val loss = 4.91825532913208
training loss = 2.4989054203033447 25300
val loss = 4.182322978973389
training loss = 2.498904228210449 25400
val loss = 4.1764750480651855
training loss = 2.4995150566101074 25500
val loss = 4.1370038986206055
training loss = 2.4989397525787354 25600
val loss = 4.177449703216553
training loss = 2.4991908073425293 25700
val loss = 4.204797267913818
training loss = 2.498969793319702 25800
val loss = 4.181703567504883
training loss = 2.498964786529541 25900
val loss = 4.176605224609375
training loss = 2.5024280548095703 26000
val loss = 4.077755928039551
training loss = 2.4989852905273438 26100
val loss = 4.176237106323242
training loss = 2.5069406032562256 26200
val loss = 4.027234077453613
training loss = 2.4990079402923584 26300
val loss = 4.174142360687256
training loss = 2.4989943504333496 26400
val loss = 4.17683219909668
training loss = 2.520352363586426 26500
val loss = 4.460871696472168
training loss = 2.4989945888519287 26600
val loss = 4.179143905639648
training loss = 2.565201997756958 26700
val loss = 4.694028854370117
training loss = 2.4989962577819824 26800
val loss = 4.183587074279785
training loss = 2.4989676475524902 26900
val loss = 4.179430961608887
training loss = 2.4991297721862793 27000
val loss = 4.200685024261475
training loss = 2.49894642829895 27100
val loss = 4.1771559715271
training loss = 2.4989540576934814 27200
val loss = 4.176143646240234
training loss = 2.498911142349243 27300
val loss = 4.179293632507324
training loss = 2.4997568130493164 27400
val loss = 4.126961708068848
training loss = 2.498892068862915 27500
val loss = 4.170449733734131
training loss = 2.49881649017334 27600
val loss = 4.179324626922607
training loss = 2.5000524520874023 27700
val loss = 4.244599342346191
training loss = 2.4987363815307617 27800
val loss = 4.180422782897949
training loss = 2.519859790802002 27900
val loss = 3.9502501487731934
training loss = 2.4986352920532227 28000
val loss = 4.183412551879883
training loss = 2.4985828399658203 28100
val loss = 4.173394203186035
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 858.6724,  884.5547,  892.3194,  913.9937,  895.0954, 1084.4226,
        1097.5408, 1144.8694, 1204.0295, 1199.0227, 1187.8738, 1226.3092,
        1297.6406, 1233.7037, 1289.6101, 1383.4835, 1335.8253, 1385.6594,
        1518.1364, 1443.6857, 1594.2892, 1590.4436, 1625.8037, 1567.4543,
        1697.1936, 1638.7479, 1670.3354, 1737.5735, 1688.8765, 1673.1239,
        1677.1797, 1687.6591, 1679.4299, 1751.2107, 1639.0426, 1740.4288,
        1678.6517, 1574.3270, 1662.2666, 1621.5831, 1639.7289, 1526.0759,
        1507.0244, 1500.8424, 1368.6227, 1344.2708, 1283.2058, 1259.9174,
        1165.2128, 1156.5315, 1055.0208,  964.8537,  961.4722,  889.3954,
         857.8479,  820.7127,  824.9233,  690.8968,  603.4499,  515.4380,
         531.7888,  450.5298,  424.8792,  370.6691,  351.3578,  347.1080,
         278.5614,  262.4327,  181.1257,  185.1618,  166.8845,  136.0937,
         149.8766,  111.1949,   91.5787,   83.7733,   45.4315,   39.7054,
          37.5352,   43.4322,   22.4736,   42.4845,   34.0811])]
2546.6771154061
4.471798558472948 11.416373174716789 87.07684885672032
val isze = 8
idinces = [37 24 11  1  3 43 57 48 36 46 68 65 80 77 60 45 27 40 16 35 21 17 62  6
  4  8 13  0 56 66 74 58 42 64 61 79 52 63 22 32 26 19 20 18 10 14 39 70
  9 72 47 30 76 67 53 55 12 44 29 71  5 54 34 41 15 59 28 82 81 38 50 69
 75  7 51  2 25 78 33 31 73 23 49]
we are doing training validation split
training loss = 347.8396911621094 100
val loss = 553.858154296875
training loss = 7.30192232131958 200
val loss = 9.247241973876953
training loss = 7.047560214996338 300
val loss = 7.993793487548828
training loss = 6.897562026977539 400
val loss = 7.333800792694092
training loss = 6.8053669929504395 500
val loss = 6.858298301696777
training loss = 6.748005390167236 600
val loss = 6.527648448944092
training loss = 6.709666728973389 700
val loss = 6.304144382476807
training loss = 6.681027412414551 800
val loss = 6.157410621643066
training loss = 6.657129287719727 900
val loss = 6.064279079437256
training loss = 6.6355438232421875 1000
val loss = 6.00750207901001
training loss = 6.615167140960693 1100
val loss = 5.974407196044922
training loss = 6.595515251159668 1200
val loss = 5.955765724182129
training loss = 6.576354503631592 1300
val loss = 5.9456658363342285
training loss = 6.557550430297852 1400
val loss = 5.9399518966674805
training loss = 6.538990497589111 1500
val loss = 5.936219215393066
training loss = 6.520576477050781 1600
val loss = 5.933010578155518
training loss = 6.502191543579102 1700
val loss = 5.929616928100586
training loss = 6.483739376068115 1800
val loss = 5.925614356994629
training loss = 6.465123176574707 1900
val loss = 5.921041488647461
training loss = 6.446249485015869 2000
val loss = 5.915869235992432
training loss = 6.4270453453063965 2100
val loss = 5.910118103027344
training loss = 6.407469749450684 2200
val loss = 5.904067039489746
training loss = 6.388248443603516 2300
val loss = 5.8489179611206055
training loss = 6.367550373077393 2400
val loss = 5.890472412109375
training loss = 6.347411632537842 2500
val loss = 5.865784645080566
training loss = 6.327033042907715 2600
val loss = 5.884738922119141
training loss = 6.3067402839660645 2700
val loss = 5.987295627593994
training loss = 6.271732330322266 2800
val loss = 5.878704071044922
training loss = 6.24004602432251 2900
val loss = 5.519172668457031
training loss = 5.815621852874756 3000
val loss = 5.899196147918701
training loss = 4.952540874481201 3100
val loss = 5.784108638763428
training loss = 3.1416704654693604 3200
val loss = 3.8893675804138184
training loss = 1.8632310628890991 3300
val loss = 2.639378547668457
training loss = 1.7492916584014893 3400
val loss = 2.372462749481201
training loss = 1.7155176401138306 3500
val loss = 2.29313325881958
training loss = 1.6932529211044312 3600
val loss = 2.2390265464782715
training loss = 1.6783063411712646 3700
val loss = 2.1915721893310547
training loss = 1.6748510599136353 3800
val loss = 2.09602689743042
training loss = 1.6601332426071167 3900
val loss = 2.136260747909546
training loss = 1.8603312969207764 4000
val loss = 2.765350341796875
training loss = 1.6499882936477661 4100
val loss = 2.102038860321045
training loss = 1.6465203762054443 4200
val loss = 2.0926053524017334
training loss = 1.6661449670791626 4300
val loss = 1.9857332706451416
training loss = 1.64111328125 4400
val loss = 2.074068307876587
training loss = 1.6390244960784912 4500
val loss = 2.06807279586792
training loss = 1.6378862857818604 4600
val loss = 2.0392603874206543
training loss = 1.6352972984313965 4700
val loss = 2.056483745574951
training loss = 1.7576258182525635 4800
val loss = 1.9164187908172607
training loss = 1.6320918798446655 4900
val loss = 2.0476603507995605
training loss = 1.6306536197662354 5000
val loss = 2.0425755977630615
training loss = 1.629361867904663 5100
val loss = 2.025790214538574
training loss = 1.627733826637268 5200
val loss = 2.033264636993408
training loss = 1.7420927286148071 5300
val loss = 1.8979661464691162
training loss = 1.6248972415924072 5400
val loss = 2.0233426094055176
training loss = 1.6235194206237793 5500
val loss = 2.021028518676758
training loss = 1.628568410873413 5600
val loss = 2.091069459915161
training loss = 1.6206568479537964 5700
val loss = 2.012450933456421
training loss = 1.6192320585250854 5800
val loss = 2.0082950592041016
training loss = 1.617849349975586 5900
val loss = 2.015241861343384
training loss = 1.6161588430404663 6000
val loss = 2.0001492500305176
training loss = 1.6146197319030762 6100
val loss = 1.9925978183746338
training loss = 1.6129440069198608 6200
val loss = 1.9944270849227905
training loss = 1.6112635135650635 6300
val loss = 1.9861855506896973
training loss = 1.609531044960022 6400
val loss = 1.9812077283859253
training loss = 1.6077595949172974 6500
val loss = 1.9820477962493896
training loss = 1.6058870553970337 6600
val loss = 1.9709473848342896
training loss = 1.6039730310440063 6700
val loss = 1.9646451473236084
training loss = 1.6021019220352173 6800
val loss = 1.968448281288147
training loss = 1.5999774932861328 6900
val loss = 1.9543323516845703
training loss = 1.5978959798812866 7000
val loss = 1.9462493658065796
training loss = 1.5958081483840942 7100
val loss = 1.9423589706420898
training loss = 1.8490784168243408 7200
val loss = 2.647813320159912
training loss = 1.5915138721466064 7300
val loss = 1.931813359260559
training loss = 1.5893173217773438 7400
val loss = 1.9234893321990967
training loss = 1.5874861478805542 7500
val loss = 1.931509017944336
training loss = 1.5849905014038086 7600
val loss = 1.9105027914047241
training loss = 1.6323857307434082 7700
val loss = 2.13262939453125
training loss = 1.58072030544281 7800
val loss = 1.8977805376052856
training loss = 1.5785999298095703 7900
val loss = 1.891369104385376
training loss = 1.5767526626586914 8000
val loss = 1.8750053644180298
training loss = 1.5745720863342285 8100
val loss = 1.8787930011749268
training loss = 1.573667287826538 8200
val loss = 1.8975436687469482
training loss = 1.5707465410232544 8300
val loss = 1.8687636852264404
training loss = 1.568895697593689 8400
val loss = 1.8614941835403442
training loss = 1.567430853843689 8500
val loss = 1.8680446147918701
training loss = 1.565503478050232 8600
val loss = 1.8503535985946655
training loss = 1.6289513111114502 8700
val loss = 2.1108932495117188
training loss = 1.5624257326126099 8800
val loss = 1.8411146402359009
training loss = 1.5609803199768066 8900
val loss = 1.8353910446166992
training loss = 1.5623112916946411 9000
val loss = 1.8719744682312012
training loss = 1.5583921670913696 9100
val loss = 1.8270291090011597
training loss = 1.557174563407898 9200
val loss = 1.822230577468872
training loss = 1.5587323904037476 9300
val loss = 1.7852792739868164
training loss = 1.5550123453140259 9400
val loss = 1.8146309852600098
training loss = 1.6062041521072388 9500
val loss = 2.036686897277832
training loss = 1.553153395652771 9600
val loss = 1.8085415363311768
training loss = 1.5522878170013428 9700
val loss = 1.8041473627090454
training loss = 1.5519990921020508 9800
val loss = 1.7871875762939453
training loss = 1.550842523574829 9900
val loss = 1.7981572151184082
training loss = 1.568713903427124 10000
val loss = 1.7235217094421387
training loss = 1.5496203899383545 10100
val loss = 1.791351556777954
training loss = 1.5490483045578003 10200
val loss = 1.7900738716125488
training loss = 1.5487946271896362 10300
val loss = 1.7789041996002197
training loss = 1.5481376647949219 10400
val loss = 1.7860908508300781
training loss = 1.5537501573562622 10500
val loss = 1.7360689640045166
training loss = 1.5474056005477905 10600
val loss = 1.78168523311615
training loss = 1.5470601320266724 10700
val loss = 1.7804043292999268
training loss = 1.5474519729614258 10800
val loss = 1.7971912622451782
training loss = 1.5465420484542847 10900
val loss = 1.777256965637207
training loss = 1.564400315284729 11000
val loss = 1.891080379486084
training loss = 1.5461652278900146 11100
val loss = 1.7746939659118652
training loss = 1.5459764003753662 11200
val loss = 1.7730399370193481
training loss = 1.5471711158752441 11300
val loss = 1.7987375259399414
training loss = 1.5457677841186523 11400
val loss = 1.7712204456329346
training loss = 1.5456453561782837 11500
val loss = 1.7698538303375244
training loss = 1.5537917613983154 11600
val loss = 1.7180482149124146
training loss = 1.5455472469329834 11700
val loss = 1.767892837524414
training loss = 1.545479655265808 11800
val loss = 1.767055869102478
training loss = 1.5457159280776978 11900
val loss = 1.7580770254135132
training loss = 1.5454800128936768 12000
val loss = 1.7661941051483154
training loss = 1.5454537868499756 12100
val loss = 1.764944314956665
training loss = 1.5457582473754883 12200
val loss = 1.7566168308258057
training loss = 1.5455517768859863 12300
val loss = 1.7647168636322021
training loss = 1.5455467700958252 12400
val loss = 1.7638906240463257
training loss = 1.548859715461731 12500
val loss = 1.8081854581832886
training loss = 1.5456691980361938 12600
val loss = 1.7634456157684326
training loss = 1.5456979274749756 12700
val loss = 1.7647278308868408
training loss = 1.5458793640136719 12800
val loss = 1.7689505815505981
training loss = 1.5458414554595947 12900
val loss = 1.7622404098510742
training loss = 1.5463312864303589 13000
val loss = 1.7764090299606323
training loss = 1.5460306406021118 13100
val loss = 1.7625842094421387
training loss = 1.546065330505371 13200
val loss = 1.7617237567901611
training loss = 1.5528314113616943 13300
val loss = 1.7148834466934204
training loss = 1.5462536811828613 13400
val loss = 1.7613263130187988
training loss = 1.5463016033172607 13500
val loss = 1.7604057788848877
training loss = 1.5464869737625122 13600
val loss = 1.757150650024414
training loss = 1.5464807748794556 13700
val loss = 1.7612495422363281
training loss = 1.5544501543045044 13800
val loss = 1.833724021911621
training loss = 1.5466668605804443 13900
val loss = 1.7610067129135132
training loss = 1.5467363595962524 14000
val loss = 1.7643650770187378
training loss = 1.5468697547912598 14100
val loss = 1.7578063011169434
training loss = 1.5468825101852417 14200
val loss = 1.7607107162475586
training loss = 1.5500338077545166 14300
val loss = 1.8031420707702637
training loss = 1.5470466613769531 14400
val loss = 1.7611424922943115
training loss = 1.5481034517288208 14500
val loss = 1.739643931388855
training loss = 1.547194004058838 14600
val loss = 1.7611923217773438
training loss = 1.5472822189331055 14700
val loss = 1.7657454013824463
training loss = 1.5473389625549316 14800
val loss = 1.7603259086608887
training loss = 1.5473613739013672 14900
val loss = 1.7605879306793213
training loss = 1.5496058464050293 15000
val loss = 1.7956498861312866
training loss = 1.547481656074524 15100
val loss = 1.7602473497390747
training loss = 1.547508716583252 15200
val loss = 1.7577629089355469
training loss = 1.5476648807525635 15300
val loss = 1.7675578594207764
training loss = 1.5475757122039795 15400
val loss = 1.7603638172149658
training loss = 1.555037021636963 15500
val loss = 1.8300364017486572
training loss = 1.5476312637329102 15600
val loss = 1.7599096298217773
training loss = 1.5487698316574097 15700
val loss = 1.7850228548049927
training loss = 1.5476783514022827 15800
val loss = 1.7643500566482544
training loss = 1.5476062297821045 15900
val loss = 1.7598143815994263
training loss = 1.5490691661834717 16000
val loss = 1.7364716529846191
training loss = 1.5475643873214722 16100
val loss = 1.7600080966949463
training loss = 1.5633559226989746 16200
val loss = 1.6930772066116333
training loss = 1.5474599599838257 16300
val loss = 1.7587794065475464
training loss = 1.5473473072052002 16400
val loss = 1.7589893341064453
training loss = 1.547336459159851 16500
val loss = 1.7555630207061768
training loss = 1.5471588373184204 16600
val loss = 1.7587897777557373
training loss = 1.6604533195495605 16700
val loss = 1.671248435974121
training loss = 1.5469152927398682 16800
val loss = 1.7571332454681396
training loss = 1.5467298030853271 16900
val loss = 1.7581515312194824
training loss = 1.546875238418579 17000
val loss = 1.7473996877670288
training loss = 1.5464305877685547 17100
val loss = 1.7569113969802856
training loss = 1.5497338771820068 17200
val loss = 1.7212518453598022
training loss = 1.546109914779663 17300
val loss = 1.7556613683700562
training loss = 1.5587795972824097 17400
val loss = 1.84984290599823
training loss = 1.5458067655563354 17500
val loss = 1.7561172246932983
training loss = 1.5456069707870483 17600
val loss = 1.7541038990020752
training loss = 1.5455129146575928 17700
val loss = 1.7571392059326172
training loss = 1.5452911853790283 17800
val loss = 1.7530467510223389
training loss = 1.6359131336212158 17900
val loss = 2.0751028060913086
training loss = 1.5449655055999756 18000
val loss = 1.7529404163360596
training loss = 1.5447371006011963 18100
val loss = 1.7512121200561523
training loss = 1.5548852682113647 18200
val loss = 1.694568157196045
training loss = 1.5443476438522339 18300
val loss = 1.7491859197616577
training loss = 1.5440970659255981 18400
val loss = 1.7512975931167603
training loss = 1.5439544916152954 18500
val loss = 1.7539176940917969
training loss = 1.543632984161377 18600
val loss = 1.747833251953125
training loss = 1.54378080368042 18700
val loss = 1.7609992027282715
training loss = 1.5431437492370605 18800
val loss = 1.7466217279434204
training loss = 1.5522584915161133 18900
val loss = 1.6929616928100586
training loss = 1.542639136314392 19000
val loss = 1.7459975481033325
training loss = 1.542678713798523 19100
val loss = 1.7326147556304932
training loss = 1.5421333312988281 19200
val loss = 1.7493765354156494
training loss = 1.541762113571167 19300
val loss = 1.7436842918395996
training loss = 1.5419940948486328 19400
val loss = 1.7298943996429443
training loss = 1.541192889213562 19500
val loss = 1.7427845001220703
training loss = 1.5412931442260742 19600
val loss = 1.731060266494751
training loss = 1.540602445602417 19700
val loss = 1.7412972450256348
training loss = 1.634077548980713 19800
val loss = 1.6539041996002197
training loss = 1.539989709854126 19900
val loss = 1.738985300064087
training loss = 1.5402146577835083 20000
val loss = 1.7577390670776367
training loss = 1.5393961668014526 20100
val loss = 1.7444182634353638
training loss = 1.538997769355774 20200
val loss = 1.7394964694976807
training loss = 1.5388081073760986 20300
val loss = 1.7342243194580078
training loss = 1.5383723974227905 20400
val loss = 1.7387182712554932
training loss = 1.5549465417861938 20500
val loss = 1.848235011100769
training loss = 1.5377377271652222 20600
val loss = 1.7377445697784424
training loss = 1.5375443696975708 20700
val loss = 1.7471195459365845
training loss = 1.5371254682540894 20800
val loss = 1.735777735710144
training loss = 1.536745548248291 20900
val loss = 1.737207293510437
training loss = 1.5424020290374756 21000
val loss = 1.6928658485412598
training loss = 1.5361164808273315 21100
val loss = 1.7370991706848145
training loss = 1.535951018333435 21200
val loss = 1.746583104133606
training loss = 1.5355545282363892 21300
val loss = 1.7318663597106934
training loss = 1.535132646560669 21400
val loss = 1.7362077236175537
training loss = 1.5465611219406128 21500
val loss = 1.8245207071304321
training loss = 1.5344939231872559 21600
val loss = 1.7363916635513306
training loss = 1.644181251525879 21700
val loss = 1.6543035507202148
training loss = 1.5338709354400635 21800
val loss = 1.7347617149353027
training loss = 1.5334995985031128 21900
val loss = 1.7350329160690308
training loss = 1.5333256721496582 22000
val loss = 1.741727590560913
training loss = 1.5329022407531738 22100
val loss = 1.7356222867965698
training loss = 1.5696464776992798 22200
val loss = 1.9106509685516357
training loss = 1.53228759765625 22300
val loss = 1.7346198558807373
training loss = 1.5327743291854858 22400
val loss = 1.7562284469604492
training loss = 1.5316706895828247 22500
val loss = 1.7372021675109863
training loss = 1.5313102006912231 22600
val loss = 1.7363166809082031
training loss = 1.5313279628753662 22700
val loss = 1.7477211952209473
training loss = 1.5306936502456665 22800
val loss = 1.7356867790222168
training loss = 1.5309234857559204 22900
val loss = 1.7517441511154175
training loss = 1.5300910472869873 23000
val loss = 1.7362065315246582
training loss = 1.5301514863967896 23100
val loss = 1.7251977920532227
training loss = 1.5294930934906006 23200
val loss = 1.7360002994537354
training loss = 1.7714189291000366 23300
val loss = 2.3733203411102295
training loss = 1.5289568901062012 23400
val loss = 1.7344307899475098
training loss = 1.5285998582839966 23500
val loss = 1.7368621826171875
training loss = 1.7772037982940674 23600
val loss = 2.3861489295959473
training loss = 1.5280393362045288 23700
val loss = 1.736318826675415
training loss = 1.5276947021484375 23800
val loss = 1.737212896347046
training loss = 1.5335090160369873 23900
val loss = 1.6941821575164795
training loss = 1.5271196365356445 24000
val loss = 1.737328290939331
training loss = 1.8511544466018677 24100
val loss = 2.528911590576172
training loss = 1.5265703201293945 24200
val loss = 1.7359991073608398
training loss = 1.526219129562378 24300
val loss = 1.7381539344787598
training loss = 1.5275444984436035 24400
val loss = 1.7671335935592651
training loss = 1.5256561040878296 24500
val loss = 1.7387702465057373
training loss = 1.6282141208648682 24600
val loss = 1.6582393646240234
training loss = 1.5250964164733887 24700
val loss = 1.7381540536880493
training loss = 1.5247600078582764 24800
val loss = 1.738386631011963
training loss = 1.5247684717178345 24900
val loss = 1.7300480604171753
training loss = 1.524208903312683 25000
val loss = 1.7394402027130127
training loss = 1.5320913791656494 25100
val loss = 1.6906087398529053
training loss = 1.5236443281173706 25200
val loss = 1.7404487133026123
training loss = 1.5494352579116821 25300
val loss = 1.6673026084899902
training loss = 1.5230774879455566 25400
val loss = 1.7410467863082886
training loss = 1.5534989833831787 25500
val loss = 1.6647663116455078
training loss = 1.5225211381912231 25600
val loss = 1.7399580478668213
training loss = 1.6318082809448242 25700
val loss = 1.6635048389434814
training loss = 1.5219745635986328 25800
val loss = 1.7418153285980225
training loss = 1.5223997831344604 25900
val loss = 1.7601087093353271
training loss = 1.521423101425171 26000
val loss = 1.742187261581421
training loss = 1.5211397409439087 26100
val loss = 1.74579918384552
training loss = 1.5209764242172241 26200
val loss = 1.736025094985962
training loss = 1.5205714702606201 26300
val loss = 1.7416579723358154
training loss = 1.5235456228256226 26400
val loss = 1.7098445892333984
training loss = 1.5200656652450562 26500
val loss = 1.7424713373184204
training loss = 1.5198830366134644 26600
val loss = 1.7348474264144897
reduced chi^2 level 2 = 1.5196267366409302
Constrained alpha: 1.8823771476745605
Constrained beta: 3.436444044113159
Constrained gamma: 12.313454627990723
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 881.6348,  901.6072,  925.9102,  945.0506,  958.2273, 1089.4716,
        1111.4602, 1100.8529, 1175.2596, 1161.1841, 1217.9788, 1191.9991,
        1221.2721, 1263.1135, 1391.6409, 1450.4982, 1386.2188, 1402.2826,
        1541.3250, 1477.2699, 1629.8140, 1620.8568, 1494.8558, 1545.7242,
        1648.6066, 1738.4791, 1573.2382, 1738.2865, 1654.8906, 1743.8876,
        1657.5345, 1688.4767, 1731.8433, 1806.8871, 1695.1371, 1708.4194,
        1721.8549, 1553.8369, 1723.8510, 1604.6206, 1645.7523, 1556.2620,
        1475.5891, 1442.0767, 1389.3225, 1347.1683, 1237.8252, 1248.1835,
        1148.3698, 1193.3231, 1092.1997, 1017.9404,  957.4260,  959.7230,
         901.6965,  879.2735,  778.2551,  676.3342,  623.9941,  553.4204,
         553.1305,  492.8430,  474.0932,  422.2776,  361.9678,  345.2112,
         241.7311,  219.2955,  199.0377,  186.0905,  159.4468,  159.0244,
         142.4641,   90.5870,   81.7196,   93.6195,   50.1300,   42.3705,
          44.2623,   37.1067,   21.0653,   39.0689,   43.3159])]
2675.692241540024
0.5112265971415381 14.669852236466689 27.911704594638852
val isze = 8
idinces = [80 62 36  1 74 60 53  7 67 39 11 58 23 44 69 30 43 16 54 57 35 41 34 40
 21 12 19 66  9 46 28 50 78  2 26 82 45 48 75  0 56  8 76  5 22 31  3 18
 70 51 37 72 27 32 33 79 38 25 17 52 29 47 63 20 42 15 61 73 64 10  4 49
 55 13 68 77 65 14 71 59 24 81  6]
we are doing training validation split
training loss = 112.99577331542969 100
val loss = 163.31825256347656
training loss = 8.066512107849121 200
val loss = 6.851396560668945
training loss = 7.654840469360352 300
val loss = 5.512187957763672
training loss = 7.486581325531006 400
val loss = 5.38336181640625
training loss = 7.311359882354736 500
val loss = 5.253933906555176
training loss = 7.137644290924072 600
val loss = 5.133256912231445
training loss = 6.972550392150879 700
val loss = 5.027390003204346
training loss = 6.8216328620910645 800
val loss = 4.940364360809326
training loss = 6.688652038574219 900
val loss = 4.873895645141602
training loss = 6.57540225982666 1000
val loss = 4.8274617195129395
training loss = 6.481751918792725 1100
val loss = 4.7985334396362305
training loss = 6.405962944030762 1200
val loss = 4.783084869384766
training loss = 6.345171928405762 1300
val loss = 4.776412010192871
training loss = 6.2960076332092285 1400
val loss = 4.7737956047058105
training loss = 6.255141735076904 1500
val loss = 4.771302223205566
training loss = 6.219690799713135 1600
val loss = 4.765993118286133
training loss = 6.187436103820801 1700
val loss = 4.75615930557251
training loss = 6.156836032867432 1800
val loss = 4.741100311279297
training loss = 6.126941680908203 1900
val loss = 4.720992088317871
training loss = 6.097255229949951 2000
val loss = 4.69638729095459
training loss = 6.067583084106445 2100
val loss = 4.668183326721191
training loss = 6.037907600402832 2200
val loss = 4.637324333190918
training loss = 6.008302688598633 2300
val loss = 4.604640960693359
training loss = 5.9788713455200195 2400
val loss = 4.570878505706787
training loss = 5.9496846199035645 2500
val loss = 4.536595821380615
training loss = 5.920751094818115 2600
val loss = 4.502190589904785
training loss = 5.891995429992676 2700
val loss = 4.467810153961182
training loss = 5.863190650939941 2800
val loss = 4.433403491973877
training loss = 5.833889484405518 2900
val loss = 4.398618698120117
training loss = 5.80328893661499 3000
val loss = 4.362645149230957
training loss = 5.769935131072998 3100
val loss = 4.323965549468994
training loss = 5.7310051918029785 3200
val loss = 4.279553413391113
training loss = 5.68034553527832 3300
val loss = 4.222993850708008
training loss = 5.602635860443115 3400
val loss = 4.1385273933410645
training loss = 5.45617151260376 3500
val loss = 3.984426975250244
training loss = 5.141937732696533 3600
val loss = 3.6644411087036133
training loss = 4.532916069030762 3700
val loss = 3.055497646331787
training loss = 3.697983503341675 3800
val loss = 2.205272674560547
training loss = 2.749361991882324 3900
val loss = 1.417029619216919
training loss = 2.0903778076171875 4000
val loss = 1.3182622194290161
training loss = 1.956953525543213 4100
val loss = 1.6064479351043701
training loss = 1.9419045448303223 4200
val loss = 1.6801341772079468
training loss = 1.9336963891983032 4300
val loss = 1.6800777912139893
training loss = 1.9270551204681396 4400
val loss = 1.6718404293060303
training loss = 1.92145836353302 4500
val loss = 1.6634844541549683
training loss = 1.916640281677246 4600
val loss = 1.6556594371795654
training loss = 1.9124056100845337 4700
val loss = 1.648176908493042
training loss = 1.9086012840270996 4800
val loss = 1.6410143375396729
training loss = 1.9051132202148438 4900
val loss = 1.634000301361084
training loss = 1.9038379192352295 5000
val loss = 1.5916460752487183
training loss = 1.8989437818527222 5100
val loss = 1.617823600769043
training loss = 1.8962793350219727 5200
val loss = 1.6160390377044678
training loss = 1.8943750858306885 5300
val loss = 1.6301915645599365
training loss = 1.8914799690246582 5400
val loss = 1.6065489053726196
training loss = 1.9037282466888428 5500
val loss = 1.5133006572723389
training loss = 1.887203574180603 5600
val loss = 1.5970364809036255
training loss = 1.885244369506836 5700
val loss = 1.593688726425171
training loss = 1.883599042892456 5800
val loss = 1.5755290985107422
training loss = 1.881535291671753 5900
val loss = 1.5851871967315674
training loss = 1.890089750289917 6000
val loss = 1.666055679321289
training loss = 1.8780847787857056 6100
val loss = 1.5770732164382935
training loss = 1.8764584064483643 6200
val loss = 1.5738650560379028
training loss = 1.8748230934143066 6300
val loss = 1.5693340301513672
training loss = 1.8733644485473633 6400
val loss = 1.5668895244598389
training loss = 1.8718843460083008 6500
val loss = 1.5632209777832031
training loss = 1.8709667921066284 6600
val loss = 1.5405840873718262
training loss = 1.8690115213394165 6700
val loss = 1.5568416118621826
training loss = 1.8676396608352661 6800
val loss = 1.5562248229980469
training loss = 1.8662370443344116 6900
val loss = 1.5485637187957764
training loss = 1.8649630546569824 7000
val loss = 1.546828269958496
training loss = 1.9884827136993408 7100
val loss = 1.9036030769348145
training loss = 1.8623837232589722 7200
val loss = 1.542816400527954
training loss = 1.8611478805541992 7300
val loss = 1.5370171070098877
training loss = 1.8611352443695068 7400
val loss = 1.5068333148956299
training loss = 1.8587228059768677 7500
val loss = 1.5312879085540771
training loss = 1.9308130741119385 7600
val loss = 1.7855403423309326
training loss = 1.8563954830169678 7700
val loss = 1.5262808799743652
training loss = 1.8552740812301636 7800
val loss = 1.522033452987671
training loss = 1.854134202003479 7900
val loss = 1.5178959369659424
training loss = 1.8531181812286377 8000
val loss = 1.5163872241973877
training loss = 1.8520703315734863 8100
val loss = 1.5124928951263428
training loss = 1.8510150909423828 8200
val loss = 1.5126111507415771
training loss = 1.850027084350586 8300
val loss = 1.5081133842468262
training loss = 1.8600517511367798 8400
val loss = 1.5957658290863037
training loss = 1.8480690717697144 8500
val loss = 1.5025709867477417
training loss = 1.8472663164138794 8600
val loss = 1.4901652336120605
training loss = 1.8462636470794678 8700
val loss = 1.5041756629943848
training loss = 1.8453296422958374 8800
val loss = 1.4948925971984863
training loss = 2.02496600151062 8900
val loss = 1.9558502435684204
reduced chi^2 level 2 = 1.8442460298538208
Constrained alpha: 1.9289995431900024
Constrained beta: 3.100605010986328
Constrained gamma: 18.08357048034668
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 864.1108,  893.0231,  960.2408,  927.0410, 1013.1882, 1066.8959,
        1112.4716, 1111.9438, 1129.9281, 1196.4761, 1219.6516, 1160.7555,
        1270.5948, 1245.6234, 1340.7058, 1374.6190, 1398.4204, 1418.2538,
        1543.1467, 1536.5636, 1533.1339, 1540.5006, 1594.8562, 1600.7128,
        1663.2843, 1770.0276, 1687.2234, 1744.1920, 1716.4027, 1620.6869,
        1649.9728, 1727.6310, 1669.3508, 1743.3081, 1666.1071, 1724.5629,
        1630.3474, 1613.3330, 1659.0814, 1652.4489, 1637.9165, 1564.8994,
        1497.2333, 1510.1787, 1300.2904, 1263.4385, 1287.4630, 1252.9753,
        1177.2191, 1206.7665, 1094.5131,  955.5183, 1004.9515,  874.3255,
         867.5769,  848.1802,  832.4774,  739.8435,  593.7474,  564.8839,
         531.1040,  453.4835,  442.6818,  398.1658,  391.8006,  323.3792,
         287.5958,  259.8030,  201.0942,  154.0334,  163.7515,  155.9385,
         127.4192,  106.6836,   90.4837,   63.1921,   46.7314,   51.9600,
          42.9904,   43.1311,   15.8077,   44.0929,   34.3803])]
2493.0013791852516
0.8932386501976469 5.4855271601361455 36.235706429783946
val isze = 8
idinces = [53 40  2 74 65 39 13 79 48 70 10 61 21 68 45 52 58 75 17 63 82 66  4 16
 78 49 37 80 36 20 38 71 35 14 26 69 77 43 19 64  9 57 25 47 44 27 67 51
 15 22  8 18 59 11  0 41 29 46 28 81 60 62 30  1 56 31 55  7 33 54 50  6
 12 73 72 23 42  3  5 76 24 34 32]
we are doing training validation split
training loss = 11.567502975463867 100
val loss = 14.130294799804688
training loss = 9.74843978881836 200
val loss = 13.262151718139648
training loss = 8.831841468811035 300
val loss = 12.343191146850586
training loss = 8.010876655578613 400
val loss = 11.549729347229004
training loss = 7.323227882385254 500
val loss = 10.919456481933594
training loss = 6.769794464111328 600
val loss = 10.443948745727539
training loss = 6.335902214050293 700
val loss = 10.099556922912598
training loss = 6.002018451690674 800
val loss = 9.859615325927734
training loss = 5.748776435852051 900
val loss = 9.69947624206543
training loss = 5.558981418609619 1000
val loss = 9.598487854003906
training loss = 5.418209075927734 1100
val loss = 9.539992332458496
training loss = 5.314753532409668 1200
val loss = 9.511029243469238
training loss = 5.2393083572387695 1300
val loss = 9.501736640930176
training loss = 5.184604644775391 1400
val loss = 9.504738807678223
training loss = 5.145029544830322 1500
val loss = 9.514602661132812
training loss = 5.116311550140381 1600
val loss = 9.52749252319336
training loss = 5.095235347747803 1700
val loss = 9.540719985961914
training loss = 5.079409122467041 1800
val loss = 9.552490234375
training loss = 5.067074775695801 1900
val loss = 9.561704635620117
training loss = 5.056959629058838 2000
val loss = 9.567707061767578
training loss = 5.048147201538086 2100
val loss = 9.570208549499512
training loss = 5.03996467590332 2200
val loss = 9.569103240966797
training loss = 5.031917572021484 2300
val loss = 9.564359664916992
training loss = 5.0236077308654785 2400
val loss = 9.556028366088867
training loss = 5.014681816101074 2500
val loss = 9.544028282165527
training loss = 5.004763126373291 2600
val loss = 9.528107643127441
training loss = 4.993399143218994 2700
val loss = 9.507766723632812
training loss = 4.979976654052734 2800
val loss = 9.482026100158691
training loss = 4.963595867156982 2900
val loss = 9.449262619018555
training loss = 4.942934513092041 3000
val loss = 9.40684700012207
training loss = 4.916019439697266 3100
val loss = 9.350706100463867
training loss = 4.880032539367676 3200
val loss = 9.27492618560791
training loss = 4.831359386444092 3300
val loss = 9.172140121459961
training loss = 4.766181945800781 3400
val loss = 9.035245895385742
training loss = 4.681267261505127 3500
val loss = 8.859975814819336
training loss = 4.573192119598389 3600
val loss = 8.643108367919922
training loss = 4.435065746307373 3700
val loss = 8.37322998046875
training loss = 4.253383636474609 3800
val loss = 8.020975112915039
training loss = 4.006933689117432 3900
val loss = 7.535483360290527
training loss = 3.668361186981201 4000
val loss = 6.845560073852539
training loss = 3.2160115242004395 4100
val loss = 5.876152038574219
training loss = 2.6770577430725098 4200
val loss = 4.625633716583252
training loss = 2.1935882568359375 4300
val loss = 3.3289289474487305
training loss = 1.9338715076446533 4400
val loss = 2.4026741981506348
training loss = 1.8603583574295044 4500
val loss = 1.9662235975265503
training loss = 1.846643090248108 4600
val loss = 1.8133444786071777
training loss = 1.842811942100525 4700
val loss = 1.7668334245681763
training loss = 1.8400322198867798 4800
val loss = 1.7525122165679932
training loss = 1.8368933200836182 4900
val loss = 1.746372938156128
training loss = 1.8328931331634521 5000
val loss = 1.7444517612457275
training loss = 1.8272790908813477 5100
val loss = 1.7405855655670166
training loss = 1.819995403289795 5200
val loss = 1.7369029521942139
training loss = 1.8094102144241333 5300
val loss = 1.741091012954712
training loss = 1.8223024606704712 5400
val loss = 1.7986027002334595
training loss = 1.7842090129852295 5500
val loss = 1.740025281906128
training loss = 1.7713733911514282 5600
val loss = 1.7331548929214478
training loss = 1.7594316005706787 5700
val loss = 1.7248671054840088
training loss = 1.7482123374938965 5800
val loss = 1.7147938013076782
training loss = 1.7384371757507324 5900
val loss = 1.7172856330871582
training loss = 1.7262874841690063 6000
val loss = 1.7003772258758545
training loss = 1.7151894569396973 6100
val loss = 1.6929008960723877
training loss = 1.70718514919281 6200
val loss = 1.6745002269744873
training loss = 1.6928861141204834 6300
val loss = 1.6826777458190918
training loss = 1.9231868982315063 6400
val loss = 2.033334493637085
training loss = 1.6702641248703003 6500
val loss = 1.6723213195800781
training loss = 1.6597089767456055 6600
val loss = 1.6803007125854492
training loss = 1.6481995582580566 6700
val loss = 1.6644032001495361
training loss = 1.6373636722564697 6800
val loss = 1.6619646549224854
training loss = 1.6278069019317627 6900
val loss = 1.6672179698944092
training loss = 1.6177277565002441 7000
val loss = 1.655860424041748
training loss = 1.6088624000549316 7100
val loss = 1.6495089530944824
training loss = 1.600573182106018 7200
val loss = 1.6510748863220215
training loss = 2.0044026374816895 7300
val loss = 2.2644107341766357
training loss = 1.5863428115844727 7400
val loss = 1.6483925580978394
training loss = 1.5800567865371704 7500
val loss = 1.6454696655273438
training loss = 1.5751681327819824 7600
val loss = 1.6381497383117676
training loss = 1.5701031684875488 7700
val loss = 1.6434199810028076
training loss = 1.5802648067474365 7800
val loss = 1.5922538042068481
training loss = 1.5625650882720947 7900
val loss = 1.6421122550964355
training loss = 1.5593585968017578 8000
val loss = 1.6421759128570557
training loss = 1.5569608211517334 8100
val loss = 1.6392359733581543
training loss = 1.5545939207077026 8200
val loss = 1.6405868530273438
training loss = 1.556821346282959 8300
val loss = 1.602325201034546
training loss = 1.5510612726211548 8400
val loss = 1.6403859853744507
training loss = 1.5495471954345703 8500
val loss = 1.6407698392868042
training loss = 1.5486009120941162 8600
val loss = 1.6348397731781006
training loss = 1.5473284721374512 8700
val loss = 1.640694499015808
training loss = 1.5537611246109009 8800
val loss = 1.6865818500518799
training loss = 1.5456569194793701 8900
val loss = 1.6397511959075928
training loss = 1.544879674911499 9000
val loss = 1.6410890817642212
training loss = 1.5586607456207275 9100
val loss = 1.7183113098144531
training loss = 1.543746829032898 9200
val loss = 1.6415209770202637
training loss = 1.5431972742080688 9300
val loss = 1.641819953918457
training loss = 1.5449224710464478 9400
val loss = 1.6204509735107422
training loss = 1.5423716306686401 9500
val loss = 1.6427366733551025
training loss = 1.5422528982162476 9600
val loss = 1.633062720298767
training loss = 1.5417218208312988 9700
val loss = 1.6397918462753296
training loss = 1.5413167476654053 9800
val loss = 1.6433169841766357
training loss = 1.5506649017333984 9900
val loss = 1.7057421207427979
training loss = 1.5407599210739136 10000
val loss = 1.6444382667541504
training loss = 1.7171350717544556 10100
val loss = 2.0275678634643555
training loss = 1.5402960777282715 10200
val loss = 1.6470247507095337
training loss = 1.5400021076202393 10300
val loss = 1.6447538137435913
training loss = 1.5405033826828003 10400
val loss = 1.6302483081817627
training loss = 1.5396068096160889 10500
val loss = 1.6458040475845337
training loss = 1.539368987083435 10600
val loss = 1.645563006401062
training loss = 1.5522196292877197 10700
val loss = 1.587419033050537
training loss = 1.539022445678711 10800
val loss = 1.6469037532806396
training loss = 1.5388127565383911 10900
val loss = 1.646045207977295
training loss = 1.5398504734039307 11000
val loss = 1.6677219867706299
training loss = 1.5384924411773682 11100
val loss = 1.6468188762664795
training loss = 1.6406323909759521 11200
val loss = 1.9046156406402588
training loss = 1.5382086038589478 11300
val loss = 1.6470332145690918
training loss = 1.5380291938781738 11400
val loss = 1.6474026441574097
training loss = 1.5386030673980713 11500
val loss = 1.6345895528793335
training loss = 1.5377663373947144 11600
val loss = 1.6477580070495605
training loss = 1.8987020254135132 11700
val loss = 1.6035683155059814
training loss = 1.5375282764434814 11800
val loss = 1.6472630500793457
training loss = 1.5373730659484863 11900
val loss = 1.6486653089523315
training loss = 1.5375369787216187 12000
val loss = 1.6583192348480225
training loss = 1.5371493101119995 12100
val loss = 1.6488502025604248
training loss = 1.5997573137283325 12200
val loss = 1.5463223457336426
training loss = 1.5369629859924316 12300
val loss = 1.6482704877853394
training loss = 1.5368226766586304 12400
val loss = 1.649373173713684
training loss = 1.548771858215332 12500
val loss = 1.7173609733581543
training loss = 1.5366417169570923 12600
val loss = 1.6492810249328613
training loss = 1.5367100238800049 12700
val loss = 1.6419262886047363
training loss = 1.5364822149276733 12800
val loss = 1.6503558158874512
training loss = 1.5363574028015137 12900
val loss = 1.6501888036727905
training loss = 1.5366501808166504 13000
val loss = 1.6527422666549683
training loss = 1.5362125635147095 13100
val loss = 1.6512281894683838
training loss = 1.5361045598983765 13200
val loss = 1.6504656076431274
training loss = 1.5419584512710571 13300
val loss = 1.6132049560546875
training loss = 1.5359776020050049 13400
val loss = 1.650765061378479
training loss = 1.5358809232711792 13500
val loss = 1.6509439945220947
training loss = 1.538055658340454 13600
val loss = 1.678755283355713
training loss = 1.535759449005127 13700
val loss = 1.6511046886444092
training loss = 1.5360586643218994 13800
val loss = 1.6400865316390991
training loss = 1.5356727838516235 13900
val loss = 1.6544526815414429
training loss = 1.5355538129806519 14000
val loss = 1.6515470743179321
training loss = 1.538208246231079 14100
val loss = 1.6845968961715698
training loss = 1.535448431968689 14200
val loss = 1.6519670486450195
training loss = 1.53537118434906 14300
val loss = 1.6510193347930908
training loss = 1.535362720489502 14400
val loss = 1.6507039070129395
training loss = 1.5352675914764404 14500
val loss = 1.6520792245864868
training loss = 1.5442529916763306 14600
val loss = 1.7042582035064697
training loss = 1.5351898670196533 14700
val loss = 1.6534210443496704
training loss = 1.53510582447052 14800
val loss = 1.6523247957229614
training loss = 1.5689481496810913 14900
val loss = 1.7813607454299927
training loss = 1.5350326299667358 15000
val loss = 1.654288411140442
training loss = 1.5349494218826294 15100
val loss = 1.6523929834365845
training loss = 1.535858154296875 15200
val loss = 1.6716481447219849
training loss = 1.5348596572875977 15300
val loss = 1.6528923511505127
reduced chi^2 level 2 = 1.8921973705291748
Constrained alpha: 1.839328646659851
Constrained beta: 3.3436172008514404
Constrained gamma: 16.221174240112305
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 797.6009,  879.4206,  879.4278,  960.1173,  987.5259, 1093.7289,
        1015.7212, 1179.3851, 1116.8347, 1214.9048, 1183.0302, 1119.3186,
        1296.4265, 1245.8750, 1325.6357, 1408.0806, 1433.6256, 1402.5266,
        1521.0007, 1444.9658, 1533.9766, 1530.1240, 1648.0095, 1665.2448,
        1631.1323, 1675.5665, 1605.5851, 1709.9941, 1672.8376, 1695.6545,
        1681.1448, 1720.2633, 1717.3612, 1683.2451, 1690.8647, 1746.8599,
        1614.6584, 1600.0376, 1665.1986, 1545.5027, 1611.5702, 1546.3779,
        1527.8851, 1464.7719, 1415.7136, 1279.3258, 1270.9688, 1172.8226,
        1181.8729, 1138.7498, 1107.6580, 1007.1602,  949.9902,  952.7368,
         865.0366,  876.7681,  854.2974,  711.9079,  657.1481,  586.3100,
         546.4347,  480.9238,  447.7888,  377.6057,  354.2764,  339.1865,
         270.7431,  258.9608,  211.3896,  156.6044,  152.9741,  159.6933,
         139.3509,  118.6188,   79.4945,   63.7428,   54.2577,   44.9045,
          35.5667,   43.5925,   30.9384,   44.3263,   43.2597])]
2532.0565960501044
1.73046786873801 3.086606823657594 15.63322814369792
val isze = 8
idinces = [ 6 26 35 17 37 57 42 61 58 79  3 59 71 31 81 22  7 24 49 51 76 80 19 65
  1 18 60 45 28 14 13 64 67 74 25 39 12  0 68 38 53 36 11 15 43 70  5 10
 78 63 50 32 34 54 82 41 40 69 75 47 55 16 56 46 29 72 66 30  9 44 23  4
  2 62 21 48 33  8 27 73 20 52 77]
we are doing training validation split
training loss = 37.37832260131836 100
val loss = 46.855770111083984
training loss = 27.567028045654297 200
val loss = 35.16095733642578
training loss = 20.567045211791992 300
val loss = 27.704099655151367
training loss = 15.871224403381348 400
val loss = 22.368267059326172
training loss = 12.73196029663086 500
val loss = 18.495397567749023
training loss = 10.607151985168457 600
val loss = 15.612030029296875
training loss = 9.148919105529785 700
val loss = 13.414996147155762
training loss = 8.136585235595703 800
val loss = 11.708864212036133
training loss = 7.427943706512451 900
val loss = 10.364021301269531
training loss = 6.92929220199585 1000
val loss = 9.291557312011719
training loss = 6.57751989364624 1100
val loss = 8.428701400756836
training loss = 6.329261302947998 1200
val loss = 7.729887008666992
training loss = 6.154271125793457 1300
val loss = 7.161360263824463
training loss = 6.0311689376831055 1400
val loss = 6.697552680969238
training loss = 5.944706439971924 1500
val loss = 6.318783283233643
training loss = 5.883935451507568 1600
val loss = 6.009614944458008
training loss = 5.840969562530518 1700
val loss = 5.757807731628418
training loss = 5.810133457183838 1800
val loss = 5.553457260131836
training loss = 5.787324905395508 1900
val loss = 5.38845682144165
training loss = 5.769593238830566 2000
val loss = 5.256089210510254
training loss = 5.754795074462891 2100
val loss = 5.150699615478516
training loss = 5.741326808929443 2200
val loss = 5.0675482749938965
training loss = 5.727939605712891 2300
val loss = 5.002535820007324
training loss = 5.713550567626953 2400
val loss = 4.952170372009277
training loss = 5.697088241577148 2500
val loss = 4.913510322570801
training loss = 5.677300930023193 2600
val loss = 4.884055137634277
training loss = 5.652561187744141 2700
val loss = 4.861839771270752
training loss = 5.6206183433532715 2800
val loss = 4.845394134521484
training loss = 5.578373908996582 2900
val loss = 4.8337202072143555
training loss = 5.521907329559326 3000
val loss = 4.825922012329102
training loss = 5.447122573852539 3100
val loss = 4.819828987121582
training loss = 5.350881576538086 3200
val loss = 4.808647155761719
training loss = 5.23071813583374 3300
val loss = 4.778126239776611
training loss = 5.081293106079102 3400
val loss = 4.710360050201416
training loss = 4.890582084655762 3500
val loss = 4.593588829040527
training loss = 4.640432834625244 3600
val loss = 4.423830986022949
training loss = 4.314897060394287 3700
val loss = 4.193143367767334
training loss = 3.915750503540039 3800
val loss = 3.878361225128174
training loss = 3.4637064933776855 3900
val loss = 3.4661269187927246
training loss = 2.9915130138397217 4000
val loss = 2.985929489135742
training loss = 2.5573534965515137 4100
val loss = 2.5008113384246826
training loss = 2.238323450088501 4200
val loss = 2.0966787338256836
training loss = 2.069103717803955 4300
val loss = 1.835338830947876
training loss = 2.0059070587158203 4400
val loss = 1.7033387422561646
training loss = 1.9867653846740723 4500
val loss = 1.6461621522903442
training loss = 1.9799046516418457 4600
val loss = 1.6224491596221924
training loss = 1.9765161275863647 4700
val loss = 1.6156809329986572
training loss = 1.9724106788635254 4800
val loss = 1.606722354888916
training loss = 2.1156063079833984 4900
val loss = 1.745085597038269
training loss = 1.965874433517456 5000
val loss = 1.6007392406463623
training loss = 1.9626127481460571 5100
val loss = 1.5977240800857544
training loss = 1.9593859910964966 5200
val loss = 1.5950497388839722
training loss = 1.9559895992279053 5300
val loss = 1.5938551425933838
training loss = 1.952946662902832 5400
val loss = 1.5898982286453247
training loss = 1.9492131471633911 5500
val loss = 1.5896811485290527
training loss = 1.9458016157150269 5600
val loss = 1.5881427526474
training loss = 1.9422874450683594 5700
val loss = 1.5859016180038452
training loss = 1.946927547454834 5800
val loss = 1.6044620275497437
training loss = 1.9352059364318848 5900
val loss = 1.5823099613189697
training loss = 1.9375667572021484 6000
val loss = 1.574826955795288
training loss = 1.9280074834823608 6100
val loss = 1.5788211822509766
training loss = 1.9242838621139526 6200
val loss = 1.5772333145141602
training loss = 1.920865535736084 6300
val loss = 1.57420015335083
training loss = 1.916965365409851 6400
val loss = 1.574181079864502
training loss = 1.9166911840438843 6500
val loss = 1.571307897567749
training loss = 1.9095791578292847 6600
val loss = 1.5711201429367065
training loss = 1.9065836668014526 6700
val loss = 1.5743855237960815
training loss = 1.902008295059204 6800
val loss = 1.568463921546936
training loss = 1.898079514503479 6900
val loss = 1.5674947500228882
training loss = 1.8964258432388306 7000
val loss = 1.5641117095947266
training loss = 1.8904075622558594 7100
val loss = 1.5650933980941772
training loss = 1.8864396810531616 7200
val loss = 1.5633026361465454
training loss = 1.882415771484375 7300
val loss = 1.5632476806640625
training loss = 1.8782566785812378 7400
val loss = 1.5622973442077637
training loss = 1.8744373321533203 7500
val loss = 1.5599277019500732
training loss = 1.8701329231262207 7600
val loss = 1.560476303100586
training loss = 2.1956465244293213 7700
val loss = 2.03336763381958
training loss = 1.8618232011795044 7800
val loss = 1.5585981607437134
training loss = 1.8575057983398438 7900
val loss = 1.5582059621810913
training loss = 1.8542639017105103 8000
val loss = 1.5613374710083008
training loss = 1.8490444421768188 8100
val loss = 1.5567830801010132
training loss = 1.8549625873565674 8200
val loss = 1.578256607055664
training loss = 1.8406912088394165 8300
val loss = 1.5554125308990479
training loss = 1.8363994359970093 8400
val loss = 1.5547183752059937
training loss = 1.8340764045715332 8500
val loss = 1.560351014137268
training loss = 1.8279988765716553 8600
val loss = 1.5532962083816528
training loss = 2.102057456970215 8700
val loss = 1.8511377573013306
training loss = 1.8196598291397095 8800
val loss = 1.5516674518585205
training loss = 1.8154115676879883 8900
val loss = 1.5514247417449951
training loss = 1.8115276098251343 9000
val loss = 1.5517853498458862
training loss = 1.8073548078536987 9100
val loss = 1.549928903579712
training loss = 1.8232096433639526 9200
val loss = 1.5889487266540527
training loss = 1.7993897199630737 9300
val loss = 1.5484157800674438
training loss = 1.8231152296066284 9400
val loss = 1.564539909362793
training loss = 1.791605830192566 9500
val loss = 1.5472657680511475
training loss = 1.7877147197723389 9600
val loss = 1.5462415218353271
training loss = 1.7847856283187866 9700
val loss = 1.5497715473175049
training loss = 1.7802282571792603 9800
val loss = 1.5447723865509033
training loss = 1.7800735235214233 9900
val loss = 1.555587649345398
training loss = 1.772957444190979 10000
val loss = 1.5431227684020996
training loss = 1.7692680358886719 10100
val loss = 1.5424854755401611
training loss = 1.7660492658615112 10200
val loss = 1.5434937477111816
training loss = 1.7625515460968018 10300
val loss = 1.540935754776001
training loss = 1.7590806484222412 10400
val loss = 1.5403106212615967
training loss = 1.760840892791748 10500
val loss = 1.5554440021514893
training loss = 1.7524292469024658 10600
val loss = 1.5388551950454712
training loss = 1.7490572929382324 10700
val loss = 1.537781000137329
training loss = 1.7459444999694824 10800
val loss = 1.5361528396606445
training loss = 1.7425702810287476 10900
val loss = 1.5367807149887085
training loss = 1.7470239400863647 11000
val loss = 1.5584689378738403
training loss = 1.7363165616989136 11100
val loss = 1.5353549718856812
training loss = 1.733130693435669 11200
val loss = 1.5348833799362183
training loss = 1.7314449548721313 11300
val loss = 1.5415058135986328
training loss = 1.726973533630371 11400
val loss = 1.5336239337921143
training loss = 1.7237738370895386 11500
val loss = 1.5336766242980957
training loss = 1.7208667993545532 11600
val loss = 1.5311241149902344
training loss = 1.7176693677902222 11700
val loss = 1.532110333442688
training loss = 1.7154502868652344 11800
val loss = 1.527694821357727
training loss = 1.7112301588058472 11900
val loss = 1.5323117971420288
training loss = 1.7077745199203491 12000
val loss = 1.5308890342712402
training loss = 1.7061487436294556 12100
val loss = 1.5241916179656982
training loss = 1.7003048658370972 12200
val loss = 1.5304337739944458
training loss = 1.6958688497543335 12300
val loss = 1.5299091339111328
training loss = 1.6910266876220703 12400
val loss = 1.530767798423767
training loss = 1.6852270364761353 12500
val loss = 1.53080153465271
training loss = 1.6832067966461182 12600
val loss = 1.5199391841888428
training loss = 1.6696221828460693 12700
val loss = 1.5331813097000122
training loss = 1.6596139669418335 12800
val loss = 1.5342386960983276
training loss = 1.6513701677322388 12900
val loss = 1.5491816997528076
training loss = 1.6391409635543823 13000
val loss = 1.5351643562316895
training loss = 1.6295853853225708 13100
val loss = 1.5340694189071655
training loss = 1.6211644411087036 13200
val loss = 1.5392343997955322
training loss = 1.6119705438613892 13300
val loss = 1.530393362045288
training loss = 1.7001911401748657 13400
val loss = 1.5639147758483887
training loss = 1.5947209596633911 13500
val loss = 1.5278962850570679
training loss = 1.5860168933868408 13600
val loss = 1.5272548198699951
training loss = 1.5771641731262207 13700
val loss = 1.5269739627838135
training loss = 1.5685324668884277 13800
val loss = 1.5272855758666992
training loss = 1.9094542264938354 13900
val loss = 2.1738240718841553
training loss = 1.5513743162155151 14000
val loss = 1.527931809425354
training loss = 1.543000340461731 14100
val loss = 1.5303421020507812
training loss = 1.5360335111618042 14200
val loss = 1.5206420421600342
training loss = 1.5272953510284424 14300
val loss = 1.5335536003112793
training loss = 1.5197792053222656 14400
val loss = 1.534687876701355
training loss = 1.5130001306533813 14500
val loss = 1.5410187244415283
training loss = 1.506446361541748 14600
val loss = 1.5397982597351074
training loss = 1.5244873762130737 14700
val loss = 1.5055469274520874
training loss = 1.4946703910827637 14800
val loss = 1.5446803569793701
training loss = 1.4893118143081665 14900
val loss = 1.5477843284606934
training loss = 1.4846644401550293 15000
val loss = 1.5436069965362549
training loss = 1.4800924062728882 15100
val loss = 1.550384283065796
training loss = 1.5252324342727661 15200
val loss = 1.5113410949707031
training loss = 1.472265601158142 15300
val loss = 1.553290605545044
training loss = 1.4687787294387817 15400
val loss = 1.5558507442474365
training loss = 1.4688314199447632 15500
val loss = 1.5892573595046997
training loss = 1.4625991582870483 15600
val loss = 1.5590577125549316
training loss = 1.4615422487258911 15700
val loss = 1.580425500869751
training loss = 1.457364797592163 15800
val loss = 1.5623265504837036
training loss = 1.4550009965896606 15900
val loss = 1.5616545677185059
training loss = 1.45294988155365 16000
val loss = 1.5685234069824219
training loss = 1.4508908987045288 16100
val loss = 1.5653454065322876
training loss = 1.4766536951065063 16200
val loss = 1.6827294826507568
training loss = 1.4473145008087158 16300
val loss = 1.5673052072525024
training loss = 1.4456778764724731 16400
val loss = 1.5683305263519287
training loss = 1.446255087852478 16500
val loss = 1.5956218242645264
training loss = 1.442724585533142 16600
val loss = 1.570129156112671
training loss = 1.4413361549377441 16700
val loss = 1.569584846496582
training loss = 1.4401766061782837 16800
val loss = 1.5657479763031006
training loss = 1.4388099908828735 16900
val loss = 1.5721757411956787
training loss = 1.4387210607528687 17000
val loss = 1.5918869972229004
training loss = 1.4365200996398926 17100
val loss = 1.5735702514648438
training loss = 1.435412883758545 17200
val loss = 1.5739655494689941
training loss = 1.4363216161727905 17300
val loss = 1.5999276638031006
training loss = 1.4334053993225098 17400
val loss = 1.5748480558395386
training loss = 1.4324138164520264 17500
val loss = 1.5753271579742432
training loss = 1.4321593046188354 17600
val loss = 1.5626758337020874
training loss = 1.4305509328842163 17700
val loss = 1.5759235620498657
training loss = 1.4296268224716187 17800
val loss = 1.5765055418014526
training loss = 1.4294321537017822 17900
val loss = 1.5923597812652588
training loss = 1.4278558492660522 18000
val loss = 1.5773353576660156
training loss = 1.6792521476745605 18100
val loss = 2.171678304672241
training loss = 1.4261300563812256 18200
val loss = 1.5780644416809082
training loss = 1.4252547025680542 18300
val loss = 1.5781924724578857
training loss = 1.4249619245529175 18400
val loss = 1.5665019750595093
training loss = 1.4236435890197754 18500
val loss = 1.5785759687423706
training loss = 1.4227973222732544 18600
val loss = 1.5789201259613037
training loss = 1.4226047992706299 18700
val loss = 1.5929601192474365
training loss = 1.4211727380752563 18800
val loss = 1.5795269012451172
training loss = 1.439115285873413 18900
val loss = 1.5288132429122925
training loss = 1.4195538759231567 19000
val loss = 1.5795972347259521
training loss = 1.442651391029358 19100
val loss = 1.527099609375
reduced chi^2 level 2 = 1.4182387590408325
Constrained alpha: 1.8262673616409302
Constrained beta: 1.803507924079895
Constrained gamma: 13.456262588500977
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 879.3061,  853.4081,  965.5773,  893.5997, 1027.3580, 1083.4624,
        1106.3722, 1097.3713, 1120.0308, 1133.6608, 1223.4834, 1186.8773,
        1250.4158, 1266.7065, 1314.9896, 1436.8589, 1427.4321, 1466.1790,
        1560.6862, 1453.0095, 1588.5323, 1542.8656, 1575.7377, 1658.3413,
        1621.3138, 1692.1952, 1640.5176, 1723.7214, 1700.7216, 1676.7114,
        1686.9406, 1763.5100, 1782.0479, 1728.4596, 1639.9176, 1755.3625,
        1616.8325, 1604.3174, 1645.7880, 1640.3401, 1691.8763, 1549.7170,
        1460.3270, 1519.4918, 1415.3151, 1327.1254, 1225.5071, 1278.1755,
        1165.4962, 1160.5488, 1053.1224,  992.5392,  988.4329,  882.2628,
         930.8447,  841.2986,  868.8459,  733.6699,  640.9512,  524.5078,
         581.6710,  450.2133,  437.1295,  392.9340,  360.4669,  363.8443,
         289.6439,  260.1591,  200.0087,  160.9797,  170.5154,  152.8414,
         147.6059,  115.9944,   96.2723,   54.0481,   48.8688,   35.9399,
          37.2789,   39.1196,   25.7597,   38.9936,   29.2892])]
2658.819276466562
1.9596300876025763 2.6576080725124207 80.18738109166738
val isze = 8
idinces = [76 13 70  2 15 31 41 33 32 45 50 24 52 68  5 60 26 57 14 49 42 55  6 27
 61 48  4 66 40 39 63 16  3 28 69 58 10 82 30  7 22 34 74 81 19 23 56 73
 67 59 64 17 46 12 20 38  8 29  1 35  0 47  9 37 36 65 43 53 25 77 18 75
 72 79 54 44 62 78 80 51 71 11 21]
we are doing training validation split
training loss = 46.483184814453125 100
val loss = 62.081851959228516
training loss = 33.17253875732422 200
val loss = 44.87970733642578
training loss = 24.173463821411133 300
val loss = 32.11846923828125
training loss = 18.218076705932617 400
val loss = 23.155302047729492
training loss = 14.235456466674805 500
val loss = 16.966093063354492
training loss = 11.528499603271484 600
val loss = 12.705216407775879
training loss = 9.65581226348877 700
val loss = 9.757467269897461
training loss = 8.345919609069824 800
val loss = 7.712239742279053
training loss = 7.426167964935303 900
val loss = 6.294794082641602
training loss = 6.781857490539551 1000
val loss = 5.317837238311768
training loss = 6.3336286544799805 1100
val loss = 4.650974750518799
training loss = 6.024563789367676 1200
val loss = 4.201388359069824
training loss = 5.812589645385742 1300
val loss = 3.9016518592834473
training loss = 5.665849208831787 1400
val loss = 3.7022757530212402
training loss = 5.559830188751221 1500
val loss = 3.5665194988250732
training loss = 5.475424766540527 1600
val loss = 3.466890335083008
training loss = 5.3974714279174805 1700
val loss = 3.382812261581421
training loss = 5.313654899597168 1800
val loss = 3.298551082611084
training loss = 5.213655471801758 1900
val loss = 3.2021167278289795
training loss = 5.088093280792236 2000
val loss = 3.0836071968078613
training loss = 4.926507472991943 2100
val loss = 2.933307647705078
training loss = 4.713991165161133 2200
val loss = 2.7382705211639404
training loss = 4.42794132232666 2300
val loss = 2.479642152786255
training loss = 4.03895378112793 2400
val loss = 2.1356215476989746
training loss = 3.5299313068389893 2500
val loss = 1.7030715942382812
training loss = 2.9597904682159424 2600
val loss = 1.2612206935882568
training loss = 2.5011088848114014 2700
val loss = 0.983223557472229
training loss = 2.2674031257629395 2800
val loss = 0.9291073083877563
training loss = 2.1899240016937256 2900
val loss = 0.9753545522689819
training loss = 2.1694626808166504 3000
val loss = 1.0221278667449951
training loss = 2.165504217147827 3100
val loss = 1.1031310558319092
training loss = 2.160459518432617 3200
val loss = 1.0665611028671265
training loss = 2.263079881668091 3300
val loss = 0.8454245328903198
training loss = 2.1571927070617676 3400
val loss = 1.0872348546981812
training loss = 2.1560962200164795 3500
val loss = 1.0893899202346802
training loss = 2.1552741527557373 3600
val loss = 1.1028602123260498
training loss = 2.1546120643615723 3700
val loss = 1.1080151796340942
training loss = 2.155139684677124 3800
val loss = 1.0805652141571045
training loss = 2.153815507888794 3900
val loss = 1.119210958480835
training loss = 2.1550705432891846 4000
val loss = 1.0829505920410156
training loss = 2.1534481048583984 4100
val loss = 1.1335673332214355
training loss = 2.153303384780884 4200
val loss = 1.133108377456665
training loss = 2.1534199714660645 4300
val loss = 1.1515917778015137
training loss = 2.1531283855438232 4400
val loss = 1.1393747329711914
training loss = 2.1586661338806152 4500
val loss = 1.0645688772201538
training loss = 2.1528425216674805 4600
val loss = 1.1389997005462646
training loss = 2.1525652408599854 4700
val loss = 1.145928144454956
training loss = 2.1887564659118652 4800
val loss = 0.9742981195449829
training loss = 2.1517274379730225 4900
val loss = 1.1490554809570312
training loss = 2.1510965824127197 5000
val loss = 1.1480789184570312
training loss = 2.1504557132720947 5100
val loss = 1.1610116958618164
training loss = 2.1493332386016846 5200
val loss = 1.1478419303894043
training loss = 2.1489784717559814 5300
val loss = 1.1148203611373901
training loss = 2.1467268466949463 5400
val loss = 1.1420092582702637
training loss = 2.1450235843658447 5500
val loss = 1.1434714794158936
training loss = 2.143038272857666 5600
val loss = 1.1389439105987549
training loss = 2.140718460083008 5700
val loss = 1.137901782989502
training loss = 2.1381356716156006 5800
val loss = 1.122000813484192
training loss = 2.1349713802337646 5900
val loss = 1.1320979595184326
training loss = 2.1313765048980713 6000
val loss = 1.1263527870178223
training loss = 2.1280312538146973 6100
val loss = 1.0923844575881958
training loss = 2.122704029083252 6200
val loss = 1.1158785820007324
training loss = 2.154564380645752 6300
val loss = 1.3641160726547241
training loss = 2.1115565299987793 6400
val loss = 1.1038891077041626
training loss = 2.104738712310791 6500
val loss = 1.097980260848999
training loss = 2.0978057384490967 6600
val loss = 1.1040849685668945
training loss = 2.0895891189575195 6700
val loss = 1.0830087661743164
training loss = 2.153710126876831 6800
val loss = 0.8622710704803467
training loss = 2.0715689659118652 6900
val loss = 1.069980502128601
training loss = 2.06144642829895 7000
val loss = 1.0647835731506348
training loss = 2.052081823348999 7100
val loss = 1.0471285581588745
training loss = 2.0418055057525635 7200
val loss = 1.0531256198883057
training loss = 2.151366949081421 7300
val loss = 0.8113245368003845
training loss = 2.022304058074951 7400
val loss = 1.0462536811828613
training loss = 2.012547492980957 7500
val loss = 1.0411632061004639
training loss = 2.004079818725586 7600
val loss = 1.0450934171676636
training loss = 1.9953941106796265 7700
val loss = 1.0338826179504395
training loss = 1.9875541925430298 7800
val loss = 1.01973557472229
training loss = 1.980164885520935 7900
val loss = 1.0266505479812622
training loss = 1.9729101657867432 8000
val loss = 1.021514654159546
training loss = 1.9678735733032227 8100
val loss = 1.0559167861938477
training loss = 1.960438847541809 8200
val loss = 1.0134813785552979
training loss = 1.9751701354980469 8300
val loss = 0.8861945271492004
training loss = 1.9498600959777832 8400
val loss = 1.0070831775665283
training loss = 1.9473130702972412 8500
val loss = 0.9565132856369019
training loss = 1.9411107301712036 8600
val loss = 1.0067323446273804
training loss = 1.9371126890182495 8700
val loss = 0.9984712600708008
training loss = 1.9348446130752563 8800
val loss = 0.9636253714561462
training loss = 1.9306461811065674 8900
val loss = 0.9949706196784973
training loss = 1.927687644958496 9000
val loss = 0.9905657768249512
training loss = 1.9254860877990723 9100
val loss = 0.9953995943069458
training loss = 1.9231164455413818 9200
val loss = 0.991462767124176
training loss = 1.9209216833114624 9300
val loss = 0.991336464881897
training loss = 1.9197548627853394 9400
val loss = 1.0157380104064941
training loss = 1.9172595739364624 9500
val loss = 0.9904426336288452
training loss = 1.9252413511276245 9600
val loss = 0.9037572145462036
training loss = 1.9141528606414795 9700
val loss = 0.9887914657592773
training loss = 1.9126546382904053 9800
val loss = 0.9904516339302063
training loss = 1.9118916988372803 9900
val loss = 0.9685889482498169
training loss = 1.910071849822998 10000
val loss = 0.9914387464523315
training loss = 1.9348005056381226 10100
val loss = 1.1820790767669678
training loss = 1.9077343940734863 10200
val loss = 0.9933986067771912
training loss = 1.9065525531768799 10300
val loss = 0.9933729767799377
training loss = 1.9056499004364014 10400
val loss = 1.0027415752410889
training loss = 1.9044249057769775 10500
val loss = 0.9949759244918823
training loss = 1.9135372638702393 10600
val loss = 1.1075124740600586
training loss = 1.9023385047912598 10700
val loss = 0.9972623586654663
training loss = 1.9012422561645508 10800
val loss = 0.9965944886207581
training loss = 1.9003862142562866 10900
val loss = 0.9864654541015625
training loss = 1.899201512336731 11000
val loss = 0.9981362819671631
training loss = 1.8984969854354858 11100
val loss = 0.9815672636032104
training loss = 1.8971525430679321 11200
val loss = 0.9959678649902344
training loss = 1.896053671836853 11300
val loss = 0.9990361928939819
training loss = 1.8973238468170166 11400
val loss = 0.9536289572715759
training loss = 1.8940438032150269 11500
val loss = 0.9983092546463013
training loss = 1.8929814100265503 11600
val loss = 0.9994360208511353
training loss = 1.8921369314193726 11700
val loss = 0.9893430471420288
training loss = 1.8910729885101318 11800
val loss = 0.9968008995056152
training loss = 1.905793309211731 11900
val loss = 1.1360557079315186
training loss = 1.889327883720398 12000
val loss = 0.995281994342804
training loss = 1.8884484767913818 12100
val loss = 0.9955610036849976
training loss = 1.8878564834594727 12200
val loss = 0.9820512533187866
training loss = 1.8869529962539673 12300
val loss = 0.992261528968811
training loss = 1.8871147632598877 12400
val loss = 1.0188119411468506
training loss = 1.8856199979782104 12500
val loss = 0.9910022020339966
training loss = 1.9133256673812866 12600
val loss = 1.1887058019638062
training loss = 1.8844155073165894 12700
val loss = 0.9858701825141907
training loss = 1.8837780952453613 12800
val loss = 0.9882381558418274
training loss = 1.8854749202728271 12900
val loss = 0.9444167613983154
training loss = 1.8827687501907349 13000
val loss = 0.9877662062644958
training loss = 1.8822020292282104 13100
val loss = 0.9867482781410217
training loss = 1.9221415519714355 13200
val loss = 0.8307520151138306
training loss = 1.8812183141708374 13300
val loss = 0.9855903387069702
training loss = 1.8806819915771484 13400
val loss = 0.9852089881896973
training loss = 1.8803231716156006 13500
val loss = 0.9780377745628357
training loss = 1.8797718286514282 13600
val loss = 0.9840444922447205
training loss = 1.8793184757232666 13700
val loss = 0.9757643342018127
training loss = 1.8789079189300537 13800
val loss = 0.9820106029510498
training loss = 1.8784147500991821 13900
val loss = 0.9827292561531067
training loss = 1.879386067390442 14000
val loss = 1.0214089155197144
training loss = 1.8775899410247803 14100
val loss = 0.9869562387466431
training loss = 1.8770791292190552 14200
val loss = 0.9811669588088989
training loss = 1.878398060798645 14300
val loss = 0.9412455558776855
training loss = 1.8762736320495605 14400
val loss = 0.9807944297790527
training loss = 1.8758065700531006 14500
val loss = 0.979508638381958
training loss = 1.8759390115737915 14600
val loss = 0.9576729536056519
training loss = 1.8750321865081787 14700
val loss = 0.9782745242118835
training loss = 1.8745787143707275 14800
val loss = 0.9771451354026794
training loss = 1.8743904829025269 14900
val loss = 0.9885839223861694
training loss = 1.8738137483596802 15000
val loss = 0.9765039086341858
training loss = 2.3108201026916504 15100
val loss = 2.127990961074829
training loss = 1.873077392578125 15200
val loss = 0.9777487516403198
training loss = 1.8726330995559692 15300
val loss = 0.9742856025695801
training loss = 1.9097758531570435 15400
val loss = 0.8246968984603882
training loss = 1.871924877166748 15500
val loss = 0.9748044013977051
training loss = 1.8714925050735474 15600
val loss = 0.9720240831375122
training loss = 1.8788694143295288 15700
val loss = 0.8920803070068359
training loss = 1.8707906007766724 15800
val loss = 0.9714537858963013
training loss = 1.8703670501708984 15900
val loss = 0.9694737195968628
training loss = 1.870840311050415 16000
val loss = 0.9427736401557922
training loss = 1.8696593046188354 16100
val loss = 0.9684788584709167
training loss = 1.873193383216858 16200
val loss = 0.9088658094406128
training loss = 1.8689733743667603 16300
val loss = 0.9699928760528564
training loss = 1.8685433864593506 16400
val loss = 0.9650734066963196
training loss = 1.8695203065872192 16500
val loss = 0.9302178621292114
training loss = 1.867845892906189 16600
val loss = 0.9632216691970825
training loss = 1.9142781496047974 16700
val loss = 1.2314014434814453
training loss = 1.8671643733978271 16800
val loss = 0.9636934399604797
training loss = 1.8667547702789307 16900
val loss = 0.9618008136749268
training loss = 1.8666679859161377 17000
val loss = 0.9730443954467773
training loss = 1.866076946258545 17100
val loss = 0.958038330078125
training loss = 1.8765995502471924 17200
val loss = 0.8672378659248352
training loss = 1.8654069900512695 17300
val loss = 0.9563637971878052
training loss = 1.8718067407608032 17400
val loss = 1.043121576309204
training loss = 1.8647572994232178 17500
val loss = 0.9547730684280396
training loss = 1.8643587827682495 17600
val loss = 0.9526647925376892
training loss = 1.8704032897949219 17700
val loss = 1.0372867584228516
training loss = 1.8637185096740723 17800
val loss = 0.9515460729598999
training loss = 1.8633301258087158 17900
val loss = 0.9522472620010376
training loss = 1.8631936311721802 18000
val loss = 0.9381272196769714
training loss = 1.8626829385757446 18100
val loss = 0.9472299814224243
training loss = 1.8626538515090942 18200
val loss = 0.9632220268249512
training loss = 1.8620368242263794 18300
val loss = 0.9452170133590698
training loss = 2.2126457691192627 18400
val loss = 1.9212946891784668
training loss = 1.8613910675048828 18500
val loss = 0.9419931173324585
training loss = 1.8610093593597412 18600
val loss = 0.9440346956253052
training loss = 1.8609143495559692 18700
val loss = 0.9528082609176636
training loss = 1.860361933708191 18800
val loss = 0.9389395713806152
training loss = 1.8620625734329224 18900
val loss = 0.8971370458602905
training loss = 1.8597339391708374 19000
val loss = 0.9362701773643494
training loss = 1.9899697303771973 19100
val loss = 1.4283888339996338
training loss = 1.8591350317001343 19200
val loss = 0.9320361614227295
training loss = 1.8587443828582764 19300
val loss = 0.9327950477600098
training loss = 1.8585129976272583 19400
val loss = 0.9286396503448486
training loss = 1.8581197261810303 19500
val loss = 0.930610716342926
training loss = 1.8584684133529663 19600
val loss = 0.9054814577102661
training loss = 1.8574939966201782 19700
val loss = 0.9274412989616394
training loss = 2.1927638053894043 19800
val loss = 0.7479887008666992
training loss = 1.8568929433822632 19900
val loss = 0.9226260185241699
training loss = 1.8565011024475098 20000
val loss = 0.9234464168548584
training loss = 1.856338381767273 20100
val loss = 0.9294885396957397
training loss = 1.8559120893478394 20200
val loss = 0.92123943567276
training loss = 2.0128073692321777 20300
val loss = 1.4723076820373535
training loss = 1.8553345203399658 20400
val loss = 0.9171057939529419
training loss = 1.8549505472183228 20500
val loss = 0.917866587638855
training loss = 1.8563157320022583 20600
val loss = 0.881059467792511
training loss = 1.8543660640716553 20700
val loss = 0.9155341386795044
training loss = 1.865470051765442 20800
val loss = 0.8308597803115845
training loss = 1.8537873029708862 20900
val loss = 0.9120094776153564
training loss = 1.8534168004989624 21000
val loss = 0.9143644571304321
training loss = 1.85336434841156 21100
val loss = 0.9228881001472473
training loss = 1.8528268337249756 21200
val loss = 0.9092798233032227
training loss = 1.8527588844299316 21300
val loss = 0.9201584458351135
training loss = 1.8522648811340332 21400
val loss = 0.9069717526435852
training loss = 2.1898787021636963 21500
val loss = 1.8393528461456299
training loss = 1.851729393005371 21600
val loss = 0.9017475843429565
training loss = 1.851352572441101 21700
val loss = 0.9039108753204346
training loss = 1.8526310920715332 21800
val loss = 0.869089663028717
training loss = 1.8508124351501465 21900
val loss = 0.9016566276550293
training loss = 1.8510972261428833 22000
val loss = 0.9278910756111145
training loss = 1.8502812385559082 22100
val loss = 0.8999282121658325
training loss = 1.8499263525009155 22200
val loss = 0.8988566398620605
training loss = 1.8501304388046265 22300
val loss = 0.9158366918563843
training loss = 1.8494267463684082 22400
val loss = 0.8962740302085876
training loss = 1.9191854000091553 22500
val loss = 1.221032738685608
training loss = 1.8489323854446411 22600
val loss = 0.8965165019035339
training loss = 1.848586082458496 22700
val loss = 0.8923358917236328
training loss = 1.8485567569732666 22800
val loss = 0.8830400705337524
training loss = 1.8481075763702393 22900
val loss = 0.8916889429092407
training loss = 1.8817845582962036 23000
val loss = 0.7589661478996277
training loss = 1.8476665019989014 23100
val loss = 0.891329824924469
training loss = 1.847341537475586 23200
val loss = 0.8882587552070618
training loss = 1.8474618196487427 23300
val loss = 0.8739168643951416
training loss = 1.8468854427337646 23400
val loss = 0.8877299427986145
training loss = 1.85071861743927 23500
val loss = 0.8344748020172119
training loss = 1.846449851989746 23600
val loss = 0.8867223262786865
training loss = 2.0646395683288574 23700
val loss = 1.5554938316345215
training loss = 1.8460824489593506 23800
val loss = 0.8872084021568298
training loss = 1.8457837104797363 23900
val loss = 0.8841943740844727
training loss = 1.8548367023468018 24000
val loss = 0.9811179637908936
training loss = 1.8454152345657349 24100
val loss = 0.8825294971466064
training loss = 1.8512529134750366 24200
val loss = 0.8181558847427368
training loss = 1.8450987339019775 24300
val loss = 0.8867571353912354
training loss = 1.8448063135147095 24400
val loss = 0.8812836408615112
training loss = 1.8455533981323242 24500
val loss = 0.907630205154419
training loss = 1.8444769382476807 24600
val loss = 0.8801987171173096
training loss = 1.8890314102172852 24700
val loss = 1.1196718215942383
training loss = 1.844191551208496 24800
val loss = 0.8813102841377258
training loss = 1.8439809083938599 24900
val loss = 0.8835734128952026
training loss = 1.8440403938293457 25000
val loss = 0.8697121143341064
training loss = 1.8437061309814453 25100
val loss = 0.8787871599197388
training loss = 1.8464136123657227 25200
val loss = 0.8347519040107727
training loss = 1.8434871435165405 25300
val loss = 0.8788774013519287
training loss = 2.159315824508667 25400
val loss = 1.7258001565933228
training loss = 1.8432964086532593 25500
val loss = 0.87491375207901
training loss = 1.843106746673584 25600
val loss = 0.8778933882713318
training loss = 1.8441917896270752 25700
val loss = 0.9071273803710938
training loss = 1.843014121055603 25800
val loss = 0.8776761889457703
training loss = 1.842862606048584 25900
val loss = 0.8773588538169861
training loss = 1.8431217670440674 26000
val loss = 0.8652892112731934
training loss = 1.8427791595458984 26100
val loss = 0.8773998022079468
training loss = 1.8916211128234863 26200
val loss = 0.7404447793960571
training loss = 1.8427307605743408 26300
val loss = 0.8802683353424072
training loss = 1.8426098823547363 26400
val loss = 0.8777406215667725
training loss = 1.8435475826263428 26500
val loss = 0.9041579365730286
training loss = 1.8425867557525635 26600
val loss = 0.8776989579200745
training loss = 1.8451474905014038 26700
val loss = 0.8369787931442261
training loss = 1.8426021337509155 26800
val loss = 0.8780389428138733
training loss = 1.8425318002700806 26900
val loss = 0.8779147863388062
training loss = 1.8431205749511719 27000
val loss = 0.8968735933303833
training loss = 1.8425896167755127 27100
val loss = 0.8777803778648376
training loss = 2.1144514083862305 27200
val loss = 1.6224348545074463
training loss = 1.842697024345398 27300
val loss = 0.8807458281517029
training loss = 1.8426389694213867 27400
val loss = 0.8779515624046326
training loss = 1.8619452714920044 27500
val loss = 0.7834659814834595
training loss = 1.8427616357803345 27600
val loss = 0.8778773546218872
training loss = 1.8427321910858154 27700
val loss = 0.8781399130821228
training loss = 1.8461352586746216 27800
val loss = 0.9313889145851135
training loss = 1.8428434133529663 27900
val loss = 0.8787609934806824
training loss = 1.928308367729187 28000
val loss = 1.2185842990875244
training loss = 1.8430064916610718 28100
val loss = 0.8753649592399597
training loss = 1.8429745435714722 28200
val loss = 0.8785049915313721
training loss = 1.8557385206222534 28300
val loss = 0.9882029294967651
training loss = 1.8431107997894287 28400
val loss = 0.8785108923912048
training loss = 1.8720855712890625 28500
val loss = 0.767607569694519
training loss = 1.843266248703003 28600
val loss = 0.874704897403717
training loss = 1.8432438373565674 28700
val loss = 0.880885124206543
training loss = 1.8435351848602295 28800
val loss = 0.8899341225624084
training loss = 1.843375325202942 28900
val loss = 0.879120409488678
training loss = 1.8518270254135132 29000
val loss = 0.8127744197845459
training loss = 1.8435231447219849 29100
val loss = 0.8800508975982666
training loss = 1.8437461853027344 29200
val loss = 0.8671227693557739
training loss = 1.843743085861206 29300
val loss = 0.8856483697891235
training loss = 1.8436834812164307 29400
val loss = 0.8797157406806946
training loss = 1.9219026565551758 29500
val loss = 1.20132315158844
training loss = 1.843835711479187 29600
val loss = 0.8782529830932617
reduced chi^2 level 2 = 1.8438310623168945
Constrained alpha: 1.869991421699524
Constrained beta: 1.7389553785324097
Constrained gamma: 12.963017463684082
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 845.9664,  861.1920,  897.4770,  965.4819, 1009.6219, 1061.5031,
        1088.5782, 1106.5697, 1127.4011, 1135.2793, 1205.3995, 1187.4487,
        1224.2555, 1303.1805, 1360.4043, 1365.4224, 1407.4800, 1505.0695,
        1529.4703, 1471.3992, 1533.2842, 1559.0237, 1584.4376, 1597.3431,
        1655.6322, 1663.4353, 1616.0923, 1663.1525, 1758.5515, 1642.9548,
        1670.2070, 1709.6934, 1678.6327, 1675.4492, 1715.3909, 1798.3381,
        1704.2471, 1609.2731, 1618.7579, 1599.7382, 1576.6022, 1656.5732,
        1483.9968, 1525.3411, 1323.9832, 1335.2809, 1259.5886, 1279.1263,
        1113.7717, 1176.6681, 1043.4508,  969.0283,  961.1248,  897.4916,
         889.2780,  901.5924,  816.8758,  647.8299,  612.6364,  533.5482,
         568.9826,  493.6003,  466.1971,  396.7859,  339.3211,  328.3878,
         274.1289,  243.5812,  178.6913,  167.3358,  166.8529,  150.1804,
         146.6584,  112.7765,   85.7372,   68.4712,   56.7538,   52.7315,
          29.5266,   43.4279,   18.5419,   42.0799,   40.2047])]
2605.2069530309036
0.9914224218110534 13.063153369980656 21.09050565764
val isze = 8
idinces = [53 64 65 54 16 61 48 38 23 81 80  1 40 71  9 35 37 13  0 75 39 24 62 43
 17 18 10 56 57 41 77 47 70 30 36 63 60  8 76 19  3 72  7 51 27 22 45 25
 68 44 32 79 74 59 29 28 46 67 31 82 34 49 14 55  6 26  5 11 12 33 58 50
  4 20 78  2 21 52 73 69 66 42 15]
we are doing training validation split
training loss = 76.48200988769531 100
val loss = 112.80279541015625
training loss = 7.120654582977295 200
val loss = 4.29691219329834
training loss = 7.069390773773193 300
val loss = 4.597006320953369
training loss = 7.020742416381836 400
val loss = 4.582456588745117
training loss = 6.970768451690674 500
val loss = 4.566319465637207
training loss = 6.921573162078857 600
val loss = 4.553285598754883
training loss = 6.874612808227539 700
val loss = 4.543906211853027
training loss = 6.830755233764648 800
val loss = 4.538052082061768
training loss = 6.790330410003662 900
val loss = 4.535118579864502
training loss = 6.753228664398193 1000
val loss = 4.534332752227783
training loss = 6.719021320343018 1100
val loss = 4.5346832275390625
training loss = 6.687084674835205 1200
val loss = 4.5352067947387695
training loss = 6.65673828125 1300
val loss = 4.53508996963501
training loss = 6.627335548400879 1400
val loss = 4.533745765686035
training loss = 6.59834098815918 1500
val loss = 4.530726432800293
training loss = 6.569345474243164 1600
val loss = 4.525971412658691
training loss = 6.540099143981934 1700
val loss = 4.519379615783691
training loss = 6.510477066040039 1800
val loss = 4.511235237121582
training loss = 6.480456829071045 1900
val loss = 4.5017170906066895
training loss = 6.450098037719727 2000
val loss = 4.49117374420166
training loss = 6.41950798034668 2100
val loss = 4.479794025421143
training loss = 6.388834476470947 2200
val loss = 4.467965126037598
training loss = 6.358222961425781 2300
val loss = 4.455948829650879
training loss = 6.327826499938965 2400
val loss = 4.443924903869629
training loss = 6.297731876373291 2500
val loss = 4.432067394256592
training loss = 6.267954349517822 2600
val loss = 4.420353889465332
training loss = 6.238349914550781 2700
val loss = 4.408827304840088
training loss = 6.208472728729248 2800
val loss = 4.397202968597412
training loss = 6.177298545837402 2900
val loss = 4.384961128234863
training loss = 6.142552375793457 3000
val loss = 4.370816707611084
training loss = 6.099133491516113 3100
val loss = 4.352349758148193
training loss = 6.03566312789917 3200
val loss = 4.324038505554199
training loss = 5.931521415710449 3300
val loss = 4.274856090545654
training loss = 5.767502784729004 3400
val loss = 4.186488151550293
training loss = 5.528954982757568 3500
val loss = 4.034392356872559
training loss = 5.150886535644531 3600
val loss = 3.7727766036987305
training loss = 4.460830211639404 3700
val loss = 3.286158800125122
training loss = 3.1665470600128174 3800
val loss = 2.3790030479431152
training loss = 2.116365432739258 3900
val loss = 1.8713184595108032
training loss = 1.9470235109329224 4000
val loss = 2.083484411239624
training loss = 1.9257736206054688 4100
val loss = 2.127656936645508
training loss = 1.9130009412765503 4200
val loss = 2.1065256595611572
training loss = 1.9027453660964966 4300
val loss = 2.081298589706421
training loss = 1.8942387104034424 4400
val loss = 2.058929443359375
training loss = 1.8870439529418945 4500
val loss = 2.039417266845703
training loss = 1.8808451890945435 4600
val loss = 2.022428035736084
training loss = 1.8754061460494995 4700
val loss = 2.007521390914917
training loss = 1.8705466985702515 4800
val loss = 1.9943487644195557
training loss = 1.866129994392395 4900
val loss = 1.982621192932129
training loss = 1.8620483875274658 5000
val loss = 1.9733376502990723
training loss = 1.858335018157959 5100
val loss = 1.9587547779083252
training loss = 1.8547959327697754 5200
val loss = 1.954561471939087
training loss = 1.8515443801879883 5300
val loss = 1.9432884454727173
training loss = 1.8484355211257935 5400
val loss = 1.9391767978668213
training loss = 1.935692548751831 5500
val loss = 2.2139689922332764
training loss = 1.8426703214645386 5600
val loss = 1.9243190288543701
training loss = 1.8399254083633423 5700
val loss = 1.9186060428619385
training loss = 1.8447250127792358 5800
val loss = 1.8620693683624268
training loss = 1.834639310836792 5900
val loss = 1.9053996801376343
training loss = 1.8320153951644897 6000
val loss = 1.898909330368042
training loss = 1.8300834894180298 6100
val loss = 1.8764088153839111
training loss = 1.8268558979034424 6200
val loss = 1.885382890701294
training loss = 1.8882149457931519 6300
val loss = 1.76910400390625
training loss = 1.8214917182922363 6400
val loss = 1.8716856241226196
training loss = 1.8194794654846191 6500
val loss = 1.8833770751953125
training loss = 1.81569504737854 6600
val loss = 1.8504014015197754
training loss = 1.8124043941497803 6700
val loss = 1.845154881477356
training loss = 1.8113123178482056 6800
val loss = 1.8695454597473145
training loss = 1.8049654960632324 6900
val loss = 1.8232351541519165
training loss = 1.8005332946777344 7000
val loss = 1.8145807981491089
training loss = 1.7960002422332764 7100
val loss = 1.8052186965942383
training loss = 1.7909945249557495 7200
val loss = 1.7866711616516113
training loss = 2.0061349868774414 7300
val loss = 1.6598777770996094
training loss = 1.781200647354126 7400
val loss = 1.7652697563171387
training loss = 1.77657151222229 7500
val loss = 1.753831148147583
training loss = 1.7869720458984375 7600
val loss = 1.8398995399475098
training loss = 1.7678965330123901 7700
val loss = 1.7317970991134644
training loss = 1.7637369632720947 7800
val loss = 1.7233333587646484
training loss = 1.7719767093658447 7900
val loss = 1.7974934577941895
training loss = 1.7556604146957397 8000
val loss = 1.7018725872039795
training loss = 1.7516138553619385 8100
val loss = 1.6900633573532104
training loss = 1.7477307319641113 8200
val loss = 1.6764521598815918
training loss = 1.7438043355941772 8300
val loss = 1.6678571701049805
training loss = 1.7518353462219238 8400
val loss = 1.7403420209884644
training loss = 1.7360554933547974 8500
val loss = 1.6437575817108154
training loss = 1.732134461402893 8600
val loss = 1.63271164894104
training loss = 1.7283742427825928 8700
val loss = 1.6227443218231201
training loss = 1.724657416343689 8800
val loss = 1.6097322702407837
training loss = 1.8699965476989746 8900
val loss = 1.9733796119689941
training loss = 1.7172977924346924 9000
val loss = 1.5836031436920166
training loss = 1.7136365175247192 9100
val loss = 1.5749938488006592
training loss = 1.7102456092834473 9200
val loss = 1.5592477321624756
training loss = 1.7068144083023071 9300
val loss = 1.551146149635315
training loss = 1.7067674398422241 9400
val loss = 1.5789375305175781
training loss = 1.7004307508468628 9500
val loss = 1.528817057609558
training loss = 1.7008728981018066 9600
val loss = 1.4871070384979248
training loss = 1.6945648193359375 9700
val loss = 1.50881028175354
training loss = 1.691755771636963 9800
val loss = 1.4975348711013794
training loss = 1.6903282403945923 9900
val loss = 1.5104289054870605
training loss = 1.6867519617080688 10000
val loss = 1.478436827659607
training loss = 1.6903607845306396 10100
val loss = 1.4253029823303223
training loss = 1.6822131872177124 10200
val loss = 1.4640753269195557
training loss = 1.6800557374954224 10300
val loss = 1.452056884765625
training loss = 1.680888295173645 10400
val loss = 1.4799225330352783
training loss = 1.676284909248352 10500
val loss = 1.436159610748291
training loss = 1.6744754314422607 10600
val loss = 1.4263383150100708
training loss = 1.6729367971420288 10700
val loss = 1.4211667776107788
training loss = 1.6714122295379639 10800
val loss = 1.4155522584915161
training loss = 1.6699248552322388 10900
val loss = 1.4054434299468994
training loss = 1.6687061786651611 11000
val loss = 1.3968174457550049
training loss = 1.6672945022583008 11100
val loss = 1.3971613645553589
training loss = 1.704779863357544 11200
val loss = 1.3051279783248901
training loss = 1.6649361848831177 11300
val loss = 1.3855445384979248
training loss = 1.6637961864471436 11400
val loss = 1.3819390535354614
training loss = 1.6628128290176392 11500
val loss = 1.3796159029006958
training loss = 1.6617828607559204 11600
val loss = 1.3718204498291016
training loss = 1.661072850227356 11700
val loss = 1.3585243225097656
training loss = 1.6599432229995728 11800
val loss = 1.3631322383880615
training loss = 1.7296662330627441 11900
val loss = 1.5865848064422607
training loss = 1.6582667827606201 12000
val loss = 1.3586065769195557
training loss = 1.6574221849441528 12100
val loss = 1.3520687818527222
training loss = 1.6574167013168335 12200
val loss = 1.3668551445007324
training loss = 1.655900001525879 12300
val loss = 1.3453829288482666
training loss = 1.7168382406234741 12400
val loss = 1.553935170173645
training loss = 1.654467225074768 12500
val loss = 1.3380986452102661
training loss = 1.653742790222168 12600
val loss = 1.3378429412841797
training loss = 1.6531096696853638 12700
val loss = 1.3341829776763916
training loss = 1.6524351835250854 12800
val loss = 1.330451250076294
training loss = 1.7336171865463257 12900
val loss = 1.582505464553833
training loss = 1.6511971950531006 13000
val loss = 1.3266563415527344
training loss = 1.650557041168213 13100
val loss = 1.32255220413208
training loss = 1.6537123918533325 13200
val loss = 1.3612350225448608
training loss = 1.6493752002716064 13300
val loss = 1.3179128170013428
training loss = 1.6574785709381104 13400
val loss = 1.3800702095031738
training loss = 1.648284673690796 13500
val loss = 1.3158985376358032
training loss = 1.6477067470550537 13600
val loss = 1.3109310865402222
training loss = 1.6472671031951904 13700
val loss = 1.31600022315979
training loss = 1.6467430591583252 13800
val loss = 1.3123257160186768
training loss = 1.6461430788040161 13900
val loss = 1.3045523166656494
training loss = 1.7641679048538208 14000
val loss = 1.200681447982788
training loss = 1.6451542377471924 14100
val loss = 1.300357460975647
training loss = 1.6446503400802612 14200
val loss = 1.300058364868164
training loss = 1.6443192958831787 14300
val loss = 1.302992820739746
training loss = 1.6437450647354126 14400
val loss = 1.2945468425750732
training loss = 1.6494790315628052 14500
val loss = 1.3462116718292236
training loss = 1.6429083347320557 14600
val loss = 1.2909460067749023
training loss = 1.642564296722412 14700
val loss = 1.2953224182128906
training loss = 1.6421622037887573 14800
val loss = 1.291888952255249
training loss = 1.6417043209075928 14900
val loss = 1.285657525062561
training loss = 1.6448239088058472 15000
val loss = 1.2522687911987305
training loss = 1.640999436378479 15100
val loss = 1.2819660902023315
training loss = 1.640617847442627 15200
val loss = 1.2804275751113892
training loss = 1.6409344673156738 15300
val loss = 1.2943768501281738
training loss = 1.6400221586227417 15400
val loss = 1.2777103185653687
training loss = 1.6396816968917847 15500
val loss = 1.2762173414230347
training loss = 1.63969886302948 15600
val loss = 1.2848353385925293
training loss = 1.6391156911849976 15700
val loss = 1.2733955383300781
training loss = 1.6388771533966064 15800
val loss = 1.272132396697998
training loss = 1.6385976076126099 15900
val loss = 1.2712386846542358
training loss = 1.6383111476898193 16000
val loss = 1.2689001560211182
training loss = 1.6381335258483887 16100
val loss = 1.2692525386810303
training loss = 1.6378782987594604 16200
val loss = 1.2668687105178833
training loss = 1.637622594833374 16300
val loss = 1.2660633325576782
training loss = 1.6374552249908447 16400
val loss = 1.2629438638687134
training loss = 1.637211799621582 16500
val loss = 1.2631006240844727
training loss = 1.6491515636444092 16600
val loss = 1.2086132764816284
training loss = 1.63683021068573 16700
val loss = 1.260791540145874
training loss = 1.6940996646881104 16800
val loss = 1.1709985733032227
training loss = 1.6364805698394775 16900
val loss = 1.2587306499481201
training loss = 1.636276125907898 17000
val loss = 1.2579439878463745
training loss = 1.6385767459869385 17100
val loss = 1.288771629333496
training loss = 1.6359646320343018 17200
val loss = 1.2564046382904053
training loss = 1.7085901498794556 17300
val loss = 1.4822998046875
training loss = 1.6356873512268066 17400
val loss = 1.2518863677978516
training loss = 1.6354879140853882 17500
val loss = 1.2531912326812744
training loss = 1.635547161102295 17600
val loss = 1.2452237606048584
training loss = 1.635216236114502 17700
val loss = 1.2518198490142822
training loss = 1.6372743844985962 17800
val loss = 1.2839568853378296
training loss = 1.63496732711792 17900
val loss = 1.2496477365493774
training loss = 1.6348042488098145 18000
val loss = 1.249226689338684
training loss = 1.6347581148147583 18100
val loss = 1.2535309791564941
training loss = 1.6345566511154175 18200
val loss = 1.2482476234436035
training loss = 1.6378172636032104 18300
val loss = 1.217917561531067
training loss = 1.634330153465271 18400
val loss = 1.2474561929702759
training loss = 1.6346068382263184 18500
val loss = 1.234735131263733
training loss = 1.634116291999817 18600
val loss = 1.2442964315414429
training loss = 1.6339621543884277 18700
val loss = 1.2453677654266357
training loss = 1.6354047060012817 18800
val loss = 1.2237021923065186
training loss = 1.6337499618530273 18900
val loss = 1.244140863418579
training loss = 1.7822009325027466 19000
val loss = 1.608828067779541
training loss = 1.6335508823394775 19100
val loss = 1.242760181427002
training loss = 1.633410930633545 19200
val loss = 1.242748498916626
training loss = 1.6371315717697144 19300
val loss = 1.2106858491897583
training loss = 1.6332045793533325 19400
val loss = 1.242032766342163
training loss = 1.977674961090088 19500
val loss = 1.9112341403961182
training loss = 1.6330065727233887 19600
val loss = 1.2412669658660889
training loss = 1.6328703165054321 19700
val loss = 1.2403714656829834
training loss = 1.6329199075698853 19800
val loss = 1.2340514659881592
training loss = 1.6326857805252075 19900
val loss = 1.2399286031723022
training loss = 1.6325488090515137 20000
val loss = 1.238910436630249
training loss = 1.6329712867736816 20100
val loss = 1.227160930633545
training loss = 1.6323646306991577 20200
val loss = 1.2386183738708496
training loss = 1.7190877199172974 20300
val loss = 1.1473464965820312
training loss = 1.6321611404418945 20400
val loss = 1.2389439344406128
training loss = 1.632022738456726 20500
val loss = 1.237076997756958
training loss = 1.6323256492614746 20600
val loss = 1.2491240501403809
training loss = 1.6318247318267822 20700
val loss = 1.2368786334991455
training loss = 1.6516005992889404 20800
val loss = 1.339876413345337
training loss = 1.6316208839416504 20900
val loss = 1.2371115684509277
training loss = 1.6314842700958252 21000
val loss = 1.2375223636627197
training loss = 1.6314365863800049 21100
val loss = 1.2390947341918945
training loss = 1.6312676668167114 21200
val loss = 1.2353448867797852
training loss = 1.6377716064453125 21300
val loss = 1.1949647665023804
training loss = 1.631052017211914 21400
val loss = 1.2350132465362549
training loss = 1.6994948387145996 21500
val loss = 1.1466920375823975
training loss = 1.6308292150497437 21600
val loss = 1.2353448867797852
training loss = 1.6306757926940918 21700
val loss = 1.233752965927124
training loss = 1.6306898593902588 21800
val loss = 1.2395637035369873
training loss = 1.6304576396942139 21900
val loss = 1.233574628829956
training loss = 1.9655734300613403 22000
val loss = 1.2148149013519287
training loss = 1.6302337646484375 22100
val loss = 1.234776496887207
training loss = 1.630069613456726 22200
val loss = 1.2328011989593506
training loss = 1.6314197778701782 22300
val loss = 1.2117680311203003
training loss = 1.629832148551941 22400
val loss = 1.2323601245880127
training loss = 1.9269169569015503 22500
val loss = 1.8309953212738037
training loss = 1.629591464996338 22600
val loss = 1.2339954376220703
training loss = 1.6294158697128296 22700
val loss = 1.2319689989089966
training loss = 1.6295677423477173 22800
val loss = 1.2235438823699951
training loss = 1.629168152809143 22900
val loss = 1.2314839363098145
training loss = 1.6509613990783691 23000
val loss = 1.3443467617034912
training loss = 1.6289279460906982 23100
val loss = 1.230089545249939
training loss = 1.6287480592727661 23200
val loss = 1.2306866645812988
training loss = 1.6298187971115112 23300
val loss = 1.2117396593093872
training loss = 1.6284873485565186 23400
val loss = 1.2307382822036743
training loss = 1.6642296314239502 23500
val loss = 1.161588191986084
training loss = 1.6282254457473755 23600
val loss = 1.2291501760482788
training loss = 1.6280462741851807 23700
val loss = 1.2286689281463623
training loss = 1.6279773712158203 23800
val loss = 1.2278777360916138
training loss = 1.6277879476547241 23900
val loss = 1.2294766902923584
training loss = 1.6779320240020752 24000
val loss = 1.149876356124878
training loss = 1.627527117729187 24100
val loss = 1.2291908264160156
training loss = 1.627350926399231 24200
val loss = 1.229994535446167
training loss = 1.62739896774292 24300
val loss = 1.2224533557891846
training loss = 1.6270948648452759 24400
val loss = 1.2281326055526733
training loss = 1.6474188566207886 24500
val loss = 1.1694324016571045
training loss = 1.6268566846847534 24600
val loss = 1.2289185523986816
training loss = 1.6266767978668213 24700
val loss = 1.227454662322998
training loss = 1.631321668624878 24800
val loss = 1.2711126804351807
training loss = 1.626441240310669 24900
val loss = 1.227339267730713
training loss = 1.6262661218643188 25000
val loss = 1.227772831916809
training loss = 1.6262178421020508 25100
val loss = 1.2296488285064697
training loss = 1.6260194778442383 25200
val loss = 1.22604238986969
training loss = 1.6499766111373901 25300
val loss = 1.1612341403961182
training loss = 1.6257879734039307 25400
val loss = 1.225114107131958
training loss = 1.6256109476089478 25500
val loss = 1.225691795349121
training loss = 1.6255433559417725 25600
val loss = 1.2249901294708252
training loss = 1.625374436378479 25700
val loss = 1.2244731187820435
training loss = 1.6299809217453003 25800
val loss = 1.1908578872680664
training loss = 1.6251413822174072 25900
val loss = 1.2242151498794556
training loss = 1.625061273574829 26000
val loss = 1.218163251876831
training loss = 1.6249841451644897 26100
val loss = 1.2190114259719849
training loss = 1.624746561050415 26200
val loss = 1.2229684591293335
training loss = 1.6579673290252686 26300
val loss = 1.3571470975875854
training loss = 1.6245347261428833 26400
val loss = 1.2220922708511353
training loss = 1.6243609189987183 26500
val loss = 1.221925973892212
training loss = 1.624370813369751 26600
val loss = 1.2257007360458374
training loss = 1.6241636276245117 26700
val loss = 1.2217663526535034
training loss = 1.62399423122406 26800
val loss = 1.2201285362243652
training loss = 1.62409245967865 26900
val loss = 1.2285819053649902
training loss = 1.6237608194351196 27000
val loss = 1.2204906940460205
training loss = 1.6283349990844727 27100
val loss = 1.1856582164764404
training loss = 1.6235393285751343 27200
val loss = 1.2196648120880127
training loss = 1.6240077018737793 27300
val loss = 1.205742359161377
training loss = 1.6233997344970703 27400
val loss = 1.2238492965698242
training loss = 1.6231844425201416 27500
val loss = 1.2190169095993042
training loss = 1.623262882232666 27600
val loss = 1.2098129987716675
training loss = 1.6230453252792358 27700
val loss = 1.2136586904525757
training loss = 1.622794508934021 27800
val loss = 1.217972993850708
training loss = 1.6247246265411377 27900
val loss = 1.1948813199996948
training loss = 1.622573971748352 28000
val loss = 1.217086911201477
training loss = 1.6383799314498901 28100
val loss = 1.3027983903884888
training loss = 1.62237548828125 28200
val loss = 1.2144334316253662
training loss = 1.6221808195114136 28300
val loss = 1.2164109945297241
training loss = 1.6222033500671387 28400
val loss = 1.2103242874145508
training loss = 1.621955156326294 28500
val loss = 1.2158067226409912
training loss = 1.6566977500915527 28600
val loss = 1.359895944595337
training loss = 1.621734619140625 28700
val loss = 1.2165664434432983
training loss = 1.6215578317642212 28800
val loss = 1.2158472537994385
training loss = 1.6215722560882568 28900
val loss = 1.210411787033081
training loss = 1.6213284730911255 29000
val loss = 1.214537262916565
training loss = 1.623068928718567 29100
val loss = 1.2391144037246704
training loss = 1.6211086511611938 29200
val loss = 1.2137970924377441
training loss = 1.6209728717803955 29300
val loss = 1.2172551155090332
training loss = 1.6210294961929321 29400
val loss = 1.219767451286316
training loss = 1.6207531690597534 29500
val loss = 1.2132742404937744
training loss = 1.6205967664718628 29600
val loss = 1.2100752592086792
training loss = 1.6205765008926392 29700
val loss = 1.2167977094650269
training loss = 1.6203540563583374 29800
val loss = 1.212151288986206
training loss = 1.621080994606018 29900
val loss = 1.1975831985473633
training loss = 1.620145559310913 30000
val loss = 1.2123128175735474
reduced chi^2 level 2 = 1.6201441287994385
Constrained alpha: 1.9094524383544922
Constrained beta: 1.5269434452056885
Constrained gamma: 12.109376907348633
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 865.8451,  854.4099,  973.4130,  989.9199,  955.9708, 1071.7697,
        1139.6176, 1079.3866, 1190.4962, 1099.5194, 1244.3923, 1189.0928,
        1266.9718, 1295.1510, 1351.2734, 1368.2069, 1421.6233, 1472.5970,
        1473.2377, 1541.1345, 1559.5240, 1531.0675, 1630.7396, 1592.3408,
        1627.9412, 1718.2823, 1584.6971, 1687.1656, 1732.3964, 1698.7258,
        1609.6959, 1824.7032, 1779.8132, 1777.3942, 1730.0736, 1677.7986,
        1654.8784, 1614.7053, 1659.0070, 1618.5790, 1587.8029, 1550.0288,
        1499.6519, 1519.3728, 1373.4375, 1327.6896, 1304.7406, 1240.1321,
        1198.5497, 1183.5558, 1064.9576,  950.4292,  997.3499,  929.0276,
         866.2084,  836.9137,  860.6146,  770.8575,  619.1017,  526.5079,
         526.8715,  474.7940,  457.2648,  396.5001,  370.2926,  336.2295,
         303.6518,  260.3775,  209.3185,  178.3226,  173.3672,  152.3067,
         145.6031,  106.8517,   97.6011,   67.0508,   47.8432,   34.4748,
          36.9601,   33.0373,   13.1600,   43.0012,   37.5451])]
2394.5997521171785
2.694585896344019 11.653934405927393 90.17204735865845
val isze = 8
idinces = [72  0  8 20 78 15 63 17 14 58 56 26 77 67 57 82  5 22  4 80 69 61 37 24
 43 16 62 28 81 79 50 48 31 60 54 71 47 19 36 64 66 49 68 11 44  7  9 10
 74 30 34  2 25 13 38 73  6 59 27 32 52 39 42 18 35 51 40 75 76  3 53 33
 23 46 70 45 12 21 41  1 29 65 55]
we are doing training validation split
training loss = 93.13162994384766 100
val loss = 90.25807189941406
training loss = 6.488741874694824 200
val loss = 7.004872798919678
training loss = 6.383400917053223 300
val loss = 7.091489315032959
training loss = 6.287414073944092 400
val loss = 7.081860542297363
training loss = 6.202657699584961 500
val loss = 7.080828666687012
training loss = 6.130476951599121 600
val loss = 7.086226940155029
training loss = 6.069932460784912 700
val loss = 7.094581127166748
training loss = 6.019050121307373 800
val loss = 7.10245943069458
training loss = 5.975595951080322 900
val loss = 7.107083320617676
training loss = 5.937480449676514 1000
val loss = 7.106456279754639
training loss = 5.902984142303467 1100
val loss = 7.099390029907227
training loss = 5.870765686035156 1200
val loss = 7.085394859313965
training loss = 5.839857578277588 1300
val loss = 7.064565658569336
training loss = 5.809597492218018 1400
val loss = 7.037359237670898
training loss = 5.779557228088379 1500
val loss = 7.004498481750488
training loss = 5.749477863311768 1600
val loss = 6.9668169021606445
training loss = 5.719214916229248 1700
val loss = 6.925146102905273
training loss = 5.688694953918457 1800
val loss = 6.880319595336914
training loss = 5.657790184020996 1900
val loss = 6.832995891571045
training loss = 5.6266045570373535 2000
val loss = 6.7843780517578125
training loss = 5.595358371734619 2100
val loss = 6.733762741088867
training loss = 5.563933849334717 2200
val loss = 6.682432174682617
training loss = 5.532393455505371 2300
val loss = 6.630795478820801
training loss = 5.500761032104492 2400
val loss = 6.580328464508057
training loss = 5.468860626220703 2500
val loss = 6.5331830978393555
training loss = 5.437047958374023 2600
val loss = 6.484940052032471
training loss = 5.405436992645264 2700
val loss = 6.436066150665283
training loss = 5.374145030975342 2800
val loss = 6.387053489685059
training loss = 5.343371868133545 2900
val loss = 6.336483955383301
training loss = 5.313187122344971 3000
val loss = 6.28725528717041
training loss = 5.283525466918945 3100
val loss = 6.237772464752197
training loss = 5.254312515258789 3200
val loss = 6.188634395599365
training loss = 5.227197170257568 3300
val loss = 6.144580364227295
training loss = 5.194853782653809 3400
val loss = 6.089274883270264
training loss = 5.160879135131836 3500
val loss = 6.031855583190918
training loss = 5.1156439781188965 3600
val loss = 5.956089973449707
training loss = 5.038060188293457 3700
val loss = 5.821565628051758
training loss = 4.903149604797363 3800
val loss = 5.5556416511535645
training loss = 4.550999641418457 3900
val loss = 4.976421356201172
training loss = 4.08296537399292 4000
val loss = 4.246886253356934
training loss = 3.2032740116119385 4100
val loss = 2.964034080505371
training loss = 2.2720541954040527 4200
val loss = 1.5890368223190308
training loss = 1.9222877025604248 4300
val loss = 1.0389560461044312
training loss = 1.886889934539795 4400
val loss = 0.9667211174964905
training loss = 1.8777226209640503 4500
val loss = 0.9470572471618652
training loss = 1.87135910987854 4600
val loss = 0.937889814376831
training loss = 1.8667720556259155 4700
val loss = 0.92157381772995
training loss = 1.8632405996322632 4800
val loss = 0.9176163077354431
training loss = 1.8603276014328003 4900
val loss = 0.9110568761825562
training loss = 1.8580834865570068 5000
val loss = 0.9081293940544128
training loss = 1.8561702966690063 5100
val loss = 0.9096033573150635
training loss = 1.854414701461792 5200
val loss = 0.9087414741516113
training loss = 1.8527641296386719 5300
val loss = 0.9017851948738098
training loss = 1.8514282703399658 5400
val loss = 0.8884706497192383
training loss = 1.8495476245880127 5500
val loss = 0.8996551632881165
training loss = 1.8479492664337158 5600
val loss = 0.8927819728851318
training loss = 1.8459872007369995 5700
val loss = 0.9029734134674072
training loss = 1.843873143196106 5800
val loss = 0.898466169834137
training loss = 1.8414901494979858 5900
val loss = 0.9019861221313477
training loss = 1.8387738466262817 6000
val loss = 0.892508864402771
training loss = 1.835609793663025 6100
val loss = 0.9000934958457947
training loss = 1.832051396369934 6200
val loss = 0.9017392992973328
training loss = 1.8283907175064087 6300
val loss = 0.8907015919685364
training loss = 1.823938012123108 6400
val loss = 0.9055799841880798
training loss = 1.819532871246338 6500
val loss = 0.9087377786636353
training loss = 1.8152514696121216 6600
val loss = 0.9012171626091003
training loss = 1.8106266260147095 6700
val loss = 0.9119894504547119
training loss = 1.808078646659851 6800
val loss = 0.8846290707588196
training loss = 1.8016259670257568 6900
val loss = 0.913972020149231
training loss = 1.7969837188720703 7000
val loss = 0.9162336587905884
training loss = 1.796106219291687 7100
val loss = 0.964963436126709
training loss = 1.7874577045440674 7200
val loss = 0.9189440608024597
training loss = 1.9338406324386597 7300
val loss = 0.7759677767753601
training loss = 1.7774686813354492 7400
val loss = 0.9188394546508789
training loss = 1.7722251415252686 7500
val loss = 0.9239723086357117
training loss = 1.7676664590835571 7600
val loss = 0.9071285128593445
training loss = 1.7615848779678345 7700
val loss = 0.9286320209503174
training loss = 1.7568551301956177 7800
val loss = 0.9115135669708252
training loss = 1.7506659030914307 7900
val loss = 0.9350948333740234
training loss = 1.7540947198867798 8000
val loss = 0.8780771493911743
training loss = 1.7396423816680908 8100
val loss = 0.9416119456291199
training loss = 1.7887871265411377 8200
val loss = 1.1686655282974243
training loss = 1.7288521528244019 8300
val loss = 0.9485129714012146
training loss = 1.7234611511230469 8400
val loss = 0.9573320150375366
training loss = 1.7502291202545166 8500
val loss = 1.1248455047607422
training loss = 1.713309407234192 8600
val loss = 0.9687771201133728
training loss = 1.7082772254943848 8700
val loss = 0.9753440618515015
training loss = 1.7123504877090454 8800
val loss = 0.9192312955856323
training loss = 1.6989444494247437 8900
val loss = 0.9885942935943604
training loss = 1.6953336000442505 9000
val loss = 1.0209360122680664
training loss = 1.6903408765792847 9100
val loss = 0.9995896220207214
training loss = 1.686202049255371 9200
val loss = 1.0116685628890991
training loss = 1.6855851411819458 9300
val loss = 0.9807739853858948
training loss = 1.6789547204971313 9400
val loss = 1.0277066230773926
training loss = 1.676021933555603 9500
val loss = 1.0550930500030518
training loss = 1.6725716590881348 9600
val loss = 1.046787977218628
training loss = 1.669566035270691 9700
val loss = 1.0528624057769775
training loss = 1.6932302713394165 9800
val loss = 0.9621184468269348
training loss = 1.6644980907440186 9900
val loss = 1.070209264755249
training loss = 1.6620852947235107 10000
val loss = 1.0787545442581177
training loss = 1.6608256101608276 10100
val loss = 1.1040380001068115
training loss = 1.6582807302474976 10200
val loss = 1.0945032835006714
training loss = 1.6564377546310425 10300
val loss = 1.1010243892669678
training loss = 1.655255913734436 10400
val loss = 1.1117260456085205
training loss = 1.6538128852844238 10500
val loss = 1.1181800365447998
training loss = 1.6524723768234253 10600
val loss = 1.1268136501312256
training loss = 1.652454137802124 10700
val loss = 1.1554774045944214
training loss = 1.6505249738693237 10800
val loss = 1.1408957242965698
training loss = 1.6701300144195557 10900
val loss = 1.0619890689849854
reduced chi^2 level 2 = 1.660508394241333
Constrained alpha: 1.8603113889694214
Constrained beta: 3.20554518699646
Constrained gamma: 20.840553283691406
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 858.6890,  920.0133,  957.9456,  987.6332, 1000.5044, 1042.4896,
        1050.1124, 1130.2842, 1134.0569, 1128.5460, 1178.8629, 1166.5658,
        1310.1687, 1247.9658, 1303.6687, 1396.7734, 1354.8390, 1389.5718,
        1483.4178, 1474.3220, 1598.3278, 1519.9207, 1633.0968, 1572.4556,
        1716.1324, 1802.7433, 1676.9198, 1710.1367, 1712.1869, 1628.3495,
        1697.1805, 1714.9432, 1703.6567, 1631.3120, 1671.2887, 1746.2117,
        1716.6309, 1539.9703, 1665.4425, 1629.4260, 1608.7412, 1502.0023,
        1423.3545, 1511.6071, 1315.4453, 1314.1914, 1340.1202, 1256.3086,
        1233.7015, 1190.7388, 1037.1248,  979.6210,  931.7277,  873.5356,
         920.1541,  845.2136,  852.2117,  735.6187,  641.3524,  564.2148,
         493.8882,  471.2361,  470.4986,  376.7115,  346.0251,  359.9648,
         274.8082,  252.6296,  220.4340,  156.1195,  166.0605,  111.9329,
         151.7261,   95.2436,   96.4089,   65.8409,   66.0917,   28.7314,
          35.1648,   34.4518,   25.3454,   38.4053,   22.1252])]
2910.8358277498273
0.9945902454155692 4.240193274317024 14.929872420600788
val isze = 8
idinces = [43 62 13 26 74 46 29 36 41 28 75 27 47 57 24  5 78 39 79 11 81 77 16  3
 48 14 68 82 55  7  4  8 50 33 66 56 65 69 64 49 17 80 71 72  0 67 12 38
 18 44 73 42 35 51 25 40 10 22 45  9 21 60 19 58 61 63 23 31 76 20  6 15
 53 59 34 52 70 37  1  2 54 30 32]
we are doing training validation split
training loss = 26.159141540527344 100
val loss = 31.161949157714844
training loss = 20.41107749938965 200
val loss = 24.6568603515625
training loss = 16.260648727416992 300
val loss = 19.43350601196289
training loss = 13.150535583496094 400
val loss = 15.482780456542969
training loss = 10.894556045532227 500
val loss = 12.590934753417969
training loss = 9.272594451904297 600
val loss = 10.495257377624512
training loss = 8.107778549194336 700
val loss = 8.978042602539062
training loss = 7.270613193511963 800
val loss = 7.8776140213012695
training loss = 6.668651103973389 900
val loss = 7.077406883239746
training loss = 6.236091613769531 1000
val loss = 6.494211196899414
training loss = 5.925848960876465 1100
val loss = 6.068591117858887
training loss = 5.7039899826049805 1200
val loss = 5.757630348205566
training loss = 5.5458760261535645 1300
val loss = 5.530154228210449
training loss = 5.433518409729004 1400
val loss = 5.363595008850098
training loss = 5.35371732711792 1500
val loss = 5.241151332855225
training loss = 5.296768665313721 1600
val loss = 5.150628089904785
training loss = 5.255499839782715 1700
val loss = 5.082990646362305
training loss = 5.224607944488525 1800
val loss = 5.031466484069824
training loss = 5.2001190185546875 1900
val loss = 4.990965843200684
training loss = 5.178983688354492 2000
val loss = 4.957565784454346
training loss = 5.158750057220459 2100
val loss = 4.92820405960083
training loss = 5.137275695800781 2200
val loss = 4.900172233581543
training loss = 5.112491130828857 2300
val loss = 4.871037006378174
training loss = 5.08219575881958 2400
val loss = 4.8382134437561035
training loss = 5.043972492218018 2500
val loss = 4.799022197723389
training loss = 4.995364189147949 2600
val loss = 4.750515937805176
training loss = 4.934362411499023 2700
val loss = 4.690395355224609
training loss = 4.859940052032471 2800
val loss = 4.616822242736816
training loss = 4.771744251251221 2900
val loss = 4.5287017822265625
training loss = 4.668631076812744 3000
val loss = 4.423522472381592
training loss = 4.547421932220459 3100
val loss = 4.297453880310059
training loss = 4.402926921844482 3200
val loss = 4.14479923248291
training loss = 4.228669166564941 3300
val loss = 3.9588398933410645
training loss = 4.017824172973633 3400
val loss = 3.7322800159454346
training loss = 3.7651963233947754 3500
val loss = 3.458714485168457
training loss = 3.471303701400757 3600
val loss = 3.136775016784668
training loss = 3.149146318435669 3700
val loss = 2.7771668434143066
training loss = 2.831267833709717 3800
val loss = 2.4107322692871094
training loss = 2.5663607120513916 3900
val loss = 2.087707042694092
training loss = 2.3917150497436523 4000
val loss = 1.8532381057739258
training loss = 2.3031787872314453 4100
val loss = 1.7142212390899658
training loss = 2.2673447132110596 4200
val loss = 1.643329381942749
training loss = 2.254378318786621 4300
val loss = 1.6096504926681519
training loss = 2.2492806911468506 4400
val loss = 1.5937292575836182
training loss = 2.2466156482696533 4500
val loss = 1.585934042930603
training loss = 2.244725227355957 4600
val loss = 1.5818620920181274
training loss = 2.255791425704956 4700
val loss = 1.6942687034606934
training loss = 2.2416751384735107 4800
val loss = 1.5782428979873657
training loss = 2.2992613315582275 4900
val loss = 1.427271842956543
training loss = 2.239004611968994 5000
val loss = 1.5769877433776855
training loss = 2.237748622894287 5100
val loss = 1.5743314027786255
training loss = 2.2365517616271973 5200
val loss = 1.5757761001586914
training loss = 2.2353780269622803 5300
val loss = 1.5716040134429932
training loss = 2.235535144805908 5400
val loss = 1.538852572441101
training loss = 2.233149766921997 5500
val loss = 1.5696295499801636
training loss = 2.4170994758605957 5600
val loss = 2.1724050045013428
training loss = 2.2310242652893066 5700
val loss = 1.5679781436920166
training loss = 2.230064868927002 5800
val loss = 1.5749082565307617
training loss = 2.229001998901367 5900
val loss = 1.5732702016830444
training loss = 2.2279105186462402 6000
val loss = 1.5661933422088623
training loss = 2.226857900619507 6100
val loss = 1.5701258182525635
training loss = 2.225720167160034 6200
val loss = 1.5643060207366943
training loss = 2.2255606651306152 6300
val loss = 1.5345714092254639
training loss = 2.2232062816619873 6400
val loss = 1.562605381011963
training loss = 2.425699472427368 6500
val loss = 1.377684473991394
training loss = 2.2199325561523438 6600
val loss = 1.5625228881835938
training loss = 2.2177348136901855 6700
val loss = 1.5587501525878906
training loss = 2.2149484157562256 6800
val loss = 1.549337387084961
training loss = 2.2111992835998535 6900
val loss = 1.5569478273391724
training loss = 2.2183821201324463 7000
val loss = 1.4669461250305176
training loss = 2.2000174522399902 7100
val loss = 1.5591152906417847
training loss = 2.1929545402526855 7200
val loss = 1.5847344398498535
training loss = 2.1846892833709717 7300
val loss = 1.565974235534668
training loss = 2.1769871711730957 7400
val loss = 1.5734977722167969
training loss = 2.1774590015411377 7500
val loss = 1.6718872785568237
training loss = 2.163039207458496 7600
val loss = 1.579859733581543
training loss = 2.1562397480010986 7700
val loss = 1.5751169919967651
training loss = 2.1495141983032227 7800
val loss = 1.5783615112304688
training loss = 2.1424224376678467 7900
val loss = 1.5862410068511963
training loss = 2.15696120262146 8000
val loss = 1.4700862169265747
training loss = 2.1274354457855225 8100
val loss = 1.592668890953064
training loss = 2.1192166805267334 8200
val loss = 1.5950994491577148
training loss = 2.1123175621032715 8300
val loss = 1.5634238719940186
training loss = 2.1021780967712402 8400
val loss = 1.6039831638336182
training loss = 2.0927698612213135 8500
val loss = 1.6079115867614746
training loss = 2.0837011337280273 8600
val loss = 1.6158818006515503
training loss = 2.074169158935547 8700
val loss = 1.6237311363220215
training loss = 2.1258397102355957 8800
val loss = 1.9405478239059448
training loss = 2.054889440536499 8900
val loss = 1.6413819789886475
training loss = 2.046877145767212 9000
val loss = 1.6944715976715088
training loss = 2.036059617996216 9100
val loss = 1.6651259660720825
training loss = 2.0267746448516846 9200
val loss = 1.6707885265350342
training loss = 2.0189270973205566 9300
val loss = 1.6974787712097168
training loss = 2.0106210708618164 9400
val loss = 1.6929463148117065
training loss = 2.013312816619873 9500
val loss = 1.8039491176605225
training loss = 1.9964218139648438 9600
val loss = 1.717247486114502
training loss = 1.9899014234542847 9700
val loss = 1.72652268409729
training loss = 1.9845361709594727 9800
val loss = 1.7449886798858643
training loss = 1.9792264699935913 9900
val loss = 1.7487834692001343
training loss = 1.9752792119979858 10000
val loss = 1.7789990901947021
training loss = 1.9707111120224 10100
val loss = 1.7683117389678955
training loss = 1.9854655265808105 10200
val loss = 1.9191625118255615
training loss = 1.9641720056533813 10300
val loss = 1.7885037660598755
training loss = 1.9612869024276733 10400
val loss = 1.7948249578475952
training loss = 1.9766513109207153 10500
val loss = 1.6854722499847412
training loss = 1.956760287284851 10600
val loss = 1.8102872371673584
training loss = 1.9547604322433472 10700
val loss = 1.816343069076538
training loss = 1.9535869359970093 10800
val loss = 1.84098219871521
training loss = 1.9516663551330566 10900
val loss = 1.8267018795013428
training loss = 1.9819645881652832 11000
val loss = 2.0425076484680176
training loss = 1.9492639303207397 11100
val loss = 1.8354090452194214
training loss = 1.9481736421585083 11200
val loss = 1.8398795127868652
training loss = 1.9490059614181519 11300
val loss = 1.8854095935821533
training loss = 1.9464844465255737 11400
val loss = 1.8464281558990479
training loss = 2.2858729362487793 11500
val loss = 2.848437786102295
training loss = 1.9451278448104858 11600
val loss = 1.8533447980880737
training loss = 1.9444743394851685 11700
val loss = 1.8538410663604736
training loss = 1.9444682598114014 11800
val loss = 1.8769112825393677
training loss = 1.9434537887573242 11900
val loss = 1.8578203916549683
reduced chi^2 level 2 = 1.9711823463439941
Constrained alpha: 1.8131259679794312
Constrained beta: 3.760669231414795
Constrained gamma: 14.520798683166504
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 865.9238,  873.3101,  946.4798,  982.5873,  977.7783, 1056.7804,
        1068.5303, 1158.9541, 1093.2423, 1249.4661, 1164.6750, 1231.8098,
        1266.6892, 1223.9324, 1341.3458, 1451.0585, 1389.3715, 1465.2271,
        1566.5637, 1427.1331, 1550.3569, 1599.7715, 1553.5983, 1622.3977,
        1564.8763, 1784.3674, 1614.2679, 1765.0869, 1724.1788, 1794.6821,
        1618.7545, 1770.4182, 1785.7123, 1699.1211, 1720.3552, 1755.0437,
        1686.1893, 1660.6265, 1672.3997, 1593.2050, 1655.0690, 1564.9856,
        1425.5142, 1516.3331, 1353.3824, 1349.9547, 1236.9086, 1253.2770,
        1192.9458, 1197.0739, 1061.7924, 1014.5466,  936.9314,  921.8024,
         910.7078,  857.7898,  848.5722,  731.1785,  606.6190,  539.3399,
         520.6787,  474.1627,  431.4006,  406.3250,  353.8181,  363.9101,
         267.8657,  236.2798,  183.1244,  171.4808,  156.3667,  160.0940,
         148.4666,   95.0331,   81.4228,   67.3420,   42.5398,   26.2433,
          35.6578,   42.7378,   20.0389,   32.5952,   37.2576])]
2906.7189796663242
4.185876578231616 11.896552158181246 84.57137128716596
val isze = 8
idinces = [ 3  6 58 76 14 13 53 65 70 15 60 51 20 64 49 55 79  0 80 31 25  5 33 21
 18 68  1 26 36 61 48 37 73 78 16  7 71 42 19 75 38 69 81 10 43  2 72 32
 52 63 24 41 28  8  4 82 67 77 50 45 54 27 57 46 12  9 35 40 59 22 11 62
 74 66 47 30 29 34 17 44 23 39 56]
we are doing training validation split
training loss = 280.85882568359375 100
val loss = 281.3431396484375
training loss = 7.331203937530518 200
val loss = 7.325474262237549
training loss = 6.838073253631592 300
val loss = 6.4805908203125
training loss = 6.527240753173828 400
val loss = 6.035138130187988
training loss = 6.342052936553955 500
val loss = 5.749864101409912
training loss = 6.235245227813721 600
val loss = 5.573944568634033
training loss = 6.171899795532227 700
val loss = 5.464957237243652
training loss = 6.131035327911377 800
val loss = 5.395078659057617
training loss = 6.1014509201049805 900
val loss = 5.348143577575684
training loss = 6.077638626098633 1000
val loss = 5.31475830078125
training loss = 6.057043075561523 1100
val loss = 5.289695739746094
training loss = 6.038546085357666 1200
val loss = 5.270162105560303
training loss = 6.021633625030518 1300
val loss = 5.253974914550781
training loss = 6.006046295166016 1400
val loss = 5.240217208862305
training loss = 5.9916181564331055 1500
val loss = 5.228244781494141
training loss = 5.978212833404541 1600
val loss = 5.217217445373535
training loss = 5.965703010559082 1700
val loss = 5.20711612701416
training loss = 5.953971862792969 1800
val loss = 5.197640419006348
training loss = 5.9429121017456055 1900
val loss = 5.188735485076904
training loss = 5.932420253753662 2000
val loss = 5.180114269256592
training loss = 5.9224042892456055 2100
val loss = 5.171860218048096
training loss = 5.912771224975586 2200
val loss = 5.163894176483154
training loss = 5.903439044952393 2300
val loss = 5.155855178833008
training loss = 5.894322872161865 2400
val loss = 5.1481709480285645
training loss = 5.885346412658691 2500
val loss = 5.140876770019531
training loss = 5.876575469970703 2600
val loss = 5.164511680603027
training loss = 5.867577075958252 2700
val loss = 5.128816604614258
training loss = 5.858574390411377 2800
val loss = 5.126426696777344
training loss = 5.871505260467529 2900
val loss = 4.701568603515625
training loss = 5.839964389801025 3000
val loss = 5.100927829742432
training loss = 5.861999988555908 3100
val loss = 5.639335632324219
training loss = 5.8203654289245605 3200
val loss = 5.0793843269348145
training loss = 5.810162544250488 3300
val loss = 5.082059860229492
training loss = 5.799737453460693 3400
val loss = 5.074056625366211
training loss = 5.789125442504883 3500
val loss = 5.057244300842285
training loss = 5.778689861297607 3600
val loss = 4.998407363891602
training loss = 5.767712593078613 3700
val loss = 5.038791656494141
training loss = 5.7573089599609375 3800
val loss = 5.080620288848877
training loss = 5.746756553649902 3900
val loss = 5.020501136779785
training loss = 5.7691826820373535 4000
val loss = 5.568871021270752
training loss = 5.727612495422363 4100
val loss = 5.001822471618652
training loss = 5.732609272003174 4200
val loss = 4.671268939971924
training loss = 5.711706161499023 4300
val loss = 4.992143630981445
training loss = 5.743059158325195 4400
val loss = 5.594032287597656
training loss = 5.700007438659668 4500
val loss = 4.978083610534668
training loss = 5.695705890655518 4600
val loss = 4.979576587677002
training loss = 5.692323684692383 4700
val loss = 4.983675956726074
training loss = 5.6898322105407715 4800
val loss = 4.974647521972656
training loss = 5.868772983551025 4900
val loss = 3.8942923545837402
training loss = 5.6864848136901855 5000
val loss = 4.975838661193848
training loss = 5.685455799102783 5100
val loss = 4.971881866455078
training loss = 5.6862993240356445 5200
val loss = 5.093588829040527
training loss = 5.684091091156006 5300
val loss = 4.9702982902526855
training loss = 5.683647155761719 5400
val loss = 4.964172840118408
training loss = 5.683345794677734 5500
val loss = 5.001865386962891
training loss = 5.682953834533691 5600
val loss = 4.970527648925781
training loss = 5.68273401260376 5700
val loss = 5.000496864318848
training loss = 5.682431697845459 5800
val loss = 4.969025611877441
training loss = 5.870846271514893 5900
val loss = 6.432005882263184
training loss = 5.681967735290527 6000
val loss = 4.97795295715332
training loss = 5.682358741760254 6100
val loss = 4.898324012756348
training loss = 5.6815056800842285 6200
val loss = 4.960698127746582
training loss = 5.681272983551025 6300
val loss = 4.966767311096191
training loss = 5.681142807006836 6400
val loss = 4.933496952056885
training loss = 5.680749416351318 6500
val loss = 4.969546318054199
training loss = 5.681774139404297 6600
val loss = 5.07891845703125
training loss = 5.680139064788818 6700
val loss = 4.96933650970459
training loss = 5.6819167137146 6800
val loss = 5.1072096824646
training loss = 5.679406642913818 6900
val loss = 4.966381072998047
training loss = 5.679051876068115 7000
val loss = 4.943134307861328
training loss = 5.678518772125244 7100
val loss = 4.986503601074219
training loss = 5.677939414978027 7200
val loss = 4.96731424331665
training loss = 5.678858280181885 7300
val loss = 4.8505964279174805
training loss = 5.676543712615967 7400
val loss = 4.967299461364746
training loss = 5.734755039215088 7500
val loss = 5.741162300109863
training loss = 5.674551010131836 7600
val loss = 4.9696197509765625
training loss = 5.673162937164307 7700
val loss = 4.966254234313965
training loss = 5.671448230743408 7800
val loss = 4.982194900512695
training loss = 5.66908597946167 7900
val loss = 4.962596416473389
training loss = 5.670485973358154 8000
val loss = 4.760024070739746
training loss = 5.660983085632324 8100
val loss = 4.959198951721191
training loss = 5.652543067932129 8200
val loss = 4.9505109786987305
training loss = 5.635864734649658 8300
val loss = 4.926401138305664
training loss = 5.587876319885254 8400
val loss = 4.911686897277832
training loss = 5.311556816101074 8500
val loss = 4.631789207458496
training loss = 4.213636875152588 8600
val loss = 3.9270074367523193
training loss = 2.673673629760742 8700
val loss = 3.3330612182617188
training loss = 2.5078043937683105 8800
val loss = 3.2795679569244385
training loss = 2.3852062225341797 8900
val loss = 2.6581733226776123
training loss = 2.2306714057922363 9000
val loss = 2.921382188796997
training loss = 2.170870304107666 9100
val loss = 2.9449994564056396
training loss = 2.1426873207092285 9200
val loss = 2.9879772663116455
training loss = 2.102975606918335 9300
val loss = 2.9128458499908447
training loss = 2.0809707641601562 9400
val loss = 2.8940975666046143
training loss = 2.0603561401367188 9500
val loss = 2.906859874725342
training loss = 2.0436949729919434 9600
val loss = 2.8512909412384033
training loss = 2.026710271835327 9700
val loss = 2.8527884483337402
training loss = 2.0148441791534424 9800
val loss = 2.822755813598633
training loss = 2.0012381076812744 9900
val loss = 2.8310623168945312
training loss = 1.9932485818862915 10000
val loss = 2.8279173374176025
training loss = 1.9812448024749756 10100
val loss = 2.813194751739502
training loss = 1.9777474403381348 10200
val loss = 2.748455762863159
training loss = 1.9652645587921143 10300
val loss = 2.7974226474761963
training loss = 2.0736279487609863 10400
val loss = 3.0658931732177734
training loss = 1.952338457107544 10500
val loss = 2.7805545330047607
training loss = 1.9469399452209473 10600
val loss = 2.805015802383423
training loss = 1.9422894716262817 10700
val loss = 2.7602100372314453
training loss = 1.9364616870880127 10800
val loss = 2.770643711090088
training loss = 1.9429911375045776 10900
val loss = 2.6983401775360107
training loss = 1.928810715675354 11000
val loss = 2.759579658508301
training loss = 2.1219778060913086 11100
val loss = 2.660734176635742
training loss = 1.9225733280181885 11200
val loss = 2.750488042831421
training loss = 1.9201273918151855 11300
val loss = 2.7305314540863037
training loss = 1.9173736572265625 11400
val loss = 2.737839460372925
training loss = 1.9139009714126587 11500
val loss = 2.742687940597534
training loss = 1.9141490459442139 11600
val loss = 2.767411470413208
training loss = 1.91041100025177 11700
val loss = 2.7355029582977295
training loss = 2.0445427894592285 11800
val loss = 2.683530569076538
training loss = 1.9063727855682373 11900
val loss = 2.8103830814361572
training loss = 1.9035059213638306 12000
val loss = 2.79331111907959
training loss = 1.9009974002838135 12100
val loss = 2.7827961444854736
training loss = 1.9011033773422241 12200
val loss = 2.7597599029541016
training loss = 1.89839768409729 12300
val loss = 2.7686684131622314
training loss = 1.8962948322296143 12400
val loss = 2.7664542198181152
training loss = 1.895987629890442 12500
val loss = 2.7535440921783447
training loss = 1.8938963413238525 12600
val loss = 2.7643775939941406
training loss = 1.8943641185760498 12700
val loss = 2.7342793941497803
training loss = 1.8920209407806396 12800
val loss = 2.759690046310425
training loss = 2.189570665359497 12900
val loss = 2.6924619674682617
training loss = 1.8905209302902222 13000
val loss = 2.7523112297058105
training loss = 1.888903260231018 13100
val loss = 2.759486198425293
training loss = 1.8904600143432617 13200
val loss = 2.7669224739074707
training loss = 1.88766610622406 13300
val loss = 2.756598949432373
training loss = 1.891835331916809 13400
val loss = 2.708038568496704
training loss = 1.8867855072021484 13500
val loss = 2.752819299697876
training loss = 1.8854069709777832 13600
val loss = 2.757718801498413
reduced chi^2 level 2 = 1.9040082693099976
Constrained alpha: 1.974427580833435
Constrained beta: 3.621084451675415
Constrained gamma: 17.24171257019043
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 844.8726,  860.5606,  911.0078,  937.0464, 1016.2915, 1078.3889,
        1099.7985, 1139.4252, 1158.4403, 1094.7379, 1175.9326, 1183.1841,
        1255.9150, 1228.1824, 1339.1309, 1409.1854, 1368.7374, 1504.5005,
        1509.2572, 1525.2402, 1569.3831, 1537.1089, 1668.9109, 1614.2604,
        1629.8872, 1691.9092, 1566.3290, 1696.8215, 1657.3972, 1642.6796,
        1655.9664, 1765.1782, 1716.7672, 1693.4642, 1806.8988, 1745.0603,
        1615.9701, 1636.1571, 1611.2678, 1515.3705, 1619.9069, 1588.2375,
        1456.2700, 1557.4697, 1403.6279, 1366.8303, 1266.8712, 1217.4530,
        1171.8506, 1174.6685, 1081.7377,  988.3941,  920.0246,  906.0516,
         905.9473,  852.7716,  819.6200,  712.2098,  653.9709,  528.8882,
         538.3745,  503.9417,  434.9344,  377.2737,  345.3805,  346.9348,
         295.6790,  248.4891,  206.8754,  176.7339,  153.8561,  136.7882,
         125.3083,   93.0127,   89.5309,   65.0243,   40.8029,   43.7300,
          35.1785,   45.9733,   29.9109,   37.8289,   41.5783])]
2710.149650394728
2.4814996744252866 7.149177382846881 5.432161892252663
val isze = 8
idinces = [20 21 24 13 35 41 51 22 42  6 81 23 43 10 50  9 17 32  5 71 75 55 16 73
 53 18 76 54  8 74 33 77 82 19 49 64 63 60  3 36 47 68 30 27 69 62 46 29
 40 45 58 65 66 25 31 57 44 67 48 15 12 79 38 34 39 14 11 26 61  7 37 56
 80  4 72  0 52 59 78 28 70  1  2]
we are doing training validation split
training loss = 37.51255798339844 100
val loss = 34.27464294433594
training loss = 24.53276824951172 200
val loss = 26.856815338134766
training loss = 17.665180206298828 300
val loss = 20.502037048339844
training loss = 13.450876235961914 400
val loss = 16.446582794189453
training loss = 10.8277587890625 500
val loss = 13.773508071899414
training loss = 9.156631469726562 600
val loss = 11.939580917358398
training loss = 8.069961547851562 700
val loss = 10.63611888885498
training loss = 7.3521504402160645 800
val loss = 9.682938575744629
training loss = 6.872537612915039 900
val loss = 8.969476699829102
training loss = 6.549433708190918 1000
val loss = 8.425580978393555
training loss = 6.330342769622803 1100
val loss = 8.004940032958984
training loss = 6.180820941925049 1200
val loss = 7.676143646240234
training loss = 6.068846225738525 1300
val loss = 7.385836601257324
training loss = 5.960797309875488 1400
val loss = 7.031489849090576
training loss = 5.908987998962402 1500
val loss = 6.803458213806152
training loss = 5.881107330322266 1600
val loss = 6.672003269195557
training loss = 5.861638069152832 1700
val loss = 6.580782413482666
training loss = 5.844860076904297 1800
val loss = 6.5030999183654785
training loss = 5.828341960906982 1900
val loss = 6.498510360717773
training loss = 5.81165885925293 2000
val loss = 6.480327606201172
training loss = 5.794455528259277 2100
val loss = 6.476080894470215
training loss = 5.776541233062744 2200
val loss = 6.444515705108643
training loss = 5.757984161376953 2300
val loss = 6.430683612823486
training loss = 5.7388434410095215 2400
val loss = 6.445444107055664
training loss = 5.718806266784668 2500
val loss = 6.402402877807617
training loss = 5.698240280151367 2600
val loss = 6.4001970291137695
training loss = 5.676998615264893 2700
val loss = 6.373708724975586
training loss = 5.655226230621338 2800
val loss = 6.369998931884766
training loss = 5.632908821105957 2900
val loss = 6.344407558441162
training loss = 5.610220909118652 3000
val loss = 6.335387229919434
training loss = 5.587260723114014 3100
val loss = 6.326871871948242
training loss = 5.564175128936768 3200
val loss = 6.300145149230957
training loss = 5.541871070861816 3300
val loss = 6.36317777633667
training loss = 5.5185956954956055 3400
val loss = 6.275363922119141
training loss = 5.496641635894775 3500
val loss = 6.26656436920166
training loss = 5.475635051727295 3600
val loss = 6.253098964691162
training loss = 5.45592737197876 3700
val loss = 6.227739334106445
training loss = 5.4377055168151855 3800
val loss = 6.23607063293457
training loss = 5.4212422370910645 3900
val loss = 6.2333831787109375
training loss = 5.406540393829346 4000
val loss = 6.228919506072998
training loss = 5.393476486206055 4100
val loss = 6.22181510925293
training loss = 5.387573719024658 4200
val loss = 6.002789497375488
training loss = 5.369770526885986 4300
val loss = 6.2244367599487305
training loss = 5.355703353881836 4400
val loss = 6.259779930114746
training loss = 5.330859661102295 4500
val loss = 6.215047836303711
training loss = 5.260629653930664 4600
val loss = 6.193469524383545
training loss = 4.982731819152832 4700
val loss = 6.26409912109375
training loss = 4.46914529800415 4800
val loss = 5.534643650054932
training loss = 3.7227838039398193 4900
val loss = 5.375974178314209
training loss = 2.5433788299560547 5000
val loss = 3.969841718673706
training loss = 1.9246851205825806 5100
val loss = 3.061979293823242
training loss = 1.899533748626709 5200
val loss = 2.9518990516662598
training loss = 1.9037911891937256 5300
val loss = 3.110017776489258
training loss = 1.8895341157913208 5400
val loss = 2.9349400997161865
training loss = 1.8848820924758911 5500
val loss = 2.9711031913757324
training loss = 1.8798179626464844 5600
val loss = 2.9406652450561523
training loss = 1.8750354051589966 5700
val loss = 2.9378676414489746
training loss = 1.873692274093628 5800
val loss = 3.03668475151062
training loss = 1.8655924797058105 5900
val loss = 2.930905818939209
training loss = 2.058699607849121 6000
val loss = 3.9573781490325928
training loss = 1.8562719821929932 6100
val loss = 2.925323486328125
training loss = 1.8516093492507935 6200
val loss = 2.912262439727783
training loss = 1.8472669124603271 6300
val loss = 2.920593023300171
training loss = 1.842879295349121 6400
val loss = 2.904890537261963
training loss = 2.0413758754730225 6500
val loss = 2.4820337295532227
training loss = 1.8340306282043457 6600
val loss = 2.8955326080322266
training loss = 1.8295408487319946 6700
val loss = 2.889353036880493
training loss = 1.8252826929092407 6800
val loss = 2.8951516151428223
training loss = 1.8207048177719116 6900
val loss = 2.872030258178711
training loss = 1.8178294897079468 7000
val loss = 2.802309989929199
training loss = 1.8117444515228271 7100
val loss = 2.8581795692443848
training loss = 1.8185778856277466 7200
val loss = 2.696908950805664
training loss = 1.8024715185165405 7300
val loss = 2.8422746658325195
training loss = 1.8650060892105103 7400
val loss = 2.5277719497680664
training loss = 1.7929531335830688 7500
val loss = 2.8322596549987793
training loss = 1.7931795120239258 7600
val loss = 2.716531991958618
training loss = 1.7832005023956299 7700
val loss = 2.8225016593933105
training loss = 1.7781156301498413 7800
val loss = 2.809091329574585
training loss = 1.773909091949463 7900
val loss = 2.759617805480957
training loss = 1.7681351900100708 8000
val loss = 2.795510768890381
training loss = 1.773061990737915 8100
val loss = 2.9600772857666016
training loss = 1.7579785585403442 8200
val loss = 2.781306743621826
training loss = 1.752751111984253 8300
val loss = 2.775984287261963
training loss = 1.7478225231170654 8400
val loss = 2.7552919387817383
training loss = 1.7427949905395508 8500
val loss = 2.7629194259643555
training loss = 1.7399276494979858 8600
val loss = 2.6855385303497314
training loss = 1.7327803373336792 8700
val loss = 2.7482810020446777
training loss = 1.727790117263794 8800
val loss = 2.743502378463745
training loss = 1.7235628366470337 8900
val loss = 2.7713263034820557
training loss = 1.7183235883712769 9000
val loss = 2.729804515838623
training loss = 1.7139379978179932 9100
val loss = 2.695943832397461
training loss = 1.7092167139053345 9200
val loss = 2.7153496742248535
training loss = 1.7049025297164917 9300
val loss = 2.73281192779541
training loss = 1.7006173133850098 9400
val loss = 2.6917688846588135
training loss = 1.6964008808135986 9500
val loss = 2.6961472034454346
training loss = 1.692376732826233 9600
val loss = 2.674537181854248
training loss = 1.6884647607803345 9700
val loss = 2.6815996170043945
training loss = 1.6859561204910278 9800
val loss = 2.6213607788085938
training loss = 1.6809875965118408 9900
val loss = 2.6739583015441895
training loss = 1.6773438453674316 10000
val loss = 2.6610913276672363
training loss = 1.6767231225967407 10100
val loss = 2.734898567199707
training loss = 1.6705710887908936 10200
val loss = 2.6472742557525635
training loss = 1.6675488948822021 10300
val loss = 2.612610101699829
training loss = 1.6641077995300293 10400
val loss = 2.6381044387817383
training loss = 1.6609406471252441 10500
val loss = 2.6256017684936523
training loss = 1.658164620399475 10600
val loss = 2.5978786945343018
training loss = 1.6550482511520386 10700
val loss = 2.6120805740356445
training loss = 1.7345606088638306 10800
val loss = 3.139209270477295
training loss = 1.6494429111480713 10900
val loss = 2.5958056449890137
training loss = 1.646665096282959 11000
val loss = 2.592357635498047
training loss = 1.6444809436798096 11100
val loss = 2.6148183345794678
training loss = 1.6414005756378174 11200
val loss = 2.5778164863586426
training loss = 1.6395763158798218 11300
val loss = 2.53265380859375
training loss = 1.6363118886947632 11400
val loss = 2.5643911361694336
training loss = 1.702445387840271 11500
val loss = 2.2903077602386475
training loss = 1.6313501596450806 11600
val loss = 2.551110029220581
training loss = 1.6288118362426758 11700
val loss = 2.542778968811035
training loss = 1.626490592956543 11800
val loss = 2.528596878051758
training loss = 1.6240345239639282 11900
val loss = 2.529953718185425
training loss = 1.6214768886566162 12000
val loss = 2.515286684036255
training loss = 1.619128704071045 12100
val loss = 2.5199742317199707
training loss = 1.6166315078735352 12200
val loss = 2.5075578689575195
training loss = 1.6139421463012695 12300
val loss = 2.501842737197876
training loss = 1.6113409996032715 12400
val loss = 2.499220848083496
training loss = 1.6084556579589844 12500
val loss = 2.481245994567871
training loss = 1.6086748838424683 12600
val loss = 2.3971967697143555
training loss = 1.6024404764175415 12700
val loss = 2.4603757858276367
training loss = 1.5990861654281616 12800
val loss = 2.442089557647705
training loss = 1.5957677364349365 12900
val loss = 2.4334447383880615
training loss = 1.5920474529266357 13000
val loss = 2.4223074913024902
training loss = 1.58899986743927 13100
val loss = 2.367993116378784
training loss = 1.5840758085250854 13200
val loss = 2.3925554752349854
training loss = 1.699249505996704 13300
val loss = 3.0179386138916016
training loss = 1.5755605697631836 13400
val loss = 2.361283779144287
training loss = 1.5712705850601196 13500
val loss = 2.3461925983428955
training loss = 1.567076325416565 13600
val loss = 2.331876754760742
training loss = 1.5632919073104858 13700
val loss = 2.318023681640625
training loss = 1.5596672296524048 13800
val loss = 2.3037848472595215
training loss = 1.5564422607421875 13900
val loss = 2.300786018371582
training loss = 1.5534743070602417 14000
val loss = 2.27889084815979
training loss = 1.552962064743042 14100
val loss = 2.3325676918029785
training loss = 1.5483648777008057 14200
val loss = 2.263187885284424
training loss = 1.5461068153381348 14300
val loss = 2.2465224266052246
training loss = 1.548661231994629 14400
val loss = 2.332334518432617
training loss = 1.542150616645813 14500
val loss = 2.2274253368377686
training loss = 1.5403480529785156 14600
val loss = 2.219417095184326
training loss = 1.5389832258224487 14700
val loss = 2.2364015579223633
training loss = 1.537025809288025 14800
val loss = 2.2041947841644287
training loss = 1.6043142080307007 14900
val loss = 1.9566729068756104
training loss = 1.5340158939361572 15000
val loss = 2.1960084438323975
training loss = 1.53260338306427 15100
val loss = 2.1857995986938477
training loss = 1.5318297147750854 15200
val loss = 2.1484076976776123
training loss = 1.5299996137619019 15300
val loss = 2.1753196716308594
training loss = 1.5336871147155762 15400
val loss = 2.2698655128479004
training loss = 1.5276768207550049 15500
val loss = 2.166675567626953
training loss = 1.5268012285232544 15600
val loss = 2.1819167137145996
training loss = 1.5256390571594238 15700
val loss = 2.1630778312683105
training loss = 1.5247455835342407 15800
val loss = 2.164053440093994
training loss = 1.523896336555481 15900
val loss = 2.1630070209503174
training loss = 1.523061752319336 16000
val loss = 2.1518115997314453
training loss = 1.5227570533752441 16100
val loss = 2.1777145862579346
training loss = 1.5216230154037476 16200
val loss = 2.1472127437591553
training loss = 1.5465151071548462 16300
val loss = 1.9733870029449463
training loss = 1.520373821258545 16400
val loss = 2.141613483428955
training loss = 1.5198123455047607 16500
val loss = 2.141380786895752
training loss = 1.5195401906967163 16600
val loss = 2.1175074577331543
training loss = 1.5188010931015015 16700
val loss = 2.1363654136657715
training loss = 1.5333884954452515 16800
val loss = 2.3166723251342773
training loss = 1.517920732498169 16900
val loss = 2.1319074630737305
training loss = 1.5177733898162842 17000
val loss = 2.1099534034729004
training loss = 1.517172932624817 17100
val loss = 2.1340200901031494
training loss = 1.516812801361084 17200
val loss = 2.127139091491699
training loss = 1.5179567337036133 17300
val loss = 2.0778913497924805
training loss = 1.5161981582641602 17400
val loss = 2.123778820037842
training loss = 1.5568112134933472 17500
val loss = 2.4480648040771484
training loss = 1.5156644582748413 17600
val loss = 2.122689723968506
training loss = 1.515419602394104 17700
val loss = 2.119520425796509
training loss = 1.516974925994873 17800
val loss = 2.175178289413452
training loss = 1.5149849653244019 17900
val loss = 2.116964817047119
training loss = 1.515175461769104 18000
val loss = 2.0899627208709717
training loss = 1.5146034955978394 18100
val loss = 2.1161394119262695
training loss = 1.5144230127334595 18200
val loss = 2.113326072692871
training loss = 1.5173320770263672 18300
val loss = 2.188645839691162
training loss = 1.5141048431396484 18400
val loss = 2.1107285022735596
training loss = 1.5139867067337036 18500
val loss = 2.116846799850464
training loss = 1.5138291120529175 18600
val loss = 2.1117632389068604
training loss = 1.513686180114746 18700
val loss = 2.1080517768859863
training loss = 1.6066627502441406 18800
val loss = 1.8501975536346436
training loss = 1.51344633102417 18900
val loss = 2.1040565967559814
training loss = 1.5133249759674072 19000
val loss = 2.104919672012329
training loss = 1.5139456987380981 19100
val loss = 2.1398861408233643
training loss = 1.5131127834320068 19200
val loss = 2.1030690670013428
training loss = 1.5402894020080566 19300
val loss = 2.3578624725341797
training loss = 1.512934684753418 19400
val loss = 2.10688853263855
training loss = 1.5128161907196045 19500
val loss = 2.100813627243042
training loss = 1.5132319927215576 19600
val loss = 2.0716333389282227
training loss = 1.5126407146453857 19700
val loss = 2.099410057067871
training loss = 1.5127612352371216 19800
val loss = 2.0793142318725586
training loss = 1.5125148296356201 19900
val loss = 2.1055612564086914
training loss = 1.5123889446258545 20000
val loss = 2.0970661640167236
training loss = 1.5537325143814087 20100
val loss = 1.8948640823364258
training loss = 1.5122357606887817 20200
val loss = 2.095120906829834
training loss = 1.5121535062789917 20300
val loss = 2.093959331512451
training loss = 1.5121867656707764 20400
val loss = 2.0816187858581543
training loss = 1.5120108127593994 20500
val loss = 2.093385696411133
training loss = 1.52593994140625 20600
val loss = 2.266587257385254
training loss = 1.5118837356567383 20700
val loss = 2.0955312252044678
training loss = 1.5117976665496826 20800
val loss = 2.0915346145629883
training loss = 1.5117769241333008 20900
val loss = 2.096503257751465
training loss = 1.5116686820983887 21000
val loss = 2.0916664600372314
training loss = 1.5116212368011475 21100
val loss = 2.082286834716797
training loss = 1.511621356010437 21200
val loss = 2.1004393100738525
training loss = 1.511466145515442 21300
val loss = 2.088223457336426
training loss = 1.5120536088943481 21400
val loss = 2.0556676387786865
training loss = 1.5113424062728882 21500
val loss = 2.0871286392211914
training loss = 1.5717058181762695 21600
val loss = 2.4947433471679688
training loss = 1.5112239122390747 21700
val loss = 2.0879335403442383
training loss = 1.51115083694458 21800
val loss = 2.085883855819702
training loss = 1.511500358581543 21900
val loss = 2.05954909324646
training loss = 1.5110325813293457 22000
val loss = 2.084195137023926
training loss = 1.5839781761169434 22100
val loss = 1.843358039855957
training loss = 1.5109237432479858 22200
val loss = 2.0831308364868164
training loss = 1.5108506679534912 22300
val loss = 2.0823721885681152
training loss = 1.519372820854187 22400
val loss = 1.9757026433944702
training loss = 1.510744333267212 22500
val loss = 2.081014633178711
training loss = 1.5106765031814575 22600
val loss = 2.080378532409668
training loss = 1.5106470584869385 22700
val loss = 2.076587677001953
training loss = 1.5105699300765991 22800
val loss = 2.0796146392822266
training loss = 1.9045777320861816 22900
val loss = 1.8653843402862549
training loss = 1.5104767084121704 23000
val loss = 2.0755982398986816
training loss = 1.5104007720947266 23100
val loss = 2.0782055854797363
training loss = 1.5951708555221558 23200
val loss = 2.5747430324554443
training loss = 1.5103036165237427 23300
val loss = 2.079292058944702
training loss = 1.5102348327636719 23400
val loss = 2.076958417892456
training loss = 1.5105806589126587 23500
val loss = 2.0514485836029053
training loss = 1.510134220123291 23600
val loss = 2.0755720138549805
training loss = 1.5929005146026611 23700
val loss = 1.8269786834716797
training loss = 1.5100384950637817 23800
val loss = 2.074627637863159
training loss = 1.509972095489502 23900
val loss = 2.073211431503296
training loss = 1.5105167627334595 24000
val loss = 2.043215274810791
training loss = 1.5098750591278076 24100
val loss = 2.0727972984313965
training loss = 1.5549468994140625 24200
val loss = 2.410900592803955
training loss = 1.5097870826721191 24300
val loss = 2.0703861713409424
training loss = 1.5097198486328125 24400
val loss = 2.0712122917175293
training loss = 1.509725570678711 24500
val loss = 2.076871633529663
training loss = 1.509631633758545 24600
val loss = 2.0710480213165283
training loss = 1.5096094608306885 24700
val loss = 2.0609757900238037
training loss = 1.5096365213394165 24800
val loss = 2.0816001892089844
training loss = 1.5094776153564453 24900
val loss = 2.0687098503112793
training loss = 1.5182085037231445 25000
val loss = 2.200021505355835
training loss = 1.5093923807144165 25100
val loss = 2.0672707557678223
training loss = 1.5093510150909424 25200
val loss = 2.0727741718292236
training loss = 1.5093737840652466 25300
val loss = 2.0566296577453613
training loss = 1.5092447996139526 25400
val loss = 2.066035032272339
training loss = 1.5202608108520508 25500
val loss = 1.9441659450531006
training loss = 1.509178638458252 25600
val loss = 2.0631394386291504
training loss = 1.5091089010238647 25700
val loss = 2.064708948135376
training loss = 1.5415178537368774 25800
val loss = 1.8797516822814941
training loss = 1.5090291500091553 25900
val loss = 2.0629966259002686
training loss = 1.5089678764343262 26000
val loss = 2.064105749130249
training loss = 1.509190559387207 26100
val loss = 2.0431063175201416
training loss = 1.5088846683502197 26200
val loss = 2.062452554702759
training loss = 1.5909699201583862 26300
val loss = 1.8163933753967285
training loss = 1.5088276863098145 26400
val loss = 2.059418201446533
training loss = 1.5087571144104004 26500
val loss = 2.060636281967163
training loss = 1.5800806283950806 26600
val loss = 1.8257532119750977
training loss = 1.5086983442306519 26700
val loss = 2.0559377670288086
training loss = 1.50862455368042 26800
val loss = 2.0591700077056885
training loss = 1.5167183876037598 26900
val loss = 2.184051275253296
training loss = 1.508556604385376 27000
val loss = 2.058408498764038
training loss = 1.508498191833496 27100
val loss = 2.0604772567749023
training loss = 1.5085275173187256 27200
val loss = 2.065209150314331
training loss = 1.5084192752838135 27300
val loss = 2.0568275451660156
training loss = 1.6582766771316528 27400
val loss = 1.7820037603378296
training loss = 1.50838303565979 27500
val loss = 2.0591049194335938
training loss = 1.508306860923767 27600
val loss = 2.055402994155884
training loss = 1.5083142518997192 27700
val loss = 2.043506145477295
training loss = 1.5082391500473022 27800
val loss = 2.0518651008605957
training loss = 1.508169174194336 27900
val loss = 2.053846836090088
training loss = 1.5085713863372803 28000
val loss = 2.028034210205078
training loss = 1.5081084966659546 28100
val loss = 2.05334734916687
training loss = 1.5080654621124268 28200
val loss = 2.0579493045806885
training loss = 1.5081502199172974 28300
val loss = 2.0646839141845703
training loss = 1.5079792737960815 28400
val loss = 2.051344156265259
training loss = 1.5490047931671143 28500
val loss = 2.363236904144287
training loss = 1.507922649383545 28600
val loss = 2.048330307006836
training loss = 1.5078495740890503 28700
val loss = 2.049898624420166
training loss = 1.5107927322387695 28800
val loss = 2.121663808822632
training loss = 1.5077860355377197 28900
val loss = 2.049351215362549
training loss = 1.5124802589416504 29000
val loss = 2.1423728466033936
training loss = 1.507771372795105 29100
val loss = 2.0563204288482666
training loss = 1.5076563358306885 29200
val loss = 2.0477218627929688
training loss = 1.548870325088501 29300
val loss = 1.8482862710952759
training loss = 1.507590413093567 29400
val loss = 2.0454020500183105
training loss = 1.5075154304504395 29500
val loss = 2.0446715354919434
training loss = 1.5077160596847534 29600
val loss = 2.0284059047698975
training loss = 1.5074468851089478 29700
val loss = 2.0454537868499756
training loss = 1.5528581142425537 29800
val loss = 1.839577078819275
training loss = 1.5073754787445068 29900
val loss = 2.0444729328155518
training loss = 1.507317304611206 30000
val loss = 2.0495004653930664
reduced chi^2 level 2 = 1.5073193311691284
Constrained alpha: 1.940210223197937
Constrained beta: 1.5238584280014038
Constrained gamma: 11.524608612060547
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 901.4136,  847.5767,  955.8074,  913.7283,  999.0438, 1057.6018,
        1097.7543, 1166.6301, 1121.1002, 1145.4827, 1168.4868, 1218.5929,
        1240.9518, 1237.4469, 1327.6885, 1402.8933, 1456.0889, 1473.3707,
        1546.9104, 1550.9110, 1640.1504, 1542.2633, 1546.2239, 1618.3801,
        1611.0233, 1767.8745, 1547.0953, 1736.4426, 1705.5171, 1684.9506,
        1682.1989, 1743.7090, 1737.6019, 1711.6725, 1785.8656, 1776.7904,
        1584.6851, 1625.4093, 1609.6753, 1575.9684, 1639.1321, 1539.7850,
        1552.8990, 1482.6458, 1418.9692, 1344.4958, 1264.2367, 1234.4930,
        1199.2112, 1143.8029, 1094.3553, 1014.1566,  957.6137,  905.2280,
         888.5488,  913.7112,  845.8503,  706.9810,  585.2099,  511.2192,
         539.3318,  488.7107,  421.1449,  370.3633,  361.9705,  313.3531,
         257.9672,  245.0194,  223.8833,  161.4693,  160.1376,  153.9889,
         148.6718,   91.2334,  106.2611,   55.3254,   46.1018,   36.5157,
          45.4208,   41.7843,   22.3680,   34.7931,   41.0286])]
2726.6532280928636
3.5747380740654897 15.120309336886846 41.50465752497243
val isze = 8
idinces = [31 10 73  5 61 28 64 32 63 49  2 60  3 80 66 42 78 41 30 48 24 82 67 55
 79 57 25 58 36 23 59  0 69 72 47  4 29 27  8 33 71 19 44 74 35 16 62 14
 56 54  9 46 53 37 70 22 75 17 52 81 40 65 76 50 15 39 34 51 12 20  7 45
 38  1 21 77  6 11 13 43 68 18 26]
we are doing training validation split
training loss = 415.8990478515625 100
val loss = 364.98248291015625
training loss = 7.395084857940674 200
val loss = 3.9980568885803223
training loss = 7.3203206062316895 300
val loss = 3.7761192321777344
training loss = 7.287189483642578 400
val loss = 3.777086019515991
training loss = 7.251373767852783 500
val loss = 3.7781436443328857
training loss = 7.213932514190674 600
val loss = 3.778827667236328
training loss = 7.175696849822998 700
val loss = 3.7789058685302734
training loss = 7.13730001449585 800
val loss = 3.777963161468506
training loss = 7.099218845367432 900
val loss = 3.775871992111206
training loss = 7.061779975891113 1000
val loss = 3.7723090648651123
training loss = 7.0251970291137695 1100
val loss = 3.767125129699707
training loss = 6.989593505859375 1200
val loss = 3.760314702987671
training loss = 6.955008506774902 1300
val loss = 3.7518742084503174
training loss = 6.921450138092041 1400
val loss = 3.7419776916503906
training loss = 6.888890266418457 1500
val loss = 3.730836868286133
training loss = 6.857283115386963 1600
val loss = 3.718679428100586
training loss = 6.826573848724365 1700
val loss = 3.7057557106018066
training loss = 6.796695709228516 1800
val loss = 3.692319393157959
training loss = 6.767586708068848 1900
val loss = 3.6785449981689453
training loss = 6.739191055297852 2000
val loss = 3.6646809577941895
training loss = 6.71143102645874 2100
val loss = 3.6507925987243652
training loss = 6.684247970581055 2200
val loss = 3.637058734893799
training loss = 6.657573223114014 2300
val loss = 3.6233901977539062
training loss = 6.631335735321045 2400
val loss = 3.609933614730835
training loss = 6.605470180511475 2500
val loss = 3.596651554107666
training loss = 6.579910755157471 2600
val loss = 3.583531379699707
training loss = 6.5545878410339355 2700
val loss = 3.570573329925537
training loss = 6.529454708099365 2800
val loss = 3.557734966278076
training loss = 6.504477024078369 2900
val loss = 3.5450851917266846
training loss = 6.479650974273682 3000
val loss = 3.532581329345703
training loss = 6.45504093170166 3100
val loss = 3.5203490257263184
training loss = 6.430785179138184 3200
val loss = 3.5085301399230957
training loss = 6.407137870788574 3300
val loss = 3.4974355697631836
training loss = 6.384444236755371 3400
val loss = 3.4875845909118652
training loss = 6.3630266189575195 3500
val loss = 3.479495048522949
training loss = 6.347095489501953 3600
val loss = 3.4135994911193848
training loss = 6.324321269989014 3700
val loss = 3.46863055229187
training loss = 6.3035783767700195 3800
val loss = 3.4474990367889404
training loss = 6.2711615562438965 3900
val loss = 3.457036018371582
training loss = 6.121898651123047 4000
val loss = 3.327789783477783
training loss = 4.641709804534912 4100
val loss = 2.7660322189331055
training loss = 2.7984681129455566 4200
val loss = 1.1506757736206055
training loss = 2.341388702392578 4300
val loss = 1.0624417066574097
training loss = 2.3246777057647705 4400
val loss = 1.0189061164855957
training loss = 2.2990081310272217 4500
val loss = 0.9643893241882324
training loss = 2.276843786239624 4600
val loss = 0.9300965070724487
training loss = 2.258267879486084 4700
val loss = 0.9136865735054016
training loss = 2.239043712615967 4800
val loss = 0.8507469892501831
training loss = 2.222142219543457 4900
val loss = 0.8159273266792297
training loss = 2.20792293548584 5000
val loss = 0.7793551683425903
training loss = 2.1947073936462402 5100
val loss = 0.7449642419815063
training loss = 2.183659791946411 5200
val loss = 0.7189972400665283
training loss = 2.174081325531006 5300
val loss = 0.6927729249000549
training loss = 2.373277425765991 5400
val loss = 0.6952093839645386
training loss = 2.1583473682403564 5500
val loss = 0.6483920812606812
training loss = 2.1519176959991455 5600
val loss = 0.6313610672950745
training loss = 2.1468722820281982 5700
val loss = 0.6203229427337646
training loss = 2.142103672027588 5800
val loss = 0.6021350622177124
training loss = 2.2159101963043213 5900
val loss = 0.5687500238418579
training loss = 2.1347458362579346 6000
val loss = 0.5792316794395447
training loss = 2.1316733360290527 6100
val loss = 0.5696433782577515
training loss = 2.129746675491333 6200
val loss = 0.5530428886413574
training loss = 2.126741886138916 6300
val loss = 0.5542470216751099
training loss = 2.1804397106170654 6400
val loss = 0.5279331207275391
training loss = 2.1229405403137207 6500
val loss = 0.543187141418457
training loss = 2.121289014816284 6600
val loss = 0.5374598503112793
training loss = 2.120157480239868 6700
val loss = 0.5366222262382507
training loss = 2.118687391281128 6800
val loss = 0.5303454399108887
training loss = 2.1175012588500977 6900
val loss = 0.5267694592475891
training loss = 2.1194498538970947 7000
val loss = 0.5425029993057251
training loss = 2.1155364513397217 7100
val loss = 0.5216482877731323
training loss = 2.1310315132141113 7200
val loss = 0.49868983030319214
training loss = 2.1139047145843506 7300
val loss = 0.5185103416442871
training loss = 2.1131246089935303 7400
val loss = 0.5164621472358704
training loss = 2.11362886428833 7500
val loss = 0.5251810550689697
training loss = 2.1118388175964355 7600
val loss = 0.5146263837814331
training loss = 2.111426830291748 7700
val loss = 0.5178875923156738
training loss = 2.110759973526001 7800
val loss = 0.5109662413597107
training loss = 2.110135555267334 7900
val loss = 0.5129934549331665
training loss = 2.1106579303741455 8000
val loss = 0.5045967698097229
training loss = 2.109156847000122 8100
val loss = 0.5132764577865601
training loss = 2.1403355598449707 8200
val loss = 0.49503424763679504
training loss = 2.1082470417022705 8300
val loss = 0.514248251914978
training loss = 2.1077494621276855 8400
val loss = 0.5154179334640503
training loss = 2.107424259185791 8500
val loss = 0.5183180570602417
training loss = 2.1068522930145264 8600
val loss = 0.5187848210334778
training loss = 2.1063241958618164 8700
val loss = 0.5206514596939087
training loss = 2.1060330867767334 8800
val loss = 0.5194262266159058
training loss = 2.1052603721618652 8900
val loss = 0.5261439085006714
training loss = 2.2516684532165527 9000
val loss = 0.5706843137741089
training loss = 2.1039648056030273 9100
val loss = 0.5330525636672974
training loss = 2.103126287460327 9200
val loss = 0.5370188355445862
training loss = 2.1031270027160645 9300
val loss = 0.5498018264770508
training loss = 2.1012821197509766 9400
val loss = 0.544760525226593
training loss = 2.168565273284912 9500
val loss = 0.5412800312042236
training loss = 2.0990378856658936 9600
val loss = 0.548302412033081
training loss = 2.097707986831665 9700
val loss = 0.549784779548645
training loss = 2.0977048873901367 9800
val loss = 0.5390592813491821
training loss = 2.0953376293182373 9900
val loss = 0.5457755327224731
training loss = 2.0941057205200195 10000
val loss = 0.5420044660568237
training loss = 2.0931811332702637 10100
val loss = 0.5355550646781921
training loss = 2.092137098312378 10200
val loss = 0.5310095548629761
training loss = 2.091888666152954 10300
val loss = 0.5316086411476135
training loss = 2.0905821323394775 10400
val loss = 0.5205036401748657
training loss = 2.0897984504699707 10500
val loss = 0.5154646635055542
training loss = 2.0906567573547363 10600
val loss = 0.5019019842147827
training loss = 2.0885512828826904 10700
val loss = 0.5076429843902588
training loss = 2.1597766876220703 10800
val loss = 0.6549612283706665
training loss = 2.0873804092407227 10900
val loss = 0.5018101930618286
training loss = 2.0871875286102295 11000
val loss = 0.4933907985687256
training loss = 2.086238145828247 11100
val loss = 0.49884524941444397
training loss = 2.085568904876709 11200
val loss = 0.4959559142589569
training loss = 2.085179328918457 11300
val loss = 0.4922700822353363
training loss = 2.084397554397583 11400
val loss = 0.4943334460258484
training loss = 2.1474850177764893 11500
val loss = 0.4847976565361023
training loss = 2.0831947326660156 11600
val loss = 0.49249976873397827
training loss = 2.082489013671875 11700
val loss = 0.49240386486053467
training loss = 2.0845232009887695 11800
val loss = 0.48038679361343384
training loss = 2.0811877250671387 11900
val loss = 0.4919608235359192
training loss = 2.104477882385254 12000
val loss = 0.4704703092575073
training loss = 2.0798258781433105 12100
val loss = 0.4925311803817749
training loss = 2.0834457874298096 12200
val loss = 0.476931631565094
training loss = 2.078420877456665 12300
val loss = 0.49262481927871704
training loss = 2.0776216983795166 12400
val loss = 0.49180418252944946
training loss = 2.0780649185180664 12500
val loss = 0.48404040932655334
training loss = 2.076171875 12600
val loss = 0.49243026971817017
training loss = 2.1223180294036865 12700
val loss = 0.47831812500953674
training loss = 2.074651002883911 12800
val loss = 0.4930906593799591
training loss = 2.0737898349761963 12900
val loss = 0.492656409740448
training loss = 2.075556993484497 13000
val loss = 0.510576605796814
training loss = 2.072242259979248 13100
val loss = 0.4938027858734131
training loss = 2.0713343620300293 13200
val loss = 0.49270352721214294
training loss = 2.0707788467407227 13300
val loss = 0.49840617179870605
training loss = 2.069690704345703 13400
val loss = 0.4942936599254608
training loss = 2.093170642852783 13500
val loss = 0.47482091188430786
training loss = 2.067971706390381 13600
val loss = 0.4955204725265503
training loss = 2.0670924186706543 13700
val loss = 0.4979861378669739
training loss = 2.066286563873291 13800
val loss = 0.4941285252571106
training loss = 2.0652482509613037 13900
val loss = 0.496145099401474
training loss = 2.1298954486846924 14000
val loss = 0.6385307908058167
training loss = 2.0634608268737793 14100
val loss = 0.4971104860305786
training loss = 2.062427520751953 14200
val loss = 0.49734264612197876
training loss = 2.1117794513702393 14300
val loss = 0.4840276837348938
training loss = 2.0605127811431885 14400
val loss = 0.49867990612983704
training loss = 2.0594117641448975 14500
val loss = 0.4988749623298645
training loss = 2.05879807472229 14600
val loss = 0.5052490830421448
training loss = 2.057399034500122 14700
val loss = 0.4999380111694336
training loss = 2.074787139892578 14800
val loss = 0.4791024625301361
training loss = 2.055337905883789 14900
val loss = 0.501329243183136
training loss = 2.0541975498199463 15000
val loss = 0.4991772174835205
training loss = 2.0532515048980713 15100
val loss = 0.5060054659843445
training loss = 2.051947832107544 15200
val loss = 0.5033184289932251
training loss = 2.059391736984253 15300
val loss = 0.5422322750091553
training loss = 2.049816608428955 15400
val loss = 0.5056859850883484
training loss = 2.0485422611236572 15500
val loss = 0.5060725212097168
training loss = 2.0505964756011963 15600
val loss = 0.5285423398017883
training loss = 2.0461807250976562 15700
val loss = 0.5085827112197876
training loss = 2.4190502166748047 15800
val loss = 1.0827207565307617
training loss = 2.0437498092651367 15900
val loss = 0.5124786496162415
training loss = 2.0423343181610107 16000
val loss = 0.5120292901992798
training loss = 2.041644811630249 16100
val loss = 0.5091952085494995
training loss = 2.0398612022399902 16200
val loss = 0.5155069231987
training loss = 2.063842535018921 16300
val loss = 0.5919726490974426
training loss = 2.0372939109802246 16400
val loss = 0.5185554027557373
training loss = 2.0358169078826904 16500
val loss = 0.5207904577255249
training loss = 2.044755697250366 16600
val loss = 0.5040083527565002
training loss = 2.033141613006592 16700
val loss = 0.5255469679832458
training loss = 2.031693935394287 16800
val loss = 0.530783474445343
training loss = 2.030581474304199 16900
val loss = 0.5341989398002625
training loss = 2.028964042663574 17000
val loss = 0.5330400466918945
training loss = 2.093137502670288 17100
val loss = 0.522449254989624
training loss = 2.0261926651000977 17200
val loss = 0.5383440852165222
training loss = 2.024613380432129 17300
val loss = 0.5420697927474976
training loss = 2.023488759994507 17400
val loss = 0.5445698499679565
training loss = 2.0219428539276123 17500
val loss = 0.5493062138557434
training loss = 2.0463500022888184 17600
val loss = 0.5289401412010193
training loss = 2.019321918487549 17700
val loss = 0.555335283279419
training loss = 2.017796754837036 17800
val loss = 0.5607655644416809
training loss = 2.122974395751953 17900
val loss = 0.7794491052627563
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 870.7702,  933.1476,  961.9413,  968.6014, 1070.3573, 1055.8878,
        1108.2828, 1081.2955, 1150.4098, 1172.4167, 1237.7471, 1176.1191,
        1278.1630, 1241.2814, 1363.9425, 1472.8940, 1365.3198, 1441.6713,
        1547.7787, 1511.6917, 1535.2896, 1553.9650, 1644.0010, 1665.6115,
        1586.3617, 1747.3113, 1622.1952, 1687.3400, 1747.5183, 1779.6433,
        1651.0353, 1699.9546, 1725.6869, 1683.8041, 1667.5988, 1780.8820,
        1673.5856, 1608.1105, 1695.5345, 1632.1238, 1632.7085, 1604.7732,
        1555.0730, 1481.8218, 1334.2548, 1394.0653, 1374.1610, 1228.8607,
        1270.2190, 1178.5834, 1083.0605,  980.1193,  901.3924,  925.9285,
         861.5997,  798.1661,  807.1418,  678.6100,  613.3844,  589.5176,
         552.1971,  472.6162,  445.6696,  419.4671,  378.9026,  337.5298,
         280.9589,  242.1012,  208.7362,  163.8657,  167.0746,  162.9292,
         143.9534,   90.9255,   88.3599,   71.9350,   49.0855,   54.8526,
          31.2180,   41.3325,   21.7115,   23.0228,   26.6975])]
2522.5075850419903
2.812727583013943 9.357370543457899 58.26045926323705
val isze = 8
idinces = [16 43  3 42  4 31 67 57 50 11  9 70 14  6 10 55 62 74 27 21 58 48 39  5
 68 49 36 59 63 47 45 46 38 65 51 73 72 12 13 53 37 64 35 54 81 79 23  8
 34  1 26 61 80 22 60 75 24 20 71 15 29  2 19 33 77 25 66 40 78 52 69  7
 30 32 76 41 56 82 44 17 28 18  0]
we are doing training validation split
training loss = 54.71976852416992 100
val loss = 81.91102600097656
training loss = 10.303457260131836 200
val loss = 15.02352237701416
training loss = 8.995920181274414 300
val loss = 12.358586311340332
training loss = 7.997980117797852 400
val loss = 10.214862823486328
training loss = 7.278924465179443 500
val loss = 8.532068252563477
training loss = 6.7755126953125 600
val loss = 7.239012718200684
training loss = 6.428097248077393 700
val loss = 6.253787517547607
training loss = 6.189681529998779 800
val loss = 5.504983901977539
training loss = 6.025740623474121 900
val loss = 4.935639381408691
training loss = 5.911801815032959 1000
val loss = 4.502209663391113
training loss = 5.830831050872803 1100
val loss = 4.171825408935547
training loss = 5.771145820617676 1200
val loss = 3.9197838306427
training loss = 5.724864959716797 1300
val loss = 3.7276089191436768
training loss = 5.686744213104248 1400
val loss = 3.5812106132507324
training loss = 5.6533589363098145 1500
val loss = 3.4699716567993164
training loss = 5.62252950668335 1600
val loss = 3.3856210708618164
training loss = 5.5928955078125 1700
val loss = 3.321746349334717
training loss = 5.563638210296631 1800
val loss = 3.2733383178710938
training loss = 5.534287452697754 1900
val loss = 3.2364630699157715
training loss = 5.50458288192749 2000
val loss = 3.208008289337158
training loss = 5.474409103393555 2100
val loss = 3.1856064796447754
training loss = 5.4437174797058105 2200
val loss = 3.1674227714538574
training loss = 5.412513732910156 2300
val loss = 3.152026891708374
training loss = 5.380800247192383 2400
val loss = 3.136632204055786
training loss = 5.348531246185303 2500
val loss = 3.1192195415496826
training loss = 5.315559387207031 2600
val loss = 3.0994510650634766
training loss = 5.281510353088379 2700
val loss = 3.077195644378662
training loss = 5.245364665985107 2800
val loss = 3.0541932582855225
training loss = 5.204378604888916 2900
val loss = 3.025660514831543
training loss = 5.150111675262451 3000
val loss = 3.0110883712768555
training loss = 5.04926061630249 3100
val loss = 2.983582019805908
training loss = 4.744457244873047 3200
val loss = 2.9720380306243896
training loss = 4.1087446212768555 3300
val loss = 2.8748297691345215
training loss = 3.212963581085205 3400
val loss = 2.221261501312256
training loss = 2.0324478149414062 3500
val loss = 1.4159224033355713
training loss = 1.608313798904419 3600
val loss = 0.9529412984848022
training loss = 1.5607271194458008 3700
val loss = 0.889538049697876
training loss = 1.5385383367538452 3800
val loss = 0.8906666040420532
training loss = 1.523309588432312 3900
val loss = 0.8994086384773254
training loss = 1.5119680166244507 4000
val loss = 0.9061942100524902
training loss = 1.518431305885315 4100
val loss = 0.9357122182846069
training loss = 1.497287392616272 4200
val loss = 0.9206722974777222
training loss = 1.5326341390609741 4300
val loss = 0.9637064337730408
training loss = 1.4888051748275757 4400
val loss = 0.9313043355941772
training loss = 1.485849380493164 4500
val loss = 0.9351769685745239
training loss = 1.4836971759796143 4600
val loss = 0.9385600090026855
training loss = 1.4817944765090942 4700
val loss = 0.9407622814178467
training loss = 1.4832346439361572 4800
val loss = 0.945453941822052
training loss = 1.479066014289856 4900
val loss = 0.9443584680557251
training loss = 1.5044852495193481 5000
val loss = 0.9795916676521301
training loss = 1.4771169424057007 5100
val loss = 0.9467129111289978
training loss = 1.4797250032424927 5200
val loss = 0.9491024017333984
training loss = 1.475625991821289 5300
val loss = 0.9482846260070801
training loss = 1.474979043006897 5400
val loss = 0.9486446380615234
training loss = 1.4872634410858154 5500
val loss = 0.9618303775787354
training loss = 1.4738209247589111 5600
val loss = 0.9496974945068359
training loss = 1.4732815027236938 5700
val loss = 0.9502062797546387
training loss = 1.4729013442993164 5800
val loss = 0.9516394138336182
training loss = 1.472264051437378 5900
val loss = 0.9509851336479187
training loss = 1.4731382131576538 6000
val loss = 0.9535017013549805
training loss = 1.4713133573532104 6100
val loss = 0.951858401298523
training loss = 1.4743942022323608 6200
val loss = 0.9585399627685547
training loss = 1.470442771911621 6300
val loss = 0.9530559778213501
training loss = 1.4699602127075195 6400
val loss = 0.9530841112136841
training loss = 1.47018563747406 6500
val loss = 0.9553923010826111
training loss = 1.469119906425476 6600
val loss = 0.9538867473602295
training loss = 1.4802278280258179 6700
val loss = 0.962862491607666
training loss = 1.4683218002319336 6800
val loss = 0.9547296166419983
training loss = 1.470163345336914 6900
val loss = 0.955817461013794
training loss = 1.4676032066345215 7000
val loss = 0.9553828239440918
training loss = 1.467177391052246 7100
val loss = 0.9557992815971375
training loss = 1.4678434133529663 7200
val loss = 0.9561253190040588
training loss = 1.4664795398712158 7300
val loss = 0.9564775228500366
training loss = 1.5680967569351196 7400
val loss = 1.07939875125885
training loss = 1.465821385383606 7500
val loss = 0.9571490287780762
training loss = 1.465476155281067 7600
val loss = 0.9575151801109314
training loss = 1.4655826091766357 7700
val loss = 0.9576122760772705
training loss = 1.4648672342300415 7800
val loss = 0.9580835103988647
training loss = 1.8410900831222534 7900
val loss = 1.4064126014709473
training loss = 1.4643009901046753 8000
val loss = 0.9587588906288147
training loss = 1.4639859199523926 8100
val loss = 0.9589873552322388
training loss = 1.473580002784729 8200
val loss = 0.9691119194030762
training loss = 1.4634534120559692 8300
val loss = 0.9594497680664062
training loss = 1.463163137435913 8400
val loss = 0.9598616361618042
training loss = 1.463088035583496 8500
val loss = 0.9600937366485596
training loss = 1.462656855583191 8600
val loss = 0.9602821469306946
training loss = 1.506000280380249 8700
val loss = 1.0002000331878662
reduced chi^2 level 2 = 1.4630645513534546
Constrained alpha: 1.9270061254501343
Constrained beta: 4.428769588470459
Constrained gamma: 22.586353302001953
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 860.8533,  866.8799,  943.0659,  937.9341,  991.1944, 1040.3125,
        1132.2273, 1147.0376, 1127.8925, 1130.2289, 1224.6063, 1232.6959,
        1162.6285, 1195.2146, 1343.8380, 1465.1324, 1370.6144, 1452.3855,
        1650.9592, 1524.0266, 1584.2977, 1510.1429, 1632.5437, 1680.4302,
        1689.4894, 1711.4645, 1654.1034, 1723.1018, 1740.8928, 1675.0552,
        1602.9672, 1782.2748, 1689.9414, 1732.8810, 1704.1815, 1739.5083,
        1645.0879, 1608.8301, 1596.4363, 1644.2035, 1614.7156, 1514.0015,
        1468.7798, 1493.1216, 1387.9563, 1335.1969, 1346.7728, 1284.8073,
        1180.4497, 1177.9648, 1085.7698, 1053.8582,  924.2700,  946.3748,
         852.2906,  845.9614,  832.4558,  698.5633,  634.6440,  533.2214,
         519.4824,  488.3731,  407.7164,  389.2528,  363.0076,  314.2357,
         320.5506,  218.4600,  197.8600,  170.6749,  168.6262,  148.3607,
         121.6031,  102.3134,   98.9149,   60.9243,   48.7902,   24.6327,
          34.6903,   45.0414,   25.4631,   41.2104,   37.3041])]
2557.5715942608495
0.11933371078116983 4.960765508467693 76.8830031469197
val isze = 8
idinces = [ 7 23 71 73 37 76  6 19 18  8 29 11 77 70 42 53 61  3 35 49 64 20 45 52
 65 74 68 57 36 30 69 25 56 21 27 24 66 33 39 50 79 46  4 63 16 54  5 38
 60 13 75  0 78 59 51 31 44 41 28 10 67 26 82 12 34 14 40 72 62 17 32 15
 55 22 58 81 47 80  2 48 43  9  1]
we are doing training validation split
training loss = 15.053832054138184 100
val loss = 6.889488697052002
training loss = 12.114324569702148 200
val loss = 4.845798015594482
training loss = 10.885090827941895 300
val loss = 3.7945516109466553
training loss = 9.773172378540039 400
val loss = 3.007233142852783
training loss = 8.828141212463379 500
val loss = 2.508535861968994
training loss = 8.054708480834961 600
val loss = 2.2670416831970215
training loss = 7.43830680847168 700
val loss = 2.2314324378967285
training loss = 6.957441806793213 800
val loss = 2.347791910171509
training loss = 6.589410781860352 900
val loss = 2.5674471855163574
training loss = 6.312649726867676 1000
val loss = 2.8489129543304443
training loss = 6.107668399810791 1100
val loss = 3.1584503650665283
training loss = 5.957406520843506 1200
val loss = 3.469355583190918
training loss = 5.847222328186035 1300
val loss = 3.7612664699554443
training loss = 5.7647600173950195 1400
val loss = 4.018991470336914
training loss = 5.699671268463135 1500
val loss = 4.231166362762451
training loss = 5.643267631530762 1600
val loss = 4.389315128326416
training loss = 5.588161468505859 1700
val loss = 4.486672401428223
training loss = 5.528042793273926 1800
val loss = 4.5184221267700195
training loss = 5.457696914672852 1900
val loss = 4.483006954193115
training loss = 5.373146057128906 2000
val loss = 4.385303020477295
training loss = 5.271257400512695 2100
val loss = 4.23848295211792
training loss = 5.148283004760742 2200
val loss = 4.0579962730407715
training loss = 4.998274803161621 2300
val loss = 3.848987102508545
training loss = 4.812506198883057 2400
val loss = 3.6007463932037354
training loss = 4.579661846160889 2500
val loss = 3.293018341064453
training loss = 4.2867302894592285 2600
val loss = 2.9081785678863525
training loss = 3.9229376316070557 2700
val loss = 2.444822311401367
training loss = 3.4918711185455322 2800
val loss = 1.9402066469192505
training loss = 3.034668207168579 2900
val loss = 1.4951612949371338
training loss = 2.64064621925354 3000
val loss = 1.2517307996749878
training loss = 2.392543077468872 3100
val loss = 1.26347017288208
training loss = 2.2954752445220947 3200
val loss = 1.2750968933105469
training loss = 2.242508888244629 3300
val loss = 1.527965784072876
training loss = 2.2239816188812256 3400
val loss = 1.5833547115325928
training loss = 2.2121505737304688 3500
val loss = 1.6181259155273438
training loss = 2.202922821044922 3600
val loss = 1.638105869293213
training loss = 2.1957547664642334 3700
val loss = 1.6209046840667725
training loss = 2.188544750213623 3800
val loss = 1.6502916812896729
training loss = 2.1821022033691406 3900
val loss = 1.6489789485931396
training loss = 2.17561411857605 4000
val loss = 1.6558756828308105
training loss = 2.1681206226348877 4100
val loss = 1.6464195251464844
training loss = 2.1609339714050293 4200
val loss = 1.6905527114868164
training loss = 2.1479969024658203 4300
val loss = 1.6308127641677856
training loss = 2.1822526454925537 4400
val loss = 1.3955957889556885
training loss = 2.1189005374908447 4500
val loss = 1.6069185733795166
training loss = 2.102421522140503 4600
val loss = 1.606614112854004
training loss = 2.087458848953247 4700
val loss = 1.5881375074386597
training loss = 2.073375940322876 4800
val loss = 1.6141586303710938
training loss = 2.1004786491394043 4900
val loss = 1.4203341007232666
training loss = 2.0511398315429688 5000
val loss = 1.6279692649841309
training loss = 2.041440963745117 5100
val loss = 1.6335852146148682
training loss = 2.033029794692993 5200
val loss = 1.627401351928711
training loss = 2.0243024826049805 5300
val loss = 1.6462218761444092
training loss = 2.067192316055298 5400
val loss = 1.9665029048919678
training loss = 2.0084407329559326 5500
val loss = 1.6527822017669678
training loss = 2.000774383544922 5600
val loss = 1.6515474319458008
training loss = 1.9941370487213135 5700
val loss = 1.6691725254058838
training loss = 1.987373948097229 5800
val loss = 1.6645493507385254
training loss = 1.9848263263702393 5900
val loss = 1.6042619943618774
training loss = 1.9760249853134155 6000
val loss = 1.6733014583587646
training loss = 2.2248377799987793 6100
val loss = 2.5086872577667236
training loss = 1.9667346477508545 6200
val loss = 1.6772899627685547
training loss = 1.9626070261001587 6300
val loss = 1.6866843700408936
training loss = 1.961466908454895 6400
val loss = 1.7446619272232056
training loss = 1.9563928842544556 6500
val loss = 1.694561243057251
training loss = 1.9536319971084595 6600
val loss = 1.700063705444336
training loss = 1.958998680114746 6700
val loss = 1.8110020160675049
training loss = 1.9495776891708374 6800
val loss = 1.7084275484085083
training loss = 1.9479764699935913 6900
val loss = 1.726628065109253
training loss = 1.9467161893844604 7000
val loss = 1.7248618602752686
training loss = 1.945380449295044 7100
val loss = 1.7189170122146606
training loss = 1.9452828168869019 7200
val loss = 1.7534245252609253
training loss = 1.943679690361023 7300
val loss = 1.7253156900405884
training loss = 1.9843884706497192 7400
val loss = 2.005247116088867
training loss = 1.9425419569015503 7500
val loss = 1.7324473857879639
training loss = 1.9419891834259033 7600
val loss = 1.7323311567306519
training loss = 1.9931334257125854 7700
val loss = 2.0536961555480957
training loss = 1.941369891166687 7800
val loss = 1.7333476543426514
training loss = 1.9410115480422974 7900
val loss = 1.7375450134277344
training loss = 2.366175651550293 8000
val loss = 2.9086952209472656
training loss = 1.9406521320343018 8100
val loss = 1.7377432584762573
training loss = 1.9403679370880127 8200
val loss = 1.741246223449707
reduced chi^2 level 2 = 1.9403533935546875
Constrained alpha: 1.8168052434921265
Constrained beta: 3.043224334716797
Constrained gamma: 27.180116653442383
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 854.9496,  859.0669,  934.0023,  883.2025, 1014.4364, 1074.9325,
        1105.7186, 1129.9160, 1205.4135, 1198.2258, 1269.6805, 1177.2919,
        1234.9653, 1251.5773, 1391.7046, 1391.7645, 1417.5973, 1464.7189,
        1478.7856, 1528.3733, 1638.6562, 1558.4980, 1547.6923, 1591.8719,
        1666.6943, 1695.9021, 1580.2523, 1738.8335, 1731.0549, 1685.5364,
        1619.5940, 1697.6056, 1679.8912, 1768.3011, 1688.9509, 1696.8302,
        1668.1843, 1672.9980, 1601.3794, 1615.3872, 1626.0107, 1564.1315,
        1522.9209, 1497.4287, 1399.8853, 1401.2700, 1262.4274, 1188.5933,
        1195.9980, 1147.2238, 1145.7173,  978.7585,  946.6839,  915.9959,
         899.3917,  856.1248,  808.7927,  670.2615,  632.0262,  533.9389,
         557.6478,  479.1084,  431.0864,  402.8499,  337.7568,  339.1371,
         313.6707,  229.0873,  215.7724,  183.0343,  144.4689,  169.9739,
         153.8246,  106.6289,   94.9265,   64.7855,   54.0461,   40.6147,
          38.5307,   30.6957,   18.9066,   38.8165,   31.4590])]
2457.648261603936
0.15803265948582557 7.965794147530869 8.121017905528872
val isze = 8
idinces = [46 15 67 71 66  3 65 41 33 81 82 80 24 37 29 21 45 74 78  9 50 56  1 62
 35 69 31  0 17 30 68 77 55 61 52 10 58 75 53 18 14 38 34 26 16 73 43 79
 27 11 63 51 40 64 20  6 76 70 32  7 59 13 25 60  2 49 22 48  5 42 19 44
  8  4 36 57 23 72 12 54 39 47 28]
we are doing training validation split
training loss = 12.651740074157715 100
val loss = 7.758408069610596
training loss = 6.782799243927002 200
val loss = 8.552972793579102
training loss = 6.574942588806152 300
val loss = 8.229904174804688
training loss = 6.370063304901123 400
val loss = 7.937577247619629
training loss = 6.181395053863525 500
val loss = 7.685762882232666
training loss = 6.015127182006836 600
val loss = 7.48339319229126
training loss = 5.872993469238281 700
val loss = 7.331209659576416
training loss = 5.754138469696045 800
val loss = 7.225106239318848
training loss = 5.656321048736572 900
val loss = 7.158559799194336
training loss = 5.576725959777832 1000
val loss = 7.123924732208252
training loss = 5.512414455413818 1100
val loss = 7.113877773284912
training loss = 5.460598468780518 1200
val loss = 7.121633529663086
training loss = 5.41877555847168 1300
val loss = 7.141212463378906
training loss = 5.384791851043701 1400
val loss = 7.167832851409912
training loss = 5.35683012008667 1500
val loss = 7.197499752044678
training loss = 5.3334126472473145 1600
val loss = 7.227324962615967
training loss = 5.313344478607178 1700
val loss = 7.255114555358887
training loss = 5.295698165893555 1800
val loss = 7.2794084548950195
training loss = 5.279751777648926 1900
val loss = 7.2993974685668945
training loss = 5.264971733093262 2000
val loss = 7.31463098526001
training loss = 5.2509636878967285 2100
val loss = 7.325048446655273
training loss = 5.237448215484619 2200
val loss = 7.330862998962402
training loss = 5.2242350578308105 2300
val loss = 7.332465648651123
training loss = 5.2111968994140625 2400
val loss = 7.330361366271973
training loss = 5.198250770568848 2500
val loss = 7.325047492980957
training loss = 5.185355186462402 2600
val loss = 7.317058086395264
training loss = 5.172471523284912 2700
val loss = 7.306940078735352
training loss = 5.159578800201416 2800
val loss = 7.295125961303711
training loss = 5.146650314331055 2900
val loss = 7.281998634338379
training loss = 5.13363790512085 3000
val loss = 7.267881393432617
training loss = 5.120476722717285 3100
val loss = 7.25291633605957
training loss = 5.107052803039551 3200
val loss = 7.237381935119629
training loss = 5.093191623687744 3300
val loss = 7.221165657043457
training loss = 5.078590393066406 3400
val loss = 7.204230785369873
training loss = 5.062771797180176 3500
val loss = 7.186352252960205
training loss = 5.044896602630615 3600
val loss = 7.166881561279297
training loss = 5.023436069488525 3700
val loss = 7.1446709632873535
training loss = 4.995325088500977 3800
val loss = 7.117171764373779
training loss = 4.953612327575684 3900
val loss = 7.07806921005249
training loss = 4.880180835723877 4000
val loss = 7.009188175201416
training loss = 4.723139762878418 4100
val loss = 6.852787971496582
training loss = 4.349668025970459 4200
val loss = 6.465480327606201
training loss = 3.6129190921783447 4300
val loss = 5.743682861328125
training loss = 2.792191743850708 4400
val loss = 5.081513404846191
training loss = 2.070054054260254 4500
val loss = 4.58741569519043
training loss = 1.6501421928405762 4600
val loss = 4.428093910217285
training loss = 1.5292545557022095 4700
val loss = 4.471261978149414
training loss = 1.5004433393478394 4800
val loss = 4.450840950012207
training loss = 1.485315203666687 4900
val loss = 4.381511688232422
training loss = 1.4738874435424805 5000
val loss = 4.308143138885498
training loss = 1.464762806892395 5100
val loss = 4.242657661437988
training loss = 1.4573887586593628 5200
val loss = 4.186239242553711
training loss = 1.451398253440857 5300
val loss = 4.138088226318359
training loss = 1.446517825126648 5400
val loss = 4.097121715545654
training loss = 1.4444706439971924 5500
val loss = 4.09307861328125
training loss = 1.4393872022628784 5600
val loss = 4.034248352050781
training loss = 1.4386509656906128 5700
val loss = 4.045402526855469
training loss = 1.4347913265228271 5800
val loss = 3.9955272674560547
training loss = 1.43311607837677 5900
val loss = 3.9765563011169434
training loss = 1.4338665008544922 6000
val loss = 3.9991393089294434
training loss = 1.4305177927017212 6100
val loss = 3.9517219066619873
training loss = 1.4298961162567139 6200
val loss = 3.957828998565674
training loss = 1.4286640882492065 6300
val loss = 3.932361602783203
training loss = 1.4279769659042358 6400
val loss = 3.9282424449920654
training loss = 1.4290512800216675 6500
val loss = 3.889364719390869
training loss = 1.426780104637146 6600
val loss = 3.9172511100769043
training loss = 1.4362905025482178 6700
val loss = 3.8435513973236084
training loss = 1.4258201122283936 6800
val loss = 3.9059410095214844
training loss = 1.4254863262176514 6900
val loss = 3.9058170318603516
training loss = 1.4275141954421997 7000
val loss = 3.9428741931915283
training loss = 1.4248186349868774 7100
val loss = 3.8958797454833984
training loss = 1.4245578050613403 7200
val loss = 3.8980274200439453
training loss = 1.4256434440612793 7300
val loss = 3.8640670776367188
training loss = 1.424000859260559 7400
val loss = 3.89263653755188
training loss = 1.4238231182098389 7500
val loss = 3.8922085762023926
training loss = 1.4248193502426147 7600
val loss = 3.918109178543091
training loss = 1.4233429431915283 7700
val loss = 3.8881473541259766
training loss = 1.4232014417648315 7800
val loss = 3.8876144886016846
training loss = 1.4231189489364624 7900
val loss = 3.8740408420562744
training loss = 1.4227999448776245 8000
val loss = 3.8838088512420654
training loss = 1.4374936819076538 8100
val loss = 3.9921059608459473
training loss = 1.4224098920822144 8200
val loss = 3.8793506622314453
training loss = 1.4223010540008545 8300
val loss = 3.8801071643829346
training loss = 1.4366990327835083 8400
val loss = 3.983555555343628
training loss = 1.4219163656234741 8500
val loss = 3.8769755363464355
training loss = 1.4218254089355469 8600
val loss = 3.87634539604187
training loss = 1.422659158706665 8700
val loss = 3.900040864944458
training loss = 1.4214874505996704 8800
val loss = 3.8727269172668457
training loss = 1.4213976860046387 8900
val loss = 3.873274087905884
training loss = 1.4213396310806274 9000
val loss = 3.8805840015411377
training loss = 1.4210803508758545 9100
val loss = 3.8701696395874023
training loss = 1.4747133255004883 9200
val loss = 3.728912830352783
training loss = 1.4207797050476074 9300
val loss = 3.868701696395874
training loss = 1.4208073616027832 9400
val loss = 3.8596677780151367
training loss = 1.4204870462417603 9500
val loss = 3.8619608879089355
training loss = 1.4203927516937256 9600
val loss = 3.8646833896636963
training loss = 1.4202231168746948 9700
val loss = 3.857377290725708
training loss = 1.4201000928878784 9800
val loss = 3.8617477416992188
training loss = 1.5936909914016724 9900
val loss = 3.6668457984924316
training loss = 1.4198038578033447 10000
val loss = 3.8570821285247803
training loss = 1.4197185039520264 10100
val loss = 3.8589675426483154
training loss = 1.421521782875061 10200
val loss = 3.8925204277038574
training loss = 1.4194036722183228 10300
val loss = 3.856173038482666
training loss = 1.4193261861801147 10400
val loss = 3.8554847240448
training loss = 1.4199113845825195 10500
val loss = 3.831542491912842
training loss = 1.419021487236023 10600
val loss = 3.8524043560028076
training loss = 1.5720114707946777 10700
val loss = 4.262691497802734
training loss = 1.4187264442443848 10800
val loss = 3.848733425140381
training loss = 1.4186427593231201 10900
val loss = 3.849301815032959
training loss = 1.4184534549713135 11000
val loss = 3.8513269424438477
training loss = 1.418336272239685 11100
val loss = 3.8460934162139893
training loss = 1.5675373077392578 11200
val loss = 3.657655954360962
training loss = 1.418036699295044 11300
val loss = 3.8412106037139893
training loss = 1.4179468154907227 11400
val loss = 3.842224597930908
training loss = 1.4179919958114624 11500
val loss = 3.852769136428833
training loss = 1.4176439046859741 11600
val loss = 3.839348316192627
training loss = 1.560333490371704 11700
val loss = 4.233246803283691
training loss = 1.4173094034194946 11800
val loss = 3.8361220359802246
training loss = 1.4172258377075195 11900
val loss = 3.8349452018737793
training loss = 1.4657284021377563 12000
val loss = 3.6969988346099854
training loss = 1.4168952703475952 12100
val loss = 3.8316853046417236
training loss = 1.4168046712875366 12200
val loss = 3.8305118083953857
training loss = 1.4213554859161377 12300
val loss = 3.778695583343506
training loss = 1.4164738655090332 12400
val loss = 3.8261332511901855
training loss = 1.4163775444030762 12500
val loss = 3.825348377227783
training loss = 1.4164060354232788 12600
val loss = 3.8354721069335938
training loss = 1.416058897972107 12700
val loss = 3.822044849395752
training loss = 1.43438720703125 12800
val loss = 3.9341819286346436
training loss = 1.4157302379608154 12900
val loss = 3.818500280380249
training loss = 1.4156321287155151 13000
val loss = 3.816605567932129
training loss = 1.4154170751571655 13100
val loss = 3.813727617263794
training loss = 1.4153203964233398 13200
val loss = 3.8131532669067383
training loss = 1.420151710510254 13300
val loss = 3.866342782974243
training loss = 1.4150105714797974 13400
val loss = 3.808781623840332
training loss = 1.4548155069351196 13500
val loss = 3.988593101501465
training loss = 1.4146983623504639 13600
val loss = 3.8010096549987793
training loss = 1.414586067199707 13700
val loss = 3.8032822608947754
training loss = 1.4376531839370728 13800
val loss = 3.706897735595703
training loss = 1.4142673015594482 13900
val loss = 3.8000545501708984
training loss = 1.4141724109649658 14000
val loss = 3.7974140644073486
training loss = 1.4164843559265137 14100
val loss = 3.833378314971924
training loss = 1.4138782024383545 14200
val loss = 3.79313325881958
training loss = 1.4379364252090454 14300
val loss = 3.6847054958343506
training loss = 1.4135935306549072 14400
val loss = 3.787118911743164
training loss = 1.4135043621063232 14500
val loss = 3.787140369415283
training loss = 1.4133174419403076 14600
val loss = 3.7813563346862793
training loss = 1.4132322072982788 14700
val loss = 3.782414674758911
training loss = 1.4822678565979004 14800
val loss = 3.637295961380005
training loss = 1.4129838943481445 14900
val loss = 3.776135206222534
training loss = 1.4129074811935425 15000
val loss = 3.7770299911499023
training loss = 1.4129722118377686 15100
val loss = 3.761908769607544
training loss = 1.4126836061477661 15200
val loss = 3.771575927734375
training loss = 1.415604829788208 15300
val loss = 3.8112874031066895
training loss = 1.412465214729309 15400
val loss = 3.7671148777008057
training loss = 1.4124164581298828 15500
val loss = 3.7655513286590576
training loss = 1.4126229286193848 15600
val loss = 3.748690128326416
training loss = 1.41224205493927 15700
val loss = 3.760509729385376
training loss = 1.4540431499481201 15800
val loss = 3.9419546127319336
training loss = 1.4120808839797974 15900
val loss = 3.755601644515991
training loss = 1.4120612144470215 16000
val loss = 3.75441312789917
training loss = 1.4136444330215454 16100
val loss = 3.72186279296875
training loss = 1.4119495153427124 16200
val loss = 3.7500081062316895
training loss = 1.5046435594558716 16300
val loss = 3.58280086517334
training loss = 1.411876916885376 16400
val loss = 3.7438390254974365
training loss = 1.411879539489746 16500
val loss = 3.744594097137451
training loss = 1.4127193689346313 16600
val loss = 3.7192025184631348
training loss = 1.4118272066116333 16700
val loss = 3.7401492595672607
training loss = 1.411859154701233 16800
val loss = 3.739654302597046
training loss = 1.4120543003082275 16900
val loss = 3.7254295349121094
training loss = 1.4118667840957642 17000
val loss = 3.7355518341064453
training loss = 1.419077754020691 17100
val loss = 3.673954486846924
training loss = 1.4118982553482056 17200
val loss = 3.732492208480835
training loss = 1.4119808673858643 17300
val loss = 3.728276252746582
training loss = 1.411952257156372 17400
val loss = 3.729745864868164
training loss = 1.4120371341705322 17500
val loss = 3.7276611328125
training loss = 1.444256067276001 17600
val loss = 3.8829221725463867
training loss = 1.4121366739273071 17700
val loss = 3.725188732147217
training loss = 1.4122344255447388 17800
val loss = 3.724857807159424
training loss = 1.4123624563217163 17900
val loss = 3.7298507690429688
training loss = 1.4123713970184326 18000
val loss = 3.7214486598968506
training loss = 1.4243168830871582 18100
val loss = 3.6476993560791016
training loss = 1.4125109910964966 18200
val loss = 3.7200045585632324
training loss = 1.4126337766647339 18300
val loss = 3.7184314727783203
training loss = 1.440399408340454 18400
val loss = 3.617518663406372
training loss = 1.4128153324127197 18500
val loss = 3.7163805961608887
training loss = 1.4130613803863525 18600
val loss = 3.724174976348877
training loss = 1.4130189418792725 18700
val loss = 3.713463306427002
training loss = 1.4131563901901245 18800
val loss = 3.7139885425567627
training loss = 1.4137500524520874 18900
val loss = 3.7308144569396973
training loss = 1.4133731126785278 19000
val loss = 3.712930202484131
training loss = 1.4206035137176514 19100
val loss = 3.655637741088867
training loss = 1.413582444190979 19200
val loss = 3.711284875869751
training loss = 1.413731575012207 19300
val loss = 3.710183620452881
training loss = 1.413940191268921 19400
val loss = 3.7194509506225586
training loss = 1.4139354228973389 19500
val loss = 3.7098116874694824
training loss = 1.5505177974700928 19600
val loss = 3.543147087097168
training loss = 1.4141429662704468 19700
val loss = 3.710104465484619
training loss = 1.4142794609069824 19800
val loss = 3.708543539047241
training loss = 1.417864203453064 19900
val loss = 3.7528421878814697
training loss = 1.4144786596298218 20000
val loss = 3.7076611518859863
training loss = 1.6025826930999756 20100
val loss = 4.163179397583008
training loss = 1.4146612882614136 20200
val loss = 3.707509756088257
training loss = 1.4147825241088867 20300
val loss = 3.7061045169830322
training loss = 1.4154081344604492 20400
val loss = 3.7234668731689453
training loss = 1.414942741394043 20500
val loss = 3.704586982727051
training loss = 1.4315677881240845 20600
val loss = 3.6217713356018066
training loss = 1.4150710105895996 20700
val loss = 3.702491044998169
training loss = 1.4151548147201538 20800
val loss = 3.703639507293701
training loss = 1.4152019023895264 20900
val loss = 3.707489252090454
training loss = 1.415216326713562 21000
val loss = 3.7015445232391357
training loss = 1.4156677722930908 21100
val loss = 3.7163891792297363
training loss = 1.4152121543884277 21200
val loss = 3.699425220489502
training loss = 1.4152339696884155 21300
val loss = 3.698984146118164
training loss = 1.4160526990890503 21400
val loss = 3.72025728225708
training loss = 1.4151383638381958 21500
val loss = 3.6963157653808594
training loss = 1.4150930643081665 21600
val loss = 3.6962313652038574
training loss = 1.4154185056686401 21700
val loss = 3.6799354553222656
training loss = 1.4148313999176025 21800
val loss = 3.6937191486358643
training loss = 1.4146941900253296 21900
val loss = 3.692685127258301
training loss = 1.419167160987854 22000
val loss = 3.643197536468506
training loss = 1.414223313331604 22100
val loss = 3.689359426498413
training loss = 1.4139506816864014 22200
val loss = 3.6891231536865234
training loss = 1.4135621786117554 22300
val loss = 3.6868748664855957
training loss = 1.4132003784179688 22400
val loss = 3.6867756843566895
training loss = 1.4235870838165283 22500
val loss = 3.769760847091675
training loss = 1.4122458696365356 22600
val loss = 3.687502384185791
training loss = 1.4117146730422974 22700
val loss = 3.6847100257873535
training loss = 1.4455056190490723 22800
val loss = 3.846205472946167
training loss = 1.410476565361023 22900
val loss = 3.6841580867767334
training loss = 1.4098302125930786 23000
val loss = 3.6833503246307373
training loss = 1.409301996231079 23100
val loss = 3.693145990371704
training loss = 1.408436894416809 23200
val loss = 3.683192491531372
training loss = 1.4300298690795898 23300
val loss = 3.8085005283355713
training loss = 1.4070072174072266 23400
val loss = 3.6828413009643555
training loss = 1.4063143730163574 23500
val loss = 3.681905746459961
training loss = 1.4055830240249634 23600
val loss = 3.6815714836120605
training loss = 1.4049279689788818 23700
val loss = 3.6794896125793457
training loss = 1.4050313234329224 23800
val loss = 3.65983510017395
training loss = 1.4036015272140503 23900
val loss = 3.6801977157592773
training loss = 1.4029431343078613 24000
val loss = 3.6749472618103027
training loss = 1.4264241456985474 24100
val loss = 3.583207130432129
training loss = 1.401670217514038 24200
val loss = 3.6705832481384277
training loss = 1.401063084602356 24300
val loss = 3.6699817180633545
training loss = 1.4007521867752075 24400
val loss = 3.6554713249206543
training loss = 1.3998559713363647 24500
val loss = 3.665703773498535
training loss = 1.4957419633865356 24600
val loss = 3.5237412452697754
training loss = 1.398686408996582 24700
val loss = 3.6597213745117188
training loss = 1.398133397102356 24800
val loss = 3.660533905029297
training loss = 1.3976713418960571 24900
val loss = 3.65250825881958
training loss = 1.39701247215271 25000
val loss = 3.653564214706421
training loss = 1.396441102027893 25100
val loss = 3.6559019088745117
training loss = 1.3986579179763794 25200
val loss = 3.6212756633758545
training loss = 1.395329475402832 25300
val loss = 3.653291940689087
training loss = 1.394827127456665 25400
val loss = 3.656736373901367
training loss = 1.3942309617996216 25500
val loss = 3.648630380630493
training loss = 1.393710970878601 25600
val loss = 3.6494269371032715
training loss = 1.5503777265548706 25700
val loss = 3.513507843017578
training loss = 1.3926441669464111 25800
val loss = 3.648392915725708
training loss = 1.392127513885498 25900
val loss = 3.646336317062378
training loss = 1.3945599794387817 26000
val loss = 3.6113266944885254
training loss = 1.3910988569259644 26100
val loss = 3.64399790763855
training loss = 1.3905926942825317 26200
val loss = 3.6437723636627197
training loss = 1.390785813331604 26300
val loss = 3.625359535217285
training loss = 1.389614224433899 26400
val loss = 3.641993761062622
training loss = 1.3891326189041138 26500
val loss = 3.6416614055633545
training loss = 1.3890976905822754 26600
val loss = 3.6258883476257324
training loss = 1.3881827592849731 26700
val loss = 3.6399617195129395
training loss = 1.387717366218567 26800
val loss = 3.639655828475952
training loss = 1.3874009847640991 26900
val loss = 3.6308841705322266
training loss = 1.3868136405944824 27000
val loss = 3.6387321949005127
training loss = 1.3863741159439087 27100
val loss = 3.6385889053344727
training loss = 1.385932445526123 27200
val loss = 3.6390602588653564
training loss = 1.385507583618164 27300
val loss = 3.637779712677002
training loss = 1.3851068019866943 27400
val loss = 3.641160011291504
training loss = 1.3846852779388428 27500
val loss = 3.637009620666504
training loss = 1.384287714958191 27600
val loss = 3.6359574794769287
training loss = 1.3839225769042969 27700
val loss = 3.640251636505127
training loss = 1.3835091590881348 27800
val loss = 3.6364731788635254
training loss = 1.3940256834030151 27900
val loss = 3.584188461303711
training loss = 1.3827637434005737 28000
val loss = 3.6348764896392822
training loss = 1.3824052810668945 28100
val loss = 3.63619065284729
training loss = 1.3821128606796265 28200
val loss = 3.630441665649414
training loss = 1.3817096948623657 28300
val loss = 3.635861873626709
training loss = 1.5320647954940796 28400
val loss = 3.5260305404663086
training loss = 1.3810455799102783 28500
val loss = 3.6342711448669434
training loss = 1.3807260990142822 28600
val loss = 3.636367082595825
training loss = 1.3807098865509033 28700
val loss = 3.646272659301758
training loss = 1.3801110982894897 28800
val loss = 3.6358883380889893
training loss = 1.4091905355453491 28900
val loss = 3.757568120956421
training loss = 1.3795241117477417 29000
val loss = 3.634721040725708
training loss = 1.3792515993118286 29100
val loss = 3.634807586669922
training loss = 1.3789546489715576 29200
val loss = 3.6347475051879883
training loss = 1.378697395324707 29300
val loss = 3.636488914489746
training loss = 1.3866808414459229 29400
val loss = 3.58992862701416
training loss = 1.378180980682373 29500
val loss = 3.6358325481414795
training loss = 1.3779385089874268 29600
val loss = 3.6373281478881836
training loss = 1.3782575130462646 29700
val loss = 3.6504769325256348
training loss = 1.37746000289917 29800
val loss = 3.6371805667877197
training loss = 1.3947139978408813 29900
val loss = 3.57906174659729
training loss = 1.3770043849945068 30000
val loss = 3.6360924243927
reduced chi^2 level 2 = 1.3770005702972412
Constrained alpha: 1.9425294399261475
Constrained beta: 3.2913970947265625
Constrained gamma: 11.330599784851074
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 874.8458,  892.1697,  913.9243,  904.2668, 1032.0741, 1118.7927,
        1107.2045, 1096.7559, 1124.7853, 1172.5583, 1161.2872, 1187.6604,
        1293.4127, 1250.1533, 1314.2637, 1457.5955, 1398.5204, 1387.2380,
        1540.7167, 1596.2999, 1518.8757, 1554.0552, 1565.5099, 1674.3177,
        1636.7686, 1692.6829, 1647.9042, 1744.4984, 1699.2727, 1677.6079,
        1688.9305, 1664.5735, 1723.9637, 1715.9601, 1665.2498, 1718.4244,
        1712.0573, 1554.3680, 1686.3636, 1630.8234, 1618.1913, 1554.4535,
        1479.0104, 1525.3540, 1340.0338, 1329.1356, 1271.8541, 1235.9429,
        1106.8163, 1137.3834, 1108.5653,  989.3496,  944.6533,  976.0740,
         909.6779,  862.9852,  829.1358,  659.5685,  608.7753,  522.7756,
         528.1431,  478.8234,  440.5805,  409.8512,  358.3770,  369.0627,
         277.8610,  246.1066,  226.0392,  180.5956,  155.5619,  153.1616,
         151.1930,  101.0840,   97.1074,   63.4384,   44.4740,   33.0444,
          32.4062,   41.5959,   19.2579,   27.2244,   29.8550])]
2658.2410507588684
1.5495520390592694 14.130156974810761 57.366785132704464
val isze = 8
idinces = [55 48 15 14 25 70 26 78 74 45 10 50 28 75 68 23 60 37  2 41  8 22 18  4
 61 53 27 66 59  0 56 72 76 43 19 77 65 63  6 16  1 30 64 52 34 47 71 62
 32 82 38 49 42 51 12 11 13 31 57 39  3 29 73 40 80 69  7 44 21 79  9 20
 67 24 17 54 36  5 35 46 81 58 33]
we are doing training validation split
training loss = 111.40484619140625 100
val loss = 77.86221313476562
training loss = 7.039183139801025 200
val loss = 8.087082862854004
training loss = 6.849961280822754 300
val loss = 8.046746253967285
training loss = 6.672689914703369 400
val loss = 7.77978515625
training loss = 6.494162082672119 500
val loss = 7.493124961853027
training loss = 6.324014663696289 600
val loss = 7.200319766998291
training loss = 6.169432640075684 700
val loss = 6.912100791931152
training loss = 6.034937858581543 800
val loss = 6.637465953826904
training loss = 5.922410011291504 900
val loss = 6.383587837219238
training loss = 5.831299781799316 1000
val loss = 6.155393123626709
training loss = 5.7591986656188965 1100
val loss = 5.955463409423828
training loss = 5.702576637268066 1200
val loss = 5.784263610839844
training loss = 5.65757417678833 1300
val loss = 5.640450954437256
training loss = 5.6206207275390625 1400
val loss = 5.521531105041504
training loss = 5.588811874389648 1500
val loss = 5.4242072105407715
training loss = 5.560037612915039 1600
val loss = 5.344904899597168
training loss = 5.532914638519287 1700
val loss = 5.280240058898926
training loss = 5.506639003753662 1800
val loss = 5.227065086364746
training loss = 5.480786323547363 1900
val loss = 5.182692527770996
training loss = 5.4551825523376465 2000
val loss = 5.14484977722168
training loss = 5.4297685623168945 2100
val loss = 5.1118011474609375
training loss = 5.404550552368164 2200
val loss = 5.0822038650512695
training loss = 5.37954044342041 2300
val loss = 5.0549726486206055
training loss = 5.354732513427734 2400
val loss = 5.029425621032715
training loss = 5.330096244812012 2500
val loss = 5.004900932312012
training loss = 5.305535793304443 2600
val loss = 4.9809675216674805
training loss = 5.280881404876709 2700
val loss = 4.957172870635986
training loss = 5.255838394165039 2800
val loss = 4.933039665222168
training loss = 5.229912281036377 2900
val loss = 4.90786075592041
training loss = 5.202272891998291 3000
val loss = 4.880645751953125
training loss = 5.17146110534668 3100
val loss = 4.8496856689453125
training loss = 5.1348347663879395 3200
val loss = 4.811844825744629
training loss = 5.0873918533325195 3300
val loss = 4.761353969573975
training loss = 5.019545078277588 3400
val loss = 4.6874847412109375
training loss = 4.9124298095703125 3500
val loss = 4.570878028869629
training loss = 4.722045421600342 3600
val loss = 4.3710784912109375
training loss = 4.324924468994141 3700
val loss = 3.9784109592437744
training loss = 3.604966640472412 3800
val loss = 3.38974928855896
training loss = 2.7494113445281982 3900
val loss = 2.971466541290283
training loss = 2.0700008869171143 4000
val loss = 2.8798038959503174
training loss = 1.8249337673187256 4100
val loss = 3.0348427295684814
training loss = 1.7722290754318237 4200
val loss = 3.1170921325683594
training loss = 1.7474405765533447 4300
val loss = 3.125488758087158
training loss = 1.7288458347320557 4400
val loss = 3.1164324283599854
training loss = 1.7140358686447144 4500
val loss = 3.106048583984375
training loss = 1.7020879983901978 4600
val loss = 3.096980571746826
training loss = 1.6923867464065552 4700
val loss = 3.089296340942383
training loss = 1.6844680309295654 4800
val loss = 3.082807779312134
training loss = 1.6779749393463135 4900
val loss = 3.077281951904297
training loss = 1.6726285219192505 5000
val loss = 3.0726442337036133
training loss = 1.6681947708129883 5100
val loss = 3.065128803253174
training loss = 1.664406418800354 5200
val loss = 3.0643203258514404
training loss = 1.6615482568740845 5300
val loss = 3.0544698238372803
training loss = 1.6582361459732056 5400
val loss = 3.0580172538757324
training loss = 1.7958883047103882 5500
val loss = 3.0946121215820312
training loss = 1.6532692909240723 5600
val loss = 3.053309917449951
training loss = 1.6511690616607666 5700
val loss = 3.050262928009033
training loss = 1.649369239807129 5800
val loss = 3.052608013153076
training loss = 1.6473424434661865 5900
val loss = 3.0453593730926514
training loss = 1.6457082033157349 6000
val loss = 3.0434529781341553
training loss = 1.644050121307373 6100
val loss = 3.040179491043091
training loss = 1.642601490020752 6200
val loss = 3.0390625
training loss = 1.6412923336029053 6300
val loss = 3.035658359527588
training loss = 1.6399152278900146 6400
val loss = 3.034621238708496
training loss = 1.6387425661087036 6500
val loss = 3.0325675010681152
training loss = 1.6426036357879639 6600
val loss = 3.0136804580688477
training loss = 1.6365035772323608 6700
val loss = 3.028265953063965
training loss = 1.6355422735214233 6800
val loss = 3.0265488624572754
training loss = 1.6346549987792969 6900
val loss = 3.0206685066223145
training loss = 1.633684515953064 7000
val loss = 3.0218770503997803
training loss = 1.6637392044067383 7100
val loss = 2.99808931350708
training loss = 1.632054328918457 7200
val loss = 3.0176401138305664
training loss = 1.6313608884811401 7300
val loss = 3.0156302452087402
training loss = 1.631728172302246 7400
val loss = 3.0048117637634277
training loss = 1.6300283670425415 7500
val loss = 3.0116076469421387
training loss = 1.6720741987228394 7600
val loss = 3.1154417991638184
training loss = 1.628867268562317 7700
val loss = 3.0086867809295654
training loss = 1.6283764839172363 7800
val loss = 3.0062050819396973
training loss = 1.6297212839126587 7900
val loss = 3.018674850463867
training loss = 1.6274445056915283 8000
val loss = 3.0027389526367188
training loss = 1.6301414966583252 8100
val loss = 3.0241572856903076
training loss = 1.6266402006149292 8200
val loss = 2.9995718002319336
training loss = 1.6263185739517212 8300
val loss = 2.9984047412872314
training loss = 1.6259582042694092 8400
val loss = 2.9971132278442383
training loss = 1.6256887912750244 8500
val loss = 2.995579242706299
training loss = 1.6412979364395142 8600
val loss = 2.973156452178955
training loss = 1.6251611709594727 8700
val loss = 2.992950439453125
training loss = 1.6361029148101807 8800
val loss = 3.0348424911499023
training loss = 1.6247169971466064 8900
val loss = 2.990478992462158
reduced chi^2 level 2 = 1.6246012449264526
Constrained alpha: 1.8526273965835571
Constrained beta: 3.9906399250030518
Constrained gamma: 21.13651466369629
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 872.6085,  893.1898,  926.4719,  953.9738, 1007.2904, 1049.2007,
        1085.7809, 1172.1028, 1195.9930, 1123.4497, 1161.8074, 1201.2517,
        1251.2056, 1271.1862, 1370.9529, 1344.5283, 1422.4120, 1496.2803,
        1595.1116, 1548.6239, 1630.2001, 1559.6906, 1609.1539, 1644.8795,
        1585.5782, 1772.2036, 1560.9794, 1719.6917, 1686.9525, 1753.1223,
        1723.0002, 1711.7725, 1699.5487, 1756.9861, 1724.7687, 1749.8490,
        1666.6345, 1588.6830, 1665.4585, 1541.6489, 1625.7264, 1584.7271,
        1596.9331, 1507.8878, 1339.5411, 1350.9091, 1305.2621, 1230.1404,
        1197.3457, 1155.5189, 1060.6407,  981.4091,  927.5669,  938.0386,
         837.9883,  826.2151,  765.8439,  745.4954,  568.6101,  477.1449,
         564.4117,  479.6071,  477.5167,  409.7345,  392.7555,  333.2805,
         271.7673,  260.2239,  194.2427,  137.0294,  176.6669,  139.4904,
         142.2421,   96.1357,   87.8046,   62.3907,   66.9168,   44.6087,
          29.2584,   49.2044,   16.0255,   49.0711,   40.5623])]
2660.816401601477
0.32876784548399984 19.003970532342773 74.57219855151833
val isze = 8
idinces = [18 81 46 66 68 72  0 53 73 36 71  1 38 39 31  8 76 20 26 25 14 55  2 82
 15 77 23  3  7 54  6 21 59 13 28 69 12 58 42 52 49 27 75 79 24  5 45 33
 64 48 67 43  9 29 51 41 47 37 35 70 60 19 11 30 40 44 32 34 16 65 50 62
 61 57  4 78 22 80 10 74 56 17 63]
we are doing training validation split
training loss = 170.6315155029297 100
val loss = 204.57318115234375
training loss = 25.812360763549805 200
val loss = 51.66151809692383
training loss = 13.823053359985352 300
val loss = 33.30717086791992
training loss = 13.324141502380371 400
val loss = 32.11812973022461
training loss = 12.770461082458496 500
val loss = 30.959781646728516
training loss = 12.166181564331055 600
val loss = 29.664430618286133
training loss = 11.525500297546387 700
val loss = 28.252750396728516
training loss = 10.864489555358887 800
val loss = 26.749345779418945
training loss = 10.201428413391113 900
val loss = 25.185380935668945
training loss = 9.556480407714844 1000
val loss = 23.59893798828125
training loss = 8.950568199157715 1100
val loss = 22.03449249267578
training loss = 8.403430938720703 1200
val loss = 20.539714813232422
training loss = 7.930984020233154 1300
val loss = 19.160593032836914
training loss = 7.542612552642822 1400
val loss = 17.935178756713867
training loss = 7.239316463470459 1500
val loss = 16.887441635131836
training loss = 7.013662338256836 1600
val loss = 16.0242862701416
training loss = 6.851923942565918 1700
val loss = 15.335916519165039
training loss = 6.737646579742432 1800
val loss = 14.80058479309082
training loss = 6.655313968658447 1900
val loss = 14.39067268371582
training loss = 6.592685699462891 2000
val loss = 14.07835578918457
training loss = 6.541477203369141 2100
val loss = 13.839131355285645
training loss = 6.496759414672852 2200
val loss = 13.653105735778809
training loss = 6.4559173583984375 2300
val loss = 13.505062103271484
training loss = 6.417612075805664 2400
val loss = 13.383584976196289
training loss = 6.381086349487305 2500
val loss = 13.280261993408203
training loss = 6.345712184906006 2600
val loss = 13.188682556152344
training loss = 6.310708045959473 2700
val loss = 13.103681564331055
training loss = 6.274888515472412 2800
val loss = 13.020301818847656
training loss = 6.236199855804443 2900
val loss = 12.932482719421387
training loss = 6.190638542175293 3000
val loss = 12.82998275756836
training loss = 6.129256248474121 3100
val loss = 12.69064712524414
training loss = 6.0295820236206055 3200
val loss = 12.458088874816895
training loss = 5.838442325592041 3300
val loss = 11.99830436706543
training loss = 5.489917278289795 3400
val loss = 11.1853666305542
training loss = 4.949346542358398 3500
val loss = 10.099793434143066
training loss = 4.1222429275512695 3600
val loss = 8.502955436706543
training loss = 3.088501453399658 3700
val loss = 6.1604461669921875
training loss = 2.495224952697754 3800
val loss = 4.154019355773926
training loss = 2.4027605056762695 3900
val loss = 3.535215139389038
training loss = 2.3850178718566895 4000
val loss = 3.4813365936279297
training loss = 2.372729539871216 4100
val loss = 3.5156190395355225
training loss = 2.362711191177368 4200
val loss = 3.5562257766723633
training loss = 2.3544270992279053 4300
val loss = 3.5933518409729004
training loss = 2.347548723220825 4400
val loss = 3.6261935234069824
training loss = 2.3418149948120117 4500
val loss = 3.654818058013916
training loss = 2.3370208740234375 4600
val loss = 3.6794052124023438
training loss = 2.333021402359009 4700
val loss = 3.7034435272216797
training loss = 2.3294177055358887 4800
val loss = 3.7146191596984863
training loss = 2.3263821601867676 4900
val loss = 3.7318010330200195
training loss = 2.3242557048797607 5000
val loss = 3.760319709777832
training loss = 2.321228504180908 5100
val loss = 3.7570924758911133
training loss = 2.3189785480499268 5200
val loss = 3.7618367671966553
training loss = 2.3168249130249023 5300
val loss = 3.7828011512756348
training loss = 2.314471483230591 5400
val loss = 3.7772469520568848
training loss = 2.3278961181640625 5500
val loss = 3.8950977325439453
training loss = 2.309840202331543 5600
val loss = 3.7897729873657227
training loss = 2.307471990585327 5700
val loss = 3.7852623462677
training loss = 2.30454158782959 5800
val loss = 3.7973856925964355
training loss = 2.3015239238739014 5900
val loss = 3.800527572631836
training loss = 2.2995102405548096 6000
val loss = 3.7755379676818848
training loss = 2.2943437099456787 6100
val loss = 3.8024158477783203
training loss = 2.4552345275878906 6200
val loss = 3.5739223957061768
training loss = 2.285930871963501 6300
val loss = 3.794604778289795
training loss = 2.2814676761627197 6400
val loss = 3.783529043197632
training loss = 2.2774715423583984 6500
val loss = 3.7954294681549072
training loss = 2.2723703384399414 6600
val loss = 3.7612953186035156
training loss = 2.2897391319274902 6700
val loss = 3.6367506980895996
training loss = 2.263005018234253 6800
val loss = 3.7394297122955322
training loss = 2.258049726486206 6900
val loss = 3.7220327854156494
training loss = 2.2531895637512207 7000
val loss = 3.725717544555664
training loss = 2.247631311416626 7100
val loss = 3.700133800506592
training loss = 2.2741265296936035 7200
val loss = 3.861064910888672
training loss = 2.2359507083892822 7300
val loss = 3.6815710067749023
training loss = 2.2294516563415527 7400
val loss = 3.6649765968322754
training loss = 2.226051092147827 7500
val loss = 3.605600357055664
training loss = 2.2157886028289795 7600
val loss = 3.6409130096435547
training loss = 2.2080371379852295 7700
val loss = 3.623220920562744
training loss = 2.2145276069641113 7800
val loss = 3.506840229034424
training loss = 2.1919474601745605 7900
val loss = 3.5892043113708496
training loss = 2.182863712310791 8000
val loss = 3.565566062927246
training loss = 2.1824800968170166 8100
val loss = 3.4567384719848633
training loss = 2.1634726524353027 8200
val loss = 3.515015125274658
training loss = 2.2095723152160645 8300
val loss = 3.2872843742370605
training loss = 2.142425298690796 8400
val loss = 3.456186294555664
training loss = 2.130774736404419 8500
val loss = 3.4254817962646484
training loss = 2.1206676959991455 8600
val loss = 3.374098777770996
training loss = 2.108128309249878 8700
val loss = 3.368635654449463
training loss = 2.122408390045166 8800
val loss = 3.522611379623413
training loss = 2.0859103202819824 8900
val loss = 3.322403907775879
training loss = 2.0742781162261963 9000
val loss = 3.3005423545837402
training loss = 2.0653483867645264 9100
val loss = 3.2869887351989746
training loss = 2.0557122230529785 9200
val loss = 3.283277988433838
training loss = 2.1772985458374023 9300
val loss = 2.9528708457946777
training loss = 2.0404489040374756 9400
val loss = 3.280240058898926
training loss = 2.0337882041931152 9500
val loss = 3.280177593231201
training loss = 2.0294365882873535 9600
val loss = 3.267348289489746
training loss = 2.02457857131958 9700
val loss = 3.28790020942688
training loss = 2.024653673171997 9800
val loss = 3.2298784255981445
training loss = 2.018523693084717 9900
val loss = 3.301114559173584
training loss = 2.0159506797790527 10000
val loss = 3.3056235313415527
training loss = 2.0149991512298584 10100
val loss = 3.291266441345215
training loss = 2.0129916667938232 10200
val loss = 3.3194022178649902
training loss = 2.013918399810791 10300
val loss = 3.383636474609375
training loss = 2.01116681098938 10400
val loss = 3.3322231769561768
training loss = 2.0104308128356934 10500
val loss = 3.339167356491089
training loss = 2.0105462074279785 10600
val loss = 3.3297457695007324
training loss = 2.0099637508392334 10700
val loss = 3.35189151763916
training loss = 2.1805765628814697 10800
val loss = 2.968526840209961
training loss = 2.009974718093872 10900
val loss = 3.3631439208984375
training loss = 2.0099661350250244 11000
val loss = 3.368917942047119
training loss = 2.0124080181121826 11100
val loss = 3.433112859725952
training loss = 2.010434150695801 11200
val loss = 3.380316972732544
training loss = 2.102954626083374 11300
val loss = 3.8105127811431885
training loss = 2.011075496673584 11400
val loss = 3.3932065963745117
training loss = 2.0113425254821777 11500
val loss = 3.3977928161621094
training loss = 2.011946201324463 11600
val loss = 3.387172222137451
training loss = 2.0120999813079834 11700
val loss = 3.403935432434082
training loss = 2.0132219791412354 11800
val loss = 3.377023220062256
training loss = 2.012892246246338 11900
val loss = 3.4119772911071777
training loss = 2.3895151615142822 12000
val loss = 2.933289051055908
training loss = 2.013707160949707 12100
val loss = 3.416688919067383
training loss = 2.014019012451172 12200
val loss = 3.4245519638061523
training loss = 2.0150933265686035 12300
val loss = 3.3996634483337402
training loss = 2.0147812366485596 12400
val loss = 3.4305620193481445
training loss = 2.0635550022125244 12500
val loss = 3.202343225479126
training loss = 2.0155248641967773 12600
val loss = 3.4383859634399414
training loss = 2.015821933746338 12700
val loss = 3.4395275115966797
training loss = 2.022493600845337 12800
val loss = 3.356966018676758
training loss = 2.0165019035339355 12900
val loss = 3.445797920227051
training loss = 2.0167837142944336 13000
val loss = 3.4472076892852783
training loss = 2.0184245109558105 13100
val loss = 3.410642623901367
training loss = 2.0173959732055664 13200
val loss = 3.452486276626587
training loss = 2.0308837890625 13300
val loss = 3.3272957801818848
training loss = 2.018012762069702 13400
val loss = 3.4642703533172607
training loss = 2.018205165863037 13500
val loss = 3.4585304260253906
training loss = 2.025664806365967 13600
val loss = 3.3694353103637695
training loss = 2.0187151432037354 13700
val loss = 3.4631330966949463
training loss = 2.0189476013183594 13800
val loss = 3.459226608276367
training loss = 2.01920747756958 13900
val loss = 3.468841552734375
training loss = 2.019380807876587 14000
val loss = 3.4670939445495605
training loss = 2.2740530967712402 14100
val loss = 4.237807273864746
training loss = 2.0198144912719727 14200
val loss = 3.469381093978882
training loss = 2.019970417022705 14300
val loss = 3.471306800842285
training loss = 2.071506977081299 14400
val loss = 3.770224094390869
training loss = 2.020357370376587 14500
val loss = 3.4765124320983887
training loss = 2.0205085277557373 14600
val loss = 3.474842071533203
training loss = 2.0207931995391846 14700
val loss = 3.4884281158447266
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 869.2125,  884.4536,  913.6152,  952.7794, 1011.8578, 1126.0273,
        1115.5612, 1163.2633, 1167.4261, 1195.2686, 1214.6619, 1183.8762,
        1280.1748, 1286.3113, 1274.5529, 1471.5947, 1393.4298, 1431.7931,
        1579.9451, 1467.7284, 1596.8713, 1531.0529, 1626.4248, 1557.7949,
        1678.9783, 1757.4547, 1629.9861, 1670.0352, 1745.8130, 1785.4227,
        1668.0480, 1729.3866, 1757.7676, 1735.7415, 1671.8253, 1739.9803,
        1696.4438, 1590.8744, 1572.2306, 1611.4885, 1568.0421, 1613.9052,
        1532.7448, 1531.0841, 1406.1711, 1302.0645, 1294.5977, 1180.2727,
        1093.1028, 1201.0524, 1093.5975, 1025.3317,  937.9672,  960.2797,
         908.0674,  878.2947,  854.0438,  683.9296,  606.4838,  540.4347,
         579.7643,  501.9150,  451.1483,  344.2781,  355.8564,  398.3228,
         291.1989,  254.5859,  204.5399,  169.6667,  157.0046,  140.4359,
         148.2137,  116.2112,  110.5151,   64.1610,   53.1601,   42.2590,
          32.4266,   42.1936,   23.5788,   24.4234,   29.8743])]
2885.4182630796413
2.2959811815642386 13.09532661618694 28.087942194928196
val isze = 8
idinces = [42 78 44 69 30 29 20 68 35 76  6 47 16 73  1 64  7 51 79 19 59 21 23 62
 56 32  9 39 28 46 67 22 52 58 41 38 82 81 37 74 17 10 13 72  8 24 49 14
 75 57 60 43 53 65 61  4 48 54 40 11 12 71 63 45 27 31 55 66 33 80 70 18
  3 26 25 36 15 34  5 50  2 77  0]
we are doing training validation split
training loss = 168.6053466796875 100
val loss = 136.4645233154297
training loss = 6.866900444030762 200
val loss = 4.538754940032959
training loss = 6.838850498199463 300
val loss = 4.487472057342529
training loss = 6.8246331214904785 400
val loss = 4.480978012084961
training loss = 6.808407783508301 500
val loss = 4.473552703857422
training loss = 6.7903828620910645 600
val loss = 4.465134620666504
training loss = 6.770740509033203 700
val loss = 4.455986976623535
training loss = 6.74960994720459 800
val loss = 4.446046829223633
training loss = 6.727122783660889 900
val loss = 4.4354119300842285
training loss = 6.703373432159424 1000
val loss = 4.424009799957275
training loss = 6.678464412689209 1100
val loss = 4.411933898925781
training loss = 6.652474403381348 1200
val loss = 4.399303436279297
training loss = 6.625473976135254 1300
val loss = 4.385931015014648
training loss = 6.597530364990234 1400
val loss = 4.371981620788574
training loss = 6.568708419799805 1500
val loss = 4.357512474060059
training loss = 6.539061069488525 1600
val loss = 4.3423919677734375
training loss = 6.508641242980957 1700
val loss = 4.326729774475098
training loss = 6.477502822875977 1800
val loss = 4.310513496398926
training loss = 6.445695877075195 1900
val loss = 4.293887138366699
training loss = 6.413276672363281 2000
val loss = 4.276757717132568
training loss = 6.3802995681762695 2100
val loss = 4.259133815765381
training loss = 6.3468546867370605 2200
val loss = 4.241053581237793
training loss = 6.313037872314453 2300
val loss = 4.2226243019104
training loss = 6.278941631317139 2400
val loss = 4.203878402709961
training loss = 6.244665622711182 2500
val loss = 4.184758186340332
training loss = 6.210299015045166 2600
val loss = 4.164789199829102
training loss = 6.175921440124512 2700
val loss = 4.143753528594971
training loss = 6.14160680770874 2800
val loss = 4.121428489685059
training loss = 6.107334136962891 2900
val loss = 4.098330497741699
training loss = 6.0728631019592285 3000
val loss = 4.075153827667236
training loss = 6.037405967712402 3100
val loss = 4.0520734786987305
training loss = 5.998772144317627 3200
val loss = 4.026657581329346
training loss = 5.951188087463379 3300
val loss = 4.001368999481201
training loss = 5.876219749450684 3400
val loss = 3.9696061611175537
training loss = 5.71899938583374 3500
val loss = 3.908475875854492
training loss = 5.415671348571777 3600
val loss = 3.7861506938934326
training loss = 4.9514994621276855 3700
val loss = 3.6847755908966064
training loss = 4.186228275299072 3800
val loss = 3.1825966835021973
training loss = 3.167400360107422 3900
val loss = 2.746990203857422
training loss = 2.5505259037017822 4000
val loss = 2.5439541339874268
training loss = 2.442535161972046 4100
val loss = 2.5038340091705322
training loss = 2.4185409545898438 4200
val loss = 2.519787073135376
training loss = 2.405538558959961 4300
val loss = 2.529768943786621
training loss = 2.396812915802002 4400
val loss = 2.540496349334717
training loss = 2.3927664756774902 4500
val loss = 2.580620288848877
training loss = 2.3865556716918945 4600
val loss = 2.5505151748657227
training loss = 2.383352041244507 4700
val loss = 2.549140453338623
training loss = 2.3810060024261475 4800
val loss = 2.5523574352264404
training loss = 2.379181385040283 4900
val loss = 2.5547146797180176
training loss = 2.3778011798858643 5000
val loss = 2.5492422580718994
training loss = 2.376695156097412 5100
val loss = 2.553435802459717
training loss = 2.385831356048584 5200
val loss = 2.6264002323150635
training loss = 2.3749630451202393 5300
val loss = 2.5505142211914062
training loss = 2.3743159770965576 5400
val loss = 2.5496764183044434
training loss = 2.3741705417633057 5500
val loss = 2.53436017036438
training loss = 2.373150587081909 5600
val loss = 2.547119617462158
training loss = 2.372687339782715 5700
val loss = 2.546649932861328
training loss = 2.372258424758911 5800
val loss = 2.538184881210327
training loss = 2.371751070022583 5900
val loss = 2.54302716255188
training loss = 2.4201629161834717 6000
val loss = 2.45268177986145
training loss = 2.370920181274414 6100
val loss = 2.541327953338623
training loss = 2.3705737590789795 6200
val loss = 2.5403904914855957
training loss = 2.3701534271240234 6300
val loss = 2.5354485511779785
training loss = 2.3698103427886963 6400
val loss = 2.5371432304382324
training loss = 2.395658016204834 6500
val loss = 2.4598159790039062
training loss = 2.3691208362579346 6600
val loss = 2.536398410797119
training loss = 2.368828296661377 6700
val loss = 2.5328640937805176
training loss = 2.3684744834899902 6800
val loss = 2.530902862548828
training loss = 2.368189811706543 6900
val loss = 2.5321121215820312
training loss = 2.3678529262542725 7000
val loss = 2.5299232006073
training loss = 2.367583990097046 7100
val loss = 2.5302257537841797
training loss = 2.3674380779266357 7200
val loss = 2.535463809967041
training loss = 2.3670568466186523 7300
val loss = 2.532137393951416
training loss = 2.366795063018799 7400
val loss = 2.5278356075286865
training loss = 2.367539405822754 7500
val loss = 2.5474398136138916
training loss = 2.3662710189819336 7600
val loss = 2.5264501571655273
training loss = 2.6375155448913574 7700
val loss = 3.111048460006714
training loss = 2.3657736778259277 7800
val loss = 2.5240211486816406
training loss = 2.3655779361724854 7900
val loss = 2.524352550506592
training loss = 2.371161699295044 8000
val loss = 2.4849202632904053
training loss = 2.3651232719421387 8100
val loss = 2.523193597793579
training loss = 2.3681118488311768 8200
val loss = 2.4917213916778564
training loss = 2.36472225189209 8300
val loss = 2.5191006660461426
training loss = 2.364534854888916 8400
val loss = 2.5215039253234863
training loss = 2.365118980407715 8500
val loss = 2.504481315612793
training loss = 2.3641421794891357 8600
val loss = 2.5204708576202393
training loss = 2.4165878295898438 8700
val loss = 2.434677839279175
training loss = 2.3637735843658447 8800
val loss = 2.5214662551879883
training loss = 2.363626718521118 8900
val loss = 2.518838405609131
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 871.8292,  857.5416,  970.1014,  974.0317,  965.7963, 1123.9742,
        1143.1670, 1103.1841, 1144.0557, 1210.2391, 1229.2582, 1176.0500,
        1246.8914, 1226.2714, 1301.8184, 1401.4738, 1393.8828, 1426.3326,
        1567.2570, 1503.3000, 1614.3148, 1526.1442, 1573.6169, 1562.8580,
        1616.8527, 1632.9462, 1608.9955, 1776.4823, 1794.4116, 1777.0242,
        1697.5558, 1661.6415, 1672.7228, 1669.2429, 1747.2115, 1727.3834,
        1637.6796, 1604.8732, 1622.1313, 1635.1718, 1632.3210, 1655.7249,
        1490.0077, 1508.4438, 1388.2614, 1386.1312, 1274.0343, 1315.4037,
        1091.0449, 1222.7655, 1066.4878,  969.5009,  945.0975,  895.0323,
         851.3055,  895.5508,  822.0240,  730.9537,  622.1990,  541.6483,
         549.8027,  469.8202,  409.6212,  399.1925,  360.6921,  325.8848,
         314.5723,  267.2145,  220.3879,  161.2855,  143.3456,  152.0621,
         165.4895,  100.5791,  103.3415,   56.4137,   50.3519,   49.0536,
          28.4691,   43.6668,   26.4086,   40.2843,   28.6882])]
3153.096584436167
4.686159442072109 10.234960039463019 15.575669710456562
val isze = 8
idinces = [38 62 37 47 68 61 81 28 12 19 71 14 16  9 79  6 58 73 77 41 48 11 46 29
 82 75 35 67 80 21 27 72 69 49 42 65 20 17 30 55  3  7 24 50 25 56 13 57
  5 26  0 40 63 54 34  1 76 22 52 33  4 53 64  2 51 23 10 60 66 39 70 32
 43 74 78 36  8 31 15 44 59 45 18]
we are doing training validation split
training loss = 61.791412353515625 100
val loss = 61.060855865478516
training loss = 7.034789562225342 200
val loss = 13.009354591369629
training loss = 6.6525492668151855 300
val loss = 11.962181091308594
training loss = 6.41510009765625 400
val loss = 11.268804550170898
training loss = 6.277376651763916 500
val loss = 10.777920722961426
training loss = 6.19879150390625 600
val loss = 10.440549850463867
training loss = 6.15192985534668 700
val loss = 10.2124605178833
training loss = 6.1207098960876465 800
val loss = 10.059793472290039
training loss = 6.096576690673828 900
val loss = 9.958049774169922
training loss = 6.075315475463867 1000
val loss = 9.88983154296875
training loss = 6.0550031661987305 1100
val loss = 9.84298324584961
training loss = 6.034817695617676 1200
val loss = 9.808969497680664
training loss = 6.014413833618164 1300
val loss = 9.782158851623535
training loss = 5.993630409240723 1400
val loss = 9.758932113647461
training loss = 5.972386360168457 1500
val loss = 9.73702621459961
training loss = 5.950619220733643 1600
val loss = 9.715099334716797
training loss = 5.928269863128662 1700
val loss = 9.692460060119629
training loss = 5.905270099639893 1800
val loss = 9.668663024902344
training loss = 5.881499767303467 1900
val loss = 9.643327713012695
training loss = 5.856680870056152 2000
val loss = 9.616147994995117
training loss = 5.8301215171813965 2100
val loss = 9.585987091064453
training loss = 5.7997331619262695 2200
val loss = 9.550025939941406
training loss = 5.757870674133301 2300
val loss = 9.497832298278809
training loss = 5.660782814025879 2400
val loss = 9.378116607666016
training loss = 5.2563934326171875 2500
val loss = 8.978904724121094
training loss = 4.452517509460449 2600
val loss = 7.634893417358398
training loss = 3.3834176063537598 2700
val loss = 5.608659744262695
training loss = 2.62127947807312 2800
val loss = 3.501565933227539
training loss = 2.4308085441589355 2900
val loss = 2.5500407218933105
training loss = 2.4224398136138916 3000
val loss = 2.3000757694244385
training loss = 2.390660524368286 3100
val loss = 2.344085216522217
training loss = 2.3886454105377197 3200
val loss = 2.326300621032715
training loss = 2.3768413066864014 3300
val loss = 2.376645565032959
training loss = 2.3723137378692627 3400
val loss = 2.381523370742798
training loss = 2.3682405948638916 3500
val loss = 2.402249336242676
training loss = 2.365081548690796 3600
val loss = 2.408385992050171
training loss = 2.3626415729522705 3700
val loss = 2.423051118850708
training loss = 2.3601737022399902 3800
val loss = 2.420579195022583
training loss = 2.360187530517578 3900
val loss = 2.4478092193603516
training loss = 2.3565075397491455 4000
val loss = 2.428487777709961
training loss = 2.3930351734161377 4100
val loss = 2.3795106410980225
training loss = 2.3536243438720703 4200
val loss = 2.433363676071167
training loss = 2.352429151535034 4300
val loss = 2.4344167709350586
training loss = 2.3530826568603516 4400
val loss = 2.4208781719207764
training loss = 2.3502204418182373 4500
val loss = 2.4366352558135986
training loss = 2.349567413330078 4600
val loss = 2.4300787448883057
training loss = 2.3483119010925293 4700
val loss = 2.4398293495178223
training loss = 2.3474528789520264 4800
val loss = 2.437575578689575
training loss = 2.360085964202881 4900
val loss = 2.402217388153076
training loss = 2.34580659866333 5000
val loss = 2.437777519226074
training loss = 2.3450772762298584 5100
val loss = 2.4372060298919678
training loss = 2.345275640487671 5200
val loss = 2.4519519805908203
training loss = 2.3436217308044434 5300
val loss = 2.436671495437622
training loss = 2.353318452835083 5400
val loss = 2.4060678482055664
training loss = 2.342259168624878 5500
val loss = 2.435914993286133
training loss = 2.4133646488189697 5600
val loss = 2.6032259464263916
training loss = 2.3409674167633057 5700
val loss = 2.4344208240509033
training loss = 2.343902587890625 5800
val loss = 2.4127745628356934
training loss = 2.339761257171631 5900
val loss = 2.4318838119506836
training loss = 2.3391854763031006 6000
val loss = 2.432295083999634
training loss = 2.3394100666046143 6100
val loss = 2.445566177368164
training loss = 2.3380420207977295 6200
val loss = 2.4312562942504883
training loss = 2.5232105255126953 6300
val loss = 2.7572808265686035
training loss = 2.336970329284668 6400
val loss = 2.429072618484497
training loss = 2.336477041244507 6500
val loss = 2.42914080619812
training loss = 2.3361716270446777 6600
val loss = 2.4236490726470947
training loss = 2.3354732990264893 6700
val loss = 2.4275715351104736
training loss = 2.3356261253356934 6800
val loss = 2.439274787902832
training loss = 2.334519624710083 6900
val loss = 2.4266018867492676
training loss = 2.336888551712036 7000
val loss = 2.447960376739502
training loss = 2.333603620529175 7100
val loss = 2.4250800609588623
training loss = 2.333202362060547 7200
val loss = 2.42397141456604
training loss = 2.3327860832214355 7300
val loss = 2.4273529052734375
training loss = 2.3323471546173096 7400
val loss = 2.422759532928467
training loss = 2.331976890563965 7500
val loss = 2.4218738079071045
training loss = 2.3365707397460938 7600
val loss = 2.4008169174194336
training loss = 2.3311846256256104 7700
val loss = 2.420780658721924
training loss = 2.3308393955230713 7800
val loss = 2.4201741218566895
training loss = 2.330573797225952 7900
val loss = 2.4158833026885986
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 829.4158,  921.6877,  978.4119,  982.2946, 1043.0807, 1079.8553,
        1111.3920, 1082.1442, 1126.4420, 1132.7130, 1229.2231, 1269.2444,
        1248.7740, 1301.5399, 1370.0405, 1368.9763, 1484.7334, 1450.5524,
        1533.9296, 1474.1443, 1592.6301, 1590.2539, 1584.2665, 1610.0140,
        1678.8253, 1701.6025, 1586.1664, 1745.1123, 1730.5574, 1738.4902,
        1667.7792, 1755.1562, 1760.0206, 1706.5518, 1681.5664, 1724.4838,
        1652.6053, 1588.4719, 1706.9368, 1683.9000, 1672.6215, 1551.2670,
        1547.5265, 1541.6008, 1350.1383, 1305.7598, 1295.8474, 1230.7157,
        1145.9495, 1180.8458, 1088.6870,  994.9135,  876.6334,  929.5339,
         930.1031,  914.0756,  823.9808,  695.8102,  592.3818,  565.4001,
         543.7076,  447.7511,  412.8693,  406.1372,  347.5736,  350.0005,
         284.7732,  261.0343,  164.8210,  172.9893,  174.6335,  135.5695,
         147.0965,  110.4966,  104.5707,   70.4314,   33.4090,   47.7288,
          46.4969,   40.9423,   24.6667,   32.8991,   21.3280])]
2690.582242379329
1.7476237164106334 12.981439015765936 98.17499858590463
val isze = 8
idinces = [28 30 41 73  1 34 22 32 60 54 80 40 81 58 56 55 53 64 29 46 39 48 70 49
 66 27 50 42 78 43 59 36 17 77 23 61 21 72  0  9 26 69 12  7  4 10 74 79
 75 44 37 13 25  3  6 33 68 47 45 18 57 82 62 67 35 76 11  2 16 71 31  5
 14  8 51 52 63 38 65 24 15 19 20]
we are doing training validation split
training loss = 72.84002685546875 100
val loss = 42.390113830566406
training loss = 7.258512496948242 200
val loss = 5.108138084411621
training loss = 7.172099590301514 300
val loss = 5.163971900939941
training loss = 7.086432456970215 400
val loss = 5.140799522399902
training loss = 7.001357555389404 500
val loss = 5.123788833618164
training loss = 6.920862674713135 600
val loss = 5.114526271820068
training loss = 6.847361087799072 700
val loss = 5.112813949584961
training loss = 6.781883716583252 800
val loss = 5.117266654968262
training loss = 6.724305152893066 900
val loss = 5.1253743171691895
training loss = 6.673705101013184 1000
val loss = 5.1343584060668945
training loss = 6.628720760345459 1100
val loss = 5.141512393951416
training loss = 6.587878704071045 1200
val loss = 5.144349098205566
training loss = 6.549844741821289 1300
val loss = 5.1415581703186035
training loss = 6.513565540313721 1400
val loss = 5.132303237915039
training loss = 6.478311538696289 1500
val loss = 5.116705894470215
training loss = 6.4436492919921875 1600
val loss = 5.095325946807861
training loss = 6.409374237060547 1700
val loss = 5.069120407104492
training loss = 6.375437259674072 1800
val loss = 5.039059638977051
training loss = 6.341864585876465 1900
val loss = 5.006288528442383
training loss = 6.3087158203125 2000
val loss = 4.9717512130737305
training loss = 6.2760114669799805 2100
val loss = 4.936334133148193
training loss = 6.243710994720459 2200
val loss = 4.900403022766113
training loss = 6.2116594314575195 2300
val loss = 4.864285945892334
training loss = 6.179527759552002 2400
val loss = 4.828033447265625
training loss = 6.146735668182373 2500
val loss = 4.791263103485107
training loss = 6.112246036529541 2600
val loss = 4.753360748291016
training loss = 6.074182033538818 2700
val loss = 4.712581634521484
training loss = 6.028879642486572 2800
val loss = 4.66588830947876
training loss = 5.968626499176025 2900
val loss = 4.6065497398376465
training loss = 5.876558303833008 3000
val loss = 4.519863128662109
training loss = 5.72025728225708 3100
val loss = 4.374972343444824
training loss = 5.4636712074279785 3200
val loss = 4.122931957244873
training loss = 5.083165168762207 3300
val loss = 3.6998467445373535
training loss = 4.518389701843262 3400
val loss = 3.0365147590637207
training loss = 3.7169930934906006 3500
val loss = 2.12483286857605
training loss = 2.875622272491455 3600
val loss = 1.19793701171875
training loss = 2.442046880722046 3700
val loss = 0.6846438050270081
training loss = 2.3546578884124756 3800
val loss = 0.5391474962234497
training loss = 2.3394317626953125 3900
val loss = 0.5045927166938782
training loss = 2.3316385746002197 4000
val loss = 0.4923003911972046
training loss = 2.3256144523620605 4100
val loss = 0.4850825369358063
training loss = 2.3207271099090576 4200
val loss = 0.47971034049987793
training loss = 2.316735029220581 4300
val loss = 0.47546130418777466
training loss = 2.3134729862213135 4400
val loss = 0.4720679819583893
training loss = 2.310809850692749 4500
val loss = 0.4693548083305359
training loss = 2.308640480041504 4600
val loss = 0.4671979248523712
training loss = 2.306877613067627 4700
val loss = 0.4654945135116577
training loss = 2.3054542541503906 4800
val loss = 0.464160293340683
training loss = 2.3043124675750732 4900
val loss = 0.4631301760673523
training loss = 2.3034071922302246 5000
val loss = 0.46234482526779175
training loss = 2.303629159927368 5100
val loss = 0.46200963854789734
training loss = 2.3014180660247803 5200
val loss = 0.4606333374977112
training loss = 2.3084628582000732 5300
val loss = 0.4673559367656708
training loss = 2.2993080615997314 5400
val loss = 0.4588589370250702
training loss = 2.298365831375122 5500
val loss = 0.4581928253173828
training loss = 2.297562837600708 5600
val loss = 0.45672500133514404
training loss = 2.2961838245391846 5700
val loss = 0.4565281867980957
training loss = 2.3089957237243652 5800
val loss = 0.48364919424057007
training loss = 2.2941699028015137 5900
val loss = 0.4551911950111389
training loss = 2.2935996055603027 6000
val loss = 0.45626142621040344
training loss = 2.292313575744629 6100
val loss = 0.4542868137359619
training loss = 2.2914795875549316 6200
val loss = 0.45339542627334595
training loss = 2.29343843460083 6300
val loss = 0.4523943066596985
training loss = 2.289863348007202 6400
val loss = 0.4524538516998291
training loss = 2.5199873447418213 6500
val loss = 0.8231195211410522
training loss = 2.2884066104888916 6600
val loss = 0.4520103931427002
training loss = 2.287787437438965 6700
val loss = 0.4514119029045105
training loss = 2.2883384227752686 6800
val loss = 0.4494146704673767
training loss = 2.28653621673584 6900
val loss = 0.4508149027824402
training loss = 2.5630221366882324 7000
val loss = 0.8962345123291016
training loss = 2.2854089736938477 7100
val loss = 0.4505343437194824
training loss = 2.2849478721618652 7200
val loss = 0.45018455386161804
training loss = 2.2873945236206055 7300
val loss = 0.4600444436073303
training loss = 2.283968687057495 7400
val loss = 0.44978633522987366
training loss = 2.283625841140747 7500
val loss = 0.4504883885383606
training loss = 2.2831547260284424 7600
val loss = 0.4487714469432831
training loss = 2.2827420234680176 7700
val loss = 0.4494350552558899
training loss = 2.2895419597625732 7800
val loss = 0.469110906124115
training loss = 2.281994342803955 7900
val loss = 0.449162095785141
training loss = 2.407649278640747 8000
val loss = 0.587019681930542
training loss = 2.2812957763671875 8100
val loss = 0.4488821029663086
training loss = 2.281027317047119 8200
val loss = 0.4489789307117462
training loss = 2.2807939052581787 8300
val loss = 0.45035016536712646
training loss = 2.2804245948791504 8400
val loss = 0.44876763224601746
training loss = 2.283564805984497 8500
val loss = 0.4468957781791687
training loss = 2.2798852920532227 8600
val loss = 0.44858449697494507
training loss = 2.3075520992279053 8700
val loss = 0.4687659740447998
training loss = 2.279378652572632 8800
val loss = 0.44884437322616577
training loss = 2.279195547103882 8900
val loss = 0.44859564304351807
training loss = 2.288393020629883 9000
val loss = 0.4505385458469391
training loss = 2.278730869293213 9100
val loss = 0.4485327899456024
training loss = 2.278581142425537 9200
val loss = 0.44847506284713745
training loss = 2.2784829139709473 9300
val loss = 0.4501483142375946
training loss = 2.2781784534454346 9400
val loss = 0.4483439326286316
training loss = 2.278054714202881 9500
val loss = 0.4484056830406189
training loss = 2.278832197189331 9600
val loss = 0.44599610567092896
training loss = 2.277693033218384 9700
val loss = 0.44835227727890015
training loss = 2.277595281600952 9800
val loss = 0.448346883058548
training loss = 2.279322862625122 9900
val loss = 0.4458165764808655
training loss = 2.277285575866699 10000
val loss = 0.44826290011405945
training loss = 2.277207851409912 10100
val loss = 0.44819778203964233
training loss = 2.2771146297454834 10200
val loss = 0.44957107305526733
training loss = 2.276953935623169 10300
val loss = 0.4482901692390442
training loss = 2.2891573905944824 10400
val loss = 0.4789602756500244
training loss = 2.2767202854156494 10500
val loss = 0.44829267263412476
training loss = 2.2766687870025635 10600
val loss = 0.4483718276023865
training loss = 2.276733160018921 10700
val loss = 0.4468516409397125
training loss = 2.276473045349121 10800
val loss = 0.44825315475463867
training loss = 2.282083034515381 10900
val loss = 0.4653729200363159
training loss = 2.276294708251953 11000
val loss = 0.4481264352798462
training loss = 2.2762691974639893 11100
val loss = 0.4481346607208252
training loss = 2.276289463043213 11200
val loss = 0.4470067620277405
training loss = 2.276110887527466 11300
val loss = 0.44825443625450134
training loss = 2.2984838485717773 11400
val loss = 0.49790823459625244
training loss = 2.2759835720062256 11500
val loss = 0.4483175575733185
training loss = 2.2782366275787354 11600
val loss = 0.4459775686264038
training loss = 2.275867462158203 11700
val loss = 0.4486905634403229
training loss = 2.2758569717407227 11800
val loss = 0.44831082224845886
training loss = 2.2764222621917725 11900
val loss = 0.4523472189903259
training loss = 2.275749683380127 12000
val loss = 0.44835740327835083
training loss = 2.2757580280303955 12100
val loss = 0.4482820928096771
training loss = 2.275707244873047 12200
val loss = 0.4475727677345276
training loss = 2.2756667137145996 12300
val loss = 0.44834065437316895
training loss = 2.3383090496063232 12400
val loss = 0.5076525211334229
training loss = 2.2755939960479736 12500
val loss = 0.4485134482383728
training loss = 2.2756142616271973 12600
val loss = 0.4483720660209656
training loss = 2.2788264751434326 12700
val loss = 0.4463856816291809
training loss = 2.275538444519043 12800
val loss = 0.44836944341659546
training loss = 2.275566816329956 12900
val loss = 0.4484226703643799
training loss = 2.2755520343780518 13000
val loss = 0.44761553406715393
training loss = 2.2755167484283447 13100
val loss = 0.4483909606933594
training loss = 2.2881643772125244 13200
val loss = 0.45337581634521484
training loss = 2.27549147605896 13300
val loss = 0.44795525074005127
training loss = 2.275508165359497 13400
val loss = 0.4484480023384094
training loss = 2.2971646785736084 13500
val loss = 0.46216851472854614
training loss = 2.2754738330841064 13600
val loss = 0.44846612215042114
training loss = 2.2755157947540283 13700
val loss = 0.44853872060775757
training loss = 2.275470018386841 13800
val loss = 0.4489021897315979
training loss = 2.275505781173706 13900
val loss = 0.4484885632991791
training loss = 2.3265600204467773 14000
val loss = 0.49474167823791504
training loss = 2.2754971981048584 14100
val loss = 0.4483599066734314
training loss = 2.275542974472046 14200
val loss = 0.4485154151916504
training loss = 2.275796413421631 14300
val loss = 0.44704949855804443
training loss = 2.275546073913574 14400
val loss = 0.4485829174518585
training loss = 2.281871795654297 14500
val loss = 0.4485021233558655
training loss = 2.2755489349365234 14600
val loss = 0.4486364424228668
training loss = 2.2757534980773926 14700
val loss = 0.45028814673423767
training loss = 2.2755749225616455 14800
val loss = 0.4483025074005127
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 867.6321,  877.4397,  933.6851,  943.1848,  965.3140, 1086.1857,
        1070.6877, 1179.7325, 1162.5111, 1148.2821, 1204.8350, 1173.9946,
        1203.3774, 1264.6809, 1311.7830, 1388.8069, 1435.8212, 1444.9884,
        1460.8300, 1524.3481, 1582.8131, 1560.7996, 1694.8530, 1588.6801,
        1702.4202, 1730.8004, 1584.8777, 1685.8927, 1705.0043, 1738.6858,
        1701.9817, 1647.6973, 1622.2264, 1784.1355, 1741.8145, 1685.8737,
        1620.9353, 1599.6068, 1641.0990, 1560.3759, 1623.3809, 1525.1683,
        1544.6541, 1524.1627, 1315.3723, 1307.9592, 1222.2863, 1258.6486,
        1171.3429, 1097.9777, 1108.5319, 1024.9147,  961.4437,  940.4784,
         854.7916,  898.1868,  842.5196,  687.1212,  551.3597,  555.4121,
         523.8425,  466.0977,  427.1862,  368.7652,  346.1358,  322.3058,
         302.3372,  240.4238,  188.7964,  144.3464,  173.8251,  129.8773,
         145.8187,  100.0333,   94.3026,   84.2574,   57.6691,   48.3029,
          33.2497,   44.4159,   22.4186,   33.2422,   21.7885])]
2750.785934486688
3.9293525831338307 7.814120456698872 76.63753292271434
val isze = 8
idinces = [ 2 25 46 30 82  0 12 63 22 79 35 57 29 65 80 49 10 72 61 20 34 39 54 14
 81 38 16 17 47 73 70 64 23 71 66 78 44 75 31 37 28 48 24 56 68 53 36 43
 19  3 59  6 60 51 67 32  8  7 21 52 18 69 33  4 45  9 55 76 15 74 41 26
 42 77 58 13 27  1  5 40 62 50 11]
we are doing training validation split
training loss = 49.79903793334961 100
val loss = 91.90354919433594
training loss = 13.936771392822266 200
val loss = 26.1993465423584
training loss = 9.712939262390137 300
val loss = 18.174293518066406
training loss = 7.043286323547363 400
val loss = 12.211475372314453
training loss = 5.572041034698486 500
val loss = 8.01585578918457
training loss = 4.918844699859619 600
val loss = 6.013896465301514
training loss = 4.485454559326172 700
val loss = 5.110105991363525
training loss = 4.121791839599609 800
val loss = 4.523863315582275
training loss = 3.8058130741119385 900
val loss = 4.061736583709717
training loss = 3.5306103229522705 1000
val loss = 3.681776285171509
training loss = 3.292964458465576 1100
val loss = 3.363020896911621
training loss = 3.091322183609009 1200
val loss = 3.1227736473083496
training loss = 2.922313690185547 1300
val loss = 2.86761212348938
training loss = 2.7866764068603516 1400
val loss = 2.761899471282959
training loss = 2.6692256927490234 1500
val loss = 2.515103816986084
training loss = 2.57936429977417 1600
val loss = 2.4404845237731934
training loss = 2.5035598278045654 1700
val loss = 2.2751059532165527
training loss = 2.4443516731262207 1800
val loss = 2.177382469177246
training loss = 2.3959851264953613 1900
val loss = 2.129387140274048
training loss = 2.356851816177368 2000
val loss = 2.0691781044006348
training loss = 2.342785596847534 2100
val loss = 2.215484142303467
training loss = 2.296698808670044 2200
val loss = 1.9727444648742676
training loss = 2.4348766803741455 2300
val loss = 2.660111904144287
training loss = 2.2520928382873535 2400
val loss = 1.9077876806259155
training loss = 2.2407398223876953 2500
val loss = 1.7888343334197998
training loss = 2.2153401374816895 2600
val loss = 1.8313469886779785
training loss = 2.2134382724761963 2700
val loss = 1.7070109844207764
training loss = 2.187001943588257 2800
val loss = 1.7900326251983643
training loss = 2.191575765609741 2900
val loss = 2.0101146697998047
training loss = 2.16545033454895 3000
val loss = 1.7895176410675049
training loss = 2.15421462059021 3100
val loss = 1.7785882949829102
training loss = 2.149202346801758 3200
val loss = 1.8610081672668457
training loss = 2.136939525604248 3300
val loss = 1.7860625982284546
training loss = 2.1266188621520996 3400
val loss = 1.7768769264221191
training loss = 2.1179513931274414 3500
val loss = 1.7271523475646973
training loss = 2.1066842079162598 3600
val loss = 1.7935500144958496
training loss = 2.095135450363159 3700
val loss = 1.7999247312545776
training loss = 2.0945944786071777 3800
val loss = 1.722839117050171
training loss = 2.071866035461426 3900
val loss = 1.8374577760696411
training loss = 2.8575634956359863 4000
val loss = 3.979804277420044
training loss = 2.0480282306671143 4100
val loss = 1.900002360343933
training loss = 2.0351309776306152 4200
val loss = 1.9318920373916626
training loss = 2.025252103805542 4300
val loss = 1.9738428592681885
training loss = 2.0138094425201416 4400
val loss = 2.005316734313965
training loss = 2.754566192626953 4500
val loss = 3.9590847492218018
training loss = 1.994681477546692 4600
val loss = 2.076545238494873
training loss = 1.9847149848937988 4700
val loss = 2.1095476150512695
training loss = 1.978554368019104 4800
val loss = 2.1574771404266357
training loss = 1.9695101976394653 4900
val loss = 2.1668457984924316
training loss = 2.0000391006469727 5000
val loss = 2.0656704902648926
training loss = 1.9561035633087158 5100
val loss = 2.2162275314331055
training loss = 1.950101375579834 5200
val loss = 2.210669994354248
training loss = 1.944786787033081 5300
val loss = 2.27068829536438
training loss = 1.9383094310760498 5400
val loss = 2.278106212615967
training loss = 1.983734369277954 5500
val loss = 2.1687614917755127
training loss = 1.9290399551391602 5600
val loss = 2.3142752647399902
training loss = 2.589034080505371 5700
val loss = 2.4688048362731934
training loss = 1.920745849609375 5800
val loss = 2.3523213863372803
training loss = 1.9211851358413696 5900
val loss = 2.43571400642395
training loss = 1.9142025709152222 6000
val loss = 2.3659095764160156
training loss = 1.9103214740753174 6100
val loss = 2.3934574127197266
training loss = 1.9614777565002441 6200
val loss = 2.6925365924835205
training loss = 1.9052146673202515 6300
val loss = 2.4198408126831055
training loss = 1.9022048711776733 6400
val loss = 2.435547351837158
training loss = 1.901479721069336 6500
val loss = 2.4744110107421875
training loss = 1.8981146812438965 6600
val loss = 2.464033603668213
training loss = 1.902001976966858 6700
val loss = 2.55418062210083
training loss = 1.894383430480957 6800
val loss = 2.4899415969848633
training loss = 1.9020814895629883 6900
val loss = 2.4185845851898193
training loss = 1.8912076950073242 7000
val loss = 2.5155131816864014
training loss = 1.9070289134979248 7100
val loss = 2.6800389289855957
training loss = 1.888547658920288 7200
val loss = 2.539954662322998
training loss = 1.8957397937774658 7300
val loss = 2.450064182281494
training loss = 1.8861682415008545 7400
val loss = 2.566164493560791
training loss = 1.961544394493103 7500
val loss = 2.388777017593384
training loss = 1.8841724395751953 7600
val loss = 2.58911395072937
training loss = 1.9040977954864502 7700
val loss = 2.7738776206970215
training loss = 1.882541537284851 7800
val loss = 2.598130464553833
reduced chi^2 level 2 = 1.8815646171569824
Constrained alpha: 4.227017879486084
Constrained beta: 2.1381313800811768
Constrained gamma: 34.52755355834961
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 873.8303,  823.7771,  850.2568,  988.1811, 1059.3948, 1049.4146,
        1084.3914, 1064.1885, 1138.3918, 1222.3826, 1202.8680, 1144.0925,
        1321.9277, 1250.6434, 1353.8715, 1400.1761, 1439.4412, 1457.5726,
        1589.8429, 1527.1807, 1604.0959, 1570.5337, 1683.1909, 1597.6418,
        1611.5529, 1711.9803, 1638.4231, 1705.6079, 1630.1449, 1630.1862,
        1641.5526, 1812.4465, 1707.0057, 1770.2587, 1730.0839, 1766.5964,
        1626.9207, 1616.6813, 1658.4027, 1596.8923, 1625.9380, 1566.8142,
        1412.2002, 1466.1221, 1414.1364, 1367.5114, 1260.5220, 1240.3113,
        1162.4988, 1236.6067, 1073.0652,  947.6059,  908.4344,  886.7864,
         878.3975,  802.1103,  841.0389,  707.8911,  617.8890,  476.6679,
         608.5110,  468.3917,  465.0289,  436.8753,  342.0581,  343.9451,
         302.0390,  252.8205,  201.0007,  166.4103,  165.7541,  150.6659,
         149.9677,   87.3914,   92.2091,   74.2344,   56.6645,   38.8002,
          29.0176,   45.7548,   14.7699,   38.0419,   45.7821])]
2866.549418510024
3.338701935027099 0.4692319202033879 34.13895029589562
val isze = 8
idinces = [35 49 50 65 48 25 71 67 53 24 56 54 40 80 11 66 14 58 32 73 75 72 13 46
 78 38 44 28  4 68 20  8 55 10 26 18 81 42 43 61 69 33 45 16 51 39 74 57
 47  7 12  6  1  9 77  2 34 60 41 76 70 27 37 36  5 17 63 82 30 23 29  0
 62 59 21 31 52 15  3 19 22 79 64]
we are doing training validation split
training loss = 232.6905975341797 100
val loss = 228.47039794921875
training loss = 180.63160705566406 200
val loss = 182.13258361816406
training loss = 17.90190887451172 300
val loss = 13.576193809509277
training loss = 12.311781883239746 400
val loss = 11.766440391540527
training loss = 7.564598083496094 500
val loss = 6.47676420211792
training loss = 5.442532062530518 600
val loss = 4.811713218688965
training loss = 4.595473766326904 700
val loss = 4.025646209716797
training loss = 4.219722747802734 800
val loss = 3.597342014312744
training loss = 4.01276969909668 900
val loss = 3.363898277282715
training loss = 3.8673553466796875 1000
val loss = 3.193293571472168
training loss = 3.7312633991241455 1100
val loss = 3.066944122314453
training loss = 3.4317026138305664 1200
val loss = 2.8958559036254883
training loss = 3.1425187587738037 1300
val loss = 2.729917049407959
training loss = 2.957425117492676 1400
val loss = 2.7400074005126953
training loss = 2.81839656829834 1500
val loss = 2.751594066619873
training loss = 2.717108726501465 1600
val loss = 2.733616828918457
training loss = 2.6412148475646973 1700
val loss = 2.766427516937256
training loss = 2.582951784133911 1800
val loss = 2.6824564933776855
training loss = 2.532846689224243 1900
val loss = 2.8822500705718994
training loss = 2.57053279876709 2000
val loss = 3.808779239654541
training loss = 2.464834690093994 2100
val loss = 2.9857983589172363
training loss = 2.44170880317688 2200
val loss = 3.0801849365234375
training loss = 2.4225001335144043 2300
val loss = 3.057945489883423
training loss = 2.411191940307617 2400
val loss = 3.283740520477295
training loss = 2.3954598903656006 2500
val loss = 3.103924036026001
training loss = 2.3857421875 2600
val loss = 3.1593105792999268
training loss = 2.422762870788574 2700
val loss = 3.878787040710449
training loss = 2.370715856552124 2800
val loss = 3.1838297843933105
training loss = 2.364962577819824 2900
val loss = 3.188292980194092
training loss = 2.3617374897003174 3000
val loss = 3.0529379844665527
training loss = 2.355649471282959 3100
val loss = 3.1674258708953857
training loss = 2.4006857872009277 3200
val loss = 3.946666717529297
training loss = 2.535223960876465 3300
val loss = 4.7647552490234375
training loss = 2.3573858737945557 3400
val loss = 3.5455079078674316
training loss = 2.342273712158203 3500
val loss = 3.155489921569824
training loss = 2.3403310775756836 3600
val loss = 3.0902280807495117
training loss = 2.3420705795288086 3700
val loss = 3.3939590454101562
training loss = 2.341754674911499 3800
val loss = 3.4281978607177734
training loss = 2.3329999446868896 3900
val loss = 3.22761869430542
training loss = 2.330798387527466 4000
val loss = 3.1178102493286133
training loss = 2.3302953243255615 4100
val loss = 3.0367417335510254
training loss = 2.4026384353637695 4200
val loss = 4.051477909088135
training loss = 2.325028657913208 4300
val loss = 3.1421751976013184
training loss = 2.324084997177124 4400
val loss = 3.2344141006469727
training loss = 2.333885669708252 4500
val loss = 2.820941925048828
training loss = 2.401512384414673 4600
val loss = 4.105016708374023
training loss = 2.321950674057007 4700
val loss = 2.977149486541748
training loss = 2.3176774978637695 4800
val loss = 3.0814690589904785
training loss = 2.3161439895629883 4900
val loss = 3.2015278339385986
training loss = 2.3147077560424805 5000
val loss = 3.1881191730499268
training loss = 2.3133749961853027 5100
val loss = 3.185117483139038
training loss = 2.3119542598724365 5200
val loss = 3.1544618606567383
training loss = 2.312286853790283 5300
val loss = 3.2661032676696777
training loss = 2.31904673576355 5400
val loss = 3.4442245960235596
training loss = 2.3334462642669678 5500
val loss = 3.651904344558716
training loss = 2.3144173622131348 5600
val loss = 3.394583225250244
training loss = 2.3841044902801514 5700
val loss = 4.05075740814209
training loss = 2.3935706615448 5800
val loss = 4.117175102233887
training loss = 2.317070245742798 5900
val loss = 2.821619987487793
training loss = 2.380594253540039 6000
val loss = 2.3993852138519287
training loss = 2.311878204345703 6100
val loss = 3.445988178253174
training loss = 2.3108768463134766 6200
val loss = 3.4418129920959473
training loss = 2.3439180850982666 6300
val loss = 3.7647147178649902
training loss = 2.2997965812683105 6400
val loss = 3.1556897163391113
training loss = 2.299407482147217 6500
val loss = 3.0999808311462402
training loss = 2.423551082611084 6600
val loss = 4.345737457275391
training loss = 2.301386833190918 6700
val loss = 2.971853256225586
training loss = 2.297682285308838 6800
val loss = 3.0642004013061523
training loss = 2.296039581298828 6900
val loss = 3.156461238861084
training loss = 2.296232223510742 7000
val loss = 3.238581418991089
training loss = 2.4533464908599854 7100
val loss = 4.523975849151611
training loss = 2.294290781021118 7200
val loss = 3.1978249549865723
training loss = 2.294049024581909 7300
val loss = 3.2253518104553223
training loss = 2.293325185775757 7400
val loss = 3.1036911010742188
training loss = 2.293555974960327 7500
val loss = 3.0631253719329834
training loss = 2.303727865219116 7600
val loss = 3.480752944946289
training loss = 2.325861692428589 7700
val loss = 3.754077434539795
training loss = 2.512700319290161 7800
val loss = 1.9949166774749756
training loss = 2.471113920211792 7900
val loss = 4.597047328948975
training loss = 2.400311231613159 8000
val loss = 2.3047478199005127
training loss = 2.3173820972442627 8100
val loss = 2.6902732849121094
training loss = 2.289088249206543 8200
val loss = 3.1772360801696777
training loss = 2.553887128829956 8300
val loss = 1.9315266609191895
training loss = 2.3041038513183594 8400
val loss = 2.816023349761963
training loss = 2.3152050971984863 8500
val loss = 3.6826858520507812
training loss = 2.3481149673461914 8600
val loss = 2.512155055999756
training loss = 2.288804769515991 8700
val loss = 3.046450614929199
training loss = 2.287693977355957 8800
val loss = 3.250593900680542
training loss = 2.299700975418091 8900
val loss = 3.519867420196533
training loss = 2.2886962890625 9000
val loss = 3.317119598388672
training loss = 2.286221981048584 9100
val loss = 3.2110729217529297
training loss = 2.2872445583343506 9200
val loss = 3.056295156478882
training loss = 2.2861435413360596 9300
val loss = 3.2382216453552246
training loss = 2.295663595199585 9400
val loss = 2.876556396484375
training loss = 2.285165548324585 9500
val loss = 3.220665693283081
training loss = 2.4182138442993164 9600
val loss = 4.373409748077393
training loss = 2.286015748977661 9700
val loss = 3.0595974922180176
training loss = 2.285104274749756 9800
val loss = 3.26513409614563
training loss = 2.2846662998199463 9900
val loss = 3.1026084423065186
training loss = 2.2849597930908203 10000
val loss = 3.080166816711426
training loss = 2.2864725589752197 10100
val loss = 3.336850166320801
training loss = 2.397035837173462 10200
val loss = 4.274853706359863
training loss = 2.300029993057251 10300
val loss = 3.566594123840332
training loss = 2.4221439361572266 10400
val loss = 4.398506164550781
training loss = 2.284653425216675 10500
val loss = 3.29679536819458
training loss = 2.295754909515381 10600
val loss = 2.857391834259033
training loss = 2.3245885372161865 10700
val loss = 2.6241164207458496
training loss = 2.282543897628784 10800
val loss = 3.1729044914245605
training loss = 2.290194511413574 10900
val loss = 3.4462037086486816
training loss = 2.2825021743774414 11000
val loss = 3.2265846729278564
training loss = 2.282670736312866 11100
val loss = 3.2471718788146973
training loss = 2.285393714904785 11200
val loss = 3.350839138031006
training loss = 2.4155619144439697 11300
val loss = 2.251577138900757
training loss = 2.3378849029541016 11400
val loss = 2.525458574295044
training loss = 2.3099255561828613 11500
val loss = 3.679810047149658
training loss = 2.287541151046753 11600
val loss = 3.407560348510742
training loss = 2.3977136611938477 11700
val loss = 4.2629876136779785
training loss = 2.281547784805298 11800
val loss = 3.21402645111084
training loss = 2.442767381668091 11900
val loss = 4.473756790161133
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 857.3750,  859.0407,  948.3367,  981.1707, 1021.2551, 1083.0724,
        1147.6338, 1137.5298, 1132.7285, 1173.5424, 1166.1764, 1178.3832,
        1230.9976, 1207.9750, 1362.9819, 1374.0282, 1484.1050, 1423.7188,
        1527.9431, 1547.4178, 1610.5383, 1528.9110, 1636.1096, 1553.5751,
        1622.7189, 1668.3180, 1620.7559, 1811.7751, 1704.0068, 1734.7217,
        1693.6437, 1683.0747, 1740.7183, 1749.3077, 1766.7700, 1790.0985,
        1658.9877, 1550.1016, 1649.9750, 1626.9856, 1577.1630, 1653.4432,
        1479.5450, 1534.4098, 1345.2705, 1289.8644, 1279.5046, 1249.2437,
        1200.4463, 1185.5966, 1096.6456, 1017.3631,  931.9448,  938.4539,
         910.0427,  867.9960,  826.5801,  722.9800,  597.5719,  521.3549,
         567.8159,  494.4178,  432.9183,  355.4951,  348.2539,  343.1338,
         277.4492,  249.5329,  209.0442,  163.0745,  157.8433,  143.6729,
         135.6584,  107.0312,   86.2462,   67.3400,   40.2609,   38.3489,
          25.3824,   50.0108,   20.9026,   32.4952,   26.0276])]
2784.2252054160163
3.4638136440593446 5.59245432863372 91.72490782847022
val isze = 8
idinces = [38 12 31  3 81  9  6 53 71 24 60 21 77 69 72 59 44  0 80 75 35 47 68  2
 48 70 50 61 45 20 29 58 42 82 52 79 46 14 34 37 43 62 36 16 39 76 51 26
 28 55 67 15 13 33 41 66  1  4 23 11  8 17 30 19 74 27 18  5 32 57 40 63
 56  7 10 73 22 49 25 64 54 65 78]
we are doing training validation split
training loss = 44.42203903198242 100
val loss = 43.89386749267578
training loss = 23.9154052734375 200
val loss = 27.84273910522461
training loss = 14.489323616027832 300
val loss = 16.080463409423828
training loss = 9.832579612731934 400
val loss = 10.471195220947266
training loss = 7.496414661407471 500
val loss = 7.600143909454346
training loss = 6.322312355041504 600
val loss = 6.053387641906738
training loss = 5.741263389587402 700
val loss = 5.192787170410156
training loss = 5.454864025115967 800
val loss = 4.69975471496582
training loss = 5.297703742980957 900
val loss = 4.400753974914551
training loss = 5.168178081512451 1000
val loss = 4.191357612609863
training loss = 4.982655048370361 1100
val loss = 3.9885964393615723
training loss = 4.63161563873291 1200
val loss = 3.6770267486572266
training loss = 3.995957136154175 1300
val loss = 3.1065826416015625
training loss = 3.1765522956848145 1400
val loss = 2.282811403274536
training loss = 2.572509527206421 1500
val loss = 1.5595669746398926
training loss = 2.3381197452545166 1600
val loss = 1.2007734775543213
training loss = 2.281552791595459 1700
val loss = 1.0826170444488525
training loss = 2.2665438652038574 1800
val loss = 1.048182487487793
training loss = 2.258782148361206 1900
val loss = 1.0369861125946045
training loss = 2.2527148723602295 2000
val loss = 1.0348212718963623
training loss = 2.247079610824585 2100
val loss = 1.0286842584609985
training loss = 2.2421040534973145 2200
val loss = 1.0282468795776367
training loss = 2.2374634742736816 2300
val loss = 1.0250298976898193
training loss = 2.2332372665405273 2400
val loss = 1.023283839225769
training loss = 2.229370594024658 2500
val loss = 1.023334264755249
training loss = 2.226051092147827 2600
val loss = 1.0199966430664062
training loss = 2.222524642944336 2700
val loss = 1.022599697113037
training loss = 2.2195000648498535 2800
val loss = 1.0220446586608887
training loss = 2.2167460918426514 2900
val loss = 1.0220167636871338
training loss = 2.2143399715423584 3000
val loss = 1.0198194980621338
training loss = 2.211925745010376 3100
val loss = 1.0213055610656738
training loss = 2.223365306854248 3200
val loss = 1.0564751625061035
training loss = 2.207930088043213 3300
val loss = 1.019895076751709
training loss = 2.247209072113037 3400
val loss = 1.0296624898910522
training loss = 2.2046687602996826 3500
val loss = 1.0176002979278564
training loss = 2.219261646270752 3600
val loss = 1.0553193092346191
training loss = 2.2020795345306396 3700
val loss = 1.0148571729660034
training loss = 2.2010059356689453 3800
val loss = 1.0123729705810547
training loss = 2.200094223022461 3900
val loss = 1.0115563869476318
training loss = 2.1992671489715576 4000
val loss = 1.0084967613220215
training loss = 2.198725700378418 4100
val loss = 1.0080503225326538
training loss = 2.198089361190796 4200
val loss = 1.003373146057129
training loss = 2.2051515579223633 4300
val loss = 0.9953134059906006
training loss = 2.197268486022949 4400
val loss = 0.99809730052948
training loss = 2.1969823837280273 4500
val loss = 0.9946963787078857
training loss = 2.196791172027588 4600
val loss = 0.9932151436805725
training loss = 2.1966075897216797 4700
val loss = 0.989212155342102
training loss = 2.197429895401001 4800
val loss = 0.9825626611709595
training loss = 2.1963953971862793 4900
val loss = 0.9838296175003052
training loss = 2.2346577644348145 5000
val loss = 0.9921705722808838
training loss = 2.1962451934814453 5100
val loss = 0.9794759750366211
training loss = 2.196197271347046 5200
val loss = 0.9766771793365479
training loss = 2.1977782249450684 5300
val loss = 0.9697458744049072
training loss = 2.1959757804870605 5400
val loss = 0.9730427861213684
training loss = 2.195899486541748 5500
val loss = 0.9712303876876831
training loss = 2.1957409381866455 5600
val loss = 0.9691786766052246
training loss = 2.195566415786743 5700
val loss = 0.9686933755874634
training loss = 2.2039794921875 5800
val loss = 0.9611937999725342
training loss = 2.195183753967285 5900
val loss = 0.9660143852233887
training loss = 2.1949660778045654 6000
val loss = 0.9657156467437744
training loss = 2.1981403827667236 6100
val loss = 0.9795411825180054
training loss = 2.1944589614868164 6200
val loss = 0.9645421504974365
training loss = 2.1942310333251953 6300
val loss = 0.9638898372650146
training loss = 2.1939916610717773 6400
val loss = 0.9618358016014099
training loss = 2.1936426162719727 6500
val loss = 0.9627736210823059
training loss = 2.196864366531372 6600
val loss = 0.9768338799476624
training loss = 2.1930220127105713 6700
val loss = 0.9620664119720459
training loss = 2.2846944332122803 6800
val loss = 1.0160382986068726
training loss = 2.1923739910125732 6900
val loss = 0.9622987508773804
training loss = 2.192054033279419 7000
val loss = 0.9613035917282104
training loss = 2.221576690673828 7100
val loss = 0.9665175676345825
training loss = 2.191385507583618 7200
val loss = 0.961105227470398
training loss = 2.1911303997039795 7300
val loss = 0.9594260454177856
training loss = 2.1907548904418945 7400
val loss = 0.9618675708770752
training loss = 2.1903910636901855 7500
val loss = 0.9602929949760437
training loss = 2.194058418273926 7600
val loss = 0.9757230877876282
training loss = 2.1897013187408447 7700
val loss = 0.9600436091423035
training loss = 2.191476345062256 7800
val loss = 0.9543335437774658
training loss = 2.189013719558716 7900
val loss = 0.959915280342102
training loss = 2.188692092895508 8000
val loss = 0.9595150947570801
training loss = 2.195547342300415 8100
val loss = 0.9534743428230286
training loss = 2.1879963874816895 8200
val loss = 0.9592360258102417
training loss = 2.187675952911377 8300
val loss = 0.9589927196502686
training loss = 2.1884961128234863 8400
val loss = 0.9546089172363281
training loss = 2.186976671218872 8500
val loss = 0.9589173793792725
training loss = 2.1866562366485596 8600
val loss = 0.958538293838501
training loss = 2.188342332839966 8700
val loss = 0.9533295631408691
training loss = 2.185971260070801 8800
val loss = 0.9583157300949097
training loss = 2.185673475265503 8900
val loss = 0.957206130027771
training loss = 2.185290575027466 9000
val loss = 0.9579533338546753
training loss = 2.184967517852783 9100
val loss = 0.9576074481010437
training loss = 2.2154760360717773 9200
val loss = 0.9649521112442017
training loss = 2.184286117553711 9300
val loss = 0.957451581954956
training loss = 2.18403697013855 9400
val loss = 0.9555306434631348
training loss = 2.1836471557617188 9500
val loss = 0.9580791592597961
training loss = 2.1832683086395264 9600
val loss = 0.95649254322052
training loss = 2.186511754989624 9700
val loss = 0.9507138133049011
training loss = 2.1825711727142334 9800
val loss = 0.9560741186141968
training loss = 2.183871269226074 9900
val loss = 0.9510040879249573
training loss = 2.1818840503692627 10000
val loss = 0.9565547704696655
training loss = 2.18151593208313 10100
val loss = 0.9553235769271851
training loss = 2.1864099502563477 10200
val loss = 0.9734097123146057
training loss = 2.1807878017425537 10300
val loss = 0.9549208879470825
training loss = 2.1812524795532227 10400
val loss = 0.9508283138275146
training loss = 2.1800360679626465 10500
val loss = 0.9546144604682922
training loss = 2.179666757583618 10600
val loss = 0.954147458076477
training loss = 2.5318822860717773 10700
val loss = 1.461268424987793
training loss = 2.1788790225982666 10800
val loss = 0.9541839361190796
training loss = 2.178487539291382 10900
val loss = 0.9535344839096069
training loss = 2.1780829429626465 11000
val loss = 0.9528993368148804
training loss = 2.177717447280884 11100
val loss = 0.9546632170677185
training loss = 2.1772239208221436 11200
val loss = 0.9528403282165527
training loss = 2.281019687652588 11300
val loss = 1.0237232446670532
training loss = 2.1763176918029785 11400
val loss = 0.9527490139007568
training loss = 2.175842046737671 11500
val loss = 0.9522129893302917
training loss = 2.1757314205169678 11600
val loss = 0.9559631943702698
training loss = 2.1748499870300293 11700
val loss = 0.9519971609115601
training loss = 2.1743409633636475 11800
val loss = 0.9511258602142334
training loss = 2.1738526821136475 11900
val loss = 0.9534268379211426
training loss = 2.1732051372528076 12000
val loss = 0.9516441822052002
training loss = 2.178889513015747 12100
val loss = 0.9719967842102051
training loss = 2.1719884872436523 12200
val loss = 0.9514931440353394
training loss = 2.171333074569702 12300
val loss = 0.9515376091003418
training loss = 2.1719954013824463 12400
val loss = 0.9589744806289673
training loss = 2.1699230670928955 12500
val loss = 0.9515774250030518
training loss = 2.1857447624206543 12600
val loss = 0.9517933130264282
training loss = 2.1683542728424072 12700
val loss = 0.9517867565155029
training loss = 2.167487382888794 12800
val loss = 0.9517329335212708
training loss = 2.1667561531066895 12900
val loss = 0.9544011354446411
training loss = 2.1656503677368164 13000
val loss = 0.9523769617080688
training loss = 2.1714389324188232 13100
val loss = 0.9480899572372437
training loss = 2.1635515689849854 13200
val loss = 0.9526095390319824
training loss = 2.1623682975769043 13300
val loss = 0.9532853364944458
training loss = 2.1613707542419434 13400
val loss = 0.9519845247268677
training loss = 2.159869432449341 13500
val loss = 0.954155683517456
training loss = 2.1584904193878174 13600
val loss = 0.9558096528053284
training loss = 2.1570236682891846 13700
val loss = 0.956175684928894
training loss = 2.1553947925567627 13800
val loss = 0.9557154178619385
training loss = 2.1589853763580322 13900
val loss = 0.9526458382606506
training loss = 2.1519129276275635 14000
val loss = 0.9569591283798218
training loss = 2.420588970184326 14100
val loss = 1.344542145729065
training loss = 2.148005723953247 14200
val loss = 0.9588817358016968
training loss = 2.145833969116211 14300
val loss = 0.9592322111129761
training loss = 2.1437416076660156 14400
val loss = 0.961829662322998
training loss = 2.1413347721099854 14500
val loss = 0.9608086347579956
training loss = 2.1388466358184814 14600
val loss = 0.9626526236534119
training loss = 2.1363892555236816 14700
val loss = 0.9615839719772339
training loss = 2.1336443424224854 14800
val loss = 0.9634292125701904
training loss = 2.1316490173339844 14900
val loss = 0.9685161709785461
training loss = 2.1281702518463135 15000
val loss = 0.9651366472244263
training loss = 2.1252050399780273 15100
val loss = 0.9661065340042114
training loss = 2.1225509643554688 15200
val loss = 0.968856930732727
training loss = 2.1193718910217285 15300
val loss = 0.9676649570465088
training loss = 2.1487929821014404 15400
val loss = 0.9859104156494141
training loss = 2.1133744716644287 15500
val loss = 0.9690964818000793
training loss = 2.110262393951416 15600
val loss = 0.9695417881011963
training loss = 2.107344150543213 15700
val loss = 0.9712096452713013
training loss = 2.1043012142181396 15800
val loss = 0.971348226070404
training loss = 2.105196475982666 15900
val loss = 0.9833643436431885
training loss = 2.0985872745513916 16000
val loss = 0.9726924300193787
training loss = 2.095698595046997 16100
val loss = 0.9731770753860474
training loss = 2.0930325984954834 16200
val loss = 0.9750043153762817
training loss = 2.0901999473571777 16300
val loss = 0.9742891192436218
training loss = 2.089588165283203 16400
val loss = 0.9734624028205872
training loss = 2.0849952697753906 16500
val loss = 0.9754868745803833
training loss = 2.1364197731018066 16600
val loss = 1.0617811679840088
training loss = 2.080113649368286 16700
val loss = 0.9764552116394043
training loss = 2.0777218341827393 16800
val loss = 0.9767045974731445
training loss = 2.075711965560913 16900
val loss = 0.9766232967376709
training loss = 2.0735368728637695 17000
val loss = 0.9776096940040588
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 889.2589,  879.9171,  932.2904,  950.1337, 1034.0792, 1115.2993,
        1065.5392, 1168.6383, 1180.1902, 1201.9491, 1182.5035, 1207.1160,
        1247.0374, 1272.0020, 1328.9520, 1455.8771, 1436.5510, 1402.7617,
        1580.5588, 1533.8638, 1605.0166, 1522.3223, 1561.1412, 1611.7986,
        1679.5660, 1744.9056, 1593.0275, 1746.6530, 1730.7463, 1709.6564,
        1649.3632, 1726.7710, 1703.1913, 1726.3716, 1721.8442, 1776.2990,
        1616.0627, 1664.4048, 1736.4650, 1664.5287, 1595.4188, 1594.0831,
        1510.4442, 1508.1659, 1385.5585, 1285.9814, 1310.9615, 1238.8038,
        1114.0874, 1122.3701, 1060.6100, 1009.1588,  943.2715,  898.1399,
         872.7026,  872.1767,  848.4563,  698.5674,  631.0104,  545.6269,
         542.4789,  486.2074,  419.7312,  409.0266,  374.1167,  311.1234,
         291.2685,  257.5968,  210.5551,  177.1517,  148.9785,  156.0606,
         110.9043,  115.2666,   95.0092,   65.0846,   50.0636,   41.8810,
          39.3180,   47.5936,   20.2270,   38.8568,   28.1724])]
2594.536707387961
4.531169191999019 17.034883705811225 97.65791626721958
val isze = 8
idinces = [38 30 54 34 63  8 32  1 76 53 28 37 33 31 14 46 23 82 74 19 29 15  5 13
 27 35 73 12 66 51 17 10 21 78 61 47 75 55  6 42 24 52 72 68 62  3 25 80
 81 77  0  2 59 60 40 45 50 67 69  9 79 39 44 22 43 64 36 71  7 20  4 56
 41 16 58 70 65 18 26 11 57 49 48]
we are doing training validation split
training loss = 536.1624145507812 100
val loss = 833.6505737304688
training loss = 12.998433113098145 200
val loss = 3.807016134262085
training loss = 10.605812072753906 300
val loss = 3.0267953872680664
training loss = 9.62087631225586 400
val loss = 3.723907947540283
training loss = 9.084739685058594 500
val loss = 4.665178298950195
training loss = 8.766051292419434 600
val loss = 5.471676826477051
training loss = 8.547685623168945 700
val loss = 6.000850200653076
training loss = 8.371017456054688 800
val loss = 6.271909236907959
training loss = 8.204805374145508 900
val loss = 6.3612565994262695
training loss = 8.026634216308594 1000
val loss = 6.373315811157227
training loss = 7.833914756774902 1100
val loss = 6.437078475952148
training loss = 7.643754482269287 1200
val loss = 6.609389305114746
training loss = 7.463459014892578 1300
val loss = 6.8002238273620605
training loss = 7.275940895080566 1400
val loss = 6.857019424438477
training loss = 7.069736480712891 1500
val loss = 6.696712493896484
training loss = 6.867405891418457 1600
val loss = 6.394930839538574
training loss = 6.69783878326416 1700
val loss = 6.101680755615234
training loss = 6.565759181976318 1800
val loss = 5.89451789855957
training loss = 6.462182998657227 1900
val loss = 5.771956443786621
training loss = 6.377983570098877 2000
val loss = 5.706207752227783
training loss = 6.308260440826416 2100
val loss = 5.694272994995117
training loss = 6.246107578277588 2200
val loss = 5.655913352966309
training loss = 6.192217826843262 2300
val loss = 5.618048667907715
training loss = 6.140505313873291 2400
val loss = 5.638962268829346
training loss = 6.120504856109619 2500
val loss = 5.540041923522949
training loss = 6.044837474822998 2600
val loss = 5.62601375579834
training loss = 5.996542930603027 2700
val loss = 5.619546890258789
training loss = 5.948679447174072 2800
val loss = 5.605727195739746
training loss = 5.897646427154541 2900
val loss = 5.60322380065918
training loss = 5.8465166091918945 3000
val loss = 5.5843353271484375
training loss = 5.792227745056152 3100
val loss = 5.584907531738281
training loss = 5.7397661209106445 3200
val loss = 5.52934455871582
training loss = 5.687331676483154 3300
val loss = 5.55533504486084
training loss = 5.650518894195557 3400
val loss = 5.594338893890381
training loss = 5.609897136688232 3500
val loss = 5.519381523132324
training loss = 5.58918571472168 3600
val loss = 5.515650749206543
training loss = 5.579761981964111 3700
val loss = 5.511896133422852
training loss = 5.57534122467041 3800
val loss = 5.529359817504883
training loss = 5.573399543762207 3900
val loss = 5.538833141326904
training loss = 5.572447299957275 4000
val loss = 5.5345988273620605
training loss = 5.5718092918396 4100
val loss = 5.5484795570373535
training loss = 5.571196556091309 4200
val loss = 5.552458763122559
training loss = 5.5705461502075195 4300
val loss = 5.548255443572998
training loss = 5.569887638092041 4400
val loss = 5.555391788482666
training loss = 5.569396018981934 4500
val loss = 5.546080589294434
training loss = 5.568154335021973 4600
val loss = 5.555576324462891
training loss = 5.5670695304870605 4700
val loss = 5.558426856994629
training loss = 5.569451808929443 4800
val loss = 5.597647190093994
training loss = 5.564096927642822 4900
val loss = 5.558465957641602
training loss = 5.5621418952941895 5000
val loss = 5.567111968994141
training loss = 5.559325695037842 5100
val loss = 5.551362037658691
training loss = 5.5555620193481445 5200
val loss = 5.5576019287109375
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 866.0522,  908.6073,  953.9665,  924.2932, 1019.5213, 1041.6504,
        1096.0177, 1094.2523, 1155.5033, 1241.8207, 1198.8046, 1170.0712,
        1279.8833, 1265.2231, 1314.1921, 1420.1969, 1406.8217, 1427.8627,
        1568.1562, 1484.1995, 1614.5797, 1520.4099, 1600.4259, 1648.3608,
        1708.8267, 1690.3087, 1633.7495, 1717.9235, 1740.9330, 1645.8538,
        1640.8878, 1727.4186, 1750.0687, 1740.8180, 1726.7024, 1848.3600,
        1647.8146, 1620.9421, 1663.5221, 1620.2455, 1639.3975, 1548.6584,
        1452.2788, 1555.6414, 1406.0647, 1365.5874, 1328.4445, 1187.4299,
        1181.7572, 1166.2488, 1061.0991,  975.8088, 1015.9399,  871.5920,
         936.2559,  894.6685,  841.0272,  686.1176,  628.6979,  550.1539,
         537.7955,  457.7958,  441.8386,  403.2645,  362.3979,  321.0826,
         275.0108,  232.0373,  192.3992,  167.5132,  155.9854,  130.6325,
         144.1390,  113.3233,   95.4638,   78.3645,   47.5855,   42.1849,
          37.6123,   54.8771,   13.0389,   32.2851,   37.0621])]
2463.231038599361
4.71739403606101 10.932913521931077 65.11347845580156
val isze = 8
idinces = [55 42 45 51 22 44 62 43 59 66 70 64  2 49  9 61 25 68 47 32 77 50 39 80
 57 58 29 13 54 60 56 48 76 28 36 72  3 24 53 19 31 17 46 82  0 74 16 38
 37  8 79 11 15 10 65  5 78 14  6 73 34 69 30 52 27  4 21 41 12 71 75 23
  7 26 33 40 63  1 81 18 35 20 67]
we are doing training validation split
training loss = 544.7144165039062 100
val loss = 990.475341796875
training loss = 351.03118896484375 200
val loss = 637.802734375
training loss = 4.362405776977539 300
val loss = 3.2260324954986572
training loss = 3.8939568996429443 400
val loss = 2.827629566192627
training loss = 3.6995904445648193 500
val loss = 2.749678134918213
training loss = 3.5734879970550537 600
val loss = 2.691880702972412
training loss = 3.310926675796509 700
val loss = 2.648205280303955
training loss = 3.139861822128296 800
val loss = 2.627067804336548
training loss = 2.9575376510620117 900
val loss = 2.5824782848358154
training loss = 2.8223159313201904 1000
val loss = 2.582102060317993
training loss = 2.6851084232330322 1100
val loss = 2.561984062194824
training loss = 2.9680941104888916 1200
val loss = 2.70790696144104
training loss = 2.489025592803955 1300
val loss = 2.5768847465515137
training loss = 2.4130194187164307 1400
val loss = 2.594327211380005
training loss = 2.3533694744110107 1500
val loss = 2.6054067611694336
training loss = 2.2993876934051514 1600
val loss = 2.6371376514434814
training loss = 2.2761199474334717 1700
val loss = 2.731071949005127
training loss = 2.219372272491455 1800
val loss = 2.684684991836548
training loss = 2.1875510215759277 1900
val loss = 2.7068324089050293
training loss = 2.1623377799987793 2000
val loss = 2.732410430908203
training loss = 2.1394731998443604 2100
val loss = 2.755164384841919
training loss = 2.1302108764648438 2200
val loss = 2.737730026245117
training loss = 2.104656219482422 2300
val loss = 2.796884775161743
training loss = 2.089796304702759 2400
val loss = 2.812708854675293
training loss = 2.078523635864258 2500
val loss = 2.829926013946533
training loss = 2.0670828819274902 2600
val loss = 2.846827507019043
training loss = 2.914716958999634 2700
val loss = 3.981499671936035
training loss = 2.048917293548584 2800
val loss = 2.8772497177124023
training loss = 2.0404257774353027 2900
val loss = 2.8837480545043945
training loss = 2.0370216369628906 3000
val loss = 2.9320690631866455
training loss = 2.026757001876831 3100
val loss = 2.902456760406494
training loss = 2.075469493865967 3200
val loss = 2.8108692169189453
training loss = 2.01484751701355 3300
val loss = 2.9176273345947266
training loss = 3.0214898586273193 3400
val loss = 2.9944674968719482
training loss = 2.004472255706787 3500
val loss = 2.923215627670288
training loss = 1.9989701509475708 3600
val loss = 2.929935932159424
training loss = 1.9949731826782227 3700
val loss = 2.939788341522217
training loss = 1.9898905754089355 3800
val loss = 2.9351747035980225
training loss = 2.0365214347839355 3900
val loss = 2.834841251373291
training loss = 1.9814320802688599 4000
val loss = 2.9397687911987305
training loss = 2.0932724475860596 4100
val loss = 3.255659580230713
training loss = 1.973457932472229 4200
val loss = 2.9422826766967773
training loss = 2.3701109886169434 4300
val loss = 3.6076154708862305
training loss = 1.9660297632217407 4400
val loss = 2.9426703453063965
training loss = 1.9618500471115112 4500
val loss = 2.9379563331604004
training loss = 1.9593652486801147 4600
val loss = 2.9284582138061523
training loss = 1.954949975013733 4700
val loss = 2.9395923614501953
training loss = 1.9526139497756958 4800
val loss = 2.927797794342041
training loss = 1.9483999013900757 4900
val loss = 2.9369330406188965
training loss = 1.9646456241607666 5000
val loss = 3.033966541290283
training loss = 1.9421916007995605 5100
val loss = 2.9339466094970703
training loss = 1.9408185482025146 5200
val loss = 2.946169853210449
training loss = 1.936456322669983 5300
val loss = 2.9297664165496826
training loss = 1.9329708814620972 5400
val loss = 2.929502010345459
training loss = 1.931217074394226 5500
val loss = 2.9222960472106934
training loss = 1.9277913570404053 5600
val loss = 2.9271090030670166
training loss = 1.996935486793518 5700
val loss = 2.819338321685791
training loss = 1.9226088523864746 5800
val loss = 2.9232215881347656
training loss = 1.927552580833435 5900
val loss = 2.876488208770752
training loss = 1.9176156520843506 6000
val loss = 2.9149105548858643
training loss = 1.9144716262817383 6100
val loss = 2.9180994033813477
reduced chi^2 level 2 = 1.9130173921585083
Constrained alpha: 4.02276086807251
Constrained beta: 2.9560141563415527
Constrained gamma: 42.86717987060547
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 855.6489,  838.0590,  908.6693,  973.3311, 1000.9885, 1039.5741,
        1086.7054, 1085.8888, 1134.7521, 1147.0312, 1187.0155, 1234.6659,
        1294.7850, 1257.1858, 1310.0962, 1440.0715, 1443.1534, 1369.4950,
        1492.7213, 1532.0419, 1642.7557, 1534.0303, 1602.6329, 1542.7253,
        1652.3329, 1704.6031, 1582.5001, 1724.4226, 1697.3748, 1768.3119,
        1657.2450, 1771.7897, 1708.8934, 1729.4493, 1741.1016, 1716.3837,
        1657.3783, 1657.2712, 1627.1128, 1523.6188, 1552.7933, 1523.0734,
        1455.7964, 1614.1057, 1382.0784, 1309.4392, 1256.0074, 1226.0989,
        1150.9976, 1258.5292, 1091.1002,  978.5278,  938.2405,  884.7902,
         880.2894,  898.7592,  830.2558,  691.6450,  676.5609,  526.5460,
         539.0430,  441.6356,  444.7691,  422.8662,  376.1399,  364.0888,
         278.6979,  252.1460,  190.1699,  163.0309,  189.4918,  160.8948,
         125.8157,  112.1020,   90.1069,   60.1016,   59.5641,   38.0377,
          32.0560,   41.8891,   15.0521,   46.6585,   34.7846])]
2610.058189057732
2.8855561302267665 5.632138632037198 27.235802680512087
val isze = 8
idinces = [66 60 35 18 46 71 58 67 22 82 40 53 55  1 50 17 41 14 68 49 12 43 63 75
 26 61 11  7 51 10 59 80 45 21 54 78 19 37 74  9 23  5 20 72 16 48 34  6
 31 69 30 28 15  0 13 57 79 65 81 56 36 42 33 39 73 32 76  4  8 24 52 38
  2 64 27 25 29  3 77 70 44 47 62]
we are doing training validation split
training loss = 41.73735046386719 100
val loss = 32.75531005859375
training loss = 26.472681045532227 200
val loss = 19.696346282958984
training loss = 18.272235870361328 300
val loss = 14.561083793640137
training loss = 13.652045249938965 400
val loss = 10.836023330688477
training loss = 10.943242073059082 500
val loss = 8.271202087402344
training loss = 9.291388511657715 600
val loss = 6.556900501251221
training loss = 8.253751754760742 700
val loss = 5.439115524291992
training loss = 7.587779998779297 800
val loss = 4.728780746459961
training loss = 7.153468608856201 900
val loss = 4.291455268859863
training loss = 6.86647891998291 1000
val loss = 4.0335774421691895
training loss = 6.674330711364746 1100
val loss = 3.890960693359375
training loss = 6.543495178222656 1200
val loss = 3.819916248321533
training loss = 6.452084541320801 1300
val loss = 3.7910141944885254
training loss = 6.385458946228027 1400
val loss = 3.7844767570495605
training loss = 6.333370685577393 1500
val loss = 3.7869629859924316
training loss = 6.287775039672852 1600
val loss = 3.7888944149017334
training loss = 6.240314960479736 1700
val loss = 3.781491994857788
training loss = 6.176984786987305 1800
val loss = 3.751164436340332
training loss = 6.061418533325195 1900
val loss = 3.661782741546631
training loss = 5.800307750701904 2000
val loss = 3.4139552116394043
training loss = 5.386826038360596 2100
val loss = 3.015834093093872
training loss = 4.9396281242370605 2200
val loss = 2.6572558879852295
training loss = 4.430663585662842 2300
val loss = 2.27836012840271
training loss = 3.887312889099121 2400
val loss = 1.8889529705047607
training loss = 3.3793764114379883 2500
val loss = 1.5582976341247559
training loss = 2.9874417781829834 2600
val loss = 1.3582122325897217
training loss = 2.7487993240356445 2700
val loss = 1.3057092428207397
training loss = 2.6348507404327393 2800
val loss = 1.3457057476043701
training loss = 2.589466094970703 2900
val loss = 1.405869722366333
training loss = 2.571787118911743 3000
val loss = 1.4489164352416992
training loss = 2.563288450241089 3100
val loss = 1.4707633256912231
training loss = 2.557619333267212 3200
val loss = 1.4787627458572388
training loss = 2.5528547763824463 3300
val loss = 1.4797272682189941
training loss = 2.5484025478363037 3400
val loss = 1.477638602256775
training loss = 2.5440616607666016 3500
val loss = 1.4743915796279907
training loss = 2.539764165878296 3600
val loss = 1.470771312713623
training loss = 2.5354835987091064 3700
val loss = 1.4671149253845215
training loss = 2.5312185287475586 3800
val loss = 1.463529348373413
training loss = 2.5269644260406494 3900
val loss = 1.4600402116775513
training loss = 2.5227267742156982 4000
val loss = 1.4566739797592163
training loss = 2.5185067653656006 4100
val loss = 1.4534339904785156
training loss = 2.5143070220947266 4200
val loss = 1.4502909183502197
training loss = 2.510131359100342 4300
val loss = 1.4472562074661255
training loss = 2.5059823989868164 4400
val loss = 1.4443426132202148
training loss = 2.5018627643585205 4500
val loss = 1.4415091276168823
training loss = 2.4994077682495117 4600
val loss = 1.4214402437210083
training loss = 2.4936037063598633 4700
val loss = 1.4383012056350708
training loss = 2.5158724784851074 4800
val loss = 1.3768106698989868
training loss = 2.4855613708496094 4900
val loss = 1.4338078498840332
training loss = 2.4817299842834473 5000
val loss = 1.4307622909545898
training loss = 2.477898597717285 5100
val loss = 1.4273405075073242
training loss = 2.474208354949951 5200
val loss = 1.4266033172607422
training loss = 2.4718875885009766 5300
val loss = 1.4070032835006714
training loss = 2.4670605659484863 5400
val loss = 1.421859622001648
training loss = 2.4641618728637695 5500
val loss = 1.407456636428833
training loss = 2.4601707458496094 5600
val loss = 1.4174079895019531
training loss = 2.459059715270996 5700
val loss = 1.439752459526062
training loss = 2.453416109085083 5800
val loss = 1.412451982498169
training loss = 2.483856201171875 5900
val loss = 1.5264086723327637
training loss = 2.446654796600342 6000
val loss = 1.4069808721542358
training loss = 2.4433484077453613 6100
val loss = 1.410300850868225
training loss = 2.439746141433716 6200
val loss = 1.4017329216003418
training loss = 2.4362194538116455 6300
val loss = 1.399808406829834
training loss = 2.434117078781128 6400
val loss = 1.3774445056915283
training loss = 2.428942918777466 6500
val loss = 1.3944168090820312
training loss = 2.4257991313934326 6600
val loss = 1.378790020942688
training loss = 2.4213640689849854 6700
val loss = 1.3901028633117676
training loss = 2.417447805404663 6800
val loss = 1.3849503993988037
training loss = 2.4135947227478027 6900
val loss = 1.3864495754241943
training loss = 2.409548282623291 7000
val loss = 1.3779144287109375
training loss = 2.509572744369507 7100
val loss = 1.2874946594238281
training loss = 2.4015309810638428 7200
val loss = 1.3713064193725586
training loss = 2.3974900245666504 7300
val loss = 1.3666458129882812
training loss = 2.5357747077941895 7400
val loss = 1.2777127027511597
training loss = 2.389312505722046 7500
val loss = 1.3578165769577026
training loss = 2.38516902923584 7600
val loss = 1.3550095558166504
training loss = 2.385784864425659 7700
val loss = 1.3206740617752075
training loss = 2.376950740814209 7800
val loss = 1.347762942314148
training loss = 2.372770309448242 7900
val loss = 1.343551516532898
training loss = 2.3701348304748535 8000
val loss = 1.3212093114852905
training loss = 2.364441394805908 8100
val loss = 1.3360130786895752
training loss = 2.3637032508850098 8200
val loss = 1.3052351474761963
training loss = 2.356102228164673 8300
val loss = 1.3263225555419922
training loss = 2.351865291595459 8400
val loss = 1.324981689453125
training loss = 2.3481674194335938 8500
val loss = 1.3312323093414307
training loss = 2.3437447547912598 8600
val loss = 1.3182992935180664
training loss = 2.351728677749634 8700
val loss = 1.3779325485229492
training loss = 2.3357372283935547 8800
val loss = 1.311685562133789
training loss = 2.3351712226867676 8900
val loss = 1.3399156332015991
training loss = 2.3280177116394043 9000
val loss = 1.307850956916809
training loss = 2.3241875171661377 9100
val loss = 1.3021345138549805
training loss = 2.3282878398895264 9200
val loss = 1.3503564596176147
training loss = 2.3169333934783936 9300
val loss = 1.2967684268951416
training loss = 2.3133389949798584 9400
val loss = 1.2932639122009277
training loss = 2.318307638168335 9500
val loss = 1.3415095806121826
training loss = 2.3065080642700195 9600
val loss = 1.287972331047058
training loss = 2.303079843521118 9700
val loss = 1.284838080406189
training loss = 2.299922227859497 9800
val loss = 1.2831974029541016
training loss = 2.2967772483825684 9900
val loss = 1.2794196605682373
training loss = 2.293710947036743 10000
val loss = 1.270559549331665
training loss = 2.2905502319335938 10100
val loss = 1.2742924690246582
training loss = 2.287468194961548 10200
val loss = 1.2715290784835815
training loss = 2.2845640182495117 10300
val loss = 1.264850378036499
training loss = 2.2815160751342773 10400
val loss = 1.2664196491241455
training loss = 2.2899763584136963 10500
val loss = 1.2218997478485107
training loss = 2.275562286376953 10600
val loss = 1.260822057723999
training loss = 2.501798629760742 10700
val loss = 1.2055187225341797
training loss = 2.2694644927978516 10800
val loss = 1.2537736892700195
training loss = 2.266216993331909 10900
val loss = 1.2508631944656372
training loss = 2.263298749923706 11000
val loss = 1.2556214332580566
training loss = 2.259531259536743 11100
val loss = 1.243133783340454
training loss = 2.2579846382141113 11200
val loss = 1.2202821969985962
training loss = 2.2523114681243896 11300
val loss = 1.2338968515396118
training loss = 2.2485148906707764 11400
val loss = 1.227910041809082
training loss = 2.246307373046875 11500
val loss = 1.2407606840133667
training loss = 2.241098165512085 11600
val loss = 1.2166998386383057
training loss = 2.456267833709717 11700
val loss = 1.5854299068450928
training loss = 2.2341980934143066 11800
val loss = 1.2071349620819092
training loss = 2.2308764457702637 11900
val loss = 1.2004027366638184
training loss = 2.2278780937194824 12000
val loss = 1.1896945238113403
training loss = 2.2246897220611572 12100
val loss = 1.1895742416381836
training loss = 2.22479510307312 12200
val loss = 1.1621211767196655
training loss = 2.2187411785125732 12300
val loss = 1.1805579662322998
training loss = 2.2157931327819824 12400
val loss = 1.173397421836853
training loss = 2.2198634147644043 12500
val loss = 1.2078696489334106
training loss = 2.2102584838867188 12600
val loss = 1.1622053384780884
training loss = 2.207496166229248 12700
val loss = 1.1568673849105835
training loss = 2.204882860183716 12800
val loss = 1.1510010957717896
training loss = 2.2023353576660156 12900
val loss = 1.1452405452728271
training loss = 2.1999125480651855 13000
val loss = 1.1431834697723389
training loss = 2.1975748538970947 13100
val loss = 1.132865071296692
training loss = 2.1953299045562744 13200
val loss = 1.1277077198028564
training loss = 2.1946961879730225 13300
val loss = 1.1377102136611938
training loss = 2.1913015842437744 13400
val loss = 1.1163278818130493
training loss = 2.2185134887695312 13500
val loss = 1.0664494037628174
training loss = 2.1877951622009277 13600
val loss = 1.1055388450622559
training loss = 2.202665328979492 13700
val loss = 1.1622943878173828
training loss = 2.184786558151245 13800
val loss = 1.09426748752594
training loss = 2.183403491973877 13900
val loss = 1.0906703472137451
training loss = 2.183767080307007 14000
val loss = 1.0719470977783203
training loss = 2.181042432785034 14100
val loss = 1.0818003416061401
training loss = 2.1799309253692627 14200
val loss = 1.0775631666183472
training loss = 2.180067539215088 14300
val loss = 1.0868616104125977
training loss = 2.178025722503662 14400
val loss = 1.0694948434829712
training loss = 2.177917242050171 14500
val loss = 1.0768464803695679
training loss = 2.176372528076172 14600
val loss = 1.0639588832855225
training loss = 2.1755664348602295 14700
val loss = 1.0583080053329468
training loss = 2.1750595569610596 14800
val loss = 1.0501782894134521
training loss = 2.1742148399353027 14900
val loss = 1.0517418384552002
training loss = 2.1810128688812256 15000
val loss = 1.0226261615753174
training loss = 2.1730494499206543 15100
val loss = 1.0448170900344849
training loss = 2.1724905967712402 15200
val loss = 1.0421862602233887
training loss = 2.17203426361084 15300
val loss = 1.040036916732788
training loss = 2.171552896499634 15400
val loss = 1.0365644693374634
training loss = 2.1711502075195312 15500
val loss = 1.0365805625915527
training loss = 2.1707324981689453 15600
val loss = 1.030500888824463
training loss = 2.17033052444458 15700
val loss = 1.0283863544464111
training loss = 2.2950518131256104 15800
val loss = 0.9882436990737915
training loss = 2.1696391105651855 15900
val loss = 1.0234706401824951
training loss = 2.169292449951172 16000
val loss = 1.0208181142807007
training loss = 2.169198513031006 16100
val loss = 1.0234943628311157
training loss = 2.168715000152588 16200
val loss = 1.01603364944458
training loss = 2.185152292251587 16300
val loss = 1.0759685039520264
training loss = 2.1682043075561523 16400
val loss = 1.0116486549377441
training loss = 2.1679418087005615 16500
val loss = 1.0079882144927979
training loss = 2.1678504943847656 16600
val loss = 1.0031862258911133
training loss = 2.1674928665161133 16700
val loss = 1.0049744844436646
training loss = 2.1679327487945557 16800
val loss = 1.0129790306091309
training loss = 2.1670944690704346 16900
val loss = 1.0008580684661865
training loss = 2.2109453678131104 17000
val loss = 1.1135543584823608
training loss = 2.1667420864105225 17100
val loss = 0.9972299337387085
training loss = 2.1665403842926025 17200
val loss = 0.9947967529296875
training loss = 2.1664841175079346 17300
val loss = 0.9898619055747986
training loss = 2.166222333908081 17400
val loss = 0.9911905527114868
training loss = 2.1663222312927246 17500
val loss = 0.9838017225265503
training loss = 2.16593074798584 17600
val loss = 0.9875296354293823
training loss = 2.1778430938720703 17700
val loss = 1.0371991395950317
training loss = 2.1656713485717773 17800
val loss = 0.9839175939559937
training loss = 2.1666924953460693 17900
val loss = 0.9699312448501587
training loss = 2.165461778640747 18000
val loss = 0.978634238243103
training loss = 2.165281295776367 18100
val loss = 0.9789475202560425
training loss = 2.165980339050293 18200
val loss = 0.9886672496795654
training loss = 2.1650683879852295 18300
val loss = 0.975753664970398
training loss = 2.1829373836517334 18400
val loss = 0.9363653659820557
training loss = 2.1648988723754883 18500
val loss = 0.9731452465057373
training loss = 2.1647682189941406 18600
val loss = 0.9712203741073608
training loss = 2.1760306358337402 18700
val loss = 1.0196533203125
training loss = 2.164600372314453 18800
val loss = 0.9683805108070374
training loss = 2.2742936611175537 18900
val loss = 1.1820279359817505
training loss = 2.164458751678467 19000
val loss = 0.9641132950782776
training loss = 2.1643333435058594 19100
val loss = 0.9640381336212158
training loss = 2.16438364982605 19200
val loss = 0.9593524932861328
training loss = 2.16420316696167 19300
val loss = 0.96123206615448
training loss = 2.1920135021209717 19400
val loss = 1.0482972860336304
training loss = 2.1640830039978027 19500
val loss = 0.9587693214416504
training loss = 2.1640655994415283 19600
val loss = 0.9534773826599121
training loss = 2.164015054702759 19700
val loss = 0.9584938287734985
training loss = 2.1638755798339844 19800
val loss = 0.9546616077423096
training loss = 2.1697518825531006 19900
val loss = 0.9883010983467102
training loss = 2.1637930870056152 20000
val loss = 0.952468991279602
training loss = 2.163702964782715 20100
val loss = 0.950663685798645
training loss = 2.1638667583465576 20200
val loss = 0.9548410177230835
training loss = 2.1636364459991455 20300
val loss = 0.9487712383270264
training loss = 2.3833866119384766 20400
val loss = 0.9364930391311646
training loss = 2.1635680198669434 20500
val loss = 0.9457532167434692
training loss = 2.1634814739227295 20600
val loss = 0.9447637796401978
training loss = 2.16372013092041 20700
val loss = 0.9385913014411926
training loss = 2.1634252071380615 20800
val loss = 0.9430441856384277
training loss = 2.1710355281829834 20900
val loss = 0.9838118553161621
training loss = 2.1633808612823486 21000
val loss = 0.940896213054657
training loss = 2.1633052825927734 21100
val loss = 0.9395713806152344
training loss = 2.1634247303009033 21200
val loss = 0.9428393244743347
training loss = 2.1632630825042725 21300
val loss = 0.9378578662872314
training loss = 2.186687707901001 21400
val loss = 1.019017219543457
training loss = 2.1632325649261475 21500
val loss = 0.9359101057052612
training loss = 2.163161039352417 21600
val loss = 0.9343071579933167
training loss = 2.163429021835327 21700
val loss = 0.9405269622802734
training loss = 2.163120746612549 21800
val loss = 0.9330422282218933
training loss = 2.166569709777832 21900
val loss = 0.9108279943466187
training loss = 2.1630859375 22000
val loss = 0.931225061416626
training loss = 2.2797603607177734 22100
val loss = 1.1696670055389404
training loss = 2.1630606651306152 22200
val loss = 0.930341899394989
training loss = 2.162980079650879 22300
val loss = 0.9282627701759338
training loss = 2.163087844848633 22400
val loss = 0.9308133721351624
training loss = 2.1629552841186523 22500
val loss = 0.9267542362213135
training loss = 2.2709908485412598 22600
val loss = 0.8769941329956055
training loss = 2.162930488586426 22700
val loss = 0.9234760999679565
training loss = 2.1628358364105225 22800
val loss = 0.923926830291748
training loss = 2.1632769107818604 22900
val loss = 0.9318032264709473
training loss = 2.1627919673919678 23000
val loss = 0.9223234057426453
training loss = 2.168203353881836 23100
val loss = 0.9570285081863403
training loss = 2.162763833999634 23200
val loss = 0.9183558821678162
training loss = 2.1626389026641846 23300
val loss = 0.9196274876594543
training loss = 2.1727349758148193 23400
val loss = 0.969515860080719
training loss = 2.1626029014587402 23500
val loss = 0.9183187484741211
training loss = 2.1624972820281982 23600
val loss = 0.9170311689376831
training loss = 2.162753105163574 23700
val loss = 0.9227348566055298
training loss = 2.1624507904052734 23800
val loss = 0.9155369400978088
training loss = 2.1623375415802 23900
val loss = 0.9144903421401978
training loss = 2.163540840148926 24000
val loss = 0.9295691251754761
training loss = 2.1622657775878906 24100
val loss = 0.9126732349395752
training loss = 2.1687591075897217 24200
val loss = 0.8814952969551086
training loss = 2.1621928215026855 24300
val loss = 0.912074863910675
training loss = 2.162060022354126 24400
val loss = 0.9101063013076782
training loss = 2.162766218185425 24500
val loss = 0.9218820333480835
training loss = 2.1619768142700195 24600
val loss = 0.9085116386413574
training loss = 2.2962636947631836 24700
val loss = 0.862337589263916
training loss = 2.161919355392456 24800
val loss = 0.9061065912246704
training loss = 2.161776065826416 24900
val loss = 0.9057186841964722
training loss = 2.1620960235595703 25000
val loss = 0.8990243673324585
training loss = 2.16170072555542 25100
val loss = 0.904325544834137
training loss = 2.3372535705566406 25200
val loss = 0.8683406114578247
training loss = 2.161637783050537 25300
val loss = 0.9028759002685547
training loss = 2.1614861488342285 25400
val loss = 0.9017897248268127
training loss = 2.162616014480591 25500
val loss = 0.9176509380340576
training loss = 2.1614136695861816 25600
val loss = 0.9002994298934937
training loss = 2.1639773845672607 25700
val loss = 0.8837592005729675
training loss = 2.161339044570923 25800
val loss = 0.8981236219406128
training loss = 2.161173105239868 25900
val loss = 0.8984013795852661
training loss = 2.1614584922790527 26000
val loss = 0.9034363627433777
training loss = 2.1611270904541016 26100
val loss = 0.8964664340019226
training loss = 2.204265594482422 26200
val loss = 1.041322946548462
training loss = 2.1610536575317383 26300
val loss = 0.8940850496292114
training loss = 2.1608688831329346 26400
val loss = 0.8935102224349976
training loss = 2.1610803604125977 26500
val loss = 0.8982881307601929
training loss = 2.1608052253723145 26600
val loss = 0.8926491141319275
training loss = 2.2169129848480225 26700
val loss = 1.0626366138458252
training loss = 2.1607439517974854 26800
val loss = 0.8911619186401367
training loss = 2.160546064376831 26900
val loss = 0.8901486992835999
training loss = 2.161132574081421 27000
val loss = 0.8805002570152283
training loss = 2.1605002880096436 27100
val loss = 0.8893016576766968
training loss = 2.295262098312378 27200
val loss = 1.2020645141601562
training loss = 2.1604394912719727 27300
val loss = 0.8893213272094727
training loss = 2.1602132320404053 27400
val loss = 0.8868799209594727
training loss = 2.1679558753967285 27500
val loss = 0.9367656707763672
training loss = 2.160320281982422 27600
val loss = 0.8866719007492065
training loss = 2.1601004600524902 27700
val loss = 0.8853039741516113
training loss = 2.5348165035247803 27800
val loss = 0.9227467775344849
training loss = 2.1600089073181152 27900
val loss = 0.8826818466186523
training loss = 2.159778594970703 28000
val loss = 0.8849348425865173
training loss = 2.1600310802459717 28100
val loss = 0.8774462938308716
training loss = 2.1596765518188477 28200
val loss = 0.8820921182632446
training loss = 2.1637985706329346 28300
val loss = 0.9178497791290283
training loss = 2.159681797027588 28400
val loss = 0.8813108205795288
training loss = 2.1594388484954834 28500
val loss = 0.8803737759590149
training loss = 2.1597633361816406 28600
val loss = 0.8856471180915833
training loss = 2.1594111919403076 28700
val loss = 0.8796266317367554
training loss = 2.161470651626587 28800
val loss = 0.8674509525299072
training loss = 2.1594467163085938 28900
val loss = 0.8805395364761353
training loss = 2.1592016220092773 29000
val loss = 0.8780931234359741
training loss = 2.1690573692321777 29100
val loss = 0.9376224279403687
training loss = 2.1592259407043457 29200
val loss = 0.8780342936515808
training loss = 2.1590051651000977 29300
val loss = 0.8768504858016968
training loss = 2.1598715782165527 29400
val loss = 0.8903220295906067
training loss = 2.15907883644104 29500
val loss = 0.8765038847923279
training loss = 2.159902334213257 29600
val loss = 0.8590774536132812
training loss = 2.1591172218322754 29700
val loss = 0.8764917850494385
training loss = 2.158916473388672 29800
val loss = 0.8754727840423584
training loss = 2.159477710723877 29900
val loss = 0.8872009515762329
training loss = 2.158961296081543 30000
val loss = 0.8753247261047363
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 850.8481,  875.2598,  932.1821,  916.1663,  988.8758, 1095.6682,
        1016.2939, 1155.8350, 1113.7002, 1182.1099, 1131.8248, 1221.1854,
        1244.0222, 1251.6503, 1286.2823, 1434.5909, 1337.3838, 1456.3370,
        1567.2186, 1461.1299, 1617.7769, 1569.4167, 1587.3014, 1708.0603,
        1587.0425, 1710.7727, 1636.6356, 1767.4574, 1712.2789, 1713.8529,
        1667.1576, 1658.0452, 1769.0873, 1732.0938, 1748.2399, 1739.4069,
        1629.3160, 1636.9191, 1689.1344, 1610.9198, 1659.5389, 1540.0804,
        1455.4298, 1479.0356, 1363.5166, 1289.5929, 1299.3424, 1281.4520,
        1164.7104, 1164.0930, 1116.0465,  916.7496,  936.8947,  934.5728,
         867.4764,  835.8649,  842.3264,  733.2545,  652.5737,  556.5939,
         507.6248,  479.1878,  455.2298,  374.9132,  367.8362,  343.8424,
         270.5701,  268.0282,  202.4909,  147.1886,  162.6527,  118.0472,
         141.9822,   89.9391,  103.8950,   84.2261,   53.4103,   38.8270,
          37.9001,   47.1347,   24.3702,   39.9758,   38.2827])]
2805.8700018942613
4.956148018364871 15.825992374909394 5.196702002922238
val isze = 8
idinces = [52  4 80 46 78 67  6 59 76 38 23 58 10 43 63  5 50 40 34 25 65 33 53 16
 49 74 36 35 66  2 56  8 75  9 27 68 21 42  0 54 29 32 81 19 64 26 47 51
 28 77  1 45 72 13 20 70 79 60 39 12 31  3  7 24 37 22 48 18 57 69 17 15
 73 61 55 30 71 44 41 62 11 82 14]
we are doing training validation split
training loss = 520.8182983398438 100
val loss = 422.90118408203125
training loss = 7.7432942390441895 200
val loss = 11.13559341430664
training loss = 6.769455432891846 300
val loss = 10.395590782165527
training loss = 6.7454514503479 400
val loss = 10.353239059448242
training loss = 6.722907066345215 500
val loss = 10.309537887573242
training loss = 6.702624797821045 600
val loss = 10.267351150512695
training loss = 6.684784412384033 700
val loss = 10.227529525756836
training loss = 6.669130802154541 800
val loss = 10.190584182739258
training loss = 6.655170917510986 900
val loss = 10.156449317932129
training loss = 6.642355442047119 1000
val loss = 10.124774932861328
training loss = 6.630175590515137 1100
val loss = 10.09530258178711
training loss = 6.618253707885742 1200
val loss = 10.067625045776367
training loss = 6.606314182281494 1300
val loss = 10.041326522827148
training loss = 6.594202518463135 1400
val loss = 10.015779495239258
training loss = 6.581847667694092 1500
val loss = 9.991016387939453
training loss = 6.569223880767822 1600
val loss = 9.96673583984375
training loss = 6.5563435554504395 1700
val loss = 9.942718505859375
training loss = 6.543217658996582 1800
val loss = 9.918617248535156
training loss = 6.529876232147217 1900
val loss = 9.894554138183594
training loss = 6.516336917877197 2000
val loss = 9.870322227478027
training loss = 6.502617835998535 2100
val loss = 9.845958709716797
training loss = 6.488733768463135 2200
val loss = 9.821374893188477
training loss = 6.474692344665527 2300
val loss = 9.796540260314941
training loss = 6.4605021476745605 2400
val loss = 9.771434783935547
training loss = 6.446164131164551 2500
val loss = 9.746163368225098
training loss = 6.4316792488098145 2600
val loss = 9.720617294311523
training loss = 6.417046070098877 2700
val loss = 9.6947021484375
training loss = 6.402266979217529 2800
val loss = 9.668622970581055
training loss = 6.387333393096924 2900
val loss = 9.642091751098633
training loss = 6.372246265411377 3000
val loss = 9.615306854248047
training loss = 6.357004642486572 3100
val loss = 9.58818244934082
training loss = 6.341617107391357 3200
val loss = 9.561037063598633
training loss = 6.326089859008789 3300
val loss = 9.533349990844727
training loss = 6.310448169708252 3400
val loss = 9.505462646484375
training loss = 6.294729232788086 3500
val loss = 9.477222442626953
training loss = 6.278992176055908 3600
val loss = 9.449040412902832
training loss = 6.263336658477783 3700
val loss = 9.416292190551758
training loss = 6.248154640197754 3800
val loss = 9.408773422241211
training loss = 6.2335734367370605 3900
val loss = 9.372303009033203
training loss = 6.220065593719482 4000
val loss = 9.373824119567871
training loss = 6.207540035247803 4100
val loss = 9.32094669342041
training loss = 6.196840286254883 4200
val loss = 9.249958992004395
training loss = 6.187091827392578 4300
val loss = 9.284750938415527
training loss = 6.195423126220703 4400
val loss = 8.876540184020996
training loss = 6.172580718994141 4500
val loss = 9.261043548583984
training loss = 6.167212009429932 4600
val loss = 9.238992691040039
training loss = 6.163064002990723 4700
val loss = 9.217817306518555
training loss = 6.159553527832031 4800
val loss = 9.23287582397461
training loss = 6.202606201171875 4900
val loss = 8.57668685913086
training loss = 6.153604507446289 5000
val loss = 9.219956398010254
training loss = 6.155356407165527 5100
val loss = 9.441140174865723
training loss = 6.14668607711792 5200
val loss = 9.221500396728516
training loss = 6.140992164611816 5300
val loss = 9.197477340698242
training loss = 6.13101863861084 5400
val loss = 9.207058906555176
training loss = 6.106505870819092 5500
val loss = 9.1297025680542
training loss = 5.999365329742432 5600
val loss = 9.033713340759277
training loss = 5.273597240447998 5700
val loss = 7.469887733459473
training loss = 4.122118949890137 5800
val loss = 5.362362861633301
training loss = 2.8311924934387207 5900
val loss = 2.6524531841278076
training loss = 2.5745625495910645 6000
val loss = 1.7799361944198608
training loss = 2.491647720336914 6100
val loss = 2.191859006881714
training loss = 2.4586455821990967 6200
val loss = 2.1441521644592285
training loss = 2.445533037185669 6300
val loss = 2.1608829498291016
training loss = 2.4374048709869385 6400
val loss = 2.146080255508423
training loss = 2.430755615234375 6500
val loss = 2.142622947692871
training loss = 2.4287867546081543 6600
val loss = 2.0200676918029785
training loss = 2.419182538986206 6700
val loss = 2.126478910446167
training loss = 2.5139379501342773 6800
val loss = 1.61881422996521
training loss = 2.4089789390563965 6900
val loss = 2.118544578552246
training loss = 2.4041926860809326 7000
val loss = 2.1001362800598145
training loss = 2.3998353481292725 7100
val loss = 2.086801528930664
training loss = 2.3954923152923584 7200
val loss = 2.09722900390625
training loss = 2.3944544792175293 7300
val loss = 1.9938607215881348
training loss = 2.3874146938323975 7400
val loss = 2.0850820541381836
training loss = 2.383551597595215 7500
val loss = 2.085995674133301
training loss = 2.379995107650757 7600
val loss = 2.0627408027648926
training loss = 2.3764469623565674 7700
val loss = 2.0725462436676025
training loss = 2.3730547428131104 7800
val loss = 2.058356761932373
training loss = 2.369830369949341 7900
val loss = 2.064779758453369
training loss = 2.4480795860290527 8000
val loss = 2.6203832626342773
training loss = 2.363729953765869 8100
val loss = 2.048445224761963
training loss = 2.360823631286621 8200
val loss = 2.0512142181396484
training loss = 2.359323024749756 8300
val loss = 1.9850064516067505
training loss = 2.355494499206543 8400
val loss = 2.0430915355682373
training loss = 2.3529229164123535 8500
val loss = 2.039947509765625
training loss = 2.3512017726898193 8600
val loss = 2.083878993988037
training loss = 2.3481621742248535 8700
val loss = 2.0328903198242188
training loss = 2.3458633422851562 8800
val loss = 2.024756669998169
training loss = 2.3437557220458984 8900
val loss = 2.0435938835144043
training loss = 2.3415510654449463 9000
val loss = 2.0234880447387695
training loss = 2.3555595874786377 9100
val loss = 2.252645492553711
training loss = 2.3375136852264404 9200
val loss = 2.0189802646636963
training loss = 2.3355796337127686 9300
val loss = 2.015669822692871
training loss = 2.3339929580688477 9400
val loss = 2.0403943061828613
training loss = 2.33196759223938 9500
val loss = 2.009456157684326
training loss = 2.332411050796509 9600
val loss = 1.9272675514221191
training loss = 2.3285415172576904 9700
val loss = 2.0003223419189453
training loss = 2.32694149017334 9800
val loss = 2.0016396045684814
training loss = 2.4156336784362793 9900
val loss = 2.5842158794403076
training loss = 2.3238041400909424 10000
val loss = 1.9994168281555176
training loss = 2.3223040103912354 10100
val loss = 1.9951701164245605
training loss = 2.3213372230529785 10200
val loss = 1.953489065170288
training loss = 2.3194799423217773 10300
val loss = 1.9897892475128174
training loss = 2.3181204795837402 10400
val loss = 1.9883196353912354
training loss = 2.3244271278381348 10500
val loss = 2.1421265602111816
training loss = 2.315479278564453 10600
val loss = 1.9834132194519043
training loss = 2.3142149448394775 10700
val loss = 1.9823685884475708
training loss = 2.315000534057617 10800
val loss = 1.9025439023971558
training loss = 2.311729669570923 10900
val loss = 1.9780256748199463
training loss = 2.538226842880249 11000
val loss = 1.3054863214492798
training loss = 2.309342861175537 11100
val loss = 1.9689395427703857
training loss = 2.3081791400909424 11200
val loss = 1.9726160764694214
training loss = 2.3131163120269775 11300
val loss = 1.8383233547210693
training loss = 2.3059306144714355 11400
val loss = 1.967005729675293
training loss = 2.3048510551452637 11500
val loss = 1.9665321111679077
training loss = 2.6203110218048096 11600
val loss = 1.2082395553588867
training loss = 2.3026363849639893 11700
val loss = 1.957971215248108
training loss = 2.3015401363372803 11800
val loss = 1.9607316255569458
training loss = 2.3019206523895264 11900
val loss = 2.0263748168945312
training loss = 2.2993500232696533 12000
val loss = 1.956896185874939
training loss = 2.299515962600708 12100
val loss = 1.8950749635696411
training loss = 2.2971389293670654 12200
val loss = 1.9609129428863525
training loss = 2.2960121631622314 12300
val loss = 1.9505553245544434
training loss = 2.299739122390747 12400
val loss = 1.8316330909729004
training loss = 2.2936882972717285 12500
val loss = 1.9464646577835083
training loss = 2.3046720027923584 12600
val loss = 2.142575263977051
training loss = 2.2912631034851074 12700
val loss = 1.9401582479476929
training loss = 2.2899985313415527 12800
val loss = 1.943186640739441
training loss = 2.288817882537842 12900
val loss = 1.9180914163589478
training loss = 2.287372589111328 13000
val loss = 1.9354499578475952
training loss = 2.4259395599365234 13100
val loss = 1.3888063430786133
training loss = 2.2845230102539062 13200
val loss = 1.9345171451568604
training loss = 2.283025026321411 13300
val loss = 1.9284017086029053
training loss = 2.2824130058288574 13400
val loss = 1.8723182678222656
training loss = 2.279815435409546 13500
val loss = 1.9229109287261963
training loss = 2.32450008392334 13600
val loss = 1.5852134227752686
training loss = 2.2763423919677734 13700
val loss = 1.9127784967422485
training loss = 2.2745120525360107 13800
val loss = 1.9149887561798096
training loss = 2.2725937366485596 13900
val loss = 1.9040358066558838
training loss = 2.27065110206604 14000
val loss = 1.9096242189407349
training loss = 2.295229911804199 14100
val loss = 2.2020952701568604
training loss = 2.2665679454803467 14200
val loss = 1.8987329006195068
training loss = 2.264437675476074 14300
val loss = 1.8993492126464844
training loss = 2.262593984603882 14400
val loss = 1.8643471002578735
training loss = 2.2600622177124023 14500
val loss = 1.8929393291473389
training loss = 2.34554123878479 14600
val loss = 1.4472541809082031
training loss = 2.2555184364318848 14700
val loss = 1.889541745185852
training loss = 2.253192663192749 14800
val loss = 1.8813183307647705
training loss = 2.267430067062378 14900
val loss = 1.6716011762619019
training loss = 2.2484371662139893 15000
val loss = 1.872117042541504
training loss = 2.245954751968384 15100
val loss = 1.8684149980545044
training loss = 2.2435853481292725 15200
val loss = 1.8812224864959717
training loss = 2.24100661277771 15300
val loss = 1.861393690109253
training loss = 2.243312120437622 15400
val loss = 1.9780118465423584
training loss = 2.2359156608581543 15500
val loss = 1.8521970510482788
training loss = 2.233261823654175 15600
val loss = 1.848122000694275
training loss = 2.231060028076172 15700
val loss = 1.8785189390182495
training loss = 2.228006601333618 15800
val loss = 1.8399016857147217
training loss = 2.2268881797790527 15900
val loss = 1.7699437141418457
training loss = 2.2225711345672607 16000
val loss = 1.8317784070968628
training loss = 2.2197439670562744 16100
val loss = 1.8271526098251343
training loss = 2.217726469039917 16200
val loss = 1.8685474395751953
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 870.5321,  839.1518,  911.5156,  959.0487,  986.0933, 1069.3809,
        1128.5035, 1114.0629, 1101.1875, 1183.5720, 1216.6692, 1198.7021,
        1247.2313, 1295.3279, 1376.8129, 1480.0143, 1358.7657, 1463.1863,
        1535.5240, 1488.5481, 1633.4366, 1488.2189, 1552.4543, 1600.0698,
        1618.0760, 1698.4073, 1684.4783, 1730.0446, 1765.7639, 1734.3774,
        1684.4921, 1767.1720, 1739.3156, 1793.2566, 1735.3988, 1788.5912,
        1726.9905, 1661.9368, 1618.2227, 1640.1425, 1681.5739, 1526.6970,
        1495.5980, 1506.6439, 1399.7196, 1361.0529, 1221.7832, 1237.3569,
        1234.1234, 1179.1799, 1078.4336,  947.3535,  984.2922,  937.9917,
         846.7083,  908.9232,  826.3917,  683.4583,  547.0323,  560.0234,
         540.1113,  499.0617,  451.8066,  355.9022,  355.3837,  347.0996,
         314.3368,  260.5539,  227.2137,  173.8876,  175.8526,  140.5479,
         160.1081,  106.6694,  111.2236,   58.9945,   62.6577,   44.5325,
          36.1599,   46.1146,   28.7842,   41.4444,   50.5659])]
2556.359745492933
2.52948745902924 19.163676933807515 66.47348596271998
val isze = 8
idinces = [36  5 48 29 38 61 40 32 26 22 37 76 24 56 14 53 19 77 35  7 46  1 30 45
 11 25 82 42 73 55 34 21 59 69 66 43 62 71 51 60 67 75 50  6 79 47 54 52
 78 74 68 13 65 44 80 16 31 39 70 17 58  8 28 33  4  9 23 81 63 57 12 41
 15  2  0 10 49 64 20  3 18 72 27]
we are doing training validation split
training loss = 441.2151184082031 100
val loss = 817.1788940429688
training loss = 80.28943634033203 200
val loss = 79.57823944091797
training loss = 12.224255561828613 300
val loss = 1.5065268278121948
training loss = 11.794912338256836 400
val loss = 1.5350096225738525
training loss = 11.352192878723145 500
val loss = 1.6634783744812012
training loss = 10.916160583496094 600
val loss = 1.8735884428024292
training loss = 10.507740020751953 700
val loss = 2.1695938110351562
training loss = 10.14309310913086 800
val loss = 2.5441744327545166
training loss = 9.831738471984863 900
val loss = 2.9777865409851074
training loss = 9.575775146484375 1000
val loss = 3.441016912460327
training loss = 9.370610237121582 1100
val loss = 3.9002697467803955
training loss = 9.20727252960205 1200
val loss = 4.324917793273926
training loss = 9.075345993041992 1300
val loss = 4.692913055419922
training loss = 8.96533489227295 1400
val loss = 4.993610382080078
training loss = 8.869909286499023 1500
val loss = 5.226419448852539
training loss = 8.784078598022461 1600
val loss = 5.398098468780518
training loss = 8.704732894897461 1700
val loss = 5.518886566162109
training loss = 8.630053520202637 1800
val loss = 5.600175380706787
training loss = 8.558975219726562 1900
val loss = 5.652083396911621
training loss = 8.490849494934082 2000
val loss = 5.683061599731445
training loss = 8.425243377685547 2100
val loss = 5.699653625488281
training loss = 8.361821174621582 2200
val loss = 5.70639705657959
training loss = 8.300341606140137 2300
val loss = 5.706768035888672
training loss = 8.240625381469727 2400
val loss = 5.702663421630859
training loss = 8.182570457458496 2500
val loss = 5.6955766677856445
training loss = 8.126171112060547 2600
val loss = 5.6867451667785645
training loss = 8.071531295776367 2700
val loss = 5.676984786987305
training loss = 8.018816947937012 2800
val loss = 5.6669511795043945
training loss = 7.968188762664795 2900
val loss = 5.65776252746582
training loss = 7.919498920440674 3000
val loss = 5.650370121002197
training loss = 7.871499061584473 3100
val loss = 5.646543502807617
training loss = 7.819209098815918 3200
val loss = 5.647934436798096
training loss = 7.740941047668457 3300
val loss = 5.661419868469238
training loss = 7.5051164627075195 3400
val loss = 5.745469570159912
training loss = 6.77104377746582 3500
val loss = 5.683782577514648
training loss = 5.054908275604248 3600
val loss = 4.2476725578308105
training loss = 2.6804656982421875 3700
val loss = 2.1272668838500977
training loss = 2.431396245956421 3800
val loss = 1.1994707584381104
training loss = 2.375070571899414 3900
val loss = 1.0905790328979492
training loss = 2.3211047649383545 4000
val loss = 0.9949174523353577
training loss = 2.2683210372924805 4100
val loss = 0.9136524200439453
training loss = 2.2166333198547363 4200
val loss = 0.8825817704200745
training loss = 2.1733405590057373 4300
val loss = 0.8575931191444397
training loss = 2.141960382461548 4400
val loss = 0.9188179969787598
training loss = 2.1150944232940674 4500
val loss = 0.8603633642196655
training loss = 2.0966696739196777 4600
val loss = 0.8048750162124634
training loss = 2.078420639038086 4700
val loss = 0.8572990298271179
training loss = 2.064180612564087 4800
val loss = 0.8522577285766602
training loss = 2.052534818649292 4900
val loss = 0.8380755186080933
training loss = 2.0418641567230225 5000
val loss = 0.8593102693557739
training loss = 2.0331084728240967 5100
val loss = 0.8529687523841858
training loss = 2.0251662731170654 5200
val loss = 0.8690175414085388
training loss = 2.019155740737915 5300
val loss = 0.8452271819114685
training loss = 2.012286901473999 5400
val loss = 0.8793871402740479
training loss = 2.014846086502075 5500
val loss = 0.9863194227218628
training loss = 2.0021557807922363 5600
val loss = 0.893561840057373
training loss = 1.9977922439575195 5700
val loss = 0.893200159072876
training loss = 2.002067804336548 5800
val loss = 0.9994018077850342
training loss = 1.9903764724731445 5900
val loss = 0.9007287621498108
training loss = 1.98719322681427 6000
val loss = 0.9113681316375732
training loss = 1.9843144416809082 6100
val loss = 0.917161762714386
training loss = 1.9815574884414673 6200
val loss = 0.9128068089485168
training loss = 1.9807853698730469 6300
val loss = 0.9582196474075317
training loss = 1.9768526554107666 6400
val loss = 0.9188978672027588
training loss = 1.997111201286316 6500
val loss = 0.8040733337402344
training loss = 1.972873568534851 6600
val loss = 0.924736499786377
training loss = 1.9711064100265503 6700
val loss = 0.9267298579216003
training loss = 1.9715722799301147 6800
val loss = 0.976085901260376
training loss = 1.9679651260375977 6900
val loss = 0.9309896230697632
training loss = 2.0012619495391846 7000
val loss = 1.1649891138076782
training loss = 1.9652775526046753 7100
val loss = 0.934312641620636
training loss = 1.9640588760375977 7200
val loss = 0.9362202882766724
training loss = 2.3152356147766113 7300
val loss = 0.8646887540817261
training loss = 1.961907148361206 7400
val loss = 0.9417824745178223
training loss = 1.9609159231185913 7500
val loss = 0.9405940175056458
training loss = 1.961929202079773 7600
val loss = 0.9861689805984497
training loss = 1.959153652191162 7700
val loss = 0.9429970383644104
training loss = 1.987030029296875 7800
val loss = 0.8198970556259155
training loss = 1.9576225280761719 7900
val loss = 0.9464383125305176
training loss = 1.9569064378738403 8000
val loss = 0.9459099173545837
training loss = 1.9568852186203003 8100
val loss = 0.922947883605957
training loss = 1.9556303024291992 8200
val loss = 0.9477136135101318
training loss = 1.9895528554916382 8300
val loss = 1.1783115863800049
training loss = 1.9545111656188965 8400
val loss = 0.9461565017700195
training loss = 1.9539718627929688 8500
val loss = 0.9491842985153198
training loss = 1.9539011716842651 8600
val loss = 0.9310030341148376
training loss = 1.9530152082443237 8700
val loss = 0.9504806995391846
training loss = 1.9942234754562378 8800
val loss = 0.8111833930015564
training loss = 1.952162265777588 8900
val loss = 0.9525734186172485
training loss = 1.9517475366592407 9000
val loss = 0.9520190358161926
training loss = 1.9520283937454224 9100
val loss = 0.9286277294158936
training loss = 1.9510042667388916 9200
val loss = 0.9525099396705627
training loss = 1.9533653259277344 9300
val loss = 1.0046111345291138
training loss = 1.9503268003463745 9400
val loss = 0.9533855319023132
reduced chi^2 level 2 = 1.9502958059310913
Constrained alpha: 1.759778380393982
Constrained beta: 3.2607996463775635
Constrained gamma: 21.94184112548828
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 831.4664,  837.0352,  903.8369,  944.8713,  951.9630, 1066.0563,
        1079.6750, 1092.2030, 1139.7273, 1174.7397, 1134.6196, 1126.4111,
        1202.2139, 1248.3077, 1286.9963, 1439.1362, 1431.5187, 1458.6072,
        1463.9614, 1439.6608, 1595.3732, 1566.0844, 1559.5605, 1569.5955,
        1664.6859, 1745.8029, 1680.9203, 1780.4861, 1786.0287, 1654.6666,
        1668.8577, 1789.9420, 1738.0370, 1719.9697, 1735.7424, 1695.6703,
        1640.3456, 1559.7301, 1624.4839, 1629.7261, 1672.2501, 1581.3870,
        1469.9326, 1540.0687, 1348.9852, 1285.4756, 1240.5281, 1225.3789,
        1181.6580, 1125.4976, 1114.4255,  987.0447,  926.4883,  918.9036,
         875.5421,  880.8712,  840.5809,  725.5512,  608.0519,  534.6561,
         548.9781,  464.7815,  451.2856,  420.3682,  368.8264,  335.6558,
         288.5609,  222.5020,  213.4155,  168.0642,  181.4170,  131.2255,
         147.0814,  105.5502,   96.9213,   67.9122,   46.4483,   44.5860,
          39.2768,   36.1341,   19.5923,   41.6420,   20.0851])]
2923.1166096964325
3.081272442663157 8.737991563490613 70.76378438049639
val isze = 8
idinces = [42 10 64 61 40 25 69 34 52 60  6 71 48 76  5 21 50 45  9 20 79 81  4 23
 78 65 33 31  3  8 46  2 63 18 80 14 12 30 39 41 72 56 53 32 15 36 22 11
 27 13 67 57 55 75  7 16 29 74 62 37 26 17 47 44 66 70 77 68  1 19 28 73
 54 24 43 59 35 51 82 38 49 58  0]
we are doing training validation split
training loss = 30.694446563720703 100
val loss = 35.52916717529297
training loss = 11.726819038391113 200
val loss = 22.896499633789062
training loss = 10.229941368103027 300
val loss = 19.596961975097656
training loss = 9.122724533081055 400
val loss = 16.803672790527344
training loss = 8.343804359436035 500
val loss = 14.579712867736816
training loss = 7.80722188949585 600
val loss = 12.844654083251953
training loss = 7.440364837646484 700
val loss = 11.501830101013184
training loss = 7.18946647644043 800
val loss = 10.465110778808594
training loss = 7.016617774963379 900
val loss = 9.664665222167969
training loss = 6.89567232131958 1000
val loss = 9.046337127685547
training loss = 6.808796405792236 1100
val loss = 8.568585395812988
training loss = 6.74390172958374 1200
val loss = 8.199542999267578
training loss = 6.692906856536865 1300
val loss = 7.915048122406006
training loss = 6.6504411697387695 1400
val loss = 7.696424961090088
training loss = 6.613012790679932 1500
val loss = 7.529212951660156
training loss = 6.578379154205322 1600
val loss = 7.401917457580566
training loss = 6.5451459884643555 1700
val loss = 7.305663585662842
training loss = 6.51246976852417 1800
val loss = 7.233243942260742
training loss = 6.479846954345703 1900
val loss = 7.178827285766602
training loss = 6.447000026702881 2000
val loss = 7.13784122467041
training loss = 6.413759708404541 2100
val loss = 7.106732368469238
training loss = 6.380000591278076 2200
val loss = 7.082732200622559
training loss = 6.345569610595703 2300
val loss = 7.0638108253479
training loss = 6.310197830200195 2400
val loss = 7.048559188842773
training loss = 6.27332878112793 2500
val loss = 7.036101818084717
training loss = 6.233795166015625 2600
val loss = 7.026224136352539
training loss = 6.189055919647217 2700
val loss = 7.01938009262085
training loss = 6.133242130279541 2800
val loss = 7.017780303955078
training loss = 6.05225944519043 2900
val loss = 7.0281782150268555
training loss = 5.915126323699951 3000
val loss = 7.066403388977051
training loss = 5.68388032913208 3100
val loss = 7.127524375915527
training loss = 5.347968101501465 3200
val loss = 7.0616302490234375
training loss = 4.867311477661133 3300
val loss = 6.7345709800720215
training loss = 4.183586120605469 3400
val loss = 6.155688285827637
training loss = 3.3538379669189453 3500
val loss = 5.272707939147949
training loss = 2.6829206943511963 3600
val loss = 4.253842353820801
training loss = 2.3991076946258545 3700
val loss = 3.5681447982788086
training loss = 2.318826198577881 3800
val loss = 3.2827486991882324
training loss = 2.2804007530212402 3900
val loss = 3.171445608139038
training loss = 2.251352071762085 4000
val loss = 3.113140344619751
training loss = 2.2273294925689697 4100
val loss = 3.071864128112793
training loss = 2.2063546180725098 4200
val loss = 3.0385961532592773
training loss = 2.1845340728759766 4300
val loss = 3.012040615081787
training loss = 2.1554501056671143 4400
val loss = 2.9809181690216064
training loss = 2.116283893585205 4500
val loss = 2.9310638904571533
training loss = 2.0674197673797607 4600
val loss = 2.8718981742858887
training loss = 2.088616132736206 4700
val loss = 2.5318825244903564
training loss = 1.9687902927398682 4800
val loss = 2.7541568279266357
training loss = 1.9286365509033203 4900
val loss = 2.7180886268615723
training loss = 1.8959239721298218 5000
val loss = 2.6664681434631348
training loss = 1.8685425519943237 5100
val loss = 2.607377529144287
training loss = 1.8446816205978394 5200
val loss = 2.6034111976623535
training loss = 1.8255459070205688 5300
val loss = 2.576249599456787
training loss = 1.8089057207107544 5400
val loss = 2.5464344024658203
training loss = 1.7953567504882812 5500
val loss = 2.5314478874206543
training loss = 1.7845968008041382 5600
val loss = 2.4783787727355957
training loss = 1.7738871574401855 5700
val loss = 2.4997730255126953
training loss = 1.7653833627700806 5800
val loss = 2.493825912475586
training loss = 1.7586731910705566 5900
val loss = 2.47084903717041
training loss = 1.7525578737258911 6000
val loss = 2.4726333618164062
training loss = 1.7493271827697754 6100
val loss = 2.5140223503112793
training loss = 1.7433346509933472 6200
val loss = 2.4576525688171387
training loss = 2.01656436920166 6300
val loss = 3.3509984016418457
training loss = 1.7366528511047363 6400
val loss = 2.4452805519104004
training loss = 1.7340682744979858 6500
val loss = 2.431285858154297
training loss = 1.7317934036254883 6600
val loss = 2.44673228263855
training loss = 1.7297425270080566 6700
val loss = 2.4357411861419678
training loss = 1.7282218933105469 6800
val loss = 2.4200382232666016
training loss = 1.726511836051941 6900
val loss = 2.429753541946411
training loss = 1.7330957651138306 7000
val loss = 2.535982131958008
training loss = 1.7239738702774048 7100
val loss = 2.425046443939209
training loss = 1.7228885889053345 7200
val loss = 2.4233686923980713
training loss = 1.7227915525436401 7300
val loss = 2.3917577266693115
training loss = 1.7210315465927124 7400
val loss = 2.4200525283813477
training loss = 2.0894837379455566 7500
val loss = 2.1783149242401123
training loss = 1.7194969654083252 7600
val loss = 2.4140191078186035
training loss = 1.7187899351119995 7700
val loss = 2.4153647422790527
training loss = 1.7621887922286987 7800
val loss = 2.237457752227783
training loss = 1.7175779342651367 7900
val loss = 2.4143474102020264
training loss = 1.7170312404632568 8000
val loss = 2.411686420440674
training loss = 1.7204281091690063 8100
val loss = 2.350193500518799
training loss = 1.7160519361495972 8200
val loss = 2.409860849380493
training loss = 1.7330090999603271 8300
val loss = 2.289764881134033
training loss = 1.7152099609375 8400
val loss = 2.40960693359375
training loss = 1.7148226499557495 8500
val loss = 2.407543659210205
training loss = 1.7146813869476318 8600
val loss = 2.392331838607788
training loss = 1.7141181230545044 8700
val loss = 2.405972480773926
training loss = 1.7141060829162598 8800
val loss = 2.423570394515991
training loss = 1.7134912014007568 8900
val loss = 2.404003620147705
training loss = 1.7454582452774048 9000
val loss = 2.6244752407073975
training loss = 1.7129521369934082 9100
val loss = 2.4002022743225098
training loss = 1.7126927375793457 9200
val loss = 2.4020442962646484
training loss = 1.713037371635437 9300
val loss = 2.427689552307129
training loss = 1.71221923828125 9400
val loss = 2.4010910987854004
training loss = 1.720636248588562 9500
val loss = 2.3139469623565674
training loss = 1.7118394374847412 9600
val loss = 2.3958492279052734
training loss = 1.7116330862045288 9700
val loss = 2.3996872901916504
training loss = 1.7127423286437988 9800
val loss = 2.3650665283203125
training loss = 1.7112740278244019 9900
val loss = 2.3992514610290527
training loss = 1.7111458778381348 10000
val loss = 2.3940608501434326
training loss = 1.7110217809677124 10100
val loss = 2.3922808170318604
training loss = 1.7108386754989624 10200
val loss = 2.3978934288024902
training loss = 1.7112523317337036 10300
val loss = 2.3771448135375977
training loss = 1.7105774879455566 10400
val loss = 2.397770881652832
training loss = 1.7104891538619995 10500
val loss = 2.400660514831543
training loss = 1.710364580154419 10600
val loss = 2.3949122428894043
training loss = 1.7102478742599487 10700
val loss = 2.3963100910186768
training loss = 1.7414932250976562 10800
val loss = 2.2506215572357178
training loss = 1.7100608348846436 10900
val loss = 2.3971989154815674
training loss = 1.7100622653961182 11000
val loss = 2.4043173789978027
training loss = 1.7099597454071045 11100
val loss = 2.4039127826690674
training loss = 1.7098108530044556 11200
val loss = 2.3950366973876953
training loss = 1.7312754392623901 11300
val loss = 2.2616376876831055
training loss = 1.7096641063690186 11400
val loss = 2.395127773284912
training loss = 1.7095961570739746 11500
val loss = 2.3943772315979004
training loss = 1.7915608882904053 11600
val loss = 2.7677040100097656
training loss = 1.7094812393188477 11700
val loss = 2.3929121494293213
training loss = 1.709429144859314 11800
val loss = 2.3936872482299805
training loss = 1.710507869720459 11900
val loss = 2.4301795959472656
training loss = 1.7093241214752197 12000
val loss = 2.393584728240967
training loss = 2.024667978286743 12100
val loss = 3.319119930267334
training loss = 1.7092480659484863 12200
val loss = 2.3970208168029785
training loss = 1.7091938257217407 12300
val loss = 2.3924508094787598
training loss = 1.7101287841796875 12400
val loss = 2.4269442558288574
training loss = 1.709109902381897 12500
val loss = 2.392538070678711
training loss = 1.7354600429534912 12600
val loss = 2.586249351501465
training loss = 1.7090309858322144 12700
val loss = 2.3934454917907715
training loss = 1.7089992761611938 12800
val loss = 2.390392780303955
training loss = 1.7091153860092163 12900
val loss = 2.40537428855896
training loss = 1.7089229822158813 13000
val loss = 2.3916327953338623
training loss = 1.7204781770706177 13100
val loss = 2.295764446258545
training loss = 1.7088375091552734 13200
val loss = 2.391700267791748
training loss = 1.7088043689727783 13300
val loss = 2.390810966491699
training loss = 1.7096562385559082 13400
val loss = 2.4229736328125
training loss = 1.7087191343307495 13500
val loss = 2.390267848968506
training loss = 1.708909511566162 13600
val loss = 2.4056882858276367
training loss = 1.7086308002471924 13700
val loss = 2.3897359371185303
training loss = 1.7085778713226318 13800
val loss = 2.3897652626037598
training loss = 1.7515428066253662 13900
val loss = 2.2161736488342285
training loss = 1.7084674835205078 14000
val loss = 2.3911490440368652
training loss = 1.7084044218063354 14100
val loss = 2.3890740871429443
training loss = 1.7107350826263428 14200
val loss = 2.442999839782715
training loss = 1.708255410194397 14300
val loss = 2.388751745223999
training loss = 1.7178499698638916 14400
val loss = 2.5020911693573
training loss = 1.7080973386764526 14500
val loss = 2.3925609588623047
training loss = 1.7079803943634033 14600
val loss = 2.387948513031006
training loss = 1.7097584009170532 14700
val loss = 2.3450355529785156
training loss = 1.7077351808547974 14800
val loss = 2.3877828121185303
training loss = 1.7076011896133423 14900
val loss = 2.387505531311035
training loss = 1.7079541683197021 15000
val loss = 2.4134323596954346
training loss = 1.7072831392288208 15100
val loss = 2.3877413272857666
training loss = 1.7071008682250977 15200
val loss = 2.387117862701416
training loss = 1.707341194152832 15300
val loss = 2.3674206733703613
training loss = 1.7066935300827026 15400
val loss = 2.387488842010498
training loss = 1.7562416791915894 15500
val loss = 2.201930046081543
training loss = 1.7062249183654785 15600
val loss = 2.3878960609436035
training loss = 1.7123210430145264 15700
val loss = 2.480353593826294
training loss = 1.7057080268859863 15800
val loss = 2.3902244567871094
training loss = 1.7054132223129272 15900
val loss = 2.388577461242676
training loss = 1.705407977104187 16000
val loss = 2.4078569412231445
training loss = 1.7048507928848267 16100
val loss = 2.3904170989990234
training loss = 1.7202779054641724 16200
val loss = 2.2762861251831055
training loss = 1.7042967081069946 16300
val loss = 2.3912758827209473
training loss = 1.7040013074874878 16400
val loss = 2.392937660217285
training loss = 1.7037841081619263 16500
val loss = 2.393876075744629
training loss = 1.7035070657730103 16600
val loss = 2.393925189971924
training loss = 1.7523983716964722 16700
val loss = 2.208594799041748
training loss = 1.7030707597732544 16800
val loss = 2.394117832183838
training loss = 1.8172656297683716 16900
val loss = 2.1582813262939453
training loss = 1.7027376890182495 17000
val loss = 2.401034355163574
training loss = 1.7025500535964966 17100
val loss = 2.3966803550720215
training loss = 1.7037113904953003 17200
val loss = 2.4380195140838623
training loss = 1.7023670673370361 17300
val loss = 2.3983583450317383
training loss = 1.7022720575332642 17400
val loss = 2.3983778953552246
training loss = 1.702264428138733 17500
val loss = 2.399975538253784
training loss = 1.702203392982483 17600
val loss = 2.3990461826324463
training loss = 1.7739015817642212 17700
val loss = 2.1908252239227295
training loss = 1.7022066116333008 17800
val loss = 2.4011611938476562
training loss = 1.702329158782959 17900
val loss = 2.4116437435150146
training loss = 1.7023308277130127 18000
val loss = 2.3922557830810547
training loss = 1.7022831439971924 18100
val loss = 2.400726079940796
training loss = 1.7061415910720825 18200
val loss = 2.4710652828216553
training loss = 1.7023956775665283 18300
val loss = 2.4016623497009277
training loss = 1.7024410963058472 18400
val loss = 2.402451515197754
training loss = 1.702735185623169 18500
val loss = 2.387347936630249
training loss = 1.702569603919983 18600
val loss = 2.4022367000579834
training loss = 1.8451825380325317 18700
val loss = 2.945875406265259
training loss = 1.7027018070220947 18800
val loss = 2.403463363647461
training loss = 1.7027490139007568 18900
val loss = 2.403613567352295
training loss = 1.7030701637268066 19000
val loss = 2.42036771774292
training loss = 1.702864170074463 19100
val loss = 2.4035439491271973
training loss = 1.7309949398040771 19200
val loss = 2.255427122116089
training loss = 1.7029757499694824 19300
val loss = 2.402562141418457
training loss = 1.7030082941055298 19400
val loss = 2.4053444862365723
training loss = 1.703194260597229 19500
val loss = 2.4170689582824707
training loss = 1.7030905485153198 19600
val loss = 2.4052464962005615
training loss = 1.7202889919281006 19700
val loss = 2.564741611480713
training loss = 1.7031652927398682 19800
val loss = 2.406923294067383
training loss = 1.7031859159469604 19900
val loss = 2.403576374053955
training loss = 1.7032824754714966 20000
val loss = 2.414479970932007
training loss = 1.7032324075698853 20100
val loss = 2.40708589553833
training loss = 1.7049486637115479 20200
val loss = 2.453479290008545
training loss = 1.7032711505889893 20300
val loss = 2.407703399658203
training loss = 1.7271249294281006 20400
val loss = 2.2718141078948975
training loss = 1.703302025794983 20500
val loss = 2.4073543548583984
training loss = 1.703298807144165 20600
val loss = 2.4076085090637207
training loss = 1.7035088539123535 20700
val loss = 2.3952369689941406
training loss = 1.7033145427703857 20800
val loss = 2.4090371131896973
training loss = 1.708740234375 20900
val loss = 2.3373026847839355
training loss = 1.7033299207687378 21000
val loss = 2.411365270614624
training loss = 1.703312635421753 21100
val loss = 2.409660816192627
training loss = 1.7038601636886597 21200
val loss = 2.3872718811035156
training loss = 1.7033098936080933 21300
val loss = 2.4104743003845215
training loss = 1.7041178941726685 21400
val loss = 2.382647752761841
training loss = 1.7033032178878784 21500
val loss = 2.412172317504883
training loss = 1.7040399312973022 21600
val loss = 2.4410829544067383
training loss = 1.7033337354660034 21700
val loss = 2.4190683364868164
training loss = 1.7032685279846191 21800
val loss = 2.4124207496643066
training loss = 1.7071077823638916 21900
val loss = 2.352583646774292
training loss = 1.7032490968704224 22000
val loss = 2.4134628772735596
training loss = 1.7922005653381348 22100
val loss = 2.812771797180176
reduced chi^2 level 2 = 1.7039856910705566
Constrained alpha: 1.684627890586853
Constrained beta: 3.9209861755371094
Constrained gamma: 14.914779663085938
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 869.7095,  919.3329,  958.6292,  953.6355, 1032.6960, 1046.9025,
        1083.9875, 1167.7238, 1152.0925, 1185.3982, 1157.1111, 1254.6622,
        1235.5311, 1335.9939, 1361.6086, 1403.8816, 1468.6429, 1496.2628,
        1561.7119, 1473.5917, 1472.1765, 1552.4207, 1543.7822, 1667.0886,
        1622.5905, 1777.9347, 1641.6632, 1744.1533, 1668.0287, 1725.8523,
        1707.3928, 1728.1094, 1643.5596, 1733.0664, 1700.9460, 1791.0763,
        1626.9489, 1579.1598, 1614.8406, 1624.7040, 1598.8174, 1526.8258,
        1496.9066, 1539.3517, 1338.9446, 1334.1216, 1212.7789, 1255.2207,
        1151.8423, 1133.2244, 1025.6399,  993.8399,  999.8224,  900.7443,
         844.1517,  892.4578,  840.5322,  668.9571,  606.1445,  523.0584,
         581.1066,  443.6103,  442.9601,  379.8379,  381.2527,  363.7792,
         273.7776,  265.1464,  208.7170,  170.9321,  129.4640,  150.0302,
         152.6024,  103.6626,   88.3196,   76.5477,   50.2047,   37.7957,
          29.8427,   44.6075,   24.4752,   47.4602,   31.7236])]
2615.7670144914073
0.06686421223813466 9.888140150810054 56.93144190093757
val isze = 8
idinces = [33 70 54 47 10 19 39 66 78 67  7 14 56 16 76 61 17 52 11  8 80 37 20 13
 59  3  1 25  6 53 74  0 40 12 62 58 69 65 79 21 72 30 24 73 35 28 48 38
 81 55 51 27  4 26 44  2 15 36 29 71 45 18 75 68 60 22 50 42 64  5 57 82
 43  9 49 41 31 32 63 23 46 34 77]
we are doing training validation split
training loss = 8.186810493469238 100
val loss = 3.027235746383667
training loss = 6.840794563293457 200
val loss = 2.2933247089385986
training loss = 6.803216934204102 300
val loss = 2.3254685401916504
training loss = 6.7628679275512695 400
val loss = 2.374541997909546
training loss = 6.722194194793701 500
val loss = 2.432359457015991
training loss = 6.683000564575195 600
val loss = 2.497439384460449
training loss = 6.6464619636535645 700
val loss = 2.567965507507324
training loss = 6.613185882568359 800
val loss = 2.6415719985961914
training loss = 6.583308219909668 900
val loss = 2.7157669067382812
training loss = 6.556591987609863 1000
val loss = 2.788038730621338
training loss = 6.53255558013916 1100
val loss = 2.8561158180236816
training loss = 6.510590076446533 1200
val loss = 2.9181320667266846
training loss = 6.490073204040527 1300
val loss = 2.9728944301605225
training loss = 6.470431804656982 1400
val loss = 3.019681453704834
training loss = 6.451182842254639 1500
val loss = 3.058469533920288
training loss = 6.431950092315674 1600
val loss = 3.089637279510498
training loss = 6.412429332733154 1700
val loss = 3.1139185428619385
training loss = 6.392385482788086 1800
val loss = 3.1322875022888184
training loss = 6.371587753295898 1900
val loss = 3.145756959915161
training loss = 6.349784851074219 2000
val loss = 3.1553268432617188
training loss = 6.326646327972412 2100
val loss = 3.1619410514831543
training loss = 6.301707744598389 2200
val loss = 3.1664154529571533
training loss = 6.2742743492126465 2300
val loss = 3.1694753170013428
training loss = 6.2432732582092285 2400
val loss = 3.171801805496216
training loss = 6.20701265335083 2500
val loss = 3.174088716506958
training loss = 6.1627397537231445 2600
val loss = 3.177187442779541
training loss = 6.105921745300293 2700
val loss = 3.1824450492858887
training loss = 6.02913761138916 2800
val loss = 3.1918063163757324
training loss = 5.920501232147217 2900
val loss = 3.2076961994171143
training loss = 5.760869026184082 3000
val loss = 3.231743574142456
training loss = 5.517289161682129 3100
val loss = 3.263946771621704
training loss = 5.138159275054932 3200
val loss = 3.299386978149414
training loss = 4.582764625549316 3300
val loss = 3.2847909927368164
training loss = 3.858181953430176 3400
val loss = 3.1094374656677246
training loss = 3.0755951404571533 3500
val loss = 2.7535574436187744
training loss = 2.5405309200286865 3600
val loss = 2.3141956329345703
training loss = 2.366576910018921 3700
val loss = 2.028787136077881
training loss = 2.329110860824585 3800
val loss = 1.9352706670761108
training loss = 2.3122332096099854 3900
val loss = 1.9218268394470215
training loss = 2.298931837081909 4000
val loss = 1.92893648147583
training loss = 2.287463665008545 4100
val loss = 1.939647912979126
training loss = 2.2774059772491455 4200
val loss = 1.9501386880874634
training loss = 2.2684919834136963 4300
val loss = 1.9596941471099854
training loss = 2.260511636734009 4400
val loss = 1.9681525230407715
training loss = 2.2532873153686523 4500
val loss = 1.975543737411499
training loss = 2.246664524078369 4600
val loss = 1.9818353652954102
training loss = 2.2405171394348145 4700
val loss = 1.9870860576629639
training loss = 2.2347326278686523 4800
val loss = 1.9913644790649414
training loss = 2.229217290878296 4900
val loss = 1.9947428703308105
training loss = 2.2238924503326416 5000
val loss = 1.9972493648529053
training loss = 2.2186927795410156 5100
val loss = 1.9990887641906738
training loss = 2.221099615097046 5200
val loss = 1.941563606262207
training loss = 2.209279775619507 5300
val loss = 2.0000226497650146
training loss = 2.204854726791382 5400
val loss = 2.0004563331604004
training loss = 2.2010185718536377 5500
val loss = 1.9851102828979492
training loss = 2.196592092514038 5600
val loss = 2.000166416168213
training loss = 2.1936938762664795 5700
val loss = 2.02882719039917
training loss = 2.188581943511963 5800
val loss = 2.0026001930236816
training loss = 2.1845879554748535 5900
val loss = 1.9981135129928589
training loss = 2.1825695037841797 6000
val loss = 2.0351459980010986
training loss = 2.176640272140503 6100
val loss = 1.9952367544174194
training loss = 2.234729528427124 6200
val loss = 2.2630667686462402
training loss = 2.168466329574585 6300
val loss = 1.9912168979644775
training loss = 2.164222478866577 6400
val loss = 1.9872567653656006
training loss = 2.1677725315093994 6500
val loss = 2.065585136413574
training loss = 2.155665636062622 6600
val loss = 1.9785701036453247
training loss = 2.1511993408203125 6700
val loss = 1.9734482765197754
training loss = 2.146819829940796 6800
val loss = 1.9618288278579712
training loss = 2.142335891723633 6900
val loss = 1.961674451828003
training loss = 2.152738571166992 7000
val loss = 1.8686249256134033
training loss = 2.1334643363952637 7100
val loss = 1.9482496976852417
training loss = 2.1289801597595215 7200
val loss = 1.9426288604736328
training loss = 2.1249396800994873 7300
val loss = 1.9530366659164429
training loss = 2.1202502250671387 7400
val loss = 1.9319965839385986
training loss = 2.116041660308838 7500
val loss = 1.9168341159820557
training loss = 2.1116719245910645 7600
val loss = 1.922054648399353
training loss = 2.1073384284973145 7700
val loss = 1.915521502494812
training loss = 2.1042158603668213 7800
val loss = 1.885390043258667
training loss = 2.099020004272461 7900
val loss = 1.9046757221221924
training loss = 2.0948057174682617 8000
val loss = 1.9036840200424194
training loss = 2.0909712314605713 8100
val loss = 1.8889847993850708
training loss = 2.087061882019043 8200
val loss = 1.8872931003570557
training loss = 2.0831098556518555 8300
val loss = 1.8813786506652832
training loss = 2.079413890838623 8400
val loss = 1.874124526977539
training loss = 2.075761318206787 8500
val loss = 1.8684637546539307
training loss = 2.1130282878875732 8600
val loss = 2.0655717849731445
training loss = 2.068892478942871 8700
val loss = 1.8557932376861572
training loss = 2.065577745437622 8800
val loss = 1.8492844104766846
training loss = 2.0626232624053955 8900
val loss = 1.8367078304290771
training loss = 2.0596237182617188 9000
val loss = 1.8363966941833496
training loss = 2.067089557647705 9100
val loss = 1.9180104732513428
training loss = 2.0542588233947754 9200
val loss = 1.8240630626678467
training loss = 2.05173659324646 9300
val loss = 1.8170803785324097
training loss = 2.049527168273926 9400
val loss = 1.8134968280792236
training loss = 2.047405958175659 9500
val loss = 1.8063913583755493
training loss = 2.283749580383301 9600
val loss = 2.4439682960510254
training loss = 2.0436019897460938 9700
val loss = 1.7958264350891113
training loss = 2.041876792907715 9800
val loss = 1.7902518510818481
training loss = 2.042011260986328 9900
val loss = 1.754969835281372
training loss = 2.0388951301574707 10000
val loss = 1.780829668045044
training loss = 2.065371513366699 10100
val loss = 1.9354987144470215
training loss = 2.0363917350769043 10200
val loss = 1.7714295387268066
training loss = 2.0352532863616943 10300
val loss = 1.7678017616271973
training loss = 2.034776210784912 10400
val loss = 1.7828052043914795
training loss = 2.0334184169769287 10500
val loss = 1.7609959840774536
training loss = 2.032561779022217 10600
val loss = 1.7573518753051758
training loss = 2.034740447998047 10700
val loss = 1.7148327827453613
training loss = 2.031122922897339 10800
val loss = 1.7516067028045654
training loss = 2.030780076980591 10900
val loss = 1.7345707416534424
training loss = 2.029973268508911 11000
val loss = 1.7525925636291504
training loss = 2.029370069503784 11100
val loss = 1.7432411909103394
training loss = 2.030743360519409 11200
val loss = 1.7719635963439941
training loss = 2.028434991836548 11300
val loss = 1.7399985790252686
training loss = 2.0279998779296875 11400
val loss = 1.7364331483840942
training loss = 2.028688907623291 11500
val loss = 1.762390375137329
training loss = 2.0272574424743652 11600
val loss = 1.7328733205795288
training loss = 2.0797226428985596 11700
val loss = 1.60503089427948
training loss = 2.026603937149048 11800
val loss = 1.7296315431594849
training loss = 2.026294708251953 11900
val loss = 1.7277145385742188
training loss = 2.0476248264312744 12000
val loss = 1.6323235034942627
training loss = 2.0257418155670166 12100
val loss = 1.7255780696868896
training loss = 2.0254714488983154 12200
val loss = 1.723193645477295
training loss = 2.026000499725342 12300
val loss = 1.7017242908477783
training loss = 2.024982213973999 12400
val loss = 1.7211936712265015
training loss = 2.132514238357544 12500
val loss = 1.5674397945404053
training loss = 2.024521827697754 12600
val loss = 1.7192308902740479
training loss = 2.024292469024658 12700
val loss = 1.7179919481277466
training loss = 2.024819850921631 12800
val loss = 1.7399471998214722
training loss = 2.0238680839538574 12900
val loss = 1.7157983779907227
training loss = 2.032975196838379 13000
val loss = 1.64349365234375
training loss = 2.023463249206543 13100
val loss = 1.7131508588790894
training loss = 2.0232527256011963 13200
val loss = 1.7131860256195068
training loss = 2.0386290550231934 13300
val loss = 1.8303325176239014
training loss = 2.0228729248046875 13400
val loss = 1.7113498449325562
training loss = 2.022674322128296 13500
val loss = 1.7108948230743408
training loss = 2.084275007247925 13600
val loss = 1.973555564880371
training loss = 2.022305727005005 13700
val loss = 1.7089405059814453
training loss = 2.0221126079559326 13800
val loss = 1.7086361646652222
training loss = 2.021968126296997 13900
val loss = 1.7042510509490967
training loss = 2.0217583179473877 14000
val loss = 1.7078090906143188
training loss = 2.021571159362793 14100
val loss = 1.7060456275939941
training loss = 2.0214502811431885 14200
val loss = 1.7122917175292969
training loss = 2.021230936050415 14300
val loss = 1.7054451704025269
training loss = 2.308560848236084 14400
val loss = 1.5797255039215088
training loss = 2.0208935737609863 14500
val loss = 1.7040812969207764
training loss = 2.0207202434539795 14600
val loss = 1.7025392055511475
training loss = 2.020922899246216 14700
val loss = 1.688699722290039
training loss = 2.020393133163452 14800
val loss = 1.702067494392395
training loss = 2.0203158855438232 14900
val loss = 1.694875955581665
training loss = 2.0200774669647217 15000
val loss = 1.7009289264678955
training loss = 2.251405715942383 15100
val loss = 1.5481715202331543
training loss = 2.019787311553955 15200
val loss = 1.6980459690093994
training loss = 2.0196197032928467 15300
val loss = 1.699253797531128
training loss = 2.0381202697753906 15400
val loss = 1.8283567428588867
training loss = 2.0193400382995605 15500
val loss = 1.697394847869873
training loss = 2.0191855430603027 15600
val loss = 1.697495937347412
training loss = 2.023664712905884 15700
val loss = 1.6492412090301514
training loss = 2.0189106464385986 15800
val loss = 1.6971895694732666
training loss = 2.0451924800872803 15900
val loss = 1.8562747240066528
training loss = 2.018664598464966 16000
val loss = 1.6995618343353271
training loss = 2.018498659133911 16100
val loss = 1.694309949874878
training loss = 2.018554210662842 16200
val loss = 1.7059439420700073
training loss = 2.0182504653930664 16300
val loss = 1.6937241554260254
training loss = 2.3125884532928467 16400
val loss = 1.5616092681884766
training loss = 2.0180206298828125 16500
val loss = 1.6955184936523438
training loss = 2.017878770828247 16600
val loss = 1.691802740097046
training loss = 2.0187041759490967 16700
val loss = 1.6676406860351562
training loss = 2.0176517963409424 16800
val loss = 1.6906661987304688
training loss = 2.0243327617645264 16900
val loss = 1.7652666568756104
training loss = 2.017449140548706 17000
val loss = 1.6936900615692139
training loss = 2.0173115730285645 17100
val loss = 1.6892879009246826
training loss = 2.0178232192993164 17200
val loss = 1.6710145473480225
training loss = 2.017102003097534 17300
val loss = 1.6889618635177612
training loss = 2.0288681983947754 17400
val loss = 1.790130853652954
training loss = 2.0169167518615723 17500
val loss = 1.6844682693481445
training loss = 2.0167856216430664 17600
val loss = 1.6868599653244019
training loss = 2.0170483589172363 17700
val loss = 1.7025516033172607
training loss = 2.0165975093841553 17800
val loss = 1.6860883235931396
training loss = 2.0166308879852295 17900
val loss = 1.6955442428588867
training loss = 2.0164482593536377 18000
val loss = 1.6804275512695312
training loss = 2.016305685043335 18100
val loss = 1.6844035387039185
training loss = 2.0200541019439697 18200
val loss = 1.638547658920288
training loss = 2.0161361694335938 18300
val loss = 1.6842838525772095
training loss = 2.016031265258789 18400
val loss = 1.68285071849823
training loss = 2.015963077545166 18500
val loss = 1.6825836896896362
training loss = 2.015862226486206 18600
val loss = 1.6821998357772827
training loss = 2.038496255874634 18700
val loss = 1.581127405166626
training loss = 2.0156986713409424 18800
val loss = 1.6825138330459595
training loss = 2.0156869888305664 18900
val loss = 1.689171314239502
training loss = 2.0155980587005615 19000
val loss = 1.6875391006469727
training loss = 2.015444755554199 19100
val loss = 1.6802736520767212
training loss = 2.0173470973968506 19200
val loss = 1.7208212614059448
training loss = 2.0152907371520996 19300
val loss = 1.6801222562789917
training loss = 2.015204429626465 19400
val loss = 1.676882028579712
training loss = 2.015170097351074 19500
val loss = 1.6837679147720337
training loss = 2.015052556991577 19600
val loss = 1.6786268949508667
training loss = 2.0930933952331543 19700
val loss = 1.521723747253418
training loss = 2.0149102210998535 19800
val loss = 1.678924560546875
training loss = 2.0148186683654785 19900
val loss = 1.6778156757354736
training loss = 2.0152719020843506 20000
val loss = 1.6974486112594604
training loss = 2.014678478240967 20100
val loss = 1.6777355670928955
training loss = 2.0210776329040527 20200
val loss = 1.7513630390167236
training loss = 2.0145769119262695 20300
val loss = 1.6722826957702637
training loss = 2.014448642730713 20400
val loss = 1.676639199256897
training loss = 2.0143978595733643 20500
val loss = 1.6790331602096558
training loss = 2.014305830001831 20600
val loss = 1.6763368844985962
training loss = 2.081925868988037 20700
val loss = 1.521164059638977
training loss = 2.014180898666382 20800
val loss = 1.678486704826355
training loss = 2.014082193374634 20900
val loss = 1.6755754947662354
training loss = 2.015471935272217 21000
val loss = 1.7098052501678467
training loss = 2.013946294784546 21100
val loss = 1.6754844188690186
training loss = 2.013925552368164 21200
val loss = 1.682598352432251
training loss = 2.0138144493103027 21300
val loss = 1.6738004684448242
training loss = 2.013716220855713 21400
val loss = 1.6750061511993408
training loss = 2.0142056941986084 21500
val loss = 1.6962119340896606
training loss = 2.0135741233825684 21600
val loss = 1.6751415729522705
training loss = 2.020937204360962 21700
val loss = 1.6104155778884888
training loss = 2.013443946838379 21800
val loss = 1.674091100692749
training loss = 2.0133464336395264 21900
val loss = 1.675007700920105
training loss = 2.013321876525879 22000
val loss = 1.6660830974578857
training loss = 2.0132055282592773 22100
val loss = 1.6767210960388184
training loss = 2.0131044387817383 22200
val loss = 1.6750857830047607
training loss = 2.0131759643554688 22300
val loss = 1.6670520305633545
training loss = 2.012967586517334 22400
val loss = 1.6755414009094238
training loss = 2.0128650665283203 22500
val loss = 1.675156593322754
training loss = 2.0143320560455322 22600
val loss = 1.7117674350738525
training loss = 2.012711524963379 22700
val loss = 1.6756722927093506
training loss = 2.1120212078094482 22800
val loss = 2.04099702835083
training loss = 2.0125529766082764 22900
val loss = 1.678353190422058
training loss = 2.0124411582946777 23000
val loss = 1.676798939704895
training loss = 2.012592315673828 23100
val loss = 1.665892481803894
training loss = 2.01226806640625 23200
val loss = 1.67685067653656
training loss = 2.035461187362671 23300
val loss = 1.833763599395752
training loss = 2.012094259262085 23400
val loss = 1.67885160446167
training loss = 2.0119752883911133 23500
val loss = 1.6781318187713623
training loss = 2.012676477432251 23600
val loss = 1.658217191696167
training loss = 2.011815309524536 23700
val loss = 1.6791224479675293
training loss = 2.0116913318634033 23800
val loss = 1.6795284748077393
training loss = 2.012169599533081 23900
val loss = 1.7027466297149658
training loss = 2.0115017890930176 24000
val loss = 1.6810845136642456
training loss = 2.011370897293091 24100
val loss = 1.6811833381652832
training loss = 2.0139496326446533 24200
val loss = 1.6422597169876099
training loss = 2.011173963546753 24300
val loss = 1.6823279857635498
training loss = 2.011037588119507 24400
val loss = 1.6817188262939453
training loss = 2.01106595993042 24500
val loss = 1.6753594875335693
training loss = 2.010812997817993 24600
val loss = 1.684760570526123
training loss = 2.020664930343628 24700
val loss = 1.6133062839508057
training loss = 2.010594367980957 24800
val loss = 1.687339186668396
training loss = 2.0104448795318604 24900
val loss = 1.6873852014541626
training loss = 2.0104691982269287 25000
val loss = 1.6802408695220947
training loss = 2.010207176208496 25100
val loss = 1.688963532447815
training loss = 2.0626015663146973 25200
val loss = 1.5492503643035889
training loss = 2.0099692344665527 25300
val loss = 1.6903700828552246
training loss = 2.0098166465759277 25400
val loss = 1.693413496017456
training loss = 2.009929895401001 25500
val loss = 1.7056251764297485
training loss = 2.0095720291137695 25600
val loss = 1.6935445070266724
training loss = 2.0262231826782227 25700
val loss = 1.826046347618103
training loss = 2.0093417167663574 25800
val loss = 1.6947174072265625
training loss = 2.009188175201416 25900
val loss = 1.6964918375015259
training loss = 2.0095698833465576 26000
val loss = 1.7180018424987793
training loss = 2.0089690685272217 26100
val loss = 1.697927713394165
training loss = 2.008820056915283 26200
val loss = 1.6992573738098145
training loss = 2.009808301925659 26300
val loss = 1.7312064170837402
training loss = 2.008605480194092 26400
val loss = 1.7007806301116943
training loss = 2.0099899768829346 26500
val loss = 1.6698412895202637
training loss = 2.008391857147217 26600
val loss = 1.7020461559295654
training loss = 2.0082528591156006 26700
val loss = 1.7035118341445923
training loss = 2.0213513374328613 26800
val loss = 1.6200761795043945
training loss = 2.0080623626708984 26900
val loss = 1.704707145690918
training loss = 2.0079305171966553 27000
val loss = 1.7056629657745361
training loss = 2.01041841506958 27100
val loss = 1.6678268909454346
training loss = 2.0077648162841797 27200
val loss = 1.7073652744293213
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 901.5861,  845.0303,  893.9194,  919.4729,  970.5281, 1064.8953,
        1118.6134, 1121.2075, 1128.6382, 1224.0647, 1267.3279, 1124.1907,
        1214.1010, 1236.0880, 1348.3086, 1352.4087, 1454.9778, 1415.6182,
        1558.5168, 1519.9573, 1593.8674, 1521.1531, 1659.4940, 1629.5853,
        1666.3291, 1717.7147, 1635.0127, 1698.2826, 1753.3466, 1738.1720,
        1635.2925, 1765.1433, 1644.9055, 1781.1211, 1771.9735, 1737.9897,
        1713.0657, 1567.2423, 1609.3323, 1674.7141, 1649.3335, 1613.2000,
        1503.3297, 1483.1429, 1340.5923, 1333.1368, 1267.5256, 1298.7803,
        1152.8522, 1198.0240, 1047.9062, 1020.7658,  975.6620,  963.3216,
         906.9880,  851.9647,  803.4256,  784.9285,  615.8264,  549.1463,
         577.8148,  478.1275,  444.7016,  424.0999,  356.5222,  319.4530,
         285.7504,  252.4324,  210.0941,  185.8246,  163.0672,  130.5081,
         104.4221,  110.5027,   95.0556,   65.5268,   62.1430,   47.5214,
          37.3258,   44.6424,   17.3704,   39.9506,   43.0745])]
2934.9297956968003
1.9802617428356906 16.97199478967717 75.91257392055391
val isze = 8
idinces = [65 49 41 31 71 18 74 29 75 10 27  9 46 43 50 51 68 63 70 62  4 25 72 79
  6 81 12 44 57 64 15 59 40 42 73 30  7 66 23 28 47 21 11 77 14 78 60 52
 69  3 24  2  0 35 48 56 32 26 61  8 54 13 37 80  5 16 39 33 20 38 67 53
 36 17 19 34 58  1 55 45 22 76 82]
we are doing training validation split
training loss = 264.2849426269531 100
val loss = 271.301513671875
training loss = 12.986109733581543 200
val loss = 18.672658920288086
training loss = 8.637331008911133 300
val loss = 12.033418655395508
training loss = 8.393645286560059 400
val loss = 11.570816040039062
training loss = 8.141500473022461 500
val loss = 11.06735610961914
training loss = 7.893467903137207 600
val loss = 10.555630683898926
training loss = 7.660641670227051 700
val loss = 10.057171821594238
training loss = 7.450618743896484 800
val loss = 9.588811874389648
training loss = 7.268000602722168 900
val loss = 9.163426399230957
training loss = 7.113908767700195 1000
val loss = 8.788772583007812
training loss = 6.985748767852783 1100
val loss = 8.465665817260742
training loss = 6.876259803771973 1200
val loss = 8.292482376098633
training loss = 6.759596347808838 1300
val loss = 7.907163143157959
training loss = 6.602859973907471 1400
val loss = 7.668640613555908
training loss = 6.388904094696045 1500
val loss = 6.788890838623047
training loss = 6.088616371154785 1600
val loss = 6.586153984069824
training loss = 5.677756309509277 1700
val loss = 5.899834632873535
training loss = 5.053410530090332 1800
val loss = 5.071646213531494
training loss = 4.21467924118042 1900
val loss = 3.8303730487823486
training loss = 3.4699275493621826 2000
val loss = 3.1195647716522217
training loss = 3.116788148880005 2100
val loss = 2.4659016132354736
training loss = 3.016223192214966 2200
val loss = 2.2810444831848145
training loss = 2.9753196239471436 2300
val loss = 2.370797634124756
training loss = 2.9515485763549805 2400
val loss = 2.2828562259674072
training loss = 2.9257519245147705 2500
val loss = 2.4342172145843506
training loss = 2.907510995864868 2600
val loss = 2.534660816192627
training loss = 2.889338731765747 2700
val loss = 2.50221586227417
training loss = 2.8760857582092285 2800
val loss = 2.486482858657837
training loss = 2.863234519958496 2900
val loss = 2.5706324577331543
training loss = 2.8575477600097656 3000
val loss = 2.7410800457000732
training loss = 2.845648765563965 3100
val loss = 2.630152940750122
training loss = 2.849649667739868 3200
val loss = 2.8990395069122314
training loss = 2.834041118621826 3300
val loss = 2.6803574562072754
training loss = 2.829373359680176 3400
val loss = 2.6996922492980957
training loss = 2.826444149017334 3500
val loss = 2.711238145828247
training loss = 2.823359727859497 3600
val loss = 2.7311315536499023
training loss = 2.8206353187561035 3700
val loss = 2.736560344696045
training loss = 2.818981647491455 3800
val loss = 2.7287068367004395
training loss = 2.8168253898620605 3900
val loss = 2.7648887634277344
training loss = 2.918830633163452 4000
val loss = 3.639951467514038
training loss = 2.8135225772857666 4100
val loss = 2.7790510654449463
training loss = 2.8118607997894287 4200
val loss = 2.786539077758789
training loss = 2.8110404014587402 4300
val loss = 2.845266342163086
training loss = 2.808886766433716 4400
val loss = 2.7893331050872803
training loss = 2.8310368061065674 4500
val loss = 2.442502975463867
training loss = 2.8059823513031006 4600
val loss = 2.792891502380371
training loss = 2.8045108318328857 4700
val loss = 2.792464256286621
training loss = 2.803790330886841 4800
val loss = 2.8567376136779785
training loss = 2.8016602993011475 4900
val loss = 2.7927634716033936
training loss = 2.864611864089966 5000
val loss = 3.466383457183838
training loss = 2.798828363418579 5100
val loss = 2.800123691558838
training loss = 2.797461748123169 5200
val loss = 2.789618968963623
training loss = 2.8361833095550537 5300
val loss = 2.349149703979492
training loss = 2.7947094440460205 5400
val loss = 2.7825257778167725
training loss = 2.793429136276245 5500
val loss = 2.7844600677490234
training loss = 2.793280839920044 5600
val loss = 2.697357177734375
training loss = 2.790820837020874 5700
val loss = 2.7810754776000977
training loss = 2.9967916011810303 5800
val loss = 4.062798023223877
training loss = 2.788322687149048 5900
val loss = 2.7808451652526855
training loss = 2.7875912189483643 6000
val loss = 2.7240777015686035
training loss = 2.7860169410705566 6100
val loss = 2.745973825454712
training loss = 2.78482985496521 6200
val loss = 2.767543315887451
training loss = 2.7853996753692627 6300
val loss = 2.664031982421875
training loss = 2.7825865745544434 6400
val loss = 2.762228488922119
training loss = 3.2375073432922363 6500
val loss = 1.593097448348999
training loss = 2.780468702316284 6600
val loss = 2.747650146484375
training loss = 2.7797036170959473 6700
val loss = 2.789605140686035
training loss = 2.778499126434326 6800
val loss = 2.773709297180176
training loss = 2.777475118637085 6900
val loss = 2.749998092651367
training loss = 2.7789554595947266 7000
val loss = 2.8655738830566406
training loss = 2.7754971981048584 7100
val loss = 2.743790626525879
training loss = 2.79050350189209 7200
val loss = 3.0543439388275146
training loss = 2.773585319519043 7300
val loss = 2.751549243927002
training loss = 2.7726457118988037 7400
val loss = 2.734203815460205
training loss = 2.777641773223877 7500
val loss = 2.920429229736328
training loss = 2.7706358432769775 7600
val loss = 2.72841215133667
training loss = 2.769742250442505 7700
val loss = 2.706960678100586
training loss = 2.768679618835449 7800
val loss = 2.7503819465637207
training loss = 2.7675139904022217 7900
val loss = 2.7186572551727295
training loss = 2.8153812885284424 8000
val loss = 2.2408010959625244
training loss = 2.7651655673980713 8100
val loss = 2.709894895553589
training loss = 2.764012098312378 8200
val loss = 2.6958725452423096
training loss = 2.7625815868377686 8300
val loss = 2.7043862342834473
training loss = 2.761314868927002 8400
val loss = 2.6991195678710938
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 830.7009,  836.3463,  957.1923,  979.7174,  995.6038, 1103.2133,
        1113.8718, 1060.7174, 1133.3358, 1176.4095, 1201.9135, 1230.4487,
        1213.2902, 1287.1416, 1356.6522, 1435.3369, 1414.1730, 1424.2946,
        1515.3027, 1566.2721, 1586.2502, 1571.4338, 1632.3966, 1580.1259,
        1589.3853, 1714.4908, 1601.8690, 1746.8936, 1713.7661, 1691.6930,
        1678.3003, 1775.5774, 1717.8644, 1722.5603, 1656.9119, 1719.4164,
        1677.3094, 1569.0137, 1640.4817, 1581.4244, 1635.6586, 1560.7850,
        1526.2502, 1474.2261, 1312.5559, 1338.0969, 1272.0206, 1189.0199,
        1109.3242, 1210.0792, 1108.1324,  966.7310,  943.2682,  908.0488,
         881.8252,  849.6439,  861.5168,  701.9972,  617.2397,  505.8376,
         543.9646,  469.0034,  476.4887,  404.6593,  379.2032,  368.9297,
         274.2258,  231.7853,  217.8266,  160.5377,  165.4933,  154.3363,
         149.1514,  136.6194,  108.2234,   75.7877,   41.4191,   38.6787,
          38.2434,   40.8399,   19.5846,   31.1250,   35.1295])]
2644.818515715413
0.5046653053248135 1.2693201917010533 97.60715570475286
val isze = 8
idinces = [56 62 69 24  0 36 14 70 68 54 34 25 77  5 46 59 40  4 78 43  8 81 51  7
 30 10 39 73 29  6 26 33 23 22  3  1 32 37  9 67 57 41 27 12 66 72 64 71
 76 55 18 35 53 13 80 58 47 28 17 74 45 20 63 16 31 50 19 79 52 65 49 61
 75 42 38 21 11 48 82 15  2 60 44]
we are doing training validation split
training loss = 21.91790199279785 100
val loss = 35.63487243652344
training loss = 12.849492073059082 200
val loss = 21.533897399902344
training loss = 7.707036972045898 300
val loss = 11.207674980163574
training loss = 5.781594276428223 400
val loss = 6.788270950317383
training loss = 4.663608074188232 500
val loss = 5.544126510620117
training loss = 3.73189377784729 600
val loss = 4.931568145751953
training loss = 3.024296760559082 700
val loss = 4.6769208908081055
training loss = 2.5731139183044434 800
val loss = 4.730716705322266
training loss = 2.3444344997406006 900
val loss = 4.899523735046387
training loss = 2.2153971195220947 1000
val loss = 5.089468002319336
training loss = 2.1347403526306152 1100
val loss = 5.20931339263916
training loss = 2.0717077255249023 1200
val loss = 5.257061004638672
training loss = 2.0273971557617188 1300
val loss = 5.325547218322754
training loss = 1.9840809106826782 1400
val loss = 5.329294681549072
training loss = 1.9554712772369385 1500
val loss = 5.3711018562316895
training loss = 1.928098201751709 1600
val loss = 5.372650146484375
training loss = 1.9101182222366333 1700
val loss = 5.3796162605285645
training loss = 1.890720248222351 1800
val loss = 5.405142307281494
training loss = 1.876659631729126 1900
val loss = 5.419771194458008
training loss = 1.8651174306869507 2000
val loss = 5.431951522827148
training loss = 1.8552722930908203 2100
val loss = 5.447254657745361
training loss = 1.8484594821929932 2200
val loss = 5.457901477813721
training loss = 1.8401730060577393 2300
val loss = 5.472202301025391
training loss = 1.8343685865402222 2400
val loss = 5.486566543579102
training loss = 1.829376220703125 2500
val loss = 5.493069648742676
training loss = 1.825182318687439 2600
val loss = 5.502521991729736
training loss = 1.8215723037719727 2700
val loss = 5.509705543518066
training loss = 1.8183863162994385 2800
val loss = 5.516692638397217
training loss = 1.8248825073242188 2900
val loss = 5.532201290130615
training loss = 1.813308596611023 3000
val loss = 5.527205944061279
training loss = 1.9325000047683716 3100
val loss = 5.620053768157959
training loss = 1.8094059228897095 3200
val loss = 5.5348076820373535
training loss = 1.8078349828720093 3300
val loss = 5.538121223449707
training loss = 1.8064038753509521 3400
val loss = 5.540909767150879
training loss = 1.8051319122314453 3500
val loss = 5.5430402755737305
training loss = 1.8042325973510742 3600
val loss = 5.545150279998779
training loss = 1.8029378652572632 3700
val loss = 5.546380043029785
training loss = 1.890312910079956 3800
val loss = 5.61503267288208
training loss = 1.8011828660964966 3900
val loss = 5.548646450042725
training loss = 1.8004601001739502 4000
val loss = 5.5497236251831055
training loss = 1.799788236618042 4100
val loss = 5.551129341125488
training loss = 1.7990955114364624 4200
val loss = 5.5503926277160645
training loss = 1.7985177040100098 4300
val loss = 5.550939083099365
training loss = 1.7981175184249878 4400
val loss = 5.551623821258545
training loss = 1.7972561120986938 4500
val loss = 5.550598621368408
training loss = 1.849023699760437 4600
val loss = 5.5987114906311035
training loss = 1.7962901592254639 4700
val loss = 5.548962116241455
training loss = 1.7958545684814453 4800
val loss = 5.54808235168457
training loss = 1.7958883047103882 4900
val loss = 5.548350811004639
training loss = 1.7950419187545776 5000
val loss = 5.545738220214844
training loss = 1.797204613685608 5100
val loss = 5.542478561401367
training loss = 1.794278621673584 5200
val loss = 5.542829990386963
training loss = 1.7939506769180298 5300
val loss = 5.5304765701293945
training loss = 1.7935547828674316 5400
val loss = 5.539522647857666
training loss = 1.7932102680206299 5500
val loss = 5.5376386642456055
training loss = 1.7937843799591064 5600
val loss = 5.537574291229248
training loss = 1.792527675628662 5700
val loss = 5.5338134765625
reduced chi^2 level 2 = 1.7924695014953613
Constrained alpha: 1.8888883590698242
Constrained beta: 3.4662392139434814
Constrained gamma: 28.979347229003906
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 848.0698,  938.0410,  995.3058,  900.6294, 1043.2137, 1031.3484,
        1134.1624, 1126.4016, 1144.6372, 1188.1918, 1188.0913, 1113.6746,
        1238.4182, 1237.0729, 1362.6896, 1374.2971, 1363.0723, 1426.7606,
        1500.6862, 1500.8000, 1611.1115, 1592.3550, 1688.8467, 1556.0051,
        1761.6399, 1683.2483, 1579.6157, 1739.6704, 1746.9315, 1690.9897,
        1750.0660, 1777.2008, 1627.4226, 1708.7714, 1684.1244, 1736.5751,
        1721.9498, 1627.9297, 1653.8469, 1683.4366, 1569.1108, 1634.1843,
        1461.6772, 1577.8191, 1373.9114, 1295.9629, 1306.6993, 1212.9237,
        1124.6243, 1148.6029, 1125.3033, 1021.1932,  993.7093,  925.7524,
         893.7100,  886.6177,  860.9777,  716.4203,  604.3984,  586.0093,
         538.4838,  422.7468,  464.4022,  404.2684,  369.1492,  368.3027,
         299.1807,  235.8535,  194.2831,  181.4546,  160.5573,  145.1504,
         140.4024,   99.9592,  101.0039,   72.9849,   65.8565,   40.4882,
          40.5898,   50.1069,   12.1706,   38.2868,   26.7987])]
2905.3493354682487
0.2317769739604375 6.164792662379144 69.4422894720068
val isze = 8
idinces = [77 54 34  0 38 46 76 43  2 22 66 68 80  9 59 16 24 44 65 61 41 10 62 32
 39 47 55  1 70 78 45  7 23 81 11 31 35 36 60 52 13 57 72 19  6 18 69 75
 20 74 58  8  5 82 49 14  4 53 21 73 33 48 51 28 67 26 64 50 63 27 37 29
 25  3 15 71 17 42 12 56 40 79 30]
we are doing training validation split
training loss = 8.962265968322754 100
val loss = 9.44611930847168
training loss = 7.273104190826416 200
val loss = 9.863533020019531
training loss = 7.057587146759033 300
val loss = 9.333915710449219
training loss = 6.803895950317383 400
val loss = 8.924084663391113
training loss = 6.585246562957764 500
val loss = 8.65416431427002
training loss = 6.390414237976074 600
val loss = 8.193202018737793
training loss = 6.2365217208862305 700
val loss = 7.972322940826416
training loss = 6.111409664154053 800
val loss = 7.688040733337402
training loss = 6.013765811920166 900
val loss = 7.475920677185059
training loss = 5.936901092529297 1000
val loss = 7.284061908721924
training loss = 5.868415355682373 1100
val loss = 7.158906936645508
training loss = 5.7982354164123535 1200
val loss = 7.005852699279785
training loss = 5.749200344085693 1300
val loss = 7.028477668762207
training loss = 5.574718475341797 1400
val loss = 6.627476692199707
training loss = 5.3941969871521 1500
val loss = 6.443319320678711
training loss = 5.078688144683838 1600
val loss = 5.857134819030762
training loss = 4.620444297790527 1700
val loss = 5.130364418029785
training loss = 3.986750602722168 1800
val loss = 4.1045355796813965
training loss = 3.306779384613037 1900
val loss = 2.971372604370117
training loss = 2.8741707801818848 2000
val loss = 2.2012648582458496
training loss = 2.6943888664245605 2100
val loss = 1.848998785018921
training loss = 2.6376969814300537 2200
val loss = 1.719106912612915
training loss = 2.615670680999756 2300
val loss = 1.6991428136825562
training loss = 2.6031229496002197 2400
val loss = 1.6934590339660645
training loss = 2.594036817550659 2500
val loss = 1.691068410873413
training loss = 2.5859010219573975 2600
val loss = 1.703248381614685
training loss = 2.5809128284454346 2700
val loss = 1.697288990020752
training loss = 2.57352352142334 2800
val loss = 1.7228444814682007
training loss = 2.5684804916381836 2900
val loss = 1.7339378595352173
training loss = 2.564986228942871 3000
val loss = 1.7363741397857666
training loss = 2.5608057975769043 3100
val loss = 1.7560465335845947
training loss = 2.6974384784698486 3200
val loss = 1.7719796895980835
training loss = 2.555367946624756 3300
val loss = 1.7786099910736084
training loss = 2.5532846450805664 3400
val loss = 1.787115216255188
training loss = 2.551711320877075 3500
val loss = 1.7982981204986572
training loss = 2.5503764152526855 3600
val loss = 1.8081581592559814
training loss = 2.553086280822754 3700
val loss = 1.7983570098876953
training loss = 2.5487005710601807 3800
val loss = 1.8251774311065674
training loss = 2.5512402057647705 3900
val loss = 1.8549799919128418
training loss = 2.547898530960083 4000
val loss = 1.8396319150924683
training loss = 2.5477194786071777 4100
val loss = 1.8475607633590698
training loss = 2.5559167861938477 4200
val loss = 1.833474040031433
training loss = 2.5477235317230225 4300
val loss = 1.8588666915893555
training loss = 2.547860860824585 4400
val loss = 1.864725947380066
training loss = 2.561863899230957 4500
val loss = 1.8485379219055176
training loss = 2.5481908321380615 4600
val loss = 1.8729344606399536
training loss = 2.5484352111816406 4700
val loss = 1.8776353597640991
training loss = 2.9780020713806152 4800
val loss = 2.1347219944000244
training loss = 2.5488357543945312 4900
val loss = 1.8829596042633057
training loss = 2.5490427017211914 5000
val loss = 1.8872597217559814
training loss = 2.549211025238037 5100
val loss = 1.8911337852478027
training loss = 2.549283981323242 5200
val loss = 1.891390323638916
training loss = 2.549417018890381 5300
val loss = 1.8939400911331177
training loss = 2.5503594875335693 5400
val loss = 1.8881067037582397
training loss = 2.5493602752685547 5500
val loss = 1.8965446949005127
training loss = 2.5493643283843994 5600
val loss = 1.8985350131988525
training loss = 2.5494556427001953 5700
val loss = 1.9030613899230957
training loss = 2.5490405559539795 5800
val loss = 1.8996390104293823
training loss = 2.5753374099731445 5900
val loss = 1.8876416683197021
training loss = 2.5485177040100098 6000
val loss = 1.9005327224731445
training loss = 2.548271417617798 6100
val loss = 1.9000880718231201
training loss = 2.54783034324646 6200
val loss = 1.9027595520019531
training loss = 2.5473763942718506 6300
val loss = 1.9010026454925537
training loss = 2.81494140625 6400
val loss = 2.3008673191070557
training loss = 2.54634428024292 6500
val loss = 1.9001078605651855
training loss = 2.5458242893218994 6600
val loss = 1.9004707336425781
training loss = 2.546403169631958 6700
val loss = 1.891851544380188
training loss = 2.544502019882202 6800
val loss = 1.899280309677124
training loss = 2.5482425689697266 6900
val loss = 1.8865532875061035
training loss = 2.543060064315796 7000
val loss = 1.8991327285766602
training loss = 2.542311668395996 7100
val loss = 1.8977059125900269
training loss = 2.5414555072784424 7200
val loss = 1.8965106010437012
training loss = 2.540620803833008 7300
val loss = 1.8959801197052002
training loss = 2.582444429397583 7400
val loss = 1.9814552068710327
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 823.9896,  822.0649,  906.8884,  946.6907,  986.5187, 1046.6046,
        1149.0345, 1141.5078, 1119.4974, 1186.6523, 1189.5370, 1248.2544,
        1282.7496, 1244.8660, 1325.3368, 1432.7570, 1452.7449, 1428.9451,
        1486.2332, 1569.1224, 1537.4111, 1581.6694, 1570.2559, 1653.5759,
        1657.7618, 1699.0527, 1618.5846, 1688.0084, 1730.6022, 1607.3794,
        1756.2667, 1831.3806, 1693.1321, 1713.2443, 1705.1915, 1734.2810,
        1736.4720, 1644.5049, 1634.2567, 1644.6211, 1605.8972, 1535.5148,
        1577.6774, 1441.2555, 1460.3512, 1334.0311, 1302.8944, 1198.2330,
        1202.0824, 1188.4204, 1103.1080, 1004.5061,  925.3267,  946.0049,
         835.0465,  892.8494,  783.0377,  705.5922,  611.2605,  560.0904,
         563.2523,  473.1726,  441.3556,  394.9435,  342.8785,  334.8879,
         293.1116,  242.7792,  194.3610,  157.4353,  157.5462,  119.3673,
         130.9780,   97.2096,   84.7715,   67.5753,   58.4689,   47.7319,
          36.1312,   33.2597,   19.3033,   30.3381,   32.8222])]
2240.598718759735
4.504501965654658 6.000190497284392 30.033855146850275
val isze = 8
idinces = [27 48 32 56 37  3  4 73 22 81 65 67 58 61 43 59 35 75 53 24 74  6 57 11
  1 13 50 38 64 79 39 60 29 49 69 46 17 77 45 16  5 40  8  2 14 44 82 80
 30 71 12 33 25 15 36 41 78 42 18 23 10 54 47  9  0 21 63 34 26 20 52  7
 62 66 72 76 55 68 19 31 28 70 51]
we are doing training validation split
training loss = 19.406770706176758 100
val loss = 27.091325759887695
training loss = 11.643006324768066 200
val loss = 13.575773239135742
training loss = 8.252443313598633 300
val loss = 8.425026893615723
training loss = 6.675812244415283 400
val loss = 5.995619773864746
training loss = 5.9181671142578125 500
val loss = 4.785463333129883
training loss = 5.547972202301025 600
val loss = 4.155943870544434
training loss = 5.366380214691162 700
val loss = 3.8171896934509277
training loss = 5.277342319488525 800
val loss = 3.629434585571289
training loss = 5.2333149909973145 900
val loss = 3.5223004817962646
training loss = 5.210703372955322 1000
val loss = 3.4593234062194824
training loss = 5.197954177856445 1100
val loss = 3.4210045337677
training loss = 5.189576148986816 1200
val loss = 3.39689302444458
training loss = 5.183040142059326 1300
val loss = 3.3810906410217285
training loss = 5.177245140075684 1400
val loss = 3.370137929916382
training loss = 5.171717166900635 1500
val loss = 3.362049102783203
training loss = 5.16627836227417 1600
val loss = 3.3556690216064453
training loss = 5.160864353179932 1700
val loss = 3.3501694202423096
training loss = 5.155462265014648 1800
val loss = 3.345296859741211
training loss = 5.150086879730225 1900
val loss = 3.3407111167907715
training loss = 5.144766807556152 2000
val loss = 3.3364248275756836
training loss = 5.139535427093506 2100
val loss = 3.332289218902588
training loss = 5.134427070617676 2200
val loss = 3.328406810760498
training loss = 5.1294779777526855 2300
val loss = 3.324770212173462
training loss = 5.124719142913818 2400
val loss = 3.32135009765625
training loss = 5.120178699493408 2500
val loss = 3.318286895751953
training loss = 5.115871906280518 2600
val loss = 3.3154685497283936
training loss = 5.113336086273193 2700
val loss = 3.2788965702056885
training loss = 5.1081318855285645 2800
val loss = 3.309971570968628
training loss = 5.104663372039795 2900
val loss = 3.309037685394287
training loss = 5.101528644561768 3000
val loss = 3.298236846923828
training loss = 5.098482131958008 3100
val loss = 3.3034887313842773
training loss = 5.099073886871338 3200
val loss = 3.251451253890991
training loss = 5.092617988586426 3300
val loss = 3.3006510734558105
training loss = 5.097522735595703 3400
val loss = 3.223755359649658
training loss = 5.085422515869141 3500
val loss = 3.296649694442749
training loss = 5.080141067504883 3600
val loss = 3.322948455810547
training loss = 5.067305564880371 3700
val loss = 3.2808940410614014
training loss = 5.036128997802734 3800
val loss = 3.2632710933685303
training loss = 4.941516399383545 3900
val loss = 3.202815055847168
training loss = 4.7111921310424805 4000
val loss = 3.003329277038574
training loss = 4.387392997741699 4100
val loss = 2.534064292907715
training loss = 3.7584774494171143 4200
val loss = 2.1249566078186035
training loss = 2.7331180572509766 4300
val loss = 1.317319631576538
training loss = 1.8197208642959595 4400
val loss = 0.9103165864944458
training loss = 1.6140073537826538 4500
val loss = 0.9065971970558167
training loss = 1.5910391807556152 4600
val loss = 0.9021279215812683
training loss = 1.5756309032440186 4700
val loss = 0.9051390886306763
training loss = 1.5889612436294556 4800
val loss = 0.7807332277297974
training loss = 1.5537911653518677 4900
val loss = 0.8998192548751831
training loss = 1.5456815958023071 5000
val loss = 0.8886992931365967
training loss = 1.5388959646224976 5100
val loss = 0.9005380272865295
training loss = 1.5330380201339722 5200
val loss = 0.8926461935043335
training loss = 1.5281652212142944 5300
val loss = 0.878975510597229
training loss = 1.5236845016479492 5400
val loss = 0.8900856971740723
training loss = 1.5228233337402344 5500
val loss = 0.8419870138168335
training loss = 1.5164042711257935 5600
val loss = 0.8888325691223145
training loss = 1.513345718383789 5700
val loss = 0.8873013257980347
training loss = 1.511157512664795 5800
val loss = 0.8642286658287048
training loss = 1.5080431699752808 5900
val loss = 0.8857188820838928
training loss = 1.698982834815979 6000
val loss = 0.7021058797836304
training loss = 1.5035127401351929 6100
val loss = 0.8839232325553894
training loss = 1.5014731884002686 6200
val loss = 0.8830450177192688
training loss = 1.5022000074386597 6300
val loss = 0.9330294132232666
training loss = 1.497663974761963 6400
val loss = 0.8807389736175537
training loss = 1.6123205423355103 6500
val loss = 0.7000870704650879
training loss = 1.4941202402114868 6600
val loss = 0.8788919448852539
training loss = 1.4924125671386719 6700
val loss = 0.8780109286308289
training loss = 1.4909451007843018 6800
val loss = 0.890591025352478
training loss = 1.4891060590744019 6900
val loss = 0.8756513595581055
training loss = 1.4918729066848755 7000
val loss = 0.818603515625
training loss = 1.4858180284500122 7100
val loss = 0.8690024018287659
training loss = 1.484162449836731 7200
val loss = 0.8717581629753113
training loss = 1.4856820106506348 7300
val loss = 0.9254323244094849
training loss = 1.4807947874069214 7400
val loss = 0.8681594729423523
training loss = 1.4790844917297363 7500
val loss = 0.8634403944015503
training loss = 1.4774014949798584 7600
val loss = 0.874608039855957
training loss = 1.4755693674087524 7700
val loss = 0.8644510507583618
training loss = 1.4791178703308105 7800
val loss = 0.8022446632385254
training loss = 1.4719301462173462 7900
val loss = 0.861194908618927
training loss = 1.5466612577438354 8000
val loss = 1.1995961666107178
training loss = 1.468171238899231 8100
val loss = 0.8568590879440308
training loss = 1.4662339687347412 8200
val loss = 0.856735348701477
training loss = 1.4643949270248413 8300
val loss = 0.8454259037971497
training loss = 1.4623781442642212 8400
val loss = 0.8518832921981812
training loss = 1.4651827812194824 8500
val loss = 0.7912448644638062
training loss = 1.4583961963653564 8600
val loss = 0.8492851853370667
training loss = 1.4563632011413574 8700
val loss = 0.8482903242111206
training loss = 1.4544202089309692 8800
val loss = 0.8523061871528625
training loss = 1.4523147344589233 8900
val loss = 0.8419196605682373
training loss = 1.4636056423187256 9000
val loss = 0.7516420483589172
training loss = 1.4483087062835693 9100
val loss = 0.8373152017593384
training loss = 1.4462835788726807 9200
val loss = 0.8356140851974487
training loss = 1.4632117748260498 9300
val loss = 0.7318160533905029
training loss = 1.442267894744873 9400
val loss = 0.8315036296844482
training loss = 1.4402564764022827 9500
val loss = 0.8296968936920166
training loss = 1.4424601793289185 9600
val loss = 0.7738646864891052
training loss = 1.4362223148345947 9700
val loss = 0.8266515135765076
training loss = 1.434181809425354 9800
val loss = 0.8270174264907837
training loss = 1.4322813749313354 9900
val loss = 0.8142935037612915
training loss = 1.4302219152450562 10000
val loss = 0.819725751876831
training loss = 1.430644154548645 10100
val loss = 0.7775524258613586
training loss = 1.4262630939483643 10200
val loss = 0.8163707852363586
training loss = 1.427685022354126 10300
val loss = 0.7649816274642944
training loss = 1.4223493337631226 10400
val loss = 0.8104439377784729
training loss = 1.4203649759292603 10500
val loss = 0.8119748830795288
training loss = 1.4184484481811523 10600
val loss = 0.8093094825744629
training loss = 1.416517734527588 10700
val loss = 0.8071975708007812
training loss = 1.4203510284423828 10800
val loss = 0.745546817779541
training loss = 1.4126890897750854 10900
val loss = 0.8039460778236389
training loss = 1.431311011314392 11000
val loss = 0.700006365776062
training loss = 1.4088423252105713 11100
val loss = 0.8019740581512451
training loss = 1.4068901538848877 11200
val loss = 0.7994548678398132
training loss = 1.4054687023162842 11300
val loss = 0.8192614912986755
training loss = 1.4029736518859863 11400
val loss = 0.7963582277297974
training loss = 1.4413836002349854 11500
val loss = 0.66546630859375
training loss = 1.3988847732543945 11600
val loss = 0.7928231954574585
training loss = 1.396696925163269 11700
val loss = 0.7930848002433777
training loss = 1.3947421312332153 11800
val loss = 0.7774303555488586
training loss = 1.3921024799346924 11900
val loss = 0.7896146774291992
training loss = 1.3915351629257202 12000
val loss = 0.8295938968658447
training loss = 1.3868145942687988 12100
val loss = 0.7863476276397705
training loss = 1.489988923072815 12200
val loss = 1.1889371871948242
training loss = 1.3805216550827026 12300
val loss = 0.7784680128097534
training loss = 1.376878261566162 12400
val loss = 0.7787481546401978
training loss = 1.3731327056884766 12500
val loss = 0.763395369052887
training loss = 1.3689143657684326 12600
val loss = 0.7712817192077637
training loss = 1.36466383934021 12700
val loss = 0.7710756659507751
training loss = 1.3605782985687256 12800
val loss = 0.7670356035232544
training loss = 1.3563694953918457 12900
val loss = 0.7592015266418457
training loss = 1.5037589073181152 13000
val loss = 0.596440315246582
training loss = 1.3476579189300537 13100
val loss = 0.7548471689224243
training loss = 1.343002200126648 13200
val loss = 0.7503814697265625
training loss = 1.3401795625686646 13300
val loss = 0.7116144895553589
training loss = 1.3333321809768677 13400
val loss = 0.7432753443717957
training loss = 1.3903883695602417 13500
val loss = 1.0242213010787964
training loss = 1.3235012292861938 13600
val loss = 0.7384167909622192
training loss = 1.3187633752822876 13700
val loss = 0.7303829193115234
training loss = 1.3177101612091064 13800
val loss = 0.6804149150848389
training loss = 1.3101941347122192 13900
val loss = 0.7231036424636841
training loss = 1.3234845399856567 14000
val loss = 0.6272947788238525
training loss = 1.3025716543197632 14100
val loss = 0.7203389406204224
training loss = 1.2990444898605347 14200
val loss = 0.7154083847999573
training loss = 1.2958158254623413 14300
val loss = 0.710436224937439
training loss = 1.292758584022522 14400
val loss = 0.7116057872772217
training loss = 1.290506362915039 14500
val loss = 0.7317428588867188
training loss = 1.2872337102890015 14600
val loss = 0.710003137588501
training loss = 1.2846893072128296 14700
val loss = 0.7071882486343384
training loss = 1.2833489179611206 14800
val loss = 0.7336736917495728
training loss = 1.2802684307098389 14900
val loss = 0.7046019434928894
training loss = 1.278552532196045 15000
val loss = 0.7202980518341064
training loss = 1.2764517068862915 15100
val loss = 0.7018221616744995
training loss = 1.274756669998169 15200
val loss = 0.7011421918869019
training loss = 1.2734345197677612 15300
val loss = 0.6855428218841553
training loss = 1.2717944383621216 15400
val loss = 0.6936721801757812
training loss = 1.270405888557434 15500
val loss = 0.6986306309700012
training loss = 1.6599146127700806 15600
val loss = 0.6083816885948181
training loss = 1.2680573463439941 15700
val loss = 0.6969596147537231
training loss = 1.2669942378997803 15800
val loss = 0.6964589953422546
training loss = 1.2676790952682495 15900
val loss = 0.6631523966789246
training loss = 1.265174150466919 16000
val loss = 0.6953809261322021
training loss = 1.2643237113952637 16100
val loss = 0.6949159502983093
training loss = 1.2640084028244019 16200
val loss = 0.7122385501861572
training loss = 1.2629284858703613 16300
val loss = 0.6940136551856995
training loss = 1.2622522115707397 16400
val loss = 0.6929380893707275
training loss = 1.261691689491272 16500
val loss = 0.6967638731002808
training loss = 1.261095404624939 16600
val loss = 0.6926960945129395
training loss = 1.2738826274871826 16700
val loss = 0.6085692048072815
training loss = 1.2600828409194946 16800
val loss = 0.6924359798431396
training loss = 1.2595914602279663 16900
val loss = 0.6892855763435364
training loss = 1.2593069076538086 17000
val loss = 0.6822658181190491
training loss = 1.258738398551941 17100
val loss = 0.6912099123001099
training loss = 1.3453179597854614 17200
val loss = 1.047943115234375
training loss = 1.257942795753479 17300
val loss = 0.6908859014511108
training loss = 1.257531762123108 17400
val loss = 0.6903042793273926
training loss = 1.2574070692062378 17500
val loss = 0.6776596307754517
training loss = 1.2568073272705078 17600
val loss = 0.6898064613342285
training loss = 1.2583589553833008 17700
val loss = 0.6536780595779419
training loss = 1.2561464309692383 17800
val loss = 0.6837412118911743
training loss = 1.255724310874939 17900
val loss = 0.6887362003326416
training loss = 1.2860227823257446 18000
val loss = 0.5713697075843811
training loss = 1.255030632019043 18100
val loss = 0.6880264282226562
training loss = 1.2546437978744507 18200
val loss = 0.6875236630439758
training loss = 1.2552974224090576 18300
val loss = 0.716170608997345
training loss = 1.2539268732070923 18400
val loss = 0.6874538660049438
training loss = 1.3808794021606445 18500
val loss = 1.1407883167266846
training loss = 1.2531867027282715 18600
val loss = 0.6861603856086731
training loss = 1.2527695894241333 18700
val loss = 0.6862720251083374
training loss = 1.2529205083847046 18800
val loss = 0.7057177424430847
training loss = 1.2519934177398682 18900
val loss = 0.6850602030754089
training loss = 1.2603703737258911 19000
val loss = 0.7754009366035461
training loss = 1.2511855363845825 19100
val loss = 0.6844562292098999
training loss = 1.3529648780822754 19200
val loss = 1.07930326461792
training loss = 1.2503610849380493 19300
val loss = 0.684646487236023
training loss = 1.2498838901519775 19400
val loss = 0.6830032467842102
training loss = 1.2495733499526978 19500
val loss = 0.6899803876876831
training loss = 1.24901282787323 19600
val loss = 0.6817326545715332
training loss = 1.253219485282898 19700
val loss = 0.7459518909454346
training loss = 1.2481154203414917 19800
val loss = 0.68135666847229
training loss = 1.2532848119735718 19900
val loss = 0.7518035173416138
training loss = 1.2472444772720337 20000
val loss = 0.6757665872573853
training loss = 1.2467072010040283 20100
val loss = 0.6789292097091675
training loss = 1.3232624530792236 20200
val loss = 1.0080454349517822
training loss = 1.245779275894165 20300
val loss = 0.6788047552108765
training loss = 1.2452433109283447 20400
val loss = 0.6754952669143677
training loss = 1.2448598146438599 20500
val loss = 0.6721829175949097
training loss = 1.2442917823791504 20600
val loss = 0.6760176420211792
training loss = 1.245993733406067 20700
val loss = 0.717143177986145
training loss = 1.243391752243042 20800
val loss = 0.6745331883430481
training loss = 1.2428511381149292 20900
val loss = 0.6736241579055786
training loss = 1.2425817251205444 21000
val loss = 0.6832261681556702
training loss = 1.2419514656066895 21100
val loss = 0.6732878684997559
training loss = 1.2486978769302368 21200
val loss = 0.7528836131095886
training loss = 1.2410770654678345 21300
val loss = 0.6727025508880615
training loss = 1.5178247690200806 21400
val loss = 1.4378173351287842
training loss = 1.240269422531128 21500
val loss = 0.6698900461196899
training loss = 1.2397854328155518 21600
val loss = 0.6707495450973511
training loss = 1.2396135330200195 21700
val loss = 0.6613506078720093
training loss = 1.2390639781951904 21800
val loss = 0.6693236827850342
training loss = 1.2386071681976318 21900
val loss = 0.669098973274231
training loss = 1.238720178604126 22000
val loss = 0.6855624914169312
training loss = 1.2379597425460815 22100
val loss = 0.66847825050354
training loss = 1.2377582788467407 22200
val loss = 0.6744796633720398
training loss = 1.2373754978179932 22300
val loss = 0.6689037084579468
training loss = 1.2369861602783203 22400
val loss = 0.667908251285553
training loss = 1.23725426197052 22500
val loss = 0.6509321331977844
training loss = 1.2365074157714844 22600
val loss = 0.6670011878013611
training loss = 1.2903773784637451 22700
val loss = 0.5319919586181641
training loss = 1.2360470294952393 22800
val loss = 0.6682647466659546
training loss = 1.2357295751571655 22900
val loss = 0.6712356805801392
training loss = 1.2357500791549683 23000
val loss = 0.6580796241760254
training loss = 1.2353259325027466 23100
val loss = 0.6662889122962952
training loss = 1.2403254508972168 23200
val loss = 0.7313202619552612
training loss = 1.2349853515625 23300
val loss = 0.6662237048149109
training loss = 1.5859689712524414 23400
val loss = 1.5619089603424072
training loss = 1.2346683740615845 23500
val loss = 0.6687482595443726
training loss = 1.2343571186065674 23600
val loss = 0.6654472351074219
training loss = 1.234727144241333 23700
val loss = 0.6824913024902344
training loss = 1.2340757846832275 23800
val loss = 0.6657153367996216
training loss = 1.2341219186782837 23900
val loss = 0.6597625613212585
training loss = 1.2338110208511353 24000
val loss = 0.6650034189224243
training loss = 1.2335494756698608 24100
val loss = 0.669242262840271
training loss = 1.2336660623550415 24200
val loss = 0.6572444438934326
training loss = 1.233294129371643 24300
val loss = 0.6657243967056274
training loss = 1.2406620979309082 24400
val loss = 0.600838303565979
training loss = 1.2330411672592163 24500
val loss = 0.6655194759368896
training loss = 1.7491101026535034 24600
val loss = 0.6494770646095276
training loss = 1.2328637838363647 24700
val loss = 0.6671520471572876
training loss = 1.2326127290725708 24800
val loss = 0.6657036542892456
training loss = 1.2344624996185303 24900
val loss = 0.707522451877594
training loss = 1.2323808670043945 25000
val loss = 0.6652158498764038
training loss = 1.2321205139160156 25100
val loss = 0.6653506755828857
training loss = 1.2324471473693848 25200
val loss = 0.6524715423583984
training loss = 1.2318987846374512 25300
val loss = 0.6656649112701416
training loss = 1.3854156732559204 25400
val loss = 1.1655789613723755
training loss = 1.2316306829452515 25500
val loss = 0.6678512692451477
training loss = 1.2313369512557983 25600
val loss = 0.6660115718841553
training loss = 1.231849193572998 25700
val loss = 0.6841764450073242
training loss = 1.2311218976974487 25800
val loss = 0.6658523082733154
training loss = 1.2308131456375122 25900
val loss = 0.6664907336235046
training loss = 1.2313458919525146 26000
val loss = 0.6474875211715698
training loss = 1.2304761409759521 26100
val loss = 0.6662330031394958
training loss = 1.2446752786636353 26200
val loss = 0.5877853631973267
training loss = 1.2301119565963745 26300
val loss = 0.6652084589004517
training loss = 1.229743242263794 26400
val loss = 0.6658754348754883
training loss = 1.2296966314315796 26500
val loss = 0.6631731390953064
training loss = 1.229315996170044 26600
val loss = 0.6664071083068848
training loss = 1.3766921758651733 26700
val loss = 1.1507819890975952
training loss = 1.2289302349090576 26800
val loss = 0.6685893535614014
training loss = 1.228506088256836 26900
val loss = 0.666428804397583
training loss = 1.3098217248916626 27000
val loss = 0.9941258430480957
training loss = 1.2279555797576904 27100
val loss = 0.6680611371994019
training loss = 1.2275059223175049 27200
val loss = 0.6706843376159668
training loss = 1.2274506092071533 27300
val loss = 0.6577635407447815
training loss = 1.2268675565719604 27400
val loss = 0.6664737462997437
reduced chi^2 level 2 = 1.2305684089660645
Constrained alpha: 1.8674407005310059
Constrained beta: 1.4556845426559448
Constrained gamma: 12.565122604370117
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 837.7354,  858.1531,  954.3719, 1000.3992, 1028.3409, 1051.0525,
        1089.0785, 1125.6433, 1131.1877, 1215.9186, 1186.3413, 1222.4890,
        1205.6093, 1282.9906, 1332.3464, 1379.6971, 1448.0647, 1444.8390,
        1550.4254, 1516.1028, 1612.2198, 1540.9908, 1647.9508, 1566.2517,
        1583.6349, 1722.2765, 1654.8485, 1694.3521, 1746.4646, 1664.6200,
        1705.6232, 1742.3400, 1705.0317, 1678.5574, 1736.9092, 1719.1642,
        1675.5239, 1631.7587, 1681.4557, 1538.9746, 1583.1931, 1580.3110,
        1523.2373, 1553.6155, 1368.1368, 1279.8209, 1272.6525, 1206.2251,
        1126.4004, 1171.5731, 1099.2543, 1006.5232,  940.4375,  882.7736,
         851.0095,  868.2216,  831.2609,  692.0496,  630.1650,  516.6550,
         613.1533,  471.5092,  445.0260,  363.0994,  374.6515,  308.4762,
         310.1714,  275.5027,  196.3801,  171.3138,  173.2143,  159.1892,
         163.8219,  117.7021,   87.2834,   71.7629,   53.9001,   41.2612,
          35.2070,   35.0827,   19.8806,   47.1420,   41.0031])]
2589.065447643765
1.184252726525909 1.7294049971023662 70.12586706039285
val isze = 8
idinces = [17 70 61 30 24  5 43 48 38 14 21  9 80 75 20 56 26 41 36 44 77 72 37  0
 62 49 42 76 22 10 53 35 39 71 25 19 58 68  2 29 11 15 46 60 51 57 65 73
 63 33 69 55 59  6 66  1  8  3 74 13 40 81 34 47 16  4 12 82 31 64 52 79
 78 45 50 27 54 28 23 67 18  7 32]
we are doing training validation split
training loss = 31.159915924072266 100
val loss = 30.95536994934082
training loss = 16.72563934326172 200
val loss = 18.156002044677734
training loss = 9.509591102600098 300
val loss = 10.52176284790039
training loss = 6.464771270751953 400
val loss = 6.778549671173096
training loss = 5.178285121917725 500
val loss = 5.242319107055664
training loss = 4.281356334686279 600
val loss = 4.422496318817139
training loss = 3.5472617149353027 700
val loss = 3.7579240798950195
training loss = 2.980532169342041 800
val loss = 3.1917362213134766
training loss = 2.606490135192871 900
val loss = 2.7545316219329834
training loss = 2.409865140914917 1000
val loss = 2.4720265865325928
training loss = 2.3091084957122803 1100
val loss = 2.3182315826416016
training loss = 2.2450480461120605 1200
val loss = 2.2593114376068115
training loss = 2.1855673789978027 1300
val loss = 2.1703782081604004
training loss = 2.1419289112091064 1400
val loss = 2.1533000469207764
training loss = 2.0979061126708984 1500
val loss = 2.0883066654205322
training loss = 2.0628440380096436 1600
val loss = 2.049938917160034
training loss = 2.0320451259613037 1700
val loss = 2.0321261882781982
training loss = 2.0048885345458984 1800
val loss = 2.0102782249450684
training loss = 1.9816867113113403 1900
val loss = 2.0021138191223145
training loss = 1.9605343341827393 2000
val loss = 1.9779174327850342
training loss = 1.9418892860412598 2100
val loss = 1.9656391143798828
training loss = 1.9258683919906616 2200
val loss = 1.9657577276229858
training loss = 1.9108004570007324 2300
val loss = 1.9481780529022217
training loss = 1.905023217201233 2400
val loss = 1.9145481586456299
training loss = 1.8860785961151123 2500
val loss = 1.9379863739013672
training loss = 1.8767578601837158 2600
val loss = 1.9213178157806396
training loss = 1.8659480810165405 2700
val loss = 1.9315571784973145
training loss = 1.8571118116378784 2800
val loss = 1.9324088096618652
training loss = 1.8489941358566284 2900
val loss = 1.9313089847564697
training loss = 1.8413196802139282 3000
val loss = 1.9337735176086426
training loss = 2.0065507888793945 3100
val loss = 2.3120217323303223
training loss = 1.8273383378982544 3200
val loss = 1.9375545978546143
training loss = 1.8208589553833008 3300
val loss = 1.940258264541626
training loss = 1.8156521320343018 3400
val loss = 1.9569172859191895
training loss = 1.8089693784713745 3500
val loss = 1.9464044570922852
training loss = 1.8399769067764282 3600
val loss = 1.9117169380187988
training loss = 1.7984062433242798 3700
val loss = 1.9520595073699951
training loss = 1.793533205986023 3800
val loss = 1.9561065435409546
training loss = 1.790569543838501 3900
val loss = 1.9774432182312012
training loss = 1.7848303318023682 4000
val loss = 1.9626331329345703
training loss = 1.835477352142334 4100
val loss = 2.127586841583252
training loss = 1.7773572206497192 4200
val loss = 1.967512845993042
training loss = 1.7739763259887695 4300
val loss = 1.9719150066375732
training loss = 1.7713593244552612 4400
val loss = 1.9677495956420898
training loss = 1.7681238651275635 4500
val loss = 1.9781261682510376
training loss = 1.7656753063201904 4600
val loss = 1.9769847393035889
training loss = 1.7631897926330566 4700
val loss = 1.9833265542984009
training loss = 1.8864498138427734 4800
val loss = 1.9837720394134521
training loss = 1.7590035200119019 4900
val loss = 1.9901036024093628
training loss = 1.757073163986206 5000
val loss = 1.9904687404632568
training loss = 1.7556111812591553 5100
val loss = 1.9990572929382324
training loss = 1.7537555694580078 5200
val loss = 1.9949177503585815
training loss = 1.754218339920044 5300
val loss = 1.9801368713378906
training loss = 1.7508779764175415 5400
val loss = 1.9984345436096191
training loss = 1.7756576538085938 5500
val loss = 1.9608651399612427
training loss = 1.7483736276626587 5600
val loss = 2.001511573791504
training loss = 1.7842167615890503 5700
val loss = 2.1250836849212646
training loss = 1.7461963891983032 5800
val loss = 2.0047106742858887
training loss = 1.745124340057373 5900
val loss = 2.0048418045043945
training loss = 1.7442681789398193 6000
val loss = 2.004399299621582
training loss = 1.7432975769042969 6100
val loss = 2.006610870361328
training loss = 1.742378830909729 6200
val loss = 2.0064053535461426
training loss = 1.7417713403701782 6300
val loss = 2.012158155441284
training loss = 1.7407827377319336 6400
val loss = 2.008270025253296
training loss = 1.790108323097229 6500
val loss = 1.9733730554580688
training loss = 1.7393136024475098 6600
val loss = 2.008223295211792
training loss = 1.738525390625 6700
val loss = 2.0101068019866943
training loss = 1.737943172454834 6800
val loss = 2.01101016998291
training loss = 1.73715341091156 6900
val loss = 2.009204626083374
training loss = 1.7367757558822632 7000
val loss = 2.0152065753936768
training loss = 1.7358317375183105 7100
val loss = 2.0087125301361084
training loss = 1.738468050956726 7200
val loss = 2.032238721847534
training loss = 1.734649658203125 7300
val loss = 2.0079398155212402
training loss = 1.7339422702789307 7400
val loss = 2.008040428161621
training loss = 1.8007173538208008 7500
val loss = 2.182213068008423
training loss = 1.7327477931976318 7600
val loss = 2.007819652557373
training loss = 1.7320483922958374 7700
val loss = 2.0069048404693604
training loss = 1.7316418886184692 7800
val loss = 2.002668857574463
reduced chi^2 level 2 = 1.7311832904815674
Constrained alpha: 1.8362172842025757
Constrained beta: 2.9177820682525635
Constrained gamma: 22.61944007873535
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 897.0748,  880.3024,  936.5903,  897.8563,  978.1466, 1090.3500,
        1095.2750, 1109.9962, 1163.8350, 1177.6160, 1179.5565, 1165.3893,
        1223.7299, 1202.0686, 1290.0734, 1383.3998, 1329.6833, 1477.9657,
        1564.7548, 1545.4238, 1607.6422, 1582.0150, 1591.2938, 1577.9247,
        1687.5972, 1738.3477, 1674.8080, 1737.5280, 1770.8442, 1722.9557,
        1725.1720, 1740.6953, 1732.1470, 1688.3624, 1770.0942, 1708.0737,
        1640.0535, 1609.4054, 1568.8057, 1605.5975, 1594.6282, 1627.4208,
        1488.2852, 1568.3387, 1354.9833, 1261.7482, 1301.8363, 1229.6599,
        1156.9607, 1146.8959, 1115.1002, 1021.6853,  977.1816,  962.8309,
         869.0740,  879.2248,  811.4525,  680.2325,  670.4077,  537.5915,
         534.1323,  458.9045,  466.6222,  378.1085,  337.5863,  339.1556,
         283.5819,  234.8905,  195.3255,  186.0921,  180.6356,  152.7680,
         139.6312,  104.5909,  108.8415,   55.1095,   43.6447,   53.8459,
          35.2689,   42.5351,   22.1468,   44.8200,   28.3915])]
2613.712864659722
3.7158864047190128 15.227271547337226 51.106587495365154
val isze = 8
idinces = [65 43 23  3  6 82 61 68 52 58 42 48 47 28  1 54 29  8 64 24 45 50 15 14
 53  4 51 25 79  9 44 35 39 70 62 78 34  5 11 22 63 33 30 60 17  0 32 37
 12 49 72 57 27 81 80 10 18 71 67 31 74 19 13 46 66 77 21 38 40 73 26 16
  7 41 75 36 20 59  2 69 76 55 56]
we are doing training validation split
training loss = 400.5842590332031 100
val loss = 368.0728759765625
training loss = 8.06502914428711 200
val loss = 8.271547317504883
training loss = 7.414202690124512 300
val loss = 6.043410301208496
training loss = 7.314518451690674 400
val loss = 6.122166633605957
training loss = 7.2174072265625 500
val loss = 6.237727165222168
training loss = 7.127352714538574 600
val loss = 6.37395715713501
training loss = 7.046750068664551 700
val loss = 6.5238356590271
training loss = 6.97620153427124 800
val loss = 6.678852081298828
training loss = 6.915003299713135 900
val loss = 6.830223083496094
training loss = 6.8616719245910645 1000
val loss = 6.970400333404541
training loss = 6.814480781555176 1100
val loss = 7.093804836273193
training loss = 6.7717766761779785 1200
val loss = 7.197574615478516
training loss = 6.732173442840576 1300
val loss = 7.281004905700684
training loss = 6.694579124450684 1400
val loss = 7.345304489135742
training loss = 6.658076286315918 1500
val loss = 7.392579078674316
training loss = 6.6217546463012695 1600
val loss = 7.4254889488220215
training loss = 6.584399223327637 1700
val loss = 7.446447372436523
training loss = 6.543925762176514 1800
val loss = 7.457254409790039
training loss = 6.4961018562316895 1900
val loss = 7.458604335784912
training loss = 6.4310102462768555 2000
val loss = 7.448017120361328
training loss = 6.32180118560791 2100
val loss = 7.413651466369629
training loss = 6.087301731109619 2200
val loss = 7.309067726135254
training loss = 5.502404689788818 2300
val loss = 6.989188194274902
training loss = 4.263841152191162 2400
val loss = 6.100373268127441
training loss = 2.838221549987793 2500
val loss = 4.5336480140686035
training loss = 2.240487813949585 2600
val loss = 3.324422836303711
training loss = 2.14756178855896 2700
val loss = 2.930169105529785
training loss = 2.128725528717041 2800
val loss = 2.8407368659973145
training loss = 2.117424964904785 2900
val loss = 2.807274103164673
training loss = 2.1081621646881104 3000
val loss = 2.793433666229248
training loss = 2.1003165245056152 3100
val loss = 2.781327486038208
training loss = 2.093566656112671 3200
val loss = 2.773973226547241
training loss = 2.087399482727051 3300
val loss = 2.766951322555542
training loss = 2.081826686859131 3400
val loss = 2.76601243019104
training loss = 2.076747179031372 3500
val loss = 2.760117769241333
training loss = 2.072166681289673 3600
val loss = 2.760518789291382
training loss = 2.070915699005127 3700
val loss = 2.743826389312744
training loss = 2.0637309551239014 3800
val loss = 2.758162021636963
training loss = 2.059953212738037 3900
val loss = 2.7583324909210205
training loss = 2.056464195251465 4000
val loss = 2.754196882247925
training loss = 2.0530481338500977 4100
val loss = 2.757277727127075
training loss = 2.050173044204712 4200
val loss = 2.7506086826324463
training loss = 2.0469448566436768 4300
val loss = 2.756030321121216
training loss = 2.0451009273529053 4400
val loss = 2.744544506072998
training loss = 2.041449546813965 4500
val loss = 2.754833221435547
training loss = 2.0399017333984375 4600
val loss = 2.7655012607574463
training loss = 2.036389112472534 4700
val loss = 2.753112316131592
training loss = 2.0340328216552734 4800
val loss = 2.753655433654785
training loss = 2.032749891281128 4900
val loss = 2.7406792640686035
training loss = 2.029388904571533 5000
val loss = 2.7520642280578613
training loss = 2.027176856994629 5100
val loss = 2.7526447772979736
training loss = 2.025176525115967 5200
val loss = 2.7453854084014893
training loss = 2.0226261615753174 5300
val loss = 2.751842975616455
training loss = 2.121147394180298 5400
val loss = 2.9584944248199463
training loss = 2.017714500427246 5500
val loss = 2.751399040222168
training loss = 2.0149495601654053 5600
val loss = 2.7533252239227295
training loss = 2.0125956535339355 5700
val loss = 2.7435946464538574
training loss = 2.0084493160247803 5800
val loss = 2.7553865909576416
training loss = 2.034168243408203 5900
val loss = 2.702237367630005
training loss = 2.0009429454803467 6000
val loss = 2.755871295928955
training loss = 1.9966793060302734 6100
val loss = 2.7564802169799805
training loss = 1.9919049739837646 6200
val loss = 2.754181385040283
training loss = 1.9863965511322021 6300
val loss = 2.7571911811828613
training loss = 1.9858647584915161 6400
val loss = 2.7967958450317383
training loss = 1.972663402557373 6500
val loss = 2.7600021362304688
training loss = 1.9637693166732788 6600
val loss = 2.7659714221954346
training loss = 1.9537004232406616 6700
val loss = 2.7668614387512207
training loss = 1.9417893886566162 6800
val loss = 2.7726385593414307
training loss = 1.985593318939209 6900
val loss = 2.7225217819213867
training loss = 1.9126441478729248 7000
val loss = 2.7901296615600586
training loss = 1.895261287689209 7100
val loss = 2.8014187812805176
training loss = 1.8781346082687378 7200
val loss = 2.8372371196746826
training loss = 1.8560770750045776 7300
val loss = 2.834871530532837
training loss = 1.83938729763031 7400
val loss = 2.889810562133789
training loss = 1.8135124444961548 7500
val loss = 2.8826212882995605
training loss = 1.7916465997695923 7600
val loss = 2.9148340225219727
training loss = 1.7716683149337769 7700
val loss = 2.9401345252990723
training loss = 1.751149296760559 7800
val loss = 2.985644817352295
training loss = 1.7457540035247803 7900
val loss = 2.9987401962280273
training loss = 1.7148802280426025 8000
val loss = 3.0694897174835205
training loss = 1.850514531135559 8100
val loss = 3.3498597145080566
training loss = 1.684848427772522 8200
val loss = 3.158848524093628
training loss = 1.6717860698699951 8300
val loss = 3.207350015640259
training loss = 1.6615540981292725 8400
val loss = 3.2544302940368652
training loss = 1.6516042947769165 8500
val loss = 3.293764114379883
training loss = 1.9363347291946411 8600
val loss = 3.7290525436401367
training loss = 1.6364686489105225 8700
val loss = 3.370774745941162
training loss = 1.6303108930587769 8800
val loss = 3.410130262374878
training loss = 1.627866506576538 8900
val loss = 3.4276537895202637
training loss = 1.6209465265274048 9000
val loss = 3.4712252616882324
training loss = 1.6170997619628906 9100
val loss = 3.4997735023498535
training loss = 1.614295482635498 9200
val loss = 3.5187435150146484
training loss = 1.6115187406539917 9300
val loss = 3.5398874282836914
training loss = 1.6090874671936035 9400
val loss = 3.559887647628784
training loss = 1.6073472499847412 9500
val loss = 3.570953369140625
training loss = 1.605476975440979 9600
val loss = 3.584733009338379
training loss = 1.60384202003479 9700
val loss = 3.597526788711548
training loss = 1.6025842428207397 9800
val loss = 3.602022409439087
training loss = 1.6012376546859741 9900
val loss = 3.6147212982177734
training loss = 1.6040822267532349 10000
val loss = 3.600156307220459
training loss = 1.599109172821045 10100
val loss = 3.6261134147644043
training loss = 1.7424051761627197 10200
val loss = 3.8635730743408203
training loss = 1.5973128080368042 10300
val loss = 3.6350555419921875
training loss = 1.5964077711105347 10400
val loss = 3.6407501697540283
training loss = 1.5966546535491943 10500
val loss = 3.648937702178955
training loss = 1.5949288606643677 10600
val loss = 3.6457102298736572
reduced chi^2 level 2 = 1.594435691833496
Constrained alpha: 1.7311084270477295
Constrained beta: 3.4270124435424805
Constrained gamma: 18.744022369384766
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 860.6349,  814.3674,  941.5980,  986.4144,  973.9904, 1141.0229,
        1174.8844, 1106.2261, 1188.9073, 1150.2023, 1227.5315, 1245.0490,
        1202.9791, 1255.4875, 1285.7521, 1387.3181, 1354.3096, 1510.6758,
        1583.5558, 1475.9326, 1599.2151, 1552.7646, 1647.0240, 1649.2511,
        1631.8796, 1698.8115, 1636.3079, 1657.6447, 1724.7152, 1647.2192,
        1730.7787, 1784.2449, 1750.9541, 1737.7515, 1661.0092, 1717.4728,
        1672.4978, 1636.3667, 1662.8395, 1576.4821, 1621.9407, 1561.9629,
        1504.4539, 1551.9639, 1338.8511, 1399.1664, 1260.0725, 1272.9371,
        1147.5551, 1106.6357, 1063.4091,  976.6099,  974.3395,  942.0983,
         865.9343,  903.0128,  845.2767,  682.6104,  607.0424,  521.6542,
         510.0339,  499.9817,  405.1219,  391.0319,  366.7670,  357.9077,
         265.9268,  244.7579,  225.2077,  187.8720,  165.5151,  126.8064,
         136.4583,   88.7106,   93.0797,   65.5012,   42.9819,   27.9672,
          34.3995,   45.5430,   21.4520,   41.6990,   41.7842])]
2758.3343074163226
2.7987672070305782 14.454875012010051 64.50263161903933
val isze = 8
idinces = [56 33  4 25 43 26 38  0 21 61  1 27 66 19 79 16 77 41 11 36  9 12 13 28
 81 35 73 54 15 50 48 52 46 31  6 62 10 67 17 57 42 60  3 58 34 24 20  8
 65 18 69 74 72 63 70 71 30 32 80 29 14 51 53 45 55 78 64 44 49 23  5 22
 76 47 82 75  2 68  7 40 59 39 37]
we are doing training validation split
training loss = 246.5811004638672 100
val loss = 386.6712951660156
training loss = 7.160403728485107 200
val loss = 6.790996074676514
training loss = 6.875184059143066 300
val loss = 5.393876075744629
training loss = 6.825633525848389 400
val loss = 5.591200828552246
training loss = 6.7765021324157715 500
val loss = 5.81726598739624
training loss = 6.729827404022217 600
val loss = 6.050671577453613
training loss = 6.686717987060547 700
val loss = 6.28237247467041
training loss = 6.6475043296813965 800
val loss = 6.50409460067749
training loss = 6.611916542053223 900
val loss = 6.70896053314209
training loss = 6.579320430755615 1000
val loss = 6.891687870025635
training loss = 6.548918724060059 1100
val loss = 7.048815727233887
training loss = 6.519901275634766 1200
val loss = 7.1790313720703125
training loss = 6.491581439971924 1300
val loss = 7.28264856338501
training loss = 6.4634318351745605 1400
val loss = 7.361353874206543
training loss = 6.435072898864746 1500
val loss = 7.417954444885254
training loss = 6.406280040740967 1600
val loss = 7.455776214599609
training loss = 6.376936912536621 1700
val loss = 7.4781293869018555
training loss = 6.346982002258301 1800
val loss = 7.488236427307129
training loss = 6.31641960144043 1900
val loss = 7.488915920257568
training loss = 6.2852559089660645 2000
val loss = 7.482553482055664
training loss = 6.253513813018799 2100
val loss = 7.471148490905762
training loss = 6.2212300300598145 2200
val loss = 7.456170082092285
training loss = 6.188420295715332 2300
val loss = 7.438682556152344
training loss = 6.155097484588623 2400
val loss = 7.419637680053711
training loss = 6.121252536773682 2500
val loss = 7.399579048156738
training loss = 6.086816310882568 2600
val loss = 7.378887176513672
training loss = 6.051592826843262 2700
val loss = 7.3578619956970215
training loss = 6.0150532722473145 2800
val loss = 7.336216449737549
training loss = 5.975771427154541 2900
val loss = 7.311966896057129
training loss = 5.929360866546631 3000
val loss = 7.277268886566162
training loss = 5.8602094650268555 3100
val loss = 7.217405796051025
training loss = 5.657455921173096 3200
val loss = 7.161093711853027
training loss = 4.174602031707764 3300
val loss = 6.529453277587891
training loss = 2.593238592147827 3400
val loss = 4.648200035095215
training loss = 2.384486675262451 3500
val loss = 4.360925674438477
training loss = 2.3734195232391357 3600
val loss = 4.3626837730407715
training loss = 2.367975950241089 3700
val loss = 4.344890594482422
training loss = 2.3624188899993896 3800
val loss = 4.3731513023376465
training loss = 2.356635332107544 3900
val loss = 4.369913578033447
training loss = 2.3505892753601074 4000
val loss = 4.384535789489746
training loss = 2.3438735008239746 4100
val loss = 4.373861789703369
training loss = 2.337057590484619 4200
val loss = 4.343733310699463
training loss = 2.32889461517334 4300
val loss = 4.375791072845459
training loss = 2.3203272819519043 4400
val loss = 4.383151531219482
training loss = 2.3110506534576416 4500
val loss = 4.394754409790039
training loss = 2.3005831241607666 4600
val loss = 4.383854866027832
training loss = 2.2895519733428955 4700
val loss = 4.353636741638184
training loss = 2.276317834854126 4800
val loss = 4.386289596557617
training loss = 2.2664904594421387 4900
val loss = 4.287271499633789
training loss = 2.246020555496216 5000
val loss = 4.384590148925781
training loss = 2.236642360687256 5100
val loss = 4.53392219543457
training loss = 2.2076079845428467 5200
val loss = 4.373708248138428
training loss = 2.2139687538146973 5300
val loss = 4.142265319824219
training loss = 2.159820318222046 5400
val loss = 4.34788179397583
training loss = 2.131063222885132 5500
val loss = 4.320411682128906
training loss = 2.102449893951416 5600
val loss = 4.2472357749938965
training loss = 2.071094512939453 5700
val loss = 4.234053611755371
training loss = 2.059396505355835 5800
val loss = 3.9999256134033203
training loss = 2.019277334213257 5900
val loss = 4.100314617156982
training loss = 2.074906826019287 6000
val loss = 4.524478912353516
training loss = 1.9897440671920776 6100
val loss = 3.985020160675049
training loss = 1.9803975820541382 6200
val loss = 3.947845458984375
training loss = 1.9747567176818848 6300
val loss = 3.9625678062438965
training loss = 1.9689128398895264 6400
val loss = 3.9074654579162598
training loss = 1.9688374996185303 6500
val loss = 3.8239665031433105
training loss = 1.9622821807861328 6600
val loss = 3.8899693489074707
training loss = 1.9598824977874756 6700
val loss = 3.893022060394287
training loss = 1.9585354328155518 6800
val loss = 3.883352279663086
training loss = 1.9571828842163086 6900
val loss = 3.8855888843536377
training loss = 1.9702190160751343 7000
val loss = 4.085373878479004
training loss = 1.955920696258545 7100
val loss = 3.890909194946289
training loss = 1.9558809995651245 7200
val loss = 3.8637185096740723
training loss = 1.9555392265319824 7300
val loss = 3.90053653717041
training loss = 1.9553955793380737 7400
val loss = 3.9030678272247314
training loss = 1.9706429243087769 7500
val loss = 4.114375114440918
training loss = 1.955739140510559 7600
val loss = 3.9150431156158447
training loss = 1.9558850526809692 7700
val loss = 3.919858932495117
training loss = 1.9568990468978882 7800
val loss = 3.9610729217529297
training loss = 1.9565762281417847 7900
val loss = 3.931108236312866
training loss = 1.9568417072296143 8000
val loss = 3.9381542205810547
training loss = 1.9574068784713745 8100
val loss = 3.9544434547424316
training loss = 1.957626223564148 8200
val loss = 3.948089122772217
training loss = 1.981982707977295 8300
val loss = 4.221412658691406
training loss = 1.9584546089172363 8400
val loss = 3.959451198577881
training loss = 1.9587773084640503 8500
val loss = 3.964465618133545
training loss = 1.959431767463684 8600
val loss = 3.964902400970459
training loss = 1.9596387147903442 8700
val loss = 3.9755194187164307
training loss = 1.9599262475967407 8800
val loss = 3.9792227745056152
training loss = 1.96101975440979 8900
val loss = 3.95821475982666
training loss = 1.9607093334197998 9000
val loss = 3.9897284507751465
training loss = 2.462730884552002 9100
val loss = 3.587564468383789
training loss = 1.9614827632904053 9200
val loss = 3.993635416030884
training loss = 1.961747646331787 9300
val loss = 4.003283500671387
training loss = 1.962714433670044 9400
val loss = 3.9742062091827393
training loss = 1.9624463319778442 9500
val loss = 4.010273456573486
training loss = 1.9750785827636719 9600
val loss = 4.201277732849121
training loss = 1.9631372690200806 9700
val loss = 4.015979290008545
training loss = 1.9633630514144897 9800
val loss = 4.021002292633057
training loss = 1.9657686948776245 9900
val loss = 3.9627251625061035
training loss = 1.963972568511963 10000
val loss = 4.028885364532471
training loss = 2.0510053634643555 10100
val loss = 4.597987174987793
training loss = 1.9645627737045288 10200
val loss = 4.036349296569824
training loss = 1.9647619724273682 10300
val loss = 4.037670612335205
training loss = 1.965895652770996 10400
val loss = 4.000186920166016
training loss = 1.9652938842773438 10500
val loss = 4.042945861816406
training loss = 2.098747968673706 10600
val loss = 3.6544151306152344
training loss = 1.965797781944275 10700
val loss = 4.050999164581299
training loss = 1.9663411378860474 10800
val loss = 4.080059051513672
training loss = 1.9663467407226562 10900
val loss = 4.040661811828613
training loss = 1.9664136171340942 11000
val loss = 4.05467414855957
training loss = 1.9677140712738037 11100
val loss = 4.109149932861328
training loss = 1.9668428897857666 11200
val loss = 4.059134006500244
training loss = 2.08123517036438 11300
val loss = 3.696131944656372
training loss = 1.9672635793685913 11400
val loss = 4.061061859130859
training loss = 1.967381238937378 11500
val loss = 4.064022064208984
training loss = 1.9677636623382568 11600
val loss = 4.084323883056641
training loss = 1.9677515029907227 11700
val loss = 4.06915283203125
reduced chi^2 level 2 = 1.967781662940979
Constrained alpha: 2.0052378177642822
Constrained beta: 2.1998040676116943
Constrained gamma: 20.173049926757812
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 851.0750,  938.5742,  977.6780,  894.8723,  968.1618, 1089.9857,
        1060.4633, 1141.2025, 1157.5928, 1141.6583, 1217.6414, 1167.1500,
        1252.8195, 1280.6251, 1321.1519, 1479.6904, 1445.4769, 1452.9247,
        1617.2075, 1472.0486, 1515.3395, 1617.0765, 1599.3958, 1610.8550,
        1712.5675, 1736.4050, 1622.4886, 1698.4635, 1728.4795, 1709.0017,
        1718.6891, 1740.6476, 1759.4540, 1723.6807, 1660.8452, 1776.7203,
        1674.3474, 1683.7716, 1677.9315, 1665.3260, 1618.3875, 1577.8086,
        1479.0944, 1488.1127, 1328.8811, 1377.6205, 1239.1742, 1203.1403,
        1192.0397, 1170.4752, 1025.3878,  965.8214,  941.1612,  955.9540,
         878.6988,  806.2293,  795.6356,  718.8232,  565.5240,  537.1747,
         555.7241,  464.5050,  450.0963,  382.6163,  365.4958,  337.2520,
         282.4680,  264.5662,  224.2962,  200.5275,  165.0251,  142.5754,
         131.8938,  117.1787,  109.1958,   42.9247,   56.1432,   36.1863,
          44.7937,   47.1794,   22.9614,   37.1280,   27.0269])]
2547.6533385500115
0.09231276689568191 15.95048778315155 22.57352877000671
val isze = 8
idinces = [43  5 71 72 24 38  2 76 44 65 66 46 18  3 41 34 31 42 29 36 19 68 62 16
 21 51 13  9 67 33  1 47 70 55 74  4 54 59 14 22 23 17 12 45 26 52 61  7
 49 56 11 57 80 77 60 78 10  6 82 48 30  8 58 28 64 32 15 73 81  0 35 40
 37 79 75 50 25 69 27 53 20 39 63]
we are doing training validation split
training loss = 115.1710205078125 100
val loss = 94.03607177734375
training loss = 10.839753150939941 200
val loss = 18.780738830566406
training loss = 9.87224006652832 300
val loss = 16.637935638427734
training loss = 9.6076078414917 400
val loss = 16.030153274536133
training loss = 9.320244789123535 500
val loss = 15.34243392944336
training loss = 9.020984649658203 600
val loss = 14.599996566772461
training loss = 8.720209121704102 700
val loss = 13.82180404663086
training loss = 8.427777290344238 800
val loss = 13.02802848815918
training loss = 8.15270709991455 900
val loss = 12.239398002624512
training loss = 7.902553081512451 1000
val loss = 11.476581573486328
training loss = 7.682811260223389 1100
val loss = 10.75847053527832
training loss = 7.496404647827148 1200
val loss = 10.100828170776367
training loss = 7.343473434448242 1300
val loss = 9.514881134033203
training loss = 7.221564292907715 1400
val loss = 9.006147384643555
training loss = 7.126250267028809 1500
val loss = 8.574667930603027
training loss = 7.052016735076904 1600
val loss = 8.215472221374512
training loss = 6.993232727050781 1700
val loss = 7.920675277709961
training loss = 6.944873332977295 1800
val loss = 7.6803083419799805
training loss = 6.902993202209473 1900
val loss = 7.4843268394470215
training loss = 6.864814758300781 2000
val loss = 7.3234734535217285
training loss = 6.828606128692627 2100
val loss = 7.189723491668701
training loss = 6.793440341949463 2200
val loss = 7.076587200164795
training loss = 6.758924961090088 2300
val loss = 6.978870391845703
training loss = 6.724992275238037 2400
val loss = 6.89268684387207
training loss = 6.691727638244629 2500
val loss = 6.815126895904541
training loss = 6.659235000610352 2600
val loss = 6.7439446449279785
training loss = 6.627581596374512 2700
val loss = 6.677584171295166
training loss = 6.596694469451904 2800
val loss = 6.614499092102051
training loss = 6.5662922859191895 2900
val loss = 6.553349018096924
training loss = 6.5357465744018555 3000
val loss = 6.492349147796631
training loss = 6.503705024719238 3100
val loss = 6.42871618270874
training loss = 6.4672017097473145 3200
val loss = 6.3563103675842285
training loss = 6.418723106384277 3300
val loss = 6.259549140930176
training loss = 6.336835861206055 3400
val loss = 6.091170310974121
training loss = 6.169824123382568 3500
val loss = 5.735520362854004
training loss = 5.871640205383301 3600
val loss = 5.142874717712402
training loss = 5.377079010009766 3700
val loss = 4.347801208496094
training loss = 4.3467888832092285 3800
val loss = 2.9085114002227783
training loss = 2.902911424636841 3900
val loss = 1.3991572856903076
training loss = 2.3551979064941406 4000
val loss = 1.0066883563995361
training loss = 2.238355875015259 4100
val loss = 0.9636570811271667
training loss = 2.1631815433502197 4200
val loss = 0.9454799890518188
training loss = 2.1075291633605957 4300
val loss = 0.9364699125289917
training loss = 2.065366268157959 4400
val loss = 0.9330228567123413
training loss = 2.0328047275543213 4500
val loss = 0.9325129389762878
training loss = 2.007207155227661 4600
val loss = 0.9334628582000732
training loss = 1.9867513179779053 4700
val loss = 0.9351094961166382
training loss = 1.9701522588729858 4800
val loss = 0.9367934465408325
training loss = 1.9565006494522095 4900
val loss = 0.9384338855743408
training loss = 1.94512939453125 5000
val loss = 0.939765453338623
training loss = 1.9365421533584595 5100
val loss = 0.907292366027832
training loss = 1.928541660308838 5200
val loss = 0.9423496723175049
training loss = 1.9312653541564941 5300
val loss = 1.079580545425415
training loss = 1.9171717166900635 5400
val loss = 0.9434146881103516
training loss = 1.9125466346740723 5500
val loss = 0.9451714754104614
training loss = 1.9085876941680908 5600
val loss = 0.9462445378303528
training loss = 1.9049972295761108 5700
val loss = 0.9426694512367249
training loss = 1.902693271636963 5800
val loss = 0.9012783765792847
training loss = 1.8988350629806519 5900
val loss = 0.94186931848526
training loss = 1.89608895778656 6000
val loss = 0.9485530853271484
training loss = 1.893572449684143 6100
val loss = 0.9418659210205078
training loss = 1.8926128149032593 6200
val loss = 0.9948241710662842
training loss = 1.8889062404632568 6300
val loss = 0.9477393627166748
training loss = 1.8866504430770874 6400
val loss = 0.9385675191879272
training loss = 1.9273133277893066 6500
val loss = 1.2626397609710693
training loss = 1.8822746276855469 6600
val loss = 0.9370129108428955
training loss = 1.8800065517425537 6700
val loss = 0.9339879155158997
training loss = 1.8778657913208008 6800
val loss = 0.9518212676048279
training loss = 1.875234603881836 6900
val loss = 0.9326320886611938
training loss = 1.8809123039245605 7000
val loss = 1.0665638446807861
training loss = 1.869565486907959 7100
val loss = 0.9277485013008118
training loss = 1.866224765777588 7200
val loss = 0.9416284561157227
training loss = 1.8624142408370972 7300
val loss = 0.9357686638832092
training loss = 1.8580868244171143 7400
val loss = 0.9225766658782959
training loss = 1.8675122261047363 7500
val loss = 0.763147234916687
training loss = 1.8486888408660889 7600
val loss = 0.9188182950019836
training loss = 1.8438878059387207 7700
val loss = 0.921960711479187
training loss = 1.8394323587417603 7800
val loss = 0.9284445643424988
training loss = 1.8351263999938965 7900
val loss = 0.9204405546188354
training loss = 1.834937334060669 8000
val loss = 1.0156508684158325
training loss = 1.82715904712677 8100
val loss = 0.9219905138015747
training loss = 1.8233213424682617 8200
val loss = 0.9277523756027222
training loss = 1.8197882175445557 8300
val loss = 0.9090067148208618
training loss = 1.8160901069641113 8400
val loss = 0.9265733957290649
training loss = 1.8246930837631226 8500
val loss = 1.1018524169921875
training loss = 1.8091844320297241 8600
val loss = 0.9298432469367981
training loss = 1.8057947158813477 8700
val loss = 0.9330090284347534
training loss = 1.8025964498519897 8800
val loss = 0.9377281069755554
training loss = 1.7994153499603271 8900
val loss = 0.9324660301208496
training loss = 1.7962864637374878 9000
val loss = 0.9308552742004395
training loss = 1.793386697769165 9100
val loss = 0.9343273639678955
training loss = 1.7904481887817383 9200
val loss = 0.9358234405517578
training loss = 1.787734031677246 9300
val loss = 0.9406548738479614
training loss = 1.7851006984710693 9400
val loss = 0.936776340007782
training loss = 2.0098652839660645 9500
val loss = 1.855025053024292
training loss = 1.7801225185394287 9600
val loss = 0.9439166188240051
training loss = 1.7777553796768188 9700
val loss = 0.9390066862106323
training loss = 1.7784477472305298 9800
val loss = 0.8606958985328674
training loss = 1.7735604047775269 9900
val loss = 0.9407459497451782
training loss = 1.7715537548065186 10000
val loss = 0.9402161836624146
training loss = 1.7697724103927612 10100
val loss = 0.9517968893051147
training loss = 1.767982006072998 10200
val loss = 0.9420650601387024
training loss = 1.7938389778137207 10300
val loss = 0.7150578498840332
training loss = 1.7648468017578125 10400
val loss = 0.9436198472976685
training loss = 1.763383388519287 10500
val loss = 0.9429017305374146
training loss = 1.7626546621322632 10600
val loss = 0.9803194403648376
training loss = 1.7608392238616943 10700
val loss = 0.9441432356834412
training loss = 1.992618441581726 10800
val loss = 1.8882498741149902
training loss = 1.7586030960083008 10900
val loss = 0.9470556378364563
training loss = 1.7575746774673462 11000
val loss = 0.9451456665992737
training loss = 1.7609432935714722 11100
val loss = 0.8485652208328247
training loss = 1.7557573318481445 11200
val loss = 0.9454187154769897
training loss = 1.822782039642334 11300
val loss = 0.6085726022720337
training loss = 1.7541801929473877 11400
val loss = 0.939863920211792
training loss = 1.7534302473068237 11500
val loss = 0.9465347528457642
training loss = 1.7527891397476196 11600
val loss = 0.9546788930892944
training loss = 1.752129316329956 11700
val loss = 0.9477577209472656
training loss = 1.784020185470581 11800
val loss = 1.2507059574127197
training loss = 1.750983715057373 11900
val loss = 0.9516606330871582
training loss = 1.7504346370697021 12000
val loss = 0.9478565454483032
training loss = 1.7654614448547363 12100
val loss = 1.1521552801132202
training loss = 1.7494735717773438 12200
val loss = 0.9472717642784119
training loss = 1.7490051984786987 12300
val loss = 0.9493224620819092
training loss = 1.7494269609451294 12400
val loss = 0.9933332204818726
training loss = 1.7481791973114014 12500
val loss = 0.9492130875587463
training loss = 1.861156702041626 12600
val loss = 0.5384896993637085
training loss = 1.7474290132522583 12700
val loss = 0.948586106300354
training loss = 1.7470595836639404 12800
val loss = 0.9505248069763184
training loss = 1.7523880004882812 12900
val loss = 0.8410218954086304
training loss = 1.746413230895996 13000
val loss = 0.9520590305328369
training loss = 1.7460827827453613 13100
val loss = 0.9511067867279053
training loss = 1.74591863155365 13200
val loss = 0.9350593686103821
training loss = 1.745501160621643 13300
val loss = 0.9528573751449585
training loss = 1.7479238510131836 13400
val loss = 1.0330228805541992
training loss = 1.7449593544006348 13500
val loss = 0.9516498446464539
training loss = 1.745707392692566 13600
val loss = 1.002602219581604
training loss = 1.7444857358932495 13700
val loss = 0.9463047981262207
training loss = 1.74421226978302 13800
val loss = 0.9544826745986938
training loss = 1.7439957857131958 13900
val loss = 0.9520609378814697
training loss = 1.7437642812728882 14000
val loss = 0.9538099765777588
training loss = 1.7547701597213745 14100
val loss = 1.1252944469451904
training loss = 1.74335777759552 14200
val loss = 0.9584702253341675
training loss = 1.7431401014328003 14300
val loss = 0.9562342762947083
training loss = 1.742972493171692 14400
val loss = 0.954281210899353
training loss = 1.7427877187728882 14500
val loss = 0.9577997922897339
training loss = 1.7425916194915771 14600
val loss = 0.9574247598648071
training loss = 1.7484939098358154 14700
val loss = 0.8451472520828247
reduced chi^2 level 2 = 1.7423722743988037
Constrained alpha: 1.7678879499435425
Constrained beta: 2.584825038909912
Constrained gamma: 14.822031021118164
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 901.0998,  874.1957,  927.3866,  956.3697,  996.3730, 1089.4259,
        1066.1816, 1103.7615, 1159.9713, 1140.7770, 1226.9330, 1172.8240,
        1240.0027, 1273.6327, 1400.9108, 1409.0186, 1394.9562, 1503.2948,
        1576.3531, 1464.4268, 1638.7852, 1570.8596, 1617.3695, 1598.0176,
        1639.6124, 1701.7150, 1591.5042, 1791.3813, 1720.4086, 1687.7772,
        1669.9285, 1757.2299, 1757.0759, 1794.2876, 1713.3364, 1761.3246,
        1643.5507, 1664.2069, 1604.1857, 1611.2345, 1572.3954, 1494.5352,
        1509.9203, 1455.4977, 1411.5503, 1327.2916, 1269.7559, 1291.0649,
        1169.6259, 1170.3303, 1053.5763, 1014.9363,  919.6649,  939.1537,
         869.6789,  795.8470,  814.0341,  692.3281,  596.4965,  521.2388,
         557.7023,  482.3725,  419.4604,  349.4105,  360.9918,  351.5032,
         302.2527,  248.8551,  193.1800,  153.3926,  162.8426,  139.7610,
         148.4366,  115.3740,   80.6863,   67.2051,   50.1109,   52.5894,
          37.8544,   44.0246,   16.9785,   39.0741,   41.7408])]
2335.246654149074
2.1507437168346444 2.1330552812280335 57.36530818879493
val isze = 8
idinces = [33 58  2 17 43  8 75  0 25 26 19 39 48 42 53 55 62 20 52 65 50 60  6  5
 77 15 47 21 76 63 72 69 61 32 37 24  3 45 59 18 74 30 64 79 80 13  7  1
 10 11 57 68 51 38 22 78 36 73 56 44 67 28 49 23 40 27 54 41 14 66  4 46
 82 16  9 12 70 29 34 81 35 71 31]
we are doing training validation split
training loss = 55.66657638549805 100
val loss = 97.60209655761719
training loss = 37.28594970703125 200
val loss = 66.21504211425781
training loss = 25.473432540893555 300
val loss = 45.81437301635742
training loss = 18.158470153808594 400
val loss = 32.76239013671875
training loss = 13.579285621643066 500
val loss = 24.328710556030273
training loss = 10.667192459106445 600
val loss = 18.757083892822266
training loss = 8.791642189025879 700
val loss = 14.989021301269531
training loss = 7.572824001312256 800
val loss = 12.385701179504395
training loss = 6.774414539337158 900
val loss = 10.552220344543457
training loss = 6.243114948272705 1000
val loss = 9.236483573913574
training loss = 5.874897003173828 1100
val loss = 8.271093368530273
training loss = 5.596026420593262 1200
val loss = 7.540596961975098
training loss = 5.353180408477783 1300
val loss = 6.963019371032715
training loss = 5.1099772453308105 1400
val loss = 6.477973461151123
training loss = 4.846215724945068 1500
val loss = 6.037052154541016
training loss = 4.552922248840332 1600
val loss = 5.596343040466309
training loss = 4.224789142608643 1700
val loss = 5.121940612792969
training loss = 3.8602283000946045 1800
val loss = 4.6011738777160645
training loss = 3.4664413928985596 1900
val loss = 4.041193008422852
training loss = 3.062241315841675 2000
val loss = 3.464232921600342
training loss = 2.677964210510254 2100
val loss = 2.9075732231140137
training loss = 2.350907325744629 2200
val loss = 2.4183521270751953
training loss = 2.1114604473114014 2300
val loss = 2.037274122238159
training loss = 1.9649007320404053 2400
val loss = 1.7776799201965332
training loss = 1.8894391059875488 2500
val loss = 1.6203722953796387
training loss = 1.8548355102539062 2600
val loss = 1.5312938690185547
training loss = 1.8389068841934204 2700
val loss = 1.481005311012268
training loss = 1.8302582502365112 2800
val loss = 1.4509077072143555
training loss = 1.8242204189300537 2900
val loss = 1.4309072494506836
training loss = 1.8191211223602295 3000
val loss = 1.4158544540405273
training loss = 1.8143850564956665 3100
val loss = 1.4031503200531006
training loss = 1.8097928762435913 3200
val loss = 1.3915050029754639
training loss = 1.8052411079406738 3300
val loss = 1.3803123235702515
training loss = 1.8006528615951538 3400
val loss = 1.3692140579223633
training loss = 1.7959518432617188 3500
val loss = 1.357978105545044
training loss = 1.7910414934158325 3600
val loss = 1.3465380668640137
training loss = 1.7857884168624878 3700
val loss = 1.334792137145996
training loss = 1.7800246477127075 3800
val loss = 1.321608304977417
training loss = 1.7736666202545166 3900
val loss = 1.3138229846954346
training loss = 1.7663731575012207 4000
val loss = 1.2968215942382812
training loss = 1.758274793624878 4100
val loss = 1.2801451683044434
training loss = 1.749010682106018 4200
val loss = 1.2738162279129028
training loss = 1.739207148551941 4300
val loss = 1.256020426750183
training loss = 1.728567361831665 4400
val loss = 1.253523588180542
training loss = 1.7215417623519897 4500
val loss = 1.2977294921875
training loss = 1.7067945003509521 4600
val loss = 1.237900972366333
training loss = 1.742903232574463 4700
val loss = 1.1155060529708862
training loss = 1.6861236095428467 4800
val loss = 1.22571861743927
training loss = 1.6764459609985352 4900
val loss = 1.2233543395996094
training loss = 1.6675872802734375 5000
val loss = 1.2227232456207275
training loss = 1.6589832305908203 5100
val loss = 1.2099937200546265
training loss = 1.6532886028289795 5200
val loss = 1.168525218963623
training loss = 1.6436378955841064 5300
val loss = 1.199397087097168
training loss = 1.6364641189575195 5400
val loss = 1.1939339637756348
training loss = 1.630103349685669 5500
val loss = 1.1768860816955566
training loss = 1.6235781908035278 5600
val loss = 1.1849126815795898
training loss = 1.7009509801864624 5700
val loss = 1.033200740814209
training loss = 1.6120195388793945 5800
val loss = 1.1766903400421143
training loss = 1.6066575050354004 5900
val loss = 1.172597050666809
training loss = 1.6021171808242798 6000
val loss = 1.1857318878173828
training loss = 1.5970858335494995 6100
val loss = 1.166203260421753
training loss = 1.592591404914856 6200
val loss = 1.1624419689178467
training loss = 1.588728904724121 6300
val loss = 1.1717097759246826
training loss = 1.584666132926941 6400
val loss = 1.157836675643921
training loss = 1.590566635131836 6500
val loss = 1.248169183731079
training loss = 1.5776501893997192 6600
val loss = 1.1572314500808716
training loss = 1.57441246509552 6700
val loss = 1.151635766029358
training loss = 1.5719102621078491 6800
val loss = 1.1675183773040771
training loss = 1.5686918497085571 6900
val loss = 1.1478817462921143
training loss = 1.566076636314392 7000
val loss = 1.1511024236679077
training loss = 1.5637158155441284 7100
val loss = 1.1424272060394287
training loss = 1.561439871788025 7200
val loss = 1.1444532871246338
training loss = 1.5678387880325317 7300
val loss = 1.2282575368881226
training loss = 1.5574363470077515 7400
val loss = 1.1420414447784424
training loss = 1.568808913230896 7500
val loss = 1.2509214878082275
training loss = 1.5539886951446533 7600
val loss = 1.137216329574585
training loss = 1.5523885488510132 7700
val loss = 1.1402161121368408
training loss = 1.5510011911392212 7800
val loss = 1.1367312669754028
training loss = 1.549651026725769 7900
val loss = 1.139110803604126
training loss = 1.5483711957931519 8000
val loss = 1.138651967048645
training loss = 1.5472567081451416 8100
val loss = 1.140535831451416
training loss = 1.5461558103561401 8200
val loss = 1.1377403736114502
training loss = 1.5451204776763916 8300
val loss = 1.134608507156372
training loss = 1.5442055463790894 8400
val loss = 1.139554500579834
training loss = 1.5432846546173096 8500
val loss = 1.1371724605560303
training loss = 1.544684648513794 8600
val loss = 1.1010651588439941
training loss = 1.5416736602783203 8700
val loss = 1.1365201473236084
training loss = 1.6532196998596191 8800
val loss = 1.5299497842788696
training loss = 1.540250539779663 8900
val loss = 1.1385917663574219
training loss = 1.539560317993164 9000
val loss = 1.1372687816619873
training loss = 1.539296269416809 9100
val loss = 1.1222789287567139
training loss = 1.5383483171463013 9200
val loss = 1.13667893409729
training loss = 1.5433951616287231 9300
val loss = 1.083155632019043
training loss = 1.5372713804244995 9400
val loss = 1.1376428604125977
training loss = 1.5367333889007568 9500
val loss = 1.1369295120239258
training loss = 1.5503644943237305 9600
val loss = 1.0575538873672485
training loss = 1.5357823371887207 9700
val loss = 1.1371912956237793
training loss = 1.5353116989135742 9800
val loss = 1.134551763534546
training loss = 1.534997820854187 9900
val loss = 1.144946575164795
training loss = 1.5344572067260742 10000
val loss = 1.1373845338821411
training loss = 1.5534205436706543 10100
val loss = 1.2680978775024414
training loss = 1.5336922407150269 10200
val loss = 1.1386839151382446
training loss = 1.533287763595581 10300
val loss = 1.1381973028182983
training loss = 1.536487102508545 10400
val loss = 1.1885855197906494
training loss = 1.5325803756713867 10500
val loss = 1.1391246318817139
training loss = 1.5322082042694092 10600
val loss = 1.137583613395691
training loss = 1.5319865942001343 10700
val loss = 1.1450542211532593
training loss = 1.5315721035003662 10800
val loss = 1.1391935348510742
training loss = 1.5838990211486816 10900
val loss = 1.3757095336914062
training loss = 1.5309721231460571 11000
val loss = 1.1372780799865723
training loss = 1.5306369066238403 11100
val loss = 1.1399779319763184
training loss = 1.5320308208465576 11200
val loss = 1.1102964878082275
training loss = 1.5300798416137695 11300
val loss = 1.1401058435440063
training loss = 1.8368103504180908 11400
val loss = 1.8954178094863892
training loss = 1.5295439958572388 11500
val loss = 1.1395390033721924
training loss = 1.5292470455169678 11600
val loss = 1.1418856382369995
training loss = 1.5290446281433105 11700
val loss = 1.1428112983703613
training loss = 1.5287593603134155 11800
val loss = 1.1414759159088135
training loss = 1.5320583581924438 11900
val loss = 1.190994381904602
training loss = 1.5282833576202393 12000
val loss = 1.1407740116119385
training loss = 1.5322322845458984 12100
val loss = 1.0966265201568604
training loss = 1.5278737545013428 12200
val loss = 1.1471006870269775
training loss = 1.527574896812439 12300
val loss = 1.1426403522491455
training loss = 1.535752773284912 12400
val loss = 1.2215452194213867
training loss = 1.52716863155365 12500
val loss = 1.1427910327911377
training loss = 1.526921033859253 12600
val loss = 1.1437348127365112
training loss = 1.526780605316162 12700
val loss = 1.140059232711792
training loss = 1.5265252590179443 12800
val loss = 1.1436902284622192
training loss = 1.5469419956207275 12900
val loss = 1.2742772102355957
training loss = 1.526139259338379 13000
val loss = 1.1446771621704102
training loss = 1.5479483604431152 13100
val loss = 1.279701590538025
training loss = 1.5257896184921265 13200
val loss = 1.1463544368743896
training loss = 1.5255563259124756 13300
val loss = 1.1446633338928223
reduced chi^2 level 2 = 1.5254861116409302
Constrained alpha: 1.8553574085235596
Constrained beta: 2.1904728412628174
Constrained gamma: 18.00484275817871
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 864.2417,  798.6300,  945.5381, 1006.1171,  997.4495, 1040.1987,
        1037.5597, 1095.8175, 1155.3438, 1170.6133, 1168.8676, 1095.5663,
        1260.1476, 1255.1876, 1311.8239, 1492.7018, 1436.6759, 1502.5917,
        1535.3491, 1515.8466, 1614.7719, 1536.7559, 1568.9971, 1600.9557,
        1636.7655, 1789.1201, 1586.3066, 1681.5465, 1764.9479, 1664.4542,
        1660.8250, 1751.7255, 1667.1411, 1730.6912, 1744.3239, 1790.9025,
        1744.2289, 1597.6759, 1582.5476, 1619.5167, 1695.5410, 1535.0629,
        1436.5812, 1447.2380, 1344.5702, 1287.4229, 1248.6014, 1189.3395,
        1182.0132, 1145.7550, 1048.5338,  985.5268,  954.6030,  929.3602,
         857.6950,  916.1865,  807.1440,  694.0422,  596.5262,  568.8783,
         558.8175,  478.7061,  416.1470,  382.8961,  380.4405,  345.7183,
         284.5695,  231.4138,  213.7627,  160.0220,  151.8734,  154.0269,
         152.9468,  110.7662,   90.0100,   64.3009,   42.2780,   46.3418,
          34.6521,   56.0149,   26.1056,   40.4737,   33.7931])]
3034.694574350315
4.503709891337334 6.131147631247751 75.69742710917905
val isze = 8
idinces = [44 46 41 12 77 13 37 61 52 11 29  4 16  7 80 45 79 48  8  3 21 62 54 42
 71 55  0 24 49  1 36 82 15 53 57 14 10 65 38  6 70 63 58 31 32 28 74 76
  9 59 30  2 43 19 78 73 20 64 17 22 75 81 69 40 25 66 47 68 33 26 23 34
 39 18 51 50 56 35 67 60 27  5 72]
we are doing training validation split
training loss = 27.979232788085938 100
val loss = 17.292922973632812
training loss = 14.472521781921387 200
val loss = 7.448760986328125
training loss = 9.448213577270508 300
val loss = 3.948786735534668
training loss = 7.58280611038208 400
val loss = 3.0471222400665283
training loss = 6.618247032165527 500
val loss = 2.672837734222412
training loss = 5.561229705810547 600
val loss = 2.1258492469787598
training loss = 4.554711818695068 700
val loss = 1.726611614227295
training loss = 3.762247085571289 800
val loss = 1.5433330535888672
training loss = 3.280637264251709 900
val loss = 1.617894172668457
training loss = 3.0663444995880127 1000
val loss = 1.78346586227417
training loss = 2.9922921657562256 1100
val loss = 1.9569082260131836
training loss = 2.966447353363037 1200
val loss = 2.0428476333618164
training loss = 2.9532928466796875 1300
val loss = 2.0891366004943848
training loss = 2.943539619445801 1400
val loss = 2.090158700942993
training loss = 2.935265302658081 1500
val loss = 2.0631048679351807
training loss = 2.9276652336120605 1600
val loss = 2.083376884460449
training loss = 2.9208719730377197 1700
val loss = 2.0755395889282227
training loss = 2.9146759510040283 1800
val loss = 2.0715222358703613
training loss = 2.909058094024658 1900
val loss = 2.0534539222717285
training loss = 2.9038798809051514 2000
val loss = 2.06097674369812
training loss = 2.9089791774749756 2100
val loss = 2.3294296264648438
training loss = 2.894831418991089 2200
val loss = 2.0536627769470215
training loss = 2.890956401824951 2300
val loss = 2.0754990577697754
training loss = 2.88720965385437 2400
val loss = 2.046578884124756
training loss = 2.8838579654693604 2500
val loss = 2.0544042587280273
training loss = 2.880758285522461 2600
val loss = 2.0449862480163574
training loss = 2.8779044151306152 2700
val loss = 2.0360841751098633
training loss = 2.8794028759002686 2800
val loss = 1.8725967407226562
training loss = 2.872706890106201 2900
val loss = 2.0321500301361084
training loss = 2.88101863861084 3000
val loss = 2.3068270683288574
training loss = 2.867980718612671 3100
val loss = 2.02506422996521
training loss = 2.8780784606933594 3200
val loss = 1.7573498487472534
training loss = 2.8633296489715576 3300
val loss = 2.0208547115325928
training loss = 2.9379143714904785 3400
val loss = 2.8130898475646973
training loss = 2.8582820892333984 3500
val loss = 2.0218777656555176
training loss = 2.8555173873901367 3600
val loss = 2.0264222621917725
training loss = 2.8522684574127197 3700
val loss = 2.0147452354431152
training loss = 2.8487589359283447 3800
val loss = 2.005863904953003
training loss = 2.844952344894409 3900
val loss = 1.965651512145996
training loss = 2.840221881866455 4000
val loss = 1.9954543113708496
training loss = 2.8350830078125 4100
val loss = 1.988785982131958
training loss = 2.829252004623413 4200
val loss = 1.9805982112884521
training loss = 2.878791570663452 4300
val loss = 2.6199326515197754
training loss = 2.8150105476379395 4400
val loss = 1.9614267349243164
training loss = 2.806110143661499 4500
val loss = 1.946196436882019
training loss = 2.7962379455566406 4600
val loss = 1.9144039154052734
training loss = 2.78458571434021 4700
val loss = 1.9132716655731201
training loss = 2.7988290786743164 4800
val loss = 1.5231852531433105
training loss = 2.755927324295044 4900
val loss = 1.8720817565917969
training loss = 2.737523078918457 5000
val loss = 1.8396451473236084
training loss = 2.7175393104553223 5100
val loss = 1.8487210273742676
training loss = 2.6930415630340576 5200
val loss = 1.768532395362854
training loss = 2.6672980785369873 5300
val loss = 1.7845909595489502
training loss = 2.637051820755005 5400
val loss = 1.6788305044174194
training loss = 2.6066811084747314 5500
val loss = 1.6268882751464844
training loss = 2.575162649154663 5600
val loss = 1.5747910737991333
training loss = 2.5501575469970703 5700
val loss = 1.690682291984558
training loss = 2.516871690750122 5800
val loss = 1.470422625541687
training loss = 2.5285298824310303 5900
val loss = 1.8998713493347168
training loss = 2.4701826572418213 6000
val loss = 1.3785357475280762
training loss = 2.4519779682159424 6100
val loss = 1.3662958145141602
training loss = 2.438979148864746 6200
val loss = 1.3010172843933105
training loss = 2.4279704093933105 6300
val loss = 1.280754804611206
training loss = 2.421926736831665 6400
val loss = 1.175898790359497
training loss = 2.4141528606414795 6500
val loss = 1.2446790933609009
training loss = 2.4092493057250977 6600
val loss = 1.2252739667892456
training loss = 2.406122922897339 6700
val loss = 1.2409077882766724
training loss = 2.4032366275787354 6800
val loss = 1.2110406160354614
training loss = 2.4055914878845215 6900
val loss = 1.3440923690795898
training loss = 2.3996598720550537 7000
val loss = 1.197845458984375
training loss = 2.3996968269348145 7100
val loss = 1.2734129428863525
training loss = 2.3973898887634277 7200
val loss = 1.1897101402282715
training loss = 2.3964452743530273 7300
val loss = 1.1874650716781616
training loss = 2.397993326187134 7400
val loss = 1.2824287414550781
training loss = 2.3951265811920166 7500
val loss = 1.1823813915252686
training loss = 2.3953609466552734 7600
val loss = 1.1224207878112793
training loss = 2.394092321395874 7700
val loss = 1.1760540008544922
training loss = 2.3935461044311523 7800
val loss = 1.1775718927383423
training loss = 2.39326810836792 7900
val loss = 1.1912342309951782
training loss = 2.3927347660064697 8000
val loss = 1.17475426197052
training loss = 2.392287015914917 8100
val loss = 1.1769640445709229
training loss = 2.392101287841797 8200
val loss = 1.1594033241271973
training loss = 2.391618251800537 8300
val loss = 1.172282338142395
training loss = 2.3912405967712402 8400
val loss = 1.1795663833618164
training loss = 2.391078233718872 8500
val loss = 1.151710033416748
training loss = 2.3906025886535645 8600
val loss = 1.169162631034851
training loss = 2.4212334156036377 8700
val loss = 0.8520650863647461
training loss = 2.3900203704833984 8800
val loss = 1.167797327041626
training loss = 2.3896732330322266 8900
val loss = 1.1667202711105347
training loss = 2.390745162963867 9000
val loss = 1.2384147644042969
training loss = 2.389150381088257 9100
val loss = 1.164964199066162
training loss = 2.3888208866119385 9200
val loss = 1.164487600326538
training loss = 2.3892781734466553 9300
val loss = 1.1141021251678467
training loss = 2.388293504714966 9400
val loss = 1.1639338731765747
training loss = 2.3879945278167725 9500
val loss = 1.1566627025604248
training loss = 2.3878235816955566 9600
val loss = 1.1536855697631836
training loss = 2.3874869346618652 9700
val loss = 1.161171555519104
training loss = 2.3983399868011475 9800
val loss = 0.9615248441696167
training loss = 2.387009382247925 9900
val loss = 1.1670587062835693
training loss = 2.3866982460021973 10000
val loss = 1.1589058637619019
training loss = 2.386638879776001 10100
val loss = 1.1359325647354126
training loss = 2.3862147331237793 10200
val loss = 1.1564463376998901
training loss = 2.409518003463745 10300
val loss = 0.8764373064041138
training loss = 2.3857569694519043 10400
val loss = 1.1641165018081665
training loss = 2.385462999343872 10500
val loss = 1.1546355485916138
training loss = 2.386370897293091 10600
val loss = 1.2219619750976562
training loss = 2.3850085735321045 10700
val loss = 1.153531551361084
training loss = 2.4442408084869385 10800
val loss = 0.7377054691314697
training loss = 2.3845877647399902 10900
val loss = 1.148742437362671
training loss = 2.3843283653259277 11000
val loss = 1.151166558265686
training loss = 2.3851730823516846 11100
val loss = 1.0902762413024902
training loss = 2.383943796157837 11200
val loss = 1.1501593589782715
training loss = 2.384230852127075 11300
val loss = 1.1046823263168335
training loss = 2.383643627166748 11400
val loss = 1.1631073951721191
training loss = 2.3833632469177246 11500
val loss = 1.147847294807434
training loss = 2.4149985313415527 11600
val loss = 1.5455348491668701
training loss = 2.38307785987854 11700
val loss = 1.152514934539795
training loss = 2.38285231590271 11800
val loss = 1.1459227800369263
training loss = 2.3826541900634766 11900
val loss = 1.1439132690429688
training loss = 2.3827357292175293 12000
val loss = 1.122443437576294
training loss = 2.3823788166046143 12100
val loss = 1.1444488763809204
training loss = 2.3821847438812256 12200
val loss = 1.1448575258255005
training loss = 2.38238525390625 12300
val loss = 1.1112117767333984
training loss = 2.38189959526062 12400
val loss = 1.1424877643585205
training loss = 2.3821005821228027 12500
val loss = 1.1785837411880493
training loss = 2.381620407104492 12600
val loss = 1.1404072046279907
training loss = 2.3814332485198975 12700
val loss = 1.1392812728881836
training loss = 2.381448268890381 12800
val loss = 1.1569359302520752
training loss = 2.3811898231506348 12900
val loss = 1.1405586004257202
training loss = 2.381007432937622 13000
val loss = 1.1405328512191772
training loss = 2.3813326358795166 13100
val loss = 1.1016829013824463
training loss = 2.3807482719421387 13200
val loss = 1.139088749885559
training loss = 2.449906826019287 13300
val loss = 0.6978998780250549
training loss = 2.3805270195007324 13400
val loss = 1.1329379081726074
training loss = 2.380333423614502 13500
val loss = 1.1373320817947388
training loss = 2.4214751720428467 13600
val loss = 0.7838329672813416
training loss = 2.380091905593872 13700
val loss = 1.134121298789978
training loss = 2.3799123764038086 13800
val loss = 1.1381322145462036
training loss = 2.379962205886841 13900
val loss = 1.1153653860092163
training loss = 2.3796651363372803 14000
val loss = 1.134655475616455
training loss = 2.3925716876983643 14100
val loss = 0.9239934682846069
training loss = 2.3794233798980713 14200
val loss = 1.134859561920166
training loss = 2.3800156116485596 14300
val loss = 1.1895534992218018
training loss = 2.3791956901550293 14400
val loss = 1.131324052810669
training loss = 2.379014492034912 14500
val loss = 1.1322723627090454
training loss = 2.3841564655303955 14600
val loss = 0.9947890043258667
training loss = 2.3787753582000732 14700
val loss = 1.132305383682251
training loss = 2.3850955963134766 14800
val loss = 0.9837825894355774
training loss = 2.3785579204559326 14900
val loss = 1.128420352935791
training loss = 2.3783810138702393 15000
val loss = 1.1297967433929443
training loss = 2.3823559284210205 15100
val loss = 1.0086028575897217
training loss = 2.3781638145446777 15200
val loss = 1.1288049221038818
training loss = 2.378283739089966 15300
val loss = 1.1623430252075195
training loss = 2.378000259399414 15400
val loss = 1.1124341487884521
training loss = 2.377763271331787 15500
val loss = 1.126389980316162
training loss = 2.3789961338043213 15600
val loss = 1.1993950605392456
training loss = 2.3775405883789062 15700
val loss = 1.1253926753997803
training loss = 2.434537887573242 15800
val loss = 1.6684532165527344
training loss = 2.3773183822631836 15900
val loss = 1.1217217445373535
training loss = 2.3778436183929443 16000
val loss = 1.1772929430007935
training loss = 2.377091646194458 16100
val loss = 1.1234999895095825
training loss = 2.376922607421875 16200
val loss = 1.1226654052734375
training loss = 2.3779826164245605 16300
val loss = 1.189582109451294
training loss = 2.3767106533050537 16400
val loss = 1.1222517490386963
training loss = 2.663482427597046 16500
val loss = 0.3923974633216858
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 866.2096,  885.0869,  940.1218,  934.8735, 1060.2803, 1074.0259,
        1078.6826, 1160.3190, 1134.8771, 1198.0791, 1116.0421, 1190.1438,
        1278.2423, 1217.9585, 1326.7433, 1344.6746, 1315.1720, 1480.6516,
        1505.9958, 1521.0344, 1617.4667, 1583.4966, 1646.2263, 1583.0753,
        1653.0316, 1673.2936, 1646.6487, 1714.7089, 1755.5160, 1710.0599,
        1715.8186, 1760.3293, 1742.3643, 1746.1517, 1726.9526, 1745.2423,
        1658.6222, 1600.6720, 1629.3882, 1635.5253, 1544.8669, 1539.2577,
        1560.1680, 1465.3036, 1448.4176, 1340.1565, 1173.2213, 1252.8914,
        1171.3265, 1220.5640, 1080.1631,  979.8180,  956.1293,  944.0005,
         886.0725,  845.1834,  888.0645,  663.5800,  610.9153,  516.7490,
         566.2341,  476.5417,  466.5442,  361.1625,  350.8257,  362.4580,
         295.9365,  252.1916,  199.0851,  173.7609,  173.0618,  152.3692,
         141.7018,  109.4447,   95.7412,   58.2152,   40.5385,   34.9060,
          33.8763,   44.0401,   25.0400,   55.6401,   44.5813])]
2544.645232440667
2.463253216955559 16.13210872629893 93.78000480524456
val isze = 8
idinces = [64 28 20 53 12 21 51 44 16 72 11 66 57 39 56 35 80 76 42 49 52 46 22  7
 63  0 30 59 77 67 17 41 79 34 38 74 14 71 43 47 54 45  2 82 25 13  1 18
 26 73 60  4 31 19 27 70 40  8 10 78 61  6 37 58 23  9 81 32  5 33 75 69
 62 48  3 68 15 24 55 36 65 50 29]
we are doing training validation split
training loss = 246.71578979492188 100
val loss = 271.11834716796875
training loss = 11.754085540771484 200
val loss = 3.6719462871551514
training loss = 9.698203086853027 300
val loss = 5.775049209594727
training loss = 9.398877143859863 400
val loss = 5.711391448974609
training loss = 9.095431327819824 500
val loss = 5.664806842803955
training loss = 8.805254936218262 600
val loss = 5.6355438232421875
training loss = 8.541869163513184 700
val loss = 5.625918388366699
training loss = 8.313973426818848 800
val loss = 5.635077476501465
training loss = 8.124873161315918 900
val loss = 5.659198760986328
training loss = 7.972796440124512 1000
val loss = 5.6923956871032715
training loss = 7.852197647094727 1100
val loss = 5.727900505065918
training loss = 7.755703926086426 1200
val loss = 5.759822368621826
training loss = 7.675926208496094 1300
val loss = 5.78396463394165
training loss = 7.606759548187256 1400
val loss = 5.798262596130371
training loss = 7.543814659118652 1500
val loss = 5.802459716796875
training loss = 7.484344482421875 1600
val loss = 5.797695159912109
training loss = 7.426805019378662 1700
val loss = 5.785541534423828
training loss = 7.370291709899902 1800
val loss = 5.767570495605469
training loss = 7.31492280960083 1900
val loss = 5.746006965637207
training loss = 7.260623455047607 2000
val loss = 5.722358703613281
training loss = 7.207406520843506 2100
val loss = 5.697449684143066
training loss = 7.155411720275879 2200
val loss = 5.672023773193359
training loss = 7.104701519012451 2300
val loss = 5.646053314208984
training loss = 7.055142402648926 2400
val loss = 5.618466377258301
training loss = 7.007021427154541 2500
val loss = 5.591761589050293
training loss = 6.960360050201416 2600
val loss = 5.566258430480957
training loss = 6.914912223815918 2700
val loss = 5.54204797744751
training loss = 6.870057106018066 2800
val loss = 5.51865291595459
training loss = 6.8246283531188965 2900
val loss = 5.504230976104736
training loss = 6.775692939758301 3000
val loss = 5.473628997802734
training loss = 6.717270851135254 3100
val loss = 5.444251537322998
training loss = 6.632558345794678 3200
val loss = 5.395683288574219
training loss = 6.478291988372803 3300
val loss = 5.380183219909668
training loss = 6.162421226501465 3400
val loss = 5.138164520263672
training loss = 5.5564422607421875 3500
val loss = 4.72865629196167
training loss = 4.345754623413086 3600
val loss = 3.6943750381469727
training loss = 2.9211995601654053 3700
val loss = 2.9951372146606445
training loss = 2.5092105865478516 3800
val loss = 2.2525854110717773
training loss = 2.49456524848938 3900
val loss = 2.2269396781921387
training loss = 2.493250846862793 4000
val loss = 2.264738082885742
training loss = 2.4908266067504883 4100
val loss = 2.235168218612671
training loss = 2.493229627609253 4200
val loss = 2.163327693939209
training loss = 2.488035202026367 4300
val loss = 2.246734380722046
training loss = 2.48667049407959 4400
val loss = 2.259394407272339
training loss = 2.4853897094726562 4500
val loss = 2.2593235969543457
training loss = 2.484042167663574 4600
val loss = 2.2617878913879395
training loss = 2.4833309650421143 4700
val loss = 2.298633575439453
training loss = 2.4812521934509277 4800
val loss = 2.2684836387634277
training loss = 2.4998035430908203 4900
val loss = 2.10789155960083
training loss = 2.478074550628662 5000
val loss = 2.269848346710205
training loss = 2.476243734359741 5100
val loss = 2.267256021499634
training loss = 2.4744157791137695 5200
val loss = 2.259666919708252
training loss = 2.4721620082855225 5300
val loss = 2.2710225582122803
training loss = 2.471351385116577 5400
val loss = 2.320673704147339
training loss = 2.4673678874969482 5500
val loss = 2.267334222793579
training loss = 2.4742636680603027 5600
val loss = 2.4066286087036133
training loss = 2.461599111557007 5700
val loss = 2.261805534362793
training loss = 2.458195924758911 5800
val loss = 2.2691092491149902
training loss = 2.454529047012329 5900
val loss = 2.24477481842041
training loss = 2.4501936435699463 6000
val loss = 2.2460427284240723
training loss = 2.445690155029297 6100
val loss = 2.2486000061035156
training loss = 2.440295457839966 6200
val loss = 2.2318344116210938
training loss = 2.4807543754577637 6300
val loss = 2.000993251800537
training loss = 2.427415132522583 6400
val loss = 2.214420795440674
training loss = 2.4193527698516846 6500
val loss = 2.2123372554779053
training loss = 2.410869598388672 6600
val loss = 2.1785244941711426
training loss = 2.40083909034729 6700
val loss = 2.179919719696045
training loss = 2.391422986984253 6800
val loss = 2.2070794105529785
training loss = 2.379359245300293 6900
val loss = 2.15368914604187
training loss = 2.369835376739502 7000
val loss = 2.200226306915283
training loss = 2.3561298847198486 7100
val loss = 2.124901294708252
training loss = 2.345484972000122 7200
val loss = 2.061250686645508
training loss = 2.331861972808838 7300
val loss = 2.0973410606384277
training loss = 2.3186898231506348 7400
val loss = 2.0777528285980225
training loss = 2.3058664798736572 7500
val loss = 2.057957649230957
training loss = 2.291790246963501 7600
val loss = 2.0344698429107666
training loss = 2.277820348739624 7700
val loss = 1.9789842367172241
training loss = 2.2623555660247803 7800
val loss = 1.9893596172332764
training loss = 2.274937629699707 7900
val loss = 1.784888505935669
training loss = 2.233074426651001 8000
val loss = 1.930715560913086
training loss = 2.219374895095825 8100
val loss = 1.9132964611053467
training loss = 2.3230140209198 8200
val loss = 1.595415472984314
training loss = 2.196777582168579 8300
val loss = 1.868356466293335
training loss = 2.18757700920105 8400
val loss = 1.8446146249771118
training loss = 2.2487170696258545 8500
val loss = 2.246088743209839
training loss = 2.173474073410034 8600
val loss = 1.8026334047317505
training loss = 2.168172597885132 8700
val loss = 1.7944992780685425
training loss = 2.164353847503662 8800
val loss = 1.765892505645752
training loss = 2.1609277725219727 8900
val loss = 1.7725064754486084
training loss = 2.158205032348633 9000
val loss = 1.763227939605713
training loss = 2.1562654972076416 9100
val loss = 1.7595281600952148
training loss = 2.1545588970184326 9200
val loss = 1.7502057552337646
training loss = 2.153122901916504 9300
val loss = 1.744747519493103
training loss = 2.158128023147583 9400
val loss = 1.852186679840088
training loss = 2.1509692668914795 9500
val loss = 1.7349565029144287
training loss = 2.150073289871216 9600
val loss = 1.7326555252075195
training loss = 2.1495728492736816 9700
val loss = 1.7488813400268555
training loss = 2.1486284732818604 9800
val loss = 1.7274231910705566
training loss = 2.1480917930603027 9900
val loss = 1.7087246179580688
training loss = 2.147488594055176 10000
val loss = 1.7336087226867676
training loss = 2.146819829940796 10100
val loss = 1.7196999788284302
training loss = 2.2439327239990234 10200
val loss = 2.251502513885498
training loss = 2.145775556564331 10300
val loss = 1.719712257385254
training loss = 2.145228624343872 10400
val loss = 1.7138323783874512
training loss = 2.2593815326690674 10500
val loss = 1.4009004831314087
training loss = 2.1442580223083496 10600
val loss = 1.7169233560562134
training loss = 2.1437110900878906 10700
val loss = 1.7087979316711426
training loss = 2.159761667251587 10800
val loss = 1.5544335842132568
training loss = 2.1427345275878906 10900
val loss = 1.707794427871704
training loss = 2.1422102451324463 11000
val loss = 1.7040214538574219
training loss = 2.1430819034576416 11100
val loss = 1.654405951499939
training loss = 2.141228199005127 11200
val loss = 1.7009003162384033
training loss = 2.2201039791107178 11300
val loss = 2.1696462631225586
training loss = 2.1402413845062256 11400
val loss = 1.6956291198730469
training loss = 2.1400961875915527 11500
val loss = 1.7223646640777588
training loss = 2.139294147491455 11600
val loss = 1.6851370334625244
training loss = 2.1387176513671875 11700
val loss = 1.6935927867889404
training loss = 2.139308214187622 11800
val loss = 1.6491143703460693
training loss = 2.1377251148223877 11900
val loss = 1.6901049613952637
training loss = 2.1802892684936523 12000
val loss = 2.0162205696105957
training loss = 2.136726140975952 12100
val loss = 1.687963843345642
training loss = 2.1519687175750732 12200
val loss = 1.537590742111206
training loss = 2.135740041732788 12300
val loss = 1.679103970527649
training loss = 2.1351890563964844 12400
val loss = 1.6840184926986694
training loss = 2.1350040435791016 12500
val loss = 1.6603420972824097
training loss = 2.1342034339904785 12600
val loss = 1.6816432476043701
training loss = 2.2113583087921143 12700
val loss = 1.402522325515747
training loss = 2.133208990097046 12800
val loss = 1.6776939630508423
training loss = 2.132673740386963 12900
val loss = 1.6782402992248535
training loss = 2.1322858333587646 13000
val loss = 1.6876866817474365
training loss = 2.1316823959350586 13100
val loss = 1.6755770444869995
training loss = 2.155587673187256 13200
val loss = 1.4943701028823853
training loss = 2.1306934356689453 13300
val loss = 1.6744067668914795
training loss = 2.1301887035369873 13400
val loss = 1.679042935371399
training loss = 2.129845142364502 13500
val loss = 1.6861449480056763
training loss = 2.12919282913208 13600
val loss = 1.669674277305603
training loss = 2.3708882331848145 13700
val loss = 1.3102811574935913
training loss = 2.1282331943511963 13800
val loss = 1.6681303977966309
training loss = 2.127713680267334 13900
val loss = 1.6669261455535889
training loss = 2.144649028778076 14000
val loss = 1.5106052160263062
training loss = 2.1267499923706055 14100
val loss = 1.6660231351852417
training loss = 2.1262264251708984 14200
val loss = 1.6641356945037842
training loss = 2.125897169113159 14300
val loss = 1.6505502462387085
training loss = 2.125309944152832 14400
val loss = 1.6623409986495972
training loss = 2.1247987747192383 14500
val loss = 1.6614327430725098
training loss = 2.1300086975097656 14600
val loss = 1.5675945281982422
training loss = 2.12388014793396 14700
val loss = 1.660793662071228
training loss = 2.1233747005462646 14800
val loss = 1.6588990688323975
training loss = 2.124087333679199 14900
val loss = 1.7036116123199463
training loss = 2.1224639415740967 15000
val loss = 1.657225489616394
training loss = 2.121964693069458 15100
val loss = 1.6578112840652466
training loss = 2.1215710639953613 15200
val loss = 1.6508517265319824
training loss = 2.121070384979248 15300
val loss = 1.6555447578430176
training loss = 2.4613571166992188 15400
val loss = 2.838557720184326
training loss = 2.1201791763305664 15500
val loss = 1.6579623222351074
training loss = 2.1196839809417725 15600
val loss = 1.653401494026184
training loss = 2.1211206912994385 15700
val loss = 1.5978997945785522
training loss = 2.1188101768493652 15800
val loss = 1.6518768072128296
training loss = 2.26511812210083 15900
val loss = 1.3239606618881226
training loss = 2.1179301738739014 16000
val loss = 1.649598240852356
training loss = 2.117457628250122 16100
val loss = 1.6498570442199707
training loss = 2.1174378395080566 16200
val loss = 1.6250717639923096
training loss = 2.116624593734741 16300
val loss = 1.6492465734481812
training loss = 2.11673903465271 16400
val loss = 1.617182731628418
training loss = 2.115837812423706 16500
val loss = 1.6381278038024902
training loss = 2.1153321266174316 16600
val loss = 1.6472880840301514
training loss = 2.1237566471099854 16700
val loss = 1.5334653854370117
training loss = 2.114513874053955 16800
val loss = 1.647247076034546
training loss = 2.1144466400146484 16900
val loss = 1.6716423034667969
training loss = 2.1137359142303467 17000
val loss = 1.63694167137146
training loss = 2.1132566928863525 17100
val loss = 1.6449384689331055
training loss = 2.1158320903778076 17200
val loss = 1.7187377214431763
training loss = 2.112473487854004 17300
val loss = 1.6435439586639404
training loss = 2.112043619155884 17400
val loss = 1.6413781642913818
training loss = 2.1119091510772705 17500
val loss = 1.662585735321045
training loss = 2.111276865005493 17600
val loss = 1.6425620317459106
training loss = 2.266394853591919 17700
val loss = 2.347233772277832
training loss = 2.110517978668213 17800
val loss = 1.6454730033874512
training loss = 2.1100926399230957 17900
val loss = 1.6408812999725342
training loss = 2.1102988719940186 18000
val loss = 1.6107759475708008
training loss = 2.1093385219573975 18100
val loss = 1.640280842781067
training loss = 2.1539013385772705 18200
val loss = 1.9703524112701416
training loss = 2.108593702316284 18300
val loss = 1.6379852294921875
training loss = 2.108287811279297 18400
val loss = 1.652410864830017
training loss = 2.1078710556030273 18500
val loss = 1.6338704824447632
training loss = 2.107459545135498 18600
val loss = 1.638859748840332
training loss = 2.1074788570404053 18700
val loss = 1.6136144399642944
training loss = 2.1067399978637695 18800
val loss = 1.6376317739486694
training loss = 2.10634708404541 18900
val loss = 1.6389011144638062
training loss = 2.106077194213867 19000
val loss = 1.6448954343795776
training loss = 2.105666160583496 19100
val loss = 1.6370456218719482
training loss = 2.1052894592285156 19200
val loss = 1.6398448944091797
training loss = 2.1050431728363037 19300
val loss = 1.6480281352996826
training loss = 2.1045830249786377 19400
val loss = 1.6359188556671143
training loss = 2.135613203048706 19500
val loss = 1.9029923677444458
training loss = 2.1039175987243652 19600
val loss = 1.636155605316162
training loss = 2.1035451889038086 19700
val loss = 1.6349722146987915
training loss = 2.1058008670806885 19800
val loss = 1.5701624155044556
training loss = 2.1028707027435303 19900
val loss = 1.634851336479187
training loss = 2.1038565635681152 20000
val loss = 1.587183952331543
training loss = 2.1022446155548096 20100
val loss = 1.643212080001831
training loss = 2.1018238067626953 20200
val loss = 1.6323809623718262
training loss = 2.1016736030578613 20300
val loss = 1.6171436309814453
training loss = 2.101161003112793 20400
val loss = 1.6327745914459229
training loss = 2.1054513454437256 20500
val loss = 1.7257626056671143
training loss = 2.10050630569458 20600
val loss = 1.6321396827697754
training loss = 2.18733811378479 20700
val loss = 2.121492385864258
training loss = 2.099867105484009 20800
val loss = 1.6272664070129395
training loss = 2.0995023250579834 20900
val loss = 1.630949854850769
training loss = 2.099839210510254 21000
val loss = 1.5994834899902344
training loss = 2.098874568939209 21100
val loss = 1.6308046579360962
training loss = 2.165186882019043 21200
val loss = 2.0468368530273438
training loss = 2.098257303237915 21300
val loss = 1.6268879175186157
training loss = 2.097907781600952 21400
val loss = 1.629921555519104
training loss = 2.09842848777771 21500
val loss = 1.6667883396148682
training loss = 2.0973050594329834 21600
val loss = 1.6298660039901733
training loss = 2.096966028213501 21700
val loss = 1.6291497945785522
training loss = 2.098533868789673 21800
val loss = 1.686497449874878
training loss = 2.096393346786499 21900
val loss = 1.629148244857788
training loss = 2.096064805984497 22000
val loss = 1.628481388092041
training loss = 2.109365940093994 22100
val loss = 1.492010235786438
training loss = 2.0954859256744385 22200
val loss = 1.628522515296936
training loss = 2.095161199569702 22300
val loss = 1.6275681257247925
training loss = 2.0982820987701416 22400
val loss = 1.5557396411895752
training loss = 2.0945870876312256 22500
val loss = 1.6279346942901611
training loss = 2.094372510910034 22600
val loss = 1.6402130126953125
training loss = 2.094083309173584 22700
val loss = 1.6156728267669678
training loss = 2.093693971633911 22800
val loss = 1.6265443563461304
training loss = 2.0944557189941406 22900
val loss = 1.585649013519287
training loss = 2.093125820159912 23000
val loss = 1.6261906623840332
training loss = 2.1412835121154785 23100
val loss = 1.3981406688690186
training loss = 2.092578887939453 23200
val loss = 1.6287498474121094
training loss = 2.092266082763672 23300
val loss = 1.6252591609954834
training loss = 2.0931601524353027 23400
val loss = 1.6702685356140137
training loss = 2.091723680496216 23500
val loss = 1.6248102188110352
training loss = 2.275452136993408 23600
val loss = 2.402628183364868
training loss = 2.0911917686462402 23700
val loss = 1.6257011890411377
training loss = 2.090892791748047 23800
val loss = 1.624018907546997
training loss = 2.091768503189087 23900
val loss = 1.6686859130859375
training loss = 2.0903687477111816 24000
val loss = 1.6235628128051758
training loss = 2.186156988143921 24100
val loss = 1.3376442193984985
training loss = 2.089862823486328 24200
val loss = 1.6230278015136719
training loss = 2.0895769596099854 24300
val loss = 1.6233258247375488
training loss = 2.0939230918884277 24400
val loss = 1.5407931804656982
training loss = 2.0890748500823975 24500
val loss = 1.6225117444992065
training loss = 2.101839303970337 24600
val loss = 1.7858103513717651
training loss = 2.088629722595215 24700
val loss = 1.6147685050964355
training loss = 2.088319778442383 24800
val loss = 1.6222617626190186
training loss = 2.551121473312378 24900
val loss = 1.3016047477722168
training loss = 2.087857484817505 25000
val loss = 1.6256599426269531
training loss = 2.087578535079956 25100
val loss = 1.6214573383331299
training loss = 2.0918197631835938 25200
val loss = 1.5402021408081055
training loss = 2.087116241455078 25300
val loss = 1.6205775737762451
training loss = 2.0868523120880127 25400
val loss = 1.6193045377731323
training loss = 2.086655616760254 25500
val loss = 1.6187573671340942
training loss = 2.086395025253296 25600
val loss = 1.6206579208374023
training loss = 2.092487335205078 25700
val loss = 1.7302252054214478
training loss = 2.085942029953003 25800
val loss = 1.6207342147827148
training loss = 2.4451847076416016 25900
val loss = 1.2803343534469604
training loss = 2.085500478744507 26000
val loss = 1.6223679780960083
training loss = 2.085244655609131 26100
val loss = 1.6196115016937256
training loss = 2.0850954055786133 26200
val loss = 1.6150660514831543
training loss = 2.0848398208618164 26300
val loss = 1.6192505359649658
training loss = 2.084595203399658 26400
val loss = 1.6193395853042603
training loss = 2.0844807624816895 26500
val loss = 1.6290490627288818
training loss = 2.0841872692108154 26600
val loss = 1.6190835237503052
training loss = 2.1748735904693604 26700
val loss = 1.3393137454986572
training loss = 2.083789587020874 26800
val loss = 1.6163032054901123
training loss = 2.0835511684417725 26900
val loss = 1.6181669235229492
training loss = 2.083611249923706 27000
val loss = 1.6348011493682861
training loss = 2.0831727981567383 27100
val loss = 1.6192268133163452
training loss = 2.082943916320801 27200
val loss = 1.6179124116897583
training loss = 2.094334602355957 27300
val loss = 1.4930500984191895
training loss = 2.0825705528259277 27400
val loss = 1.6167633533477783
training loss = 2.082343816757202 27500
val loss = 1.6174362897872925
training loss = 2.08301043510437 27600
val loss = 1.5815485715866089
training loss = 2.0819833278656006 27700
val loss = 1.6168832778930664
training loss = 2.0883398056030273 27800
val loss = 1.7287044525146484
training loss = 2.0816543102264404 27900
val loss = 1.6088786125183105
training loss = 2.08140230178833 28000
val loss = 1.6162861585617065
training loss = 2.0847012996673584 28100
val loss = 1.6956833600997925
training loss = 2.081050157546997 28200
val loss = 1.616735816001892
training loss = 2.111260175704956 28300
val loss = 1.8765485286712646
training loss = 2.0807242393493652 28400
val loss = 1.6093581914901733
training loss = 2.0804924964904785 28500
val loss = 1.6157479286193848
training loss = 2.0839927196502686 28600
val loss = 1.6972856521606445
training loss = 2.080160617828369 28700
val loss = 1.6148065328598022
training loss = 2.0804696083068848 28800
val loss = 1.5861884355545044
training loss = 2.079834222793579 28900
val loss = 1.6146187782287598
training loss = 2.07963228225708 29000
val loss = 1.6146090030670166
training loss = 2.1224606037139893 29100
val loss = 1.931465744972229
training loss = 2.079324722290039 29200
val loss = 1.6132566928863525
training loss = 2.079124689102173 29300
val loss = 1.6142332553863525
training loss = 2.082397937774658 29400
val loss = 1.692818522453308
training loss = 2.0788209438323975 29500
val loss = 1.613487958908081
training loss = 2.0796456336975098 29600
val loss = 1.6556450128555298
training loss = 2.0785233974456787 29700
val loss = 1.6084413528442383
training loss = 2.0783162117004395 29800
val loss = 1.6132639646530151
training loss = 2.079324245452881 29900
val loss = 1.657592535018921
training loss = 2.0780248641967773 30000
val loss = 1.6129846572875977
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 862.7928,  865.1863,  902.1812,  909.0234,  997.7048, 1072.9769,
        1076.8744, 1123.2045, 1137.0652, 1191.8423, 1244.4783, 1185.9330,
        1202.6177, 1222.1903, 1296.4674, 1407.5996, 1423.5369, 1343.0068,
        1581.3799, 1527.3092, 1616.2961, 1531.4774, 1584.9557, 1656.9219,
        1664.5458, 1702.4950, 1578.8910, 1761.3378, 1669.3440, 1742.5958,
        1675.8801, 1734.0793, 1785.9487, 1815.5131, 1771.8594, 1758.4259,
        1711.1851, 1631.9954, 1646.4801, 1601.2748, 1654.9467, 1575.6666,
        1412.2241, 1535.3535, 1370.6133, 1285.0316, 1227.4657, 1294.4365,
        1146.8268, 1156.8003, 1125.3871, 1024.0697,  984.1812,  862.4484,
         884.4755,  835.7274,  853.3200,  703.4077,  547.6555,  564.7013,
         564.1906,  519.4126,  448.7437,  392.9369,  342.5482,  329.5087,
         294.9084,  209.7324,  195.8170,  165.9109,  157.1987,  139.7038,
         144.2948,  102.0693,   73.9750,   84.6474,   39.7822,   38.7379,
          41.8232,   36.8963,   15.1404,   42.6542,   37.2098])]
2921.240004312615
3.1209048121635536 10.718120376777412 98.09742326308913
val isze = 8
idinces = [24 22 21 80  2 49 58 53 29 25 11 75 27 77 18 15 57  5 20 65 68 62 19  3
 64 50 70  6 23 34 13 48 16 36 33 37 60  8 17 10 66 56 46 54 71 74 30 59
 32  1 78 45 28 14 12 72 76  4 31 44 61 67 69  9  7 38 41 81 26 51 35 63
 79  0 82 55 39 42 47 73 40 43 52]
we are doing training validation split
training loss = 104.39463806152344 100
val loss = 79.523681640625
training loss = 8.92824935913086 200
val loss = 12.14793872833252
training loss = 8.14516830444336 300
val loss = 11.745197296142578
training loss = 7.597336769104004 400
val loss = 11.279470443725586
training loss = 7.231845855712891 500
val loss = 10.90609359741211
training loss = 6.994194507598877 600
val loss = 10.609231948852539
training loss = 6.840774059295654 700
val loss = 10.374320983886719
training loss = 6.7403883934021 800
val loss = 10.189033508300781
training loss = 6.672094821929932 900
val loss = 10.04354476928711
training loss = 6.622519016265869 1000
val loss = 9.929293632507324
training loss = 6.583492279052734 1100
val loss = 9.839179992675781
training loss = 6.550262451171875 1200
val loss = 9.767644882202148
training loss = 6.520185470581055 1300
val loss = 9.709855079650879
training loss = 6.491863250732422 1400
val loss = 9.661998748779297
training loss = 6.464585781097412 1500
val loss = 9.62129020690918
training loss = 6.437999248504639 1600
val loss = 9.585359573364258
training loss = 6.411931037902832 1700
val loss = 9.552743911743164
training loss = 6.386274337768555 1800
val loss = 9.522077560424805
training loss = 6.3609538078308105 1900
val loss = 9.492715835571289
training loss = 6.335910797119141 2000
val loss = 9.464111328125
training loss = 6.311085224151611 2100
val loss = 9.4359130859375
training loss = 6.286422252655029 2200
val loss = 9.407862663269043
training loss = 6.261870861053467 2300
val loss = 9.380050659179688
training loss = 6.237384796142578 2400
val loss = 9.352055549621582
training loss = 6.2129364013671875 2500
val loss = 9.324155807495117
training loss = 6.188499450683594 2600
val loss = 9.296319961547852
training loss = 6.164046287536621 2700
val loss = 9.26856803894043
training loss = 6.139525413513184 2800
val loss = 9.240629196166992
training loss = 6.11476469039917 2900
val loss = 9.212668418884277
training loss = 6.089259147644043 3000
val loss = 9.184012413024902
training loss = 6.0615034103393555 3100
val loss = 9.152463912963867
training loss = 6.026545524597168 3200
val loss = 9.111555099487305
training loss = 5.963046073913574 3300
val loss = 9.034456253051758
training loss = 5.740717887878418 3400
val loss = 8.7753267288208
training loss = 5.1567912101745605 3500
val loss = 7.944129943847656
training loss = 4.064816474914551 3600
val loss = 6.217199325561523
training loss = 2.798522710800171 3700
val loss = 3.3644776344299316
training loss = 2.496920585632324 3800
val loss = 3.0518860816955566
training loss = 2.4462528228759766 3900
val loss = 3.4966607093811035
training loss = 2.3187623023986816 4000
val loss = 3.287327289581299
training loss = 2.4199836254119873 4100
val loss = 2.6935698986053467
training loss = 2.206040620803833 4200
val loss = 3.36806058883667
training loss = 2.155085802078247 4300
val loss = 3.5845112800598145
training loss = 2.094512939453125 4400
val loss = 3.4447317123413086
training loss = 2.040881395339966 4500
val loss = 3.5977563858032227
training loss = 1.992936611175537 4600
val loss = 3.587421178817749
training loss = 1.9490678310394287 4700
val loss = 3.6256766319274902
training loss = 1.9150655269622803 4800
val loss = 3.712745189666748
training loss = 1.8851066827774048 4900
val loss = 3.7714273929595947
training loss = 1.862387776374817 5000
val loss = 3.8653178215026855
training loss = 1.8432426452636719 5100
val loss = 3.8773746490478516
training loss = 2.379132032394409 5200
val loss = 2.977543830871582
training loss = 1.8170949220657349 5300
val loss = 3.969484329223633
training loss = 1.8077189922332764 5400
val loss = 3.9994473457336426
training loss = 1.8017061948776245 5500
val loss = 4.1019744873046875
training loss = 1.7944355010986328 5600
val loss = 4.053714275360107
training loss = 1.8314461708068848 5700
val loss = 4.548938751220703
training loss = 1.7853015661239624 5800
val loss = 4.096013069152832
training loss = 1.7816507816314697 5900
val loss = 4.1125006675720215
training loss = 1.779097080230713 6000
val loss = 4.165698528289795
training loss = 1.7758874893188477 6100
val loss = 4.137403964996338
training loss = 1.9323387145996094 6200
val loss = 5.127076148986816
training loss = 1.7712939977645874 6300
val loss = 4.154036998748779
training loss = 1.7692838907241821 6400
val loss = 4.1685333251953125
training loss = 1.7675403356552124 6500
val loss = 4.163320541381836
training loss = 1.7658402919769287 6600
val loss = 4.1768999099731445
training loss = 1.7658170461654663 6700
val loss = 4.098752975463867
training loss = 1.7629042863845825 6800
val loss = 4.187621593475342
training loss = 1.888168215751648 6900
val loss = 5.072934150695801
training loss = 1.7603837251663208 7000
val loss = 4.19908332824707
training loss = 1.7591615915298462 7100
val loss = 4.198954105377197
training loss = 1.8643701076507568 7200
val loss = 4.989165306091309
training loss = 1.7570135593414307 7300
val loss = 4.20934534072876
training loss = 1.7559473514556885 7400
val loss = 4.21058464050293
training loss = 1.7550610303878784 7500
val loss = 4.19913911819458
training loss = 1.754025936126709 7600
val loss = 4.213679790496826
training loss = 2.207414150238037 7700
val loss = 3.2611281871795654
training loss = 1.7522257566452026 7800
val loss = 4.209434509277344
training loss = 1.7512520551681519 7900
val loss = 4.221035480499268
training loss = 1.7754950523376465 8000
val loss = 3.909604787826538
training loss = 1.7494735717773438 8100
val loss = 4.224303722381592
training loss = 1.7485557794570923 8200
val loss = 4.213457107543945
training loss = 1.7477580308914185 8300
val loss = 4.209290504455566
training loss = 1.7467432022094727 8400
val loss = 4.226490020751953
training loss = 1.7887858152389526 8500
val loss = 3.821105718612671
training loss = 1.7450040578842163 8600
val loss = 4.227547645568848
training loss = 1.7440648078918457 8700
val loss = 4.225168228149414
training loss = 1.743323802947998 8800
val loss = 4.223000526428223
reduced chi^2 level 2 = 1.742475986480713
Constrained alpha: 1.7105306386947632
Constrained beta: 2.9021108150482178
Constrained gamma: 27.582902908325195
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 854.1335,  833.1898,  967.8193,  914.1634,  985.6635, 1061.0354,
        1084.6188, 1093.6099, 1202.7483, 1141.1805, 1219.0454, 1175.6843,
        1265.8423, 1267.9537, 1355.4189, 1347.8120, 1412.1644, 1382.2411,
        1582.4701, 1576.8031, 1579.6980, 1572.2332, 1545.5948, 1609.2404,
        1600.5428, 1696.3253, 1625.6602, 1743.2693, 1747.9789, 1721.0756,
        1667.2395, 1759.2668, 1696.2839, 1730.7123, 1751.4934, 1657.0717,
        1642.5206, 1594.5677, 1614.0232, 1663.4363, 1626.8115, 1558.3439,
        1508.3329, 1511.5908, 1371.2437, 1345.2299, 1243.9717, 1249.2576,
        1116.5074, 1149.2068, 1045.3923, 1002.5760,  994.1276,  932.6565,
         827.0092,  857.6037,  829.9006,  680.2086,  575.7315,  544.1378,
         557.3605,  472.7129,  423.9570,  380.4292,  359.0971,  344.5108,
         275.0504,  267.5865,  198.0403,  190.9958,  152.7439,  154.4336,
         138.6702,  109.9780,   85.6028,   76.6877,   45.1068,   35.6104,
          44.2827,   52.8035,   15.3618,   41.9458,   33.6698])]
2598.5281025585564
1.7103106519417577 3.9930453734220728 47.51021096480081
val isze = 8
idinces = [69 82 22 81 74 70 79 24 66 40 46 26 27  8 50 47 65 30  7 77 15 61  2 51
  0 18 29 28 39 59 71 49 45 76 64 43 32 34 35 37  6 44 36 60 19 23  3 72
 33 57 63 20 13 12 67 21  9 38  5 17 62 11 56 25 73 16 31 54 14  1 68 55
 10  4 53 58 52 80 78 75 42 48 41]
we are doing training validation split
training loss = 25.2194881439209 100
val loss = 13.515539169311523
training loss = 19.998477935791016 200
val loss = 10.274791717529297
training loss = 15.911097526550293 300
val loss = 8.355375289916992
training loss = 12.814729690551758 400
val loss = 7.824796676635742
training loss = 10.561175346374512 500
val loss = 8.121458053588867
training loss = 8.943075180053711 600
val loss = 8.847795486450195
training loss = 7.785462379455566 700
val loss = 9.765966415405273
training loss = 6.957549095153809 800
val loss = 10.741159439086914
training loss = 6.365322589874268 900
val loss = 11.698915481567383
training loss = 5.941835880279541 1000
val loss = 12.599470138549805
training loss = 5.639395713806152 1100
val loss = 13.42288589477539
training loss = 5.423850059509277 1200
val loss = 14.160526275634766
training loss = 5.270583629608154 1300
val loss = 14.810483932495117
training loss = 5.161762237548828 1400
val loss = 15.374500274658203
training loss = 5.084409713745117 1500
val loss = 15.856460571289062
training loss = 5.029057025909424 1600
val loss = 16.26138687133789
training loss = 4.988780975341797 1700
val loss = 16.594722747802734
training loss = 4.958494186401367 1800
val loss = 16.861997604370117
training loss = 4.93444299697876 1900
val loss = 17.068357467651367
training loss = 4.9138031005859375 2000
val loss = 17.218366622924805
training loss = 4.894355773925781 2100
val loss = 17.31562042236328
training loss = 4.874232292175293 2200
val loss = 17.362239837646484
training loss = 4.851634979248047 2300
val loss = 17.358312606811523
training loss = 4.824570178985596 2400
val loss = 17.300739288330078
training loss = 4.790495872497559 2500
val loss = 17.181900024414062
training loss = 4.74587345123291 2600
val loss = 16.987178802490234
training loss = 4.685626029968262 2700
val loss = 16.692249298095703
training loss = 4.602715969085693 2800
val loss = 16.261157989501953
training loss = 4.488768100738525 2900
val loss = 15.651976585388184
training loss = 4.337167263031006 3000
val loss = 14.842063903808594
training loss = 4.147944450378418 3100
val loss = 13.872081756591797
training loss = 3.9277148246765137 3200
val loss = 12.849318504333496
training loss = 3.682899236679077 3300
val loss = 11.85554313659668
training loss = 3.4175639152526855 3400
val loss = 10.876612663269043
training loss = 3.1370468139648438 3500
val loss = 9.85697078704834
training loss = 2.8508174419403076 3600
val loss = 8.770492553710938
training loss = 2.574035406112671 3700
val loss = 7.637642860412598
training loss = 2.3265960216522217 3800
val loss = 6.5213823318481445
training loss = 2.127650022506714 3900
val loss = 5.512971878051758
training loss = 1.9866818189620972 4000
val loss = 4.698274612426758
training loss = 1.8982703685760498 4100
val loss = 4.118329048156738
training loss = 1.8466988801956177 4200
val loss = 3.757404088973999
training loss = 1.815813422203064 4300
val loss = 3.565551280975342
training loss = 1.794850468635559 4400
val loss = 3.4881772994995117
training loss = 1.778382420539856 4500
val loss = 3.4816577434539795
training loss = 1.764123797416687 4600
val loss = 3.516000747680664
training loss = 1.751190185546875 4700
val loss = 3.5723328590393066
training loss = 1.739230751991272 4800
val loss = 3.639529228210449
training loss = 1.7280889749526978 4900
val loss = 3.711371898651123
training loss = 1.7176886796951294 5000
val loss = 3.784473180770874
training loss = 1.7079757452011108 5100
val loss = 3.8571200370788574
training loss = 1.6989201307296753 5200
val loss = 3.9284000396728516
training loss = 1.6904888153076172 5300
val loss = 3.997854471206665
training loss = 1.6826585531234741 5400
val loss = 4.065077781677246
training loss = 1.6754063367843628 5500
val loss = 4.129934310913086
training loss = 1.6696487665176392 5600
val loss = 4.179800987243652
training loss = 1.6618568897247314 5700
val loss = 4.2587995529174805
training loss = 1.6558295488357544 5800
val loss = 4.315235614776611
training loss = 1.6500277519226074 5900
val loss = 4.381898880004883
training loss = 1.644625186920166 6000
val loss = 4.432027816772461
training loss = 1.6397897005081177 6100
val loss = 4.497821807861328
training loss = 1.6347745656967163 6200
val loss = 4.537661552429199
training loss = 1.6332799196243286 6300
val loss = 4.614950180053711
training loss = 1.626160740852356 6400
val loss = 4.635236740112305
training loss = 1.622329592704773 6500
val loss = 4.677186012268066
training loss = 1.6188428401947021 6600
val loss = 4.71707820892334
training loss = 1.6167088747024536 6700
val loss = 4.77971887588501
training loss = 1.612464189529419 6800
val loss = 4.793641090393066
training loss = 1.6097140312194824 6900
val loss = 4.826517105102539
training loss = 1.6073887348175049 7000
val loss = 4.874431610107422
training loss = 1.6046096086502075 7100
val loss = 4.890421390533447
training loss = 1.6024473905563354 7200
val loss = 4.916816711425781
training loss = 1.600519061088562 7300
val loss = 4.95387077331543
training loss = 1.5985054969787598 7400
val loss = 4.968087196350098
training loss = 1.609505295753479 7500
val loss = 5.063003063201904
training loss = 1.5951026678085327 7600
val loss = 5.013866424560547
training loss = 1.5936750173568726 7700
val loss = 5.031475067138672
training loss = 1.6267751455307007 7800
val loss = 5.176165580749512
training loss = 1.591006875038147 7900
val loss = 5.06849479675293
training loss = 1.5899015665054321 8000
val loss = 5.082674503326416
training loss = 1.6552183628082275 8100
val loss = 5.269164562225342
training loss = 1.58784818649292 8200
val loss = 5.1123857498168945
training loss = 1.5870074033737183 8300
val loss = 5.122915744781494
training loss = 1.587016224861145 8400
val loss = 5.15712833404541
training loss = 1.5853772163391113 8500
val loss = 5.146944046020508
training loss = 1.5847477912902832 8600
val loss = 5.154052734375
training loss = 1.5840131044387817 8700
val loss = 5.168143272399902
training loss = 1.583465576171875 8800
val loss = 5.174778938293457
training loss = 1.5882893800735474 8900
val loss = 5.232117176055908
training loss = 1.5823959112167358 9000
val loss = 5.191987037658691
training loss = 1.810673713684082 9100
val loss = 5.02055025100708
training loss = 1.5814768075942993 9200
val loss = 5.205915927886963
training loss = 1.581124186515808 9300
val loss = 5.211474895477295
training loss = 1.580910086631775 9400
val loss = 5.214325904846191
reduced chi^2 level 2 = 1.5804665088653564
Constrained alpha: 1.7393977642059326
Constrained beta: 4.7382683753967285
Constrained gamma: 22.105865478515625
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 875.7626,  869.5712,  965.3042,  898.9545,  998.7637, 1133.7500,
        1067.8510, 1097.5737, 1161.0905, 1238.4205, 1221.9213, 1269.7555,
        1261.6602, 1255.0568, 1355.5739, 1381.5162, 1373.5261, 1431.7301,
        1477.5323, 1498.3099, 1610.7356, 1569.9274, 1600.4381, 1582.9891,
        1623.0281, 1728.7216, 1690.6237, 1774.6337, 1651.8861, 1708.3695,
        1700.7675, 1738.6212, 1696.6956, 1736.7972, 1731.7858, 1750.6990,
        1667.6093, 1561.3604, 1636.2817, 1623.7518, 1692.8276, 1574.8271,
        1490.5986, 1523.3317, 1314.3635, 1356.6737, 1212.7933, 1242.5399,
        1178.6688, 1211.5122, 1074.9482,  992.5271,  985.0609,  854.8135,
         906.7181,  910.9746,  833.3685,  711.0259,  620.1938,  542.5391,
         575.8185,  499.7211,  475.0352,  412.3045,  331.3276,  352.5774,
         304.4236,  255.7069,  219.0937,  170.6383,  170.5863,  142.0819,
         135.3818,  117.6166,   92.2907,   72.0250,   37.7593,   55.9210,
          31.7710,   44.9760,   15.7045,   37.5077,   38.0391])]
2555.0949885847667
1.6062371405690086 1.8094615600076058 50.280309766660345
val isze = 8
idinces = [27 32 35 41 17 30  5 79 18 14 28 74 37 22 23 71 10 15 75 68 38 58 46 63
 53 62 20 13 80 48 77  8 65 40 31 29 57 34 59 39 50 52 26 33 43  7 42  4
 11 64  1 61 60  3 78 69 67  6 47 24 36 12  2 44 51  0 25 70 21 66 56 82
 19 72 45 76 73  9 55 16 49 81 54]
we are doing training validation split
training loss = 43.5229377746582 100
val loss = 56.10480499267578
training loss = 26.772459030151367 200
val loss = 36.75746154785156
training loss = 16.831546783447266 300
val loss = 24.72450065612793
training loss = 11.104069709777832 400
val loss = 17.94320297241211
training loss = 7.893733024597168 500
val loss = 14.075593948364258
training loss = 6.219784736633301 600
val loss = 11.88685417175293
training loss = 5.4351115226745605 700
val loss = 10.656953811645508
training loss = 5.018259525299072 800
val loss = 9.853023529052734
training loss = 4.680121421813965 900
val loss = 9.169973373413086
training loss = 4.352880954742432 1000
val loss = 8.517094612121582
training loss = 4.03153657913208 1100
val loss = 7.870767593383789
training loss = 3.7153377532958984 1200
val loss = 7.218385696411133
training loss = 3.4074010848999023 1300
val loss = 6.560357093811035
training loss = 3.1187326908111572 1400
val loss = 5.913015842437744
training loss = 2.8646981716156006 1500
val loss = 5.304790019989014
training loss = 2.6573941707611084 1600
val loss = 4.764939308166504
training loss = 2.500501871109009 1700
val loss = 4.311766624450684
training loss = 2.3893749713897705 1800
val loss = 3.9500644207000732
training loss = 2.3142621517181396 1900
val loss = 3.6738662719726562
training loss = 2.2641396522521973 2000
val loss = 3.4707162380218506
training loss = 2.229522228240967 2100
val loss = 3.3261098861694336
training loss = 2.2036352157592773 2200
val loss = 3.225435733795166
training loss = 2.1822946071624756 2300
val loss = 3.1567437648773193
training loss = 2.1631929874420166 2400
val loss = 3.1106948852539062
training loss = 2.14517879486084 2500
val loss = 3.0797317028045654
training loss = 2.127746343612671 2600
val loss = 3.0588951110839844
training loss = 2.1107304096221924 2700
val loss = 3.0445592403411865
training loss = 2.094132423400879 2800
val loss = 3.033557653427124
training loss = 2.0780985355377197 2900
val loss = 3.0013680458068848
training loss = 2.0624682903289795 3000
val loss = 3.0205235481262207
training loss = 2.050166606903076 3100
val loss = 2.871122121810913
training loss = 2.033231258392334 3200
val loss = 2.992501735687256
training loss = 2.030595064163208 3300
val loss = 3.2792863845825195
training loss = 2.0065488815307617 3400
val loss = 2.974616050720215
training loss = 2.004709482192993 3500
val loss = 3.2468020915985107
training loss = 1.9820798635482788 3600
val loss = 2.952929973602295
training loss = 1.9754692316055298 3700
val loss = 2.7663865089416504
training loss = 1.9593404531478882 3800
val loss = 2.930518627166748
training loss = 1.9502640962600708 3900
val loss = 2.8170599937438965
training loss = 1.9378918409347534 4000
val loss = 2.9134180545806885
training loss = 1.9312907457351685 4100
val loss = 3.0582098960876465
training loss = 1.9175914525985718 4200
val loss = 2.898609161376953
training loss = 1.9175456762313843 4300
val loss = 2.6586337089538574
training loss = 1.8984200954437256 4400
val loss = 2.8733320236206055
training loss = 1.8892427682876587 4500
val loss = 2.8723137378692627
training loss = 1.880521297454834 4600
val loss = 2.8973262310028076
training loss = 1.8717374801635742 4700
val loss = 2.8610339164733887
training loss = 1.8648709058761597 4800
val loss = 2.9473330974578857
training loss = 1.8553345203399658 4900
val loss = 2.8495140075683594
training loss = 1.8983510732650757 5000
val loss = 3.4486608505249023
training loss = 1.8399381637573242 5100
val loss = 2.833861827850342
training loss = 1.8342057466506958 5200
val loss = 2.7388057708740234
training loss = 1.8255072832107544 5300
val loss = 2.8258109092712402
training loss = 1.8185899257659912 5400
val loss = 2.8207101821899414
training loss = 1.8137379884719849 5500
val loss = 2.912869453430176
training loss = 1.8054866790771484 5600
val loss = 2.8097100257873535
training loss = 1.9201339483261108 5700
val loss = 2.194242477416992
training loss = 1.7931958436965942 5800
val loss = 2.8006110191345215
training loss = 1.7872625589370728 5900
val loss = 2.795022964477539
training loss = 1.7816659212112427 6000
val loss = 2.7940549850463867
training loss = 1.7761552333831787 6100
val loss = 2.784325122833252
training loss = 1.8127864599227905 6200
val loss = 3.2863669395446777
training loss = 1.7658135890960693 6300
val loss = 2.7743611335754395
training loss = 1.7608028650283813 6400
val loss = 2.7695531845092773
training loss = 1.7561800479888916 6500
val loss = 2.768524646759033
training loss = 1.7515804767608643 6600
val loss = 2.7634053230285645
training loss = 1.7473224401474 6700
val loss = 2.7442429065704346
training loss = 1.7430810928344727 6800
val loss = 2.756812572479248
training loss = 1.7664079666137695 6900
val loss = 3.137887954711914
training loss = 1.7353646755218506 7000
val loss = 2.7525172233581543
training loss = 1.7316784858703613 7100
val loss = 2.730381965637207
training loss = 1.7283581495285034 7200
val loss = 2.7525248527526855
training loss = 1.7249813079833984 7300
val loss = 2.742267608642578
training loss = 1.7232283353805542 7400
val loss = 2.668389320373535
training loss = 1.719089388847351 7500
val loss = 2.737797737121582
training loss = 1.7174475193023682 7600
val loss = 2.8110203742980957
training loss = 1.7139034271240234 7700
val loss = 2.7433879375457764
training loss = 1.7113672494888306 7800
val loss = 2.7323415279388428
training loss = 1.7266038656234741 7900
val loss = 3.0283150672912598
training loss = 1.706991195678711 8000
val loss = 2.7321102619171143
training loss = 1.704848289489746 8100
val loss = 2.729435443878174
training loss = 1.7038506269454956 8200
val loss = 2.664480447769165
training loss = 1.7011983394622803 8300
val loss = 2.7262439727783203
training loss = 1.6993573904037476 8400
val loss = 2.727631092071533
training loss = 1.6983126401901245 8500
val loss = 2.7672791481018066
training loss = 1.6962780952453613 8600
val loss = 2.7258191108703613
training loss = 2.1389968395233154 8700
val loss = 4.66914176940918
training loss = 1.693516731262207 8800
val loss = 2.7201685905456543
training loss = 1.6920636892318726 8900
val loss = 2.7262773513793945
training loss = 1.6915256977081299 9000
val loss = 2.776689052581787
training loss = 1.6895647048950195 9100
val loss = 2.725785732269287
training loss = 1.7051273584365845 9200
val loss = 2.99828839302063
training loss = 1.6872636079788208 9300
val loss = 2.726105213165283
training loss = 1.6859956979751587 9400
val loss = 2.7258529663085938
training loss = 1.6853289604187012 9500
val loss = 2.7552645206451416
training loss = 1.683967113494873 9600
val loss = 2.73075008392334
training loss = 1.8100862503051758 9700
val loss = 3.606088399887085
training loss = 1.6819735765457153 9800
val loss = 2.7403206825256348
training loss = 1.6808160543441772 9900
val loss = 2.735602855682373
training loss = 1.6842235326766968 10000
val loss = 2.6123547554016113
training loss = 1.6789348125457764 10100
val loss = 2.739539861679077
training loss = 1.677849531173706 10200
val loss = 2.7510745525360107
training loss = 1.6771961450576782 10300
val loss = 2.7631335258483887
training loss = 1.676040530204773 10400
val loss = 2.746262311935425
training loss = 1.6753664016723633 10500
val loss = 2.7353296279907227
training loss = 1.6743539571762085 10600
val loss = 2.7505786418914795
training loss = 1.6879754066467285 10700
val loss = 2.530125617980957
training loss = 1.6728535890579224 10800
val loss = 2.7527518272399902
training loss = 1.6719547510147095 10900
val loss = 2.7596683502197266
training loss = 1.6852171421051025 11000
val loss = 3.0092005729675293
training loss = 1.6706160306930542 11100
val loss = 2.763225793838501
training loss = 1.669797420501709 11200
val loss = 2.7688958644866943
training loss = 1.6694862842559814 11300
val loss = 2.785223960876465
training loss = 1.668672800064087 11400
val loss = 2.773439407348633
training loss = 1.6723923683166504 11500
val loss = 2.912295341491699
training loss = 1.6675573587417603 11600
val loss = 2.7672834396362305
training loss = 1.6668452024459839 11700
val loss = 2.782632350921631
training loss = 1.6667383909225464 11800
val loss = 2.7504491806030273
training loss = 1.6658339500427246 11900
val loss = 2.785503387451172
training loss = 1.6689728498458862 12000
val loss = 2.907942295074463
training loss = 1.6649140119552612 12100
val loss = 2.7912559509277344
training loss = 1.6655724048614502 12200
val loss = 2.724539279937744
training loss = 1.664079189300537 12300
val loss = 2.7898874282836914
training loss = 1.6635220050811768 12400
val loss = 2.796337604522705
training loss = 1.7007406949996948 12500
val loss = 3.214597702026367
training loss = 1.6627522706985474 12600
val loss = 2.797858238220215
reduced chi^2 level 2 = 1.6626948118209839
Constrained alpha: 1.884285569190979
Constrained beta: 1.13193941116333
Constrained gamma: 16.672740936279297
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 890.7152,  841.5985,  988.3345,  962.3269, 1031.9633, 1067.2898,
        1096.7820, 1149.4498, 1135.9896, 1180.5411, 1176.4463, 1180.7695,
        1232.1807, 1201.9557, 1357.5885, 1389.0527, 1428.6475, 1453.3369,
        1502.5875, 1427.5389, 1579.2063, 1529.2882, 1589.2312, 1642.7009,
        1655.3038, 1722.6508, 1654.5546, 1767.2689, 1712.6696, 1722.4755,
        1704.3463, 1687.3326, 1648.9269, 1730.4576, 1668.4491, 1786.7583,
        1653.8282, 1641.4241, 1560.3656, 1614.9430, 1679.2249, 1459.9672,
        1518.0432, 1465.5999, 1353.1838, 1419.6349, 1343.4019, 1177.3169,
        1194.0829, 1205.7439, 1089.8059,  969.8571,  987.1437,  906.9384,
         942.2738,  869.0734,  812.8287,  701.1360,  564.5224,  531.0622,
         569.8536,  483.6148,  441.5108,  372.1151,  354.6143,  326.3665,
         298.1954,  258.4158,  215.5911,  175.5234,  169.6836,  124.1389,
         139.3082,  111.4836,   95.5755,   56.4836,   59.1082,   57.1375,
          38.1645,   49.0077,   20.0941,   40.0285,   32.4444])]
2559.8680150435657
1.6104107479704215 12.034012051054079 64.38165778880675
val isze = 8
idinces = [32 49 35 16  5 45  1 46  6 68 33 11 80 72 82 25  8 14 10 54 43 65 66 48
 81 23 73 47  9 24 27 44 61 56 20 36  2 71 77 12 76 19 52 26 55 53 39 28
 79 31 51 18 15 57 63 60  7 70 13 40 62 17 64  4 34 41 67 59  0 58 30 50
 42 37 69 22 78 75 38 74 21 29  3]
we are doing training validation split
training loss = 82.14031219482422 100
val loss = 124.63691711425781
training loss = 6.480447292327881 200
val loss = 6.440295219421387
training loss = 6.458429336547852 300
val loss = 6.695797920227051
training loss = 6.441742420196533 400
val loss = 6.727657794952393
training loss = 6.422876358032227 500
val loss = 6.7583537101745605
training loss = 6.402116775512695 600
val loss = 6.7873735427856445
training loss = 6.379671573638916 700
val loss = 6.8133931159973145
training loss = 6.355714797973633 800
val loss = 6.83553409576416
training loss = 6.33039665222168 900
val loss = 6.853178024291992
training loss = 6.30385684967041 1000
val loss = 6.8660569190979
training loss = 6.2762322425842285 1100
val loss = 6.874128818511963
training loss = 6.2476606369018555 1200
val loss = 6.877644062042236
training loss = 6.218289852142334 1300
val loss = 6.87687873840332
training loss = 6.188262462615967 1400
val loss = 6.872308731079102
training loss = 6.157730579376221 1500
val loss = 6.864427089691162
training loss = 6.126840114593506 1600
val loss = 6.85385799407959
training loss = 6.095730304718018 1700
val loss = 6.841049671173096
training loss = 6.064521789550781 1800
val loss = 6.826589107513428
training loss = 6.03331184387207 1900
val loss = 6.810973167419434
training loss = 6.00214958190918 2000
val loss = 6.794628143310547
training loss = 5.971022129058838 2100
val loss = 6.778044700622559
training loss = 5.939821243286133 2200
val loss = 6.761640548706055
training loss = 5.908291339874268 2300
val loss = 6.745745658874512
training loss = 5.875952243804932 2400
val loss = 6.730746269226074
training loss = 5.841943264007568 2500
val loss = 6.7170281410217285
training loss = 5.804721832275391 2600
val loss = 6.70505952835083
training loss = 5.761356830596924 2700
val loss = 6.69576358795166
training loss = 5.7059736251831055 2800
val loss = 6.691049575805664
training loss = 5.626272201538086 2900
val loss = 6.695217132568359
training loss = 5.497820854187012 3000
val loss = 6.714485168457031
training loss = 5.278194904327393 3100
val loss = 6.739055633544922
training loss = 4.878234386444092 3200
val loss = 6.70686674118042
training loss = 4.166397571563721 3300
val loss = 6.463260650634766
training loss = 3.2579421997070312 3400
val loss = 5.640805721282959
training loss = 2.396226644515991 3500
val loss = 4.525363922119141
training loss = 1.936866283416748 3600
val loss = 3.585879325866699
training loss = 1.8342822790145874 3700
val loss = 3.1377978324890137
training loss = 1.8205021619796753 3800
val loss = 3.0017263889312744
training loss = 1.816312551498413 3900
val loss = 2.9653737545013428
training loss = 1.8132456541061401 4000
val loss = 2.9528648853302
training loss = 1.8105247020721436 4100
val loss = 2.9458301067352295
training loss = 1.8080129623413086 4200
val loss = 2.940504789352417
training loss = 1.8056573867797852 4300
val loss = 2.936143398284912
training loss = 1.8034263849258423 4400
val loss = 2.932539701461792
training loss = 1.8012933731079102 4500
val loss = 2.9295356273651123
training loss = 1.799241542816162 4600
val loss = 2.926981210708618
training loss = 1.7972595691680908 4700
val loss = 2.9247982501983643
training loss = 1.7953426837921143 4800
val loss = 2.922879695892334
training loss = 1.7935229539871216 4900
val loss = 2.9250025749206543
training loss = 1.7915902137756348 5000
val loss = 2.91542911529541
training loss = 1.7898136377334595 5100
val loss = 2.9141900539398193
training loss = 1.7879687547683716 5200
val loss = 2.913186550140381
training loss = 1.786255121231079 5300
val loss = 2.909320116043091
training loss = 1.7845432758331299 5400
val loss = 2.908499002456665
training loss = 1.7828636169433594 5500
val loss = 2.9023642539978027
training loss = 1.7918001413345337 5600
val loss = 2.979419469833374
training loss = 1.7797274589538574 5700
val loss = 2.8961031436920166
training loss = 1.7782841920852661 5800
val loss = 2.893831729888916
training loss = 1.8176839351654053 5900
val loss = 2.8223540782928467
training loss = 1.7754749059677124 6000
val loss = 2.8883886337280273
training loss = 1.774191975593567 6100
val loss = 2.8862128257751465
training loss = 1.7958500385284424 6200
val loss = 3.0058863162994385
training loss = 1.7716929912567139 6300
val loss = 2.8810300827026367
training loss = 1.7705250978469849 6400
val loss = 2.880617618560791
training loss = 1.7697707414627075 6500
val loss = 2.8656160831451416
training loss = 1.7681987285614014 6600
val loss = 2.87728214263916
training loss = 2.129774570465088 6700
val loss = 2.9946537017822266
training loss = 1.7659035921096802 6800
val loss = 2.875706672668457
training loss = 1.7647732496261597 6900
val loss = 2.8743302822113037
training loss = 1.7640025615692139 7000
val loss = 2.8841567039489746
training loss = 1.7623955011367798 7100
val loss = 2.873436450958252
training loss = 1.7611805200576782 7200
val loss = 2.8726539611816406
training loss = 1.9379836320877075 7300
val loss = 2.843230724334717
training loss = 1.7585164308547974 7400
val loss = 2.8698699474334717
training loss = 1.757093906402588 7500
val loss = 2.8731040954589844
training loss = 2.1532294750213623 7600
val loss = 3.845484972000122
training loss = 1.7539085149765015 7700
val loss = 2.8745265007019043
training loss = 1.7521865367889404 7800
val loss = 2.8761801719665527
training loss = 1.7585362195968628 7900
val loss = 2.8277223110198975
training loss = 1.7483700513839722 8000
val loss = 2.879305839538574
training loss = 1.779504656791687 8100
val loss = 2.801919460296631
training loss = 1.7442092895507812 8200
val loss = 2.8817718029022217
training loss = 1.7420709133148193 8300
val loss = 2.8861637115478516
training loss = 1.7418880462646484 8400
val loss = 2.923083543777466
training loss = 1.7376961708068848 8500
val loss = 2.8905832767486572
training loss = 1.7391459941864014 8600
val loss = 2.940523624420166
training loss = 1.733413577079773 8700
val loss = 2.8969290256500244
training loss = 1.7313346862792969 8800
val loss = 2.8950161933898926
training loss = 1.7670669555664062 8900
val loss = 2.803947925567627
training loss = 1.727296233177185 9000
val loss = 2.897942066192627
training loss = 1.725393533706665 9100
val loss = 2.903219699859619
training loss = 1.7234694957733154 9200
val loss = 2.9028007984161377
training loss = 1.7216136455535889 9300
val loss = 2.9003207683563232
training loss = 1.7200040817260742 9400
val loss = 2.9129891395568848
training loss = 1.718072533607483 9500
val loss = 2.90228533744812
training loss = 1.716370701789856 9600
val loss = 2.906844139099121
training loss = 1.714707851409912 9700
val loss = 2.899845838546753
training loss = 1.7130632400512695 9800
val loss = 2.906325340270996
training loss = 1.759384036064148 9900
val loss = 3.1437466144561768
training loss = 1.7098987102508545 10000
val loss = 2.9101552963256836
training loss = 1.7083665132522583 10100
val loss = 2.9112021923065186
training loss = 1.7073144912719727 10200
val loss = 2.895768880844116
training loss = 1.705411672592163 10300
val loss = 2.9151721000671387
training loss = 1.7132821083068848 10400
val loss = 3.002131700515747
training loss = 1.7025734186172485 10500
val loss = 2.9212167263031006
training loss = 1.701198697090149 10600
val loss = 2.921638250350952
training loss = 1.7004563808441162 10700
val loss = 2.9445853233337402
training loss = 1.698546290397644 10800
val loss = 2.926342487335205
training loss = 1.7114721536636353 10900
val loss = 3.043076992034912
training loss = 1.6960093975067139 11000
val loss = 2.9311022758483887
training loss = 1.6947927474975586 11100
val loss = 2.9366886615753174
training loss = 1.6936872005462646 11200
val loss = 2.9275693893432617
training loss = 1.692447543144226 11300
val loss = 2.9385905265808105
training loss = 1.6933603286743164 11400
val loss = 2.9799141883850098
training loss = 1.6902097463607788 11500
val loss = 2.9441967010498047
training loss = 1.689422607421875 11600
val loss = 2.9316883087158203
training loss = 1.6881227493286133 11700
val loss = 2.9429588317871094
training loss = 1.687071681022644 11800
val loss = 2.9516782760620117
training loss = 1.6984937191009521 11900
val loss = 3.0694539546966553
training loss = 1.6851170063018799 12000
val loss = 2.956331491470337
training loss = 1.6841715574264526 12100
val loss = 2.959407091140747
training loss = 1.6845272779464722 12200
val loss = 2.9950003623962402
training loss = 1.6823680400848389 12300
val loss = 2.9644393920898438
reduced chi^2 level 2 = 1.682315707206726
Constrained alpha: 1.9254088401794434
Constrained beta: 2.897702693939209
Constrained gamma: 17.722238540649414
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 848.1777,  858.2507,  952.1829,  957.8898, 1029.8304, 1032.7794,
        1117.9119, 1145.1893, 1177.1495, 1203.2634, 1230.1245, 1163.0448,
        1270.4910, 1219.5128, 1311.3521, 1394.1095, 1355.5027, 1541.9550,
        1593.6888, 1471.3348, 1589.4908, 1500.8754, 1641.9529, 1648.9766,
        1665.3616, 1709.4562, 1564.2416, 1750.2939, 1718.6183, 1682.1046,
        1656.5502, 1792.7570, 1775.3152, 1672.2814, 1677.2439, 1763.5387,
        1640.9186, 1632.6599, 1701.8529, 1555.3735, 1595.5956, 1563.0878,
        1548.1475, 1516.3389, 1431.8320, 1310.0244, 1274.7916, 1260.1486,
        1164.4386, 1142.3931, 1120.1741,  945.2482,  905.6565,  939.9077,
         892.0861,  879.9803,  797.7921,  748.4726,  602.7333,  554.4862,
         549.0646,  461.1414,  429.4447,  377.7764,  333.1241,  364.3427,
         273.0613,  252.8940,  207.4959,  157.5625,  157.7764,  140.8440,
         161.0791,  117.5733,  117.1397,   62.5538,   50.9957,   41.5381,
          42.9059,   43.9611,   18.6002,   29.9234,   51.8078])]
3013.9143213523553
4.098853753341397 12.995466254282992 70.89409589747918
val isze = 8
idinces = [61 80 12 16 27  9 30 26 60 74 79 51  2 82 67 48 21 72 10 55 76 81 65 40
 58 13 24  3 54 38 52 36  6 75 14  7 57 28  4 45 49 39 37 22 33 64 42 50
 43 19  8  0 71  5 32 18 77 31 23 62 20 69 70 25 11 41 68 17 47 46 29  1
 35 53 44 66 56 63 34 78 59 15 73]
we are doing training validation split
training loss = 286.923828125 100
val loss = 118.349609375
training loss = 7.99117374420166 200
val loss = 4.0564284324646
training loss = 7.958271503448486 300
val loss = 3.9888648986816406
training loss = 7.929633140563965 400
val loss = 3.993215560913086
training loss = 7.898706436157227 500
val loss = 3.997243881225586
training loss = 7.86641788482666 600
val loss = 4.001255512237549
training loss = 7.8334808349609375 700
val loss = 4.005044460296631
training loss = 7.800456523895264 800
val loss = 4.008425712585449
training loss = 7.767766952514648 900
val loss = 4.011275768280029
training loss = 7.735718727111816 1000
val loss = 4.013434410095215
training loss = 7.704524993896484 1100
val loss = 4.014865875244141
training loss = 7.674306869506836 1200
val loss = 4.015506744384766
training loss = 7.645140171051025 1300
val loss = 4.015379905700684
training loss = 7.617039680480957 1400
val loss = 4.014503002166748
training loss = 7.589995384216309 1500
val loss = 4.012932777404785
training loss = 7.5639567375183105 1600
val loss = 4.0107269287109375
training loss = 7.538870811462402 1700
val loss = 4.007966995239258
training loss = 7.514666557312012 1800
val loss = 4.0046892166137695
training loss = 7.4912614822387695 1900
val loss = 4.001014232635498
training loss = 7.468578338623047 2000
val loss = 3.9969444274902344
training loss = 7.446530342102051 2100
val loss = 3.992554187774658
training loss = 7.425039291381836 2200
val loss = 3.987884044647217
training loss = 7.40402364730835 2300
val loss = 3.982962131500244
training loss = 7.3834075927734375 2400
val loss = 3.9778237342834473
training loss = 7.363124370574951 2500
val loss = 3.9725093841552734
training loss = 7.343113899230957 2600
val loss = 3.9669718742370605
training loss = 7.323330402374268 2700
val loss = 3.9613089561462402
training loss = 7.303738117218018 2800
val loss = 3.9554929733276367
training loss = 7.284326553344727 2900
val loss = 3.949547290802002
training loss = 7.2651190757751465 3000
val loss = 3.9434823989868164
training loss = 7.246179580688477 3100
val loss = 3.937438488006592
training loss = 7.227625370025635 3200
val loss = 3.931370258331299
training loss = 7.2096405029296875 3300
val loss = 3.9254183769226074
training loss = 7.192490577697754 3400
val loss = 3.9198179244995117
training loss = 7.176496982574463 3500
val loss = 3.9147262573242188
training loss = 7.162021636962891 3600
val loss = 3.909817695617676
training loss = 7.1497111320495605 3700
val loss = 3.906649589538574
training loss = 7.139616012573242 3800
val loss = 3.9025449752807617
training loss = 7.132208824157715 3900
val loss = 3.9114906787872314
training loss = 7.125821113586426 4000
val loss = 3.898977041244507
training loss = 7.139261245727539 4100
val loss = 3.851057529449463
training loss = 7.118159294128418 4200
val loss = 3.898735523223877
training loss = 7.164456367492676 4300
val loss = 4.07518196105957
training loss = 7.113543510437012 4400
val loss = 3.8977532386779785
training loss = 7.111536026000977 4500
val loss = 3.899085760116577
training loss = 7.109408378601074 4600
val loss = 3.9070894718170166
training loss = 7.105665683746338 4700
val loss = 3.897305727005005
training loss = 7.100113868713379 4800
val loss = 3.9143242835998535
training loss = 7.0824055671691895 4900
val loss = 3.888474702835083
training loss = 7.13303804397583 5000
val loss = 4.191268444061279
training loss = 6.634635925292969 5100
val loss = 3.7471871376037598
training loss = 5.918336868286133 5200
val loss = 3.9180145263671875
training loss = 3.781242847442627 5300
val loss = 2.2252142429351807
training loss = 2.594224214553833 5400
val loss = 1.7955386638641357
training loss = 2.556427478790283 5500
val loss = 1.8860423564910889
training loss = 2.545898199081421 5600
val loss = 2.0320053100585938
training loss = 2.4892728328704834 5700
val loss = 1.9477262496948242
training loss = 2.4092934131622314 5800
val loss = 2.054556131362915
training loss = 2.3483448028564453 5900
val loss = 2.187093496322632
training loss = 2.3115735054016113 6000
val loss = 2.242189407348633
training loss = 2.283261299133301 6100
val loss = 2.2975220680236816
training loss = 2.2593302726745605 6200
val loss = 2.3157334327697754
training loss = 2.5641860961914062 6300
val loss = 3.603855609893799
training loss = 2.2204432487487793 6400
val loss = 2.383456230163574
training loss = 2.2044217586517334 6500
val loss = 2.4135003089904785
training loss = 2.1908047199249268 6600
val loss = 2.4085636138916016
training loss = 2.1781582832336426 6700
val loss = 2.475294589996338
training loss = 2.4052023887634277 6800
val loss = 1.983996033668518
training loss = 2.158025026321411 6900
val loss = 2.525815486907959
training loss = 2.149693250656128 7000
val loss = 2.549264430999756
training loss = 2.142817258834839 7100
val loss = 2.594095468521118
training loss = 2.136516571044922 7200
val loss = 2.605135440826416
training loss = 2.132460594177246 7300
val loss = 2.5650339126586914
training loss = 2.126617908477783 7400
val loss = 2.643988609313965
training loss = 2.1279852390289307 7500
val loss = 2.7978057861328125
training loss = 2.119265556335449 7600
val loss = 2.676220178604126
training loss = 2.116196632385254 7700
val loss = 2.69204044342041
training loss = 2.117007255554199 7800
val loss = 2.810576915740967
training loss = 2.111346960067749 7900
val loss = 2.7162282466888428
training loss = 2.109269618988037 8000
val loss = 2.7273013591766357
training loss = 2.10910964012146 8100
val loss = 2.805868625640869
training loss = 2.1059932708740234 8200
val loss = 2.7436790466308594
training loss = 2.104541540145874 8300
val loss = 2.7518715858459473
training loss = 2.1051628589630127 8400
val loss = 2.679187774658203
training loss = 2.102149248123169 8500
val loss = 2.7650623321533203
training loss = 2.1446444988250732 8600
val loss = 3.2138655185699463
training loss = 2.10019588470459 8700
val loss = 2.7761378288269043
training loss = 2.099330425262451 8800
val loss = 2.7709898948669434
training loss = 2.0986454486846924 8900
val loss = 2.766127109527588
training loss = 2.0977864265441895 9000
val loss = 2.78834867477417
training loss = 2.097998857498169 9100
val loss = 2.736264944076538
training loss = 2.096484422683716 9200
val loss = 2.7955422401428223
training loss = 2.12222957611084 9300
val loss = 2.5218002796173096
training loss = 2.0953567028045654 9400
val loss = 2.7988009452819824
training loss = 2.0948033332824707 9500
val loss = 2.801069736480713
training loss = 2.0945544242858887 9600
val loss = 2.7786853313446045
training loss = 2.09384822845459 9700
val loss = 2.80887508392334
training loss = 2.1046628952026367 9800
val loss = 3.0231900215148926
training loss = 2.0929977893829346 9900
val loss = 2.810523509979248
training loss = 2.095893383026123 10000
val loss = 2.9301187992095947
training loss = 2.0922598838806152 10100
val loss = 2.8256571292877197
training loss = 2.09181809425354 10200
val loss = 2.8172645568847656
training loss = 2.091968536376953 10300
val loss = 2.857226848602295
(1, 75)
(1, 8)
