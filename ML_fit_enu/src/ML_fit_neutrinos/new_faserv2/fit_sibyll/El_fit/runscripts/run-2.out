84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 872.5817,  838.1095,  953.9719,  939.7217, 1006.3277, 1050.4764,
        1103.9094, 1096.8499, 1111.3010, 1239.7316, 1230.6312, 1147.5750,
        1262.1163, 1249.3718, 1381.0125, 1486.7692, 1415.2020, 1466.2479,
        1521.0435, 1550.0044, 1603.4237, 1526.4518, 1668.8402, 1653.7694,
        1738.9879, 1707.2683, 1598.5278, 1750.5800, 1695.5415, 1739.5999,
        1677.8972, 1753.9960, 1693.3877, 1739.8901, 1699.8273, 1742.3174,
        1676.7334, 1548.1543, 1624.1620, 1565.1864, 1607.1013, 1527.5941,
        1456.8900, 1557.5443, 1327.3865, 1346.1309, 1312.4969, 1215.4432,
        1179.0793, 1175.8090, 1087.1497, 1019.9555,  983.6621,  960.9086,
         886.0530,  872.7026,  816.5308,  681.2427,  592.7288,  570.3052,
         590.9748,  466.3490,  467.8955,  403.3233,  378.7905,  340.1350,
         286.8599,  261.2844,  207.0678,  159.2089,  182.3639,  140.0930,
         162.4893,  102.1697,  110.0387,   65.9794,   55.3723,   39.1397,
          30.7360,   36.5040,   17.3718,   40.8443,   34.9595])]
2435.0483074282092
3.3307020781542414 12.05829643786695 23.297493345997644
val isze = 8
idinces = [22 34 24  1 69 28 47 81 43 35 16 82 36 44  6 40 67 74 33 61  3 58 60 80
 54 15 32 45 65 30 20 13 57 73 59 56 70 19 75 72 41 62 10 51 23 18 53 11
 52 50  2 26 25 31 46 17 63 42 66 68 64  8 37 21 48 71 14  9 55 49 12 29
  7  5 39 27 38 79 78 77  0 76  4]
we are doing training validation split
training loss = 359.7444763183594 100
val loss = 364.7789611816406
training loss = 8.501349449157715 200
val loss = 20.175655364990234
training loss = 7.524189472198486 300
val loss = 18.335914611816406
training loss = 6.818591594696045 400
val loss = 16.78302764892578
training loss = 6.302577972412109 500
val loss = 15.506046295166016
training loss = 5.942694187164307 600
val loss = 14.49514102935791
training loss = 5.700634956359863 700
val loss = 13.721831321716309
training loss = 5.539494514465332 800
val loss = 13.146173477172852
training loss = 5.4282636642456055 900
val loss = 12.724865913391113
training loss = 5.344170093536377 1000
val loss = 12.417830467224121
training loss = 5.271718978881836 1100
val loss = 12.190803527832031
training loss = 5.197304725646973 1200
val loss = 12.012008666992188
training loss = 5.084954738616943 1300
val loss = 11.827327728271484
training loss = 4.610983848571777 1400
val loss = 11.404650688171387
training loss = 3.6127068996429443 1500
val loss = 10.085501670837402
training loss = 2.6870431900024414 1600
val loss = 8.434286117553711
training loss = 2.1319580078125 1700
val loss = 7.191761016845703
training loss = 1.9179232120513916 1800
val loss = 6.456987380981445
training loss = 1.8551058769226074 1900
val loss = 6.113092422485352
training loss = 1.836104393005371 2000
val loss = 5.948572158813477
training loss = 1.8275675773620605 2100
val loss = 5.865474224090576
training loss = 1.8218181133270264 2200
val loss = 5.814444541931152
training loss = 1.8168506622314453 2300
val loss = 5.769850730895996
training loss = 1.8123096227645874 2400
val loss = 5.730329990386963
training loss = 1.8080389499664307 2500
val loss = 5.692730903625488
training loss = 1.803955078125 2600
val loss = 5.654026508331299
training loss = 1.8001255989074707 2700
val loss = 5.620358467102051
training loss = 1.7964211702346802 2800
val loss = 5.58351993560791
training loss = 1.7930251359939575 2900
val loss = 5.552019119262695
training loss = 1.7897127866744995 3000
val loss = 5.519061088562012
training loss = 1.7866909503936768 3100
val loss = 5.489884376525879
training loss = 1.7909191846847534 3200
val loss = 5.449416160583496
training loss = 1.7810758352279663 3300
val loss = 5.435359001159668
training loss = 1.7786682844161987 3400
val loss = 5.414727210998535
training loss = 1.7761117219924927 3500
val loss = 5.388092994689941
training loss = 1.7739288806915283 3600
val loss = 5.369118690490723
training loss = 1.7717413902282715 3700
val loss = 5.347349166870117
training loss = 1.7751858234405518 3800
val loss = 5.319974899291992
training loss = 1.7679134607315063 3900
val loss = 5.311987400054932
training loss = 1.7662516832351685 4000
val loss = 5.298650741577148
training loss = 1.7646095752716064 4100
val loss = 5.282489776611328
training loss = 1.7631748914718628 4200
val loss = 5.270451545715332
training loss = 1.7623926401138306 4300
val loss = 5.2562031745910645
training loss = 1.760580062866211 4400
val loss = 5.247750282287598
training loss = 1.7667503356933594 4500
val loss = 5.261728763580322
training loss = 1.758375883102417 4600
val loss = 5.228377342224121
training loss = 1.757982611656189 4700
val loss = 5.224234580993652
training loss = 1.7564982175827026 4800
val loss = 5.211604595184326
training loss = 1.7556657791137695 4900
val loss = 5.204004287719727
training loss = 1.755249261856079 5000
val loss = 5.201674461364746
training loss = 1.7541186809539795 5100
val loss = 5.189785957336426
training loss = 1.860792875289917 5200
val loss = 5.3557233810424805
training loss = 1.7527203559875488 5300
val loss = 5.176774978637695
training loss = 1.7520883083343506 5400
val loss = 5.170495986938477
training loss = 1.7723394632339478 5500
val loss = 5.213261604309082
training loss = 1.7508327960968018 5600
val loss = 5.158707618713379
training loss = 1.7502645254135132 5700
val loss = 5.152736663818359
training loss = 1.7503793239593506 5800
val loss = 5.153390407562256
training loss = 1.7491105794906616 5900
val loss = 5.141636371612549
training loss = 1.7525300979614258 6000
val loss = 5.131119728088379
training loss = 1.7480058670043945 6100
val loss = 5.1309709548950195
training loss = 1.7475173473358154 6200
val loss = 5.126255989074707
training loss = 1.7470250129699707 6300
val loss = 5.1225738525390625
training loss = 1.7464510202407837 6400
val loss = 5.115330219268799
training loss = 1.7473161220550537 6500
val loss = 5.119561672210693
training loss = 1.7454452514648438 6600
val loss = 5.105568885803223
training loss = 1.7543420791625977 6700
val loss = 5.0967535972595215
training loss = 1.7444812059402466 6800
val loss = 5.096696853637695
training loss = 1.7440342903137207 6900
val loss = 5.091764450073242
training loss = 1.743554711341858 7000
val loss = 5.0870819091796875
training loss = 1.7431124448776245 7100
val loss = 5.082444190979004
training loss = 1.7427234649658203 7200
val loss = 5.078739643096924
training loss = 1.742232322692871 7300
val loss = 5.073995113372803
training loss = 1.7421413660049438 7400
val loss = 5.0726494789123535
training loss = 1.7414288520812988 7500
val loss = 5.0673112869262695
training loss = 1.7410099506378174 7600
val loss = 5.061796188354492
training loss = 1.7406014204025269 7700
val loss = 5.059813022613525
training loss = 1.740228295326233 7800
val loss = 5.054204940795898
training loss = 1.8346720933914185 7900
val loss = 5.201251983642578
training loss = 1.7394869327545166 8000
val loss = 5.046863555908203
training loss = 1.7392522096633911 8100
val loss = 5.044866561889648
training loss = 1.7388192415237427 8200
val loss = 5.03964900970459
training loss = 1.7384599447250366 8300
val loss = 5.036603927612305
training loss = 1.738221526145935 8400
val loss = 5.033719539642334
training loss = 1.7378084659576416 8500
val loss = 5.030353546142578
training loss = 1.7395427227020264 8600
val loss = 5.023548126220703
training loss = 1.7371915578842163 8700
val loss = 5.024263858795166
training loss = 1.7369176149368286 8800
val loss = 5.021300315856934
training loss = 1.7366312742233276 8900
val loss = 5.02016544342041
training loss = 1.736355185508728 9000
val loss = 5.015781402587891
training loss = 1.8546271324157715 9100
val loss = 5.0953192710876465
training loss = 1.7358113527297974 9200
val loss = 5.010732650756836
training loss = 1.7355823516845703 9300
val loss = 5.00808048248291
training loss = 1.8759965896606445 9400
val loss = 5.111494541168213
training loss = 1.735108733177185 9500
val loss = 5.004005432128906
training loss = 1.734887719154358 9600
val loss = 5.000840187072754
training loss = 1.735044240951538 9700
val loss = 5.00294828414917
training loss = 1.7344423532485962 9800
val loss = 4.996578216552734
training loss = 1.755772352218628 9900
val loss = 5.039717197418213
training loss = 1.7340267896652222 10000
val loss = 4.992527961730957
training loss = 1.733849287033081 10100
val loss = 4.990290641784668
training loss = 1.734409213066101 10200
val loss = 4.986828327178955
training loss = 1.7334712743759155 10300
val loss = 4.9865264892578125
training loss = 1.8236610889434814 10400
val loss = 5.128121376037598
training loss = 1.7331193685531616 10500
val loss = 4.982927322387695
training loss = 1.734397053718567 10600
val loss = 4.977200031280518
training loss = 1.732791781425476 10700
val loss = 4.9801177978515625
training loss = 1.732649326324463 10800
val loss = 4.977724075317383
training loss = 1.7327336072921753 10900
val loss = 4.976142883300781
training loss = 1.7323365211486816 11000
val loss = 4.974761962890625
training loss = 1.7322179079055786 11100
val loss = 4.973128795623779
training loss = 1.7331331968307495 11200
val loss = 4.979045867919922
training loss = 1.7319390773773193 11300
val loss = 4.970322608947754
training loss = 1.731995701789856 11400
val loss = 4.970795154571533
training loss = 1.7317287921905518 11500
val loss = 4.969140529632568
training loss = 1.7315765619277954 11600
val loss = 4.966216087341309
training loss = 1.731768012046814 11700
val loss = 4.969693183898926
training loss = 1.7313436269760132 11800
val loss = 4.963838577270508
training loss = 1.812869906425476 11900
val loss = 5.01237678527832
training loss = 1.7311214208602905 12000
val loss = 4.961509704589844
training loss = 1.7310429811477661 12100
val loss = 4.959742546081543
training loss = 1.731002688407898 12200
val loss = 4.958281993865967
training loss = 1.730836033821106 12300
val loss = 4.957967281341553
training loss = 1.732897162437439 12400
val loss = 4.953890800476074
training loss = 1.7306469678878784 12500
val loss = 4.955950736999512
training loss = 1.775048851966858 12600
val loss = 4.975447654724121
training loss = 1.730462908744812 12700
val loss = 4.954115867614746
training loss = 1.730398416519165 12800
val loss = 4.952994346618652
training loss = 1.730454921722412 12900
val loss = 4.9545416831970215
training loss = 1.7302348613739014 13000
val loss = 4.951127052307129
training loss = 1.7395858764648438 13100
val loss = 4.947782516479492
training loss = 1.7300769090652466 13200
val loss = 4.949629783630371
training loss = 1.7300442457199097 13300
val loss = 4.947771072387695
training loss = 1.729983925819397 13400
val loss = 4.949408054351807
training loss = 1.7298779487609863 13500
val loss = 4.946915626525879
training loss = 1.7352055311203003 13600
val loss = 4.942761421203613
training loss = 1.7297412157058716 13700
val loss = 4.9454851150512695
training loss = 1.82766854763031 13800
val loss = 5.011092185974121
training loss = 1.7296133041381836 13900
val loss = 4.944167137145996
training loss = 1.7295759916305542 14000
val loss = 4.943301200866699
training loss = 1.729645848274231 14100
val loss = 4.945005416870117
training loss = 1.729451298713684 14200
val loss = 4.941816329956055
training loss = 1.7439303398132324 14300
val loss = 4.941418647766113
training loss = 1.7293367385864258 14400
val loss = 4.940668106079102
training loss = 1.7293038368225098 14500
val loss = 4.939908027648926
training loss = 1.729346513748169 14600
val loss = 4.938338279724121
training loss = 1.729191541671753 14700
val loss = 4.938614845275879
training loss = 1.7644275426864624 14800
val loss = 4.949851989746094
training loss = 1.7290974855422974 14900
val loss = 4.937955856323242
training loss = 1.7290617227554321 15000
val loss = 4.936731815338135
training loss = 1.7290502786636353 15100
val loss = 4.935876369476318
training loss = 1.7289575338363647 15200
val loss = 4.935796737670898
training loss = 1.7289329767227173 15300
val loss = 4.934986114501953
training loss = 1.7288665771484375 15400
val loss = 4.935174942016602
training loss = 1.7288422584533691 15500
val loss = 4.934129238128662
training loss = 1.7534884214401245 15600
val loss = 4.983791351318359
training loss = 1.7287516593933105 15700
val loss = 4.9331769943237305
training loss = 1.7287300825119019 15800
val loss = 4.932503700256348
training loss = 1.7287052869796753 15900
val loss = 4.932018756866455
training loss = 1.7286497354507446 16000
val loss = 4.9316253662109375
training loss = 1.7475769519805908 16100
val loss = 4.930744171142578
training loss = 1.728563666343689 16200
val loss = 4.930866241455078
training loss = 1.7285466194152832 16300
val loss = 4.930191993713379
training loss = 1.729127287864685 16400
val loss = 4.935511112213135
training loss = 1.72847580909729 16500
val loss = 4.929326057434082
training loss = 1.7335734367370605 16600
val loss = 4.9238128662109375
training loss = 1.728403925895691 16700
val loss = 4.928607940673828
training loss = 1.7955681085586548 16800
val loss = 4.966928482055664
training loss = 1.7283390760421753 16900
val loss = 4.9281134605407715
training loss = 1.728326678276062 17000
val loss = 4.927389144897461
training loss = 1.728386640548706 17100
val loss = 4.92576265335083
training loss = 1.7282575368881226 17200
val loss = 4.926341533660889
training loss = 1.728222131729126 17300
val loss = 4.926128387451172
training loss = 1.7281919717788696 17400
val loss = 4.925606727600098
training loss = 1.7478294372558594 17500
val loss = 4.925845146179199
training loss = 1.7281291484832764 17600
val loss = 4.924925804138184
training loss = 1.7281222343444824 17700
val loss = 4.923871994018555
training loss = 1.7281147241592407 17800
val loss = 4.925537109375
training loss = 1.7280542850494385 17900
val loss = 4.923596382141113
training loss = 1.735515832901001 18000
val loss = 4.945291519165039
training loss = 1.7279890775680542 18100
val loss = 4.922940254211426
training loss = 1.727983832359314 18200
val loss = 4.922074317932129
training loss = 1.727936863899231 18300
val loss = 4.922205924987793
training loss = 1.7279205322265625 18400
val loss = 4.921794414520264
training loss = 1.734420657157898 18500
val loss = 4.917698383331299
training loss = 1.7278587818145752 18600
val loss = 4.92119836807251
training loss = 1.7278516292572021 18700
val loss = 4.920393943786621
training loss = 1.7278544902801514 18800
val loss = 4.92201566696167
training loss = 1.7277894020080566 18900
val loss = 4.920041084289551
training loss = 1.7333647012710571 19000
val loss = 4.914687633514404
training loss = 1.7277275323867798 19100
val loss = 4.919493675231934
training loss = 1.7277193069458008 19200
val loss = 4.918961048126221
training loss = 1.7282320261001587 19300
val loss = 4.924047946929932
training loss = 1.7276582717895508 19400
val loss = 4.918391227722168
training loss = 1.727671504020691 19500
val loss = 4.918582916259766
training loss = 1.72762131690979 19600
val loss = 4.917496681213379
training loss = 1.7275936603546143 19700
val loss = 4.917289733886719
training loss = 1.727593183517456 19800
val loss = 4.919032096862793
training loss = 1.7275390625 19900
val loss = 4.916685104370117
training loss = 1.7369425296783447 20000
val loss = 4.941982269287109
training loss = 1.7274857759475708 20100
val loss = 4.9161224365234375
training loss = 1.7275222539901733 20200
val loss = 4.91463041305542
training loss = 1.727434515953064 20300
val loss = 4.915972709655762
training loss = 1.7274235486984253 20400
val loss = 4.915013313293457
training loss = 1.7278372049331665 20500
val loss = 4.919157981872559
training loss = 1.7273682355880737 20600
val loss = 4.914386749267578
training loss = 1.7367700338363647 20700
val loss = 4.9105634689331055
training loss = 1.7273173332214355 20800
val loss = 4.914968013763428
training loss = 1.727292537689209 20900
val loss = 4.913476467132568
training loss = 1.7272835969924927 21000
val loss = 4.912961959838867
training loss = 1.7274943590164185 21100
val loss = 4.915848731994629
training loss = 1.727227807044983 21200
val loss = 4.912471771240234
training loss = 1.8720766305923462 21300
val loss = 5.130434989929199
training loss = 1.7271637916564941 21400
val loss = 4.912049293518066
training loss = 1.7271568775177002 21500
val loss = 4.911479949951172
training loss = 1.7339390516281128 21600
val loss = 4.9306535720825195
training loss = 1.7271010875701904 21700
val loss = 4.9111785888671875
training loss = 1.7270902395248413 21800
val loss = 4.9105048179626465
training loss = 1.7270472049713135 21900
val loss = 4.911109924316406
training loss = 1.7270325422286987 22000
val loss = 4.909906387329102
training loss = 1.7272205352783203 22100
val loss = 4.907209396362305
training loss = 1.7269808053970337 22200
val loss = 4.909643173217773
training loss = 1.7269814014434814 22300
val loss = 4.90827751159668
training loss = 1.72699773311615 22400
val loss = 4.907788276672363
training loss = 1.7269128561019897 22500
val loss = 4.9082350730896
training loss = 1.7279329299926758 22600
val loss = 4.905399322509766
training loss = 1.7268534898757935 22700
val loss = 4.907721519470215
training loss = 1.7271236181259155 22800
val loss = 4.909894943237305
training loss = 1.7268315553665161 22900
val loss = 4.906464576721191
training loss = 1.7267885208129883 23000
val loss = 4.906464576721191
training loss = 1.7267651557922363 23100
val loss = 4.907255172729492
training loss = 1.7267351150512695 23200
val loss = 4.905916690826416
training loss = 1.7277600765228271 23300
val loss = 4.902871131896973
training loss = 1.7266781330108643 23400
val loss = 4.90535831451416
training loss = 1.7768484354019165 23500
val loss = 4.9289469718933105
training loss = 1.7266203165054321 23600
val loss = 4.904897689819336
training loss = 1.7266117334365845 23700
val loss = 4.904447078704834
training loss = 1.726696252822876 23800
val loss = 4.906182289123535
training loss = 1.726549744606018 23900
val loss = 4.903595924377441
training loss = 1.7275643348693848 24000
val loss = 4.910404205322266
training loss = 1.7264939546585083 24100
val loss = 4.902897834777832
training loss = 1.7532362937927246 24200
val loss = 4.907277584075928
training loss = 1.7264314889907837 24300
val loss = 4.902387619018555
training loss = 1.726420521736145 24400
val loss = 4.901863098144531
training loss = 1.7267965078353882 24500
val loss = 4.905713081359863
training loss = 1.7263591289520264 24600
val loss = 4.901237487792969
training loss = 1.7300552129745483 24700
val loss = 4.91401481628418
training loss = 1.7263009548187256 24800
val loss = 4.900508880615234
training loss = 1.7262866497039795 24900
val loss = 4.9001851081848145
training loss = 1.7262414693832397 25000
val loss = 4.899967193603516
training loss = 1.7262203693389893 25100
val loss = 4.899486541748047
training loss = 1.8709965944290161 25200
val loss = 5.008419036865234
training loss = 1.7261552810668945 25300
val loss = 4.899229049682617
training loss = 1.7261415719985962 25400
val loss = 4.898375034332275
training loss = 1.7264481782913208 25500
val loss = 4.901534080505371
training loss = 1.7260791063308716 25600
val loss = 4.897660732269287
training loss = 1.7370959520339966 25700
val loss = 4.894535064697266
training loss = 1.7260226011276245 25800
val loss = 4.896825790405273
training loss = 1.726004719734192 25900
val loss = 4.896254539489746
training loss = 1.7260311841964722 26000
val loss = 4.897891044616699
training loss = 1.7259405851364136 26100
val loss = 4.895816802978516
training loss = 1.7271226644515991 26200
val loss = 4.902703285217285
training loss = 1.7258785963058472 26300
val loss = 4.89516019821167
training loss = 1.756738305091858 26400
val loss = 4.9549760818481445
training loss = 1.7258130311965942 26500
val loss = 4.894369602203369
training loss = 1.725830316543579 26600
val loss = 4.893043518066406
training loss = 1.7258095741271973 26700
val loss = 4.892880439758301
training loss = 1.7257332801818848 26800
val loss = 4.89320182800293
training loss = 1.725685715675354 26900
val loss = 4.893421173095703
training loss = 1.7256687879562378 27000
val loss = 4.892431259155273
training loss = 1.7421499490737915 27100
val loss = 4.9306817054748535
training loss = 1.7256006002426147 27200
val loss = 4.8917646408081055
training loss = 1.725583553314209 27300
val loss = 4.891056537628174
training loss = 1.7256414890289307 27400
val loss = 4.889926433563232
training loss = 1.7255159616470337 27500
val loss = 4.890512943267822
training loss = 1.7255263328552246 27600
val loss = 4.891794681549072
training loss = 1.7254486083984375 27700
val loss = 4.889732360839844
training loss = 1.7476485967636108 27800
val loss = 4.936009883880615
training loss = 1.7253950834274292 27900
val loss = 4.889839172363281
training loss = 1.7253583669662476 28000
val loss = 4.88847017288208
training loss = 1.7289094924926758 28100
val loss = 4.884598255157471
training loss = 1.7252907752990723 28200
val loss = 4.887784481048584
training loss = 1.7318799495697021 28300
val loss = 4.9069647789001465
training loss = 1.7252246141433716 28400
val loss = 4.887202262878418
training loss = 1.7255594730377197 28500
val loss = 4.889420509338379
training loss = 1.7251474857330322 28600
val loss = 4.886395454406738
training loss = 1.7251278162002563 28700
val loss = 4.8855671882629395
training loss = 1.7475852966308594 28800
val loss = 4.933863639831543
training loss = 1.7250512838363647 28900
val loss = 4.884751319885254
training loss = 1.7250293493270874 29000
val loss = 4.884228706359863
training loss = 1.7262862920761108 29100
val loss = 4.881156921386719
training loss = 1.7249540090560913 29200
val loss = 4.883495330810547
training loss = 1.749570369720459 29300
val loss = 4.888117790222168
training loss = 1.7248824834823608 29400
val loss = 4.882807731628418
training loss = 1.7249093055725098 29500
val loss = 4.88304328918457
training loss = 1.7248177528381348 29600
val loss = 4.881426811218262
training loss = 1.7247803211212158 29700
val loss = 4.881099700927734
training loss = 1.7249561548233032 29800
val loss = 4.884038925170898
training loss = 1.7246967554092407 29900
val loss = 4.880287170410156
training loss = 1.7274154424667358 30000
val loss = 4.875080108642578
reduced chi^2 level 2 = 1.729130506515503
Constrained alpha: 1.9000352621078491
Constrained beta: 4.053763389587402
Constrained gamma: 12.032797813415527
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 847.7451,  815.4682,  953.6652,  906.9885,  971.1612, 1079.3142,
        1095.1577, 1107.0536, 1190.7000, 1209.2582, 1184.1598, 1225.9332,
        1243.8553, 1302.3069, 1275.6206, 1414.5586, 1447.8317, 1447.0082,
        1540.6249, 1485.3313, 1598.5360, 1513.5521, 1649.7795, 1589.8391,
        1631.7490, 1706.2354, 1632.6266, 1819.1802, 1662.9828, 1710.3604,
        1632.7327, 1815.1603, 1693.0370, 1760.3906, 1709.6552, 1760.5281,
        1697.4323, 1589.7944, 1623.7017, 1608.7905, 1639.5811, 1539.0540,
        1490.4474, 1496.0248, 1339.8712, 1343.0338, 1226.0002, 1216.4205,
        1163.9559, 1150.9613, 1063.8389,  967.7213,  914.4758,  896.8697,
         875.6426,  886.9775,  867.0936,  684.7710,  628.8790,  570.0170,
         561.5340,  453.3446,  433.6210,  357.2698,  356.3061,  340.6108,
         291.4965,  231.1617,  209.1096,  173.0623,  182.6404,  133.3658,
         148.0112,  114.3482,  101.1038,   63.8013,   47.6180,   34.5731,
          34.3568,   44.4813,   18.4142,   31.8724,   43.7076])]
2755.652254522377
4.587047268528697 14.80001237452342 11.69827671998258
val isze = 8
idinces = [ 4 51 48 62 49 57  8 19 63 28 11 65  6 47 15 41 56  9 70 66  3 55 73 59
 42 36 81 13 79 31 34 69 54 35 53 45 71 23 40 20 17 58 38 18 32 30 33 80
 60 10 44 50  0 39 12  2 68 64  5 26 67 25  7 22 77  1 43 76 52 75 61 14
 21 72 16 37 24 29 74 46 82 27 78]
we are doing training validation split
training loss = 562.1240844726562 100
val loss = 579.0826416015625
training loss = 9.246813774108887 200
val loss = 4.7203803062438965
training loss = 7.275259017944336 300
val loss = 6.774314880371094
training loss = 7.193514823913574 400
val loss = 6.6783366203308105
training loss = 7.130171775817871 500
val loss = 6.61872673034668
training loss = 7.083143711090088 600
val loss = 6.569723606109619
training loss = 7.048863887786865 700
val loss = 6.530284881591797
training loss = 7.023695468902588 800
val loss = 6.498716354370117
training loss = 7.004596710205078 900
val loss = 6.473261833190918
training loss = 6.989279747009277 1000
val loss = 6.452651023864746
training loss = 6.976168155670166 1100
val loss = 6.435576438903809
training loss = 6.964252948760986 1200
val loss = 6.421143054962158
training loss = 6.952932357788086 1300
val loss = 6.408451080322266
training loss = 6.941859722137451 1400
val loss = 6.397089004516602
training loss = 6.930858135223389 1500
val loss = 6.386645317077637
training loss = 6.9198408126831055 1600
val loss = 6.376713275909424
training loss = 6.908764839172363 1700
val loss = 6.367169380187988
training loss = 6.897620677947998 1800
val loss = 6.357946872711182
training loss = 6.886396884918213 1900
val loss = 6.348797798156738
training loss = 6.87510347366333 2000
val loss = 6.339502334594727
training loss = 6.863733768463135 2100
val loss = 6.330438613891602
training loss = 6.852298259735107 2200
val loss = 6.321179389953613
training loss = 6.840789318084717 2300
val loss = 6.312107086181641
training loss = 6.8292036056518555 2400
val loss = 6.302874565124512
training loss = 6.817534446716309 2500
val loss = 6.293567657470703
training loss = 6.805757999420166 2600
val loss = 6.284136772155762
training loss = 6.793841361999512 2700
val loss = 6.274772644042969
training loss = 6.781716346740723 2800
val loss = 6.265044689178467
training loss = 6.76922607421875 2900
val loss = 6.2551469802856445
training loss = 6.755953311920166 3000
val loss = 6.244406700134277
training loss = 6.74042272567749 3100
val loss = 6.231189727783203
training loss = 6.714535713195801 3200
val loss = 6.205621719360352
training loss = 6.5933003425598145 3300
val loss = 6.063694000244141
training loss = 5.687045097351074 3400
val loss = 5.419415473937988
training loss = 3.8506853580474854 3500
val loss = 3.122352123260498
training loss = 2.823878765106201 3600
val loss = 2.026552677154541
training loss = 2.6843819618225098 3700
val loss = 2.02173113822937
training loss = 2.6707067489624023 3800
val loss = 2.1572136878967285
training loss = 2.654120445251465 3900
val loss = 2.110793113708496
training loss = 2.684429883956909 4000
val loss = 2.38588809967041
training loss = 2.6307249069213867 4100
val loss = 2.0168776512145996
training loss = 2.6210601329803467 4200
val loss = 2.007307291030884
training loss = 2.646331787109375 4300
val loss = 2.2853190898895264
training loss = 2.6032259464263916 4400
val loss = 1.958346962928772
training loss = 2.5961735248565674 4500
val loss = 1.9954569339752197
training loss = 2.587371587753296 4600
val loss = 1.9264215230941772
training loss = 2.581247091293335 4700
val loss = 1.9628002643585205
training loss = 2.573176622390747 4800
val loss = 1.8949445486068726
training loss = 2.566504716873169 4900
val loss = 1.8807711601257324
training loss = 2.5602633953094482 5000
val loss = 1.8627007007598877
training loss = 2.554288864135742 5100
val loss = 1.8512380123138428
training loss = 2.5709338188171387 5200
val loss = 1.6634989976882935
training loss = 2.5428125858306885 5300
val loss = 1.8281484842300415
training loss = 2.538045644760132 5400
val loss = 1.8537640571594238
training loss = 2.532050371170044 5500
val loss = 1.8040105104446411
training loss = 2.52691650390625 5600
val loss = 1.7945556640625
training loss = 2.52188777923584 5700
val loss = 1.7887336015701294
training loss = 2.5170626640319824 5800
val loss = 1.7751712799072266
training loss = 2.5622384548187256 5900
val loss = 2.110114336013794
training loss = 2.507704257965088 6000
val loss = 1.759294033050537
training loss = 2.5031466484069824 6100
val loss = 1.7472155094146729
training loss = 2.4988787174224854 6200
val loss = 1.7286982536315918
training loss = 2.4944987297058105 6300
val loss = 1.733267068862915
training loss = 2.4922852516174316 6400
val loss = 1.67038094997406
training loss = 2.486262083053589 6500
val loss = 1.719172477722168
training loss = 2.483654499053955 6600
val loss = 1.6654797792434692
training loss = 2.4783711433410645 6700
val loss = 1.7025091648101807
training loss = 2.474531650543213 6800
val loss = 1.6992573738098145
training loss = 2.4708902835845947 6900
val loss = 1.6831769943237305
training loss = 2.4672131538391113 7000
val loss = 1.6857631206512451
training loss = 2.4657604694366455 7100
val loss = 1.6246356964111328
training loss = 2.460237741470337 7200
val loss = 1.6738944053649902
training loss = 2.4573545455932617 7300
val loss = 1.6442525386810303
training loss = 2.4535422325134277 7400
val loss = 1.666210651397705
training loss = 2.450312614440918 7500
val loss = 1.669419527053833
training loss = 2.4470908641815186 7600
val loss = 1.6533602476119995
training loss = 2.4439315795898438 7700
val loss = 1.648270606994629
training loss = 2.441310167312622 7800
val loss = 1.6188523769378662
training loss = 2.4378459453582764 7900
val loss = 1.6391810178756714
training loss = 2.4418208599090576 8000
val loss = 1.743561029434204
training loss = 2.4318623542785645 8100
val loss = 1.6311240196228027
training loss = 2.442257881164551 8200
val loss = 1.7843873500823975
training loss = 2.4259321689605713 8300
val loss = 1.6196362972259521
training loss = 2.423335075378418 8400
val loss = 1.6440141201019287
training loss = 2.419971466064453 8500
val loss = 1.6063815355300903
training loss = 2.416837692260742 8600
val loss = 1.60944402217865
training loss = 2.4140279293060303 8700
val loss = 1.5855295658111572
training loss = 2.4105818271636963 8800
val loss = 1.6011606454849243
training loss = 2.407933473587036 8900
val loss = 1.563486099243164
training loss = 2.4036858081817627 9000
val loss = 1.595192551612854
training loss = 2.399915933609009 9100
val loss = 1.5883021354675293
training loss = 2.4096033573150635 9200
val loss = 1.4572968482971191
training loss = 2.3914945125579834 9300
val loss = 1.578813076019287
training loss = 2.3865771293640137 9400
val loss = 1.5745556354522705
training loss = 2.381903648376465 9500
val loss = 1.5448623895645142
training loss = 2.375943422317505 9600
val loss = 1.5647863149642944
training loss = 2.3697197437286377 9700
val loss = 1.560012936592102
training loss = 2.3635685443878174 9800
val loss = 1.5694959163665771
training loss = 2.3568403720855713 9900
val loss = 1.5480291843414307
training loss = 2.362603187561035 10000
val loss = 1.4250881671905518
training loss = 2.343350410461426 10100
val loss = 1.5345324277877808
training loss = 2.33650279045105 10200
val loss = 1.5272016525268555
training loss = 2.3301069736480713 10300
val loss = 1.5254061222076416
training loss = 2.3237385749816895 10400
val loss = 1.5133532285690308
training loss = 2.3371384143829346 10500
val loss = 1.6931917667388916
training loss = 2.311565637588501 10600
val loss = 1.498894453048706
training loss = 2.3055896759033203 10700
val loss = 1.4895391464233398
training loss = 2.3002445697784424 10800
val loss = 1.4772727489471436
training loss = 2.294948101043701 10900
val loss = 1.4730967283248901
training loss = 2.2897567749023438 11000
val loss = 1.4804871082305908
training loss = 2.2848494052886963 11100
val loss = 1.4648256301879883
training loss = 2.279946804046631 11200
val loss = 1.448830485343933
training loss = 2.2765402793884277 11300
val loss = 1.4042152166366577
training loss = 2.271228790283203 11400
val loss = 1.432931661605835
training loss = 2.266939163208008 11500
val loss = 1.4256336688995361
training loss = 2.264597177505493 11600
val loss = 1.3739014863967896
training loss = 2.2591822147369385 11700
val loss = 1.4113185405731201
training loss = 2.255371570587158 11800
val loss = 1.3966596126556396
training loss = 2.2520999908447266 11900
val loss = 1.3916841745376587
training loss = 2.2487263679504395 12000
val loss = 1.390796422958374
training loss = 2.24588680267334 12100
val loss = 1.358636736869812
training loss = 2.2424700260162354 12200
val loss = 1.3832688331604004
training loss = 2.239450454711914 12300
val loss = 1.3721652030944824
training loss = 2.2368366718292236 12400
val loss = 1.3818726539611816
training loss = 2.2339491844177246 12500
val loss = 1.3602967262268066
training loss = 2.2431023120880127 12600
val loss = 1.4893317222595215
training loss = 2.2289154529571533 12700
val loss = 1.3483893871307373
training loss = 2.226451873779297 12800
val loss = 1.3463070392608643
training loss = 2.22428035736084 12900
val loss = 1.3412103652954102
training loss = 2.2220544815063477 13000
val loss = 1.3352441787719727
training loss = 2.223102569580078 13100
val loss = 1.394540548324585
training loss = 2.2180304527282715 13200
val loss = 1.3266946077346802
training loss = 2.337463140487671 13300
val loss = 1.830038070678711
training loss = 2.21431565284729 13400
val loss = 1.3206994533538818
training loss = 2.2124736309051514 13500
val loss = 1.3137037754058838
training loss = 2.2109858989715576 13600
val loss = 1.3019336462020874
training loss = 2.2093002796173096 13700
val loss = 1.3067419528961182
training loss = 2.2076759338378906 13800
val loss = 1.2979907989501953
training loss = 2.206385612487793 13900
val loss = 1.292464256286621
training loss = 2.2048933506011963 14000
val loss = 1.2964978218078613
training loss = 2.2036426067352295 14100
val loss = 1.3091856241226196
training loss = 2.202272653579712 14200
val loss = 1.284471035003662
training loss = 2.2009334564208984 14300
val loss = 1.2871458530426025
training loss = 2.2404086589813232 14400
val loss = 1.5429961681365967
training loss = 2.19858717918396 14500
val loss = 1.2802629470825195
training loss = 2.1973845958709717 14600
val loss = 1.2784559726715088
training loss = 2.1974635124206543 14700
val loss = 1.2427241802215576
training loss = 2.195357084274292 14800
val loss = 1.2734813690185547
training loss = 2.194274663925171 14900
val loss = 1.2696245908737183
training loss = 2.193389415740967 15000
val loss = 1.2661139965057373
training loss = 2.192389726638794 15100
val loss = 1.2658450603485107
training loss = 2.206134796142578 15200
val loss = 1.4066722393035889
training loss = 2.190650224685669 15300
val loss = 1.2619754076004028
training loss = 2.1897239685058594 15400
val loss = 1.2608599662780762
training loss = 2.1891143321990967 15500
val loss = 1.2460496425628662
training loss = 2.188147783279419 15600
val loss = 1.25431489944458
training loss = 2.196751594543457 15700
val loss = 1.1619359254837036
training loss = 2.1866326332092285 15800
val loss = 1.2494680881500244
training loss = 2.5274736881256104 15900
val loss = 2.218029499053955
training loss = 2.185244560241699 16000
val loss = 1.248948097229004
training loss = 2.1844770908355713 16100
val loss = 1.2442338466644287
training loss = 2.183897018432617 16200
val loss = 1.2477006912231445
training loss = 2.183182954788208 16300
val loss = 1.2408068180084229
training loss = 2.1824517250061035 16400
val loss = 1.2363550662994385
training loss = 2.1820454597473145 16500
val loss = 1.2468092441558838
training loss = 2.1812753677368164 16600
val loss = 1.2339129447937012
training loss = 2.202084541320801 16700
val loss = 1.4070813655853271
training loss = 2.180208206176758 16800
val loss = 1.2288434505462646
training loss = 2.179647922515869 16900
val loss = 1.2285418510437012
training loss = 2.179414987564087 17000
val loss = 1.2300705909729004
training loss = 2.1789770126342773 17100
val loss = 1.2233998775482178
training loss = 2.1786048412323 17200
val loss = 1.221252679824829
training loss = 2.178755283355713 17300
val loss = 1.2026698589324951
training loss = 2.178236961364746 17400
val loss = 1.217905044555664
training loss = 2.1780307292938232 17500
val loss = 1.215547800064087
training loss = 2.1780219078063965 17600
val loss = 1.2159669399261475
training loss = 2.17789626121521 17700
val loss = 1.2134478092193604
training loss = 2.1799252033233643 17800
val loss = 1.261395812034607
training loss = 2.1778759956359863 17900
val loss = 1.2097270488739014
training loss = 2.1778340339660645 18000
val loss = 1.2109696865081787
training loss = 2.1779448986053467 18100
val loss = 1.20673406124115
training loss = 2.1779255867004395 18200
val loss = 1.2090803384780884
training loss = 2.1854915618896484 18300
val loss = 1.3043513298034668
training loss = 2.178072214126587 18400
val loss = 1.2067551612854004
training loss = 2.1780896186828613 18500
val loss = 1.2072638273239136
training loss = 2.178490400314331 18600
val loss = 1.2231535911560059
training loss = 2.1782679557800293 18700
val loss = 1.2070231437683105
training loss = 2.189250946044922 18800
val loss = 1.1121916770935059
training loss = 2.178483486175537 18900
val loss = 1.2099696397781372
training loss = 2.178500175476074 19000
val loss = 1.2060906887054443
training loss = 2.1855356693267822 19100
val loss = 1.2981842756271362
training loss = 2.1786816120147705 19200
val loss = 1.2074379920959473
training loss = 2.178713321685791 19300
val loss = 1.205584168434143
training loss = 2.1791224479675293 19400
val loss = 1.1897457838058472
training loss = 2.178907632827759 19500
val loss = 1.2050399780273438
training loss = 2.1795992851257324 19600
val loss = 1.17935311794281
training loss = 2.1791188716888428 19700
val loss = 1.1980063915252686
training loss = 2.1791036128997803 19800
val loss = 1.2040894031524658
training loss = 2.1802351474761963 19900
val loss = 1.2365226745605469
training loss = 2.179263114929199 20000
val loss = 1.2041094303131104
training loss = 2.18306565284729 20100
val loss = 1.269554615020752
training loss = 2.1794698238372803 20200
val loss = 1.1981141567230225
training loss = 2.1794519424438477 20300
val loss = 1.2030301094055176
training loss = 2.5052502155303955 20400
val loss = 2.123410224914551
training loss = 2.179593086242676 20500
val loss = 1.202561378479004
training loss = 2.1796021461486816 20600
val loss = 1.2019761800765991
training loss = 2.182015895843506 20700
val loss = 1.251720666885376
training loss = 2.1797454357147217 20800
val loss = 1.2017154693603516
training loss = 2.179745674133301 20900
val loss = 1.2006042003631592
training loss = 2.180034637451172 21000
val loss = 1.2135751247406006
training loss = 2.1798624992370605 21100
val loss = 1.2004051208496094
training loss = 2.456230640411377 21200
val loss = 2.021358013153076
training loss = 2.1799871921539307 21300
val loss = 1.2020690441131592
training loss = 2.1799607276916504 21400
val loss = 1.1988379955291748
training loss = 2.193070888519287 21500
val loss = 1.10035240650177
training loss = 2.1800293922424316 21600
val loss = 1.1967145204544067
training loss = 2.1799979209899902 21700
val loss = 1.1953202486038208
training loss = 2.180231809616089 21800
val loss = 1.1837010383605957
training loss = 2.1800167560577393 21900
val loss = 1.1951558589935303
training loss = 2.202709197998047 22000
val loss = 1.3704288005828857
training loss = 2.179992914199829 22100
val loss = 1.194361925125122
training loss = 2.17989182472229 22200
val loss = 1.1920666694641113
training loss = 2.1799163818359375 22300
val loss = 1.1935992240905762
training loss = 2.1797525882720947 22400
val loss = 1.1893203258514404
training loss = 2.179551124572754 22500
val loss = 1.187650203704834
training loss = 2.180870532989502 22600
val loss = 1.1488139629364014
training loss = 2.179063081741333 22700
val loss = 1.183262825012207
training loss = 2.4054465293884277 22800
val loss = 0.950780987739563
training loss = 2.178133487701416 22900
val loss = 1.1776657104492188
training loss = 2.1773552894592285 23000
val loss = 1.1746747493743896
training loss = 2.1807565689086914 23100
val loss = 1.1106946468353271
training loss = 2.174974203109741 23200
val loss = 1.167401909828186
training loss = 2.1733760833740234 23300
val loss = 1.1801482439041138
training loss = 2.170757293701172 23400
val loss = 1.1633713245391846
training loss = 2.1678287982940674 23500
val loss = 1.156460165977478
training loss = 2.176300048828125 23600
val loss = 1.271527886390686
training loss = 2.161022186279297 23700
val loss = 1.1519371271133423
training loss = 2.157426357269287 23800
val loss = 1.1508443355560303
training loss = 2.1543564796447754 23900
val loss = 1.170042872428894
training loss = 2.1502392292022705 24000
val loss = 1.1453592777252197
training loss = 2.50380539894104 24100
val loss = 0.9569094777107239
training loss = 2.1425697803497314 24200
val loss = 1.1357109546661377
training loss = 2.1383745670318604 24300
val loss = 1.1299128532409668
training loss = 2.13962459564209 24400
val loss = 1.0617014169692993
training loss = 2.1293399333953857 24500
val loss = 1.1169545650482178
training loss = 2.287177324295044 24600
val loss = 1.6346489191055298
training loss = 2.119267702102661 24700
val loss = 1.1035034656524658
training loss = 2.113769292831421 24800
val loss = 1.09633207321167
training loss = 2.1085307598114014 24900
val loss = 1.1050317287445068
training loss = 2.102383613586426 25000
val loss = 1.0816645622253418
training loss = 2.0977835655212402 25100
val loss = 1.045762062072754
training loss = 2.0903255939483643 25200
val loss = 1.0668129920959473
training loss = 2.0839312076568604 25300
val loss = 1.0597026348114014
training loss = 2.0780787467956543 25400
val loss = 1.041900873184204
training loss = 2.071610689163208 25500
val loss = 1.0466243028640747
training loss = 2.0650761127471924 25600
val loss = 1.0415019989013672
training loss = 2.059046745300293 25700
val loss = 1.0261449813842773
training loss = 2.0526480674743652 25800
val loss = 1.0273971557617188
training loss = 2.047340154647827 25900
val loss = 0.9999529719352722
training loss = 2.040461301803589 26000
val loss = 1.0162184238433838
training loss = 2.0524377822875977 26100
val loss = 1.1282994747161865
training loss = 2.028846025466919 26200
val loss = 1.009127140045166
training loss = 2.0230748653411865 26300
val loss = 1.0008602142333984
training loss = 2.020789623260498 26400
val loss = 0.9607889652252197
training loss = 2.0123705863952637 26500
val loss = 0.9925363659858704
training loss = 2.294076681137085 26600
val loss = 0.8989747762680054
training loss = 2.0020456314086914 26700
val loss = 0.986436128616333
training loss = 1.9966866970062256 26800
val loss = 0.9799933433532715
training loss = 1.9923901557922363 26900
val loss = 0.972186267375946
training loss = 1.9875404834747314 27000
val loss = 0.9739848375320435
training loss = 2.0992987155914307 27100
val loss = 0.8575462102890015
training loss = 1.9788662195205688 27200
val loss = 0.9698500633239746
training loss = 1.974473476409912 27300
val loss = 0.9649633169174194
training loss = 1.9715837240219116 27400
val loss = 0.9505001902580261
training loss = 1.9674630165100098 27500
val loss = 0.9605811834335327
training loss = 1.964285969734192 27600
val loss = 0.9747926592826843
training loss = 1.9612016677856445 27700
val loss = 0.9611904621124268
training loss = 1.9580039978027344 27800
val loss = 0.9546874165534973
training loss = 2.425759792327881 27900
val loss = 1.8525140285491943
training loss = 1.9528898000717163 28000
val loss = 0.953951895236969
training loss = 1.9502341747283936 28100
val loss = 0.9499624967575073
training loss = 1.9603019952774048 28200
val loss = 0.8887735605239868
training loss = 1.945866346359253 28300
val loss = 0.9466360211372375
training loss = 2.118988513946533 28400
val loss = 0.8505100607872009
training loss = 1.9421005249023438 28500
val loss = 0.9447543621063232
training loss = 1.9400994777679443 28600
val loss = 0.9440984129905701
training loss = 1.9399018287658691 28700
val loss = 0.966790497303009
training loss = 1.9371483325958252 28800
val loss = 0.9422950744628906
training loss = 1.9354242086410522 28900
val loss = 0.9404172301292419
training loss = 1.9345273971557617 29000
val loss = 0.9337072372436523
training loss = 1.9328784942626953 29100
val loss = 0.9403492212295532
training loss = 1.9324924945831299 29200
val loss = 0.9254459142684937
training loss = 1.9305871725082397 29300
val loss = 0.9388017058372498
training loss = 1.929213523864746 29400
val loss = 0.9403570890426636
training loss = 1.9286340475082397 29500
val loss = 0.9357339143753052
training loss = 1.9274253845214844 29600
val loss = 0.937759280204773
training loss = 1.9726827144622803 29700
val loss = 0.8499031066894531
training loss = 1.9255281686782837 29800
val loss = 0.9362820386886597
training loss = 1.9243868589401245 29900
val loss = 0.9385843276977539
training loss = 1.923926591873169 30000
val loss = 0.9432780742645264
reduced chi^2 level 2 = 1.9239758253097534
Constrained alpha: 1.8268040418624878
Constrained beta: 0.019412405788898468
Constrained gamma: 11.936179161071777
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 821.2006,  868.2452,  991.8741,  954.3622,  943.4288, 1045.2228,
        1152.4473, 1151.9685, 1168.0889, 1171.5350, 1184.2622, 1194.9073,
        1218.8420, 1222.6619, 1360.0000, 1447.5256, 1413.5204, 1409.8494,
        1558.6577, 1517.8698, 1586.8472, 1531.7914, 1574.3389, 1673.6448,
        1700.6072, 1697.1343, 1617.2239, 1770.1161, 1748.6920, 1730.6890,
        1698.8828, 1689.0135, 1737.9938, 1685.4286, 1721.1737, 1732.3174,
        1656.2421, 1638.2175, 1608.9430, 1542.4427, 1710.3113, 1561.3007,
        1502.5012, 1477.9586, 1331.0292, 1305.9910, 1289.2748, 1239.5848,
        1192.5291, 1139.7672, 1126.4176,  998.3451,  963.8390,  952.7406,
         883.2855,  844.1430,  814.6751,  636.5305,  598.8168,  529.8037,
         526.0226,  472.8743,  431.5717,  376.2891,  324.0276,  362.4088,
         264.3901,  279.4509,  230.6032,  164.4811,  173.0233,  145.6484,
         133.9561,  123.1286,   97.6945,   62.8247,   49.8462,   36.4626,
          35.4225,   51.8223,   25.5965,   45.5693,   42.0698])]
2679.9340395075487
3.7182393062726304 19.448379657636117 37.255710418518774
val isze = 8
idinces = [21 51 47 73  7 66  0 11 44 52 79 15 50 35 12 69 62 56 18 61 68  2 17  6
 78  8 16 70 22 58 63 71 75 39 19 53 76  4 38 81  9 72 40  5 45 55 59  3
 82 36 54 20 13 77 49 27 42 60 29 30 31 33 46 67 57 28 34 64 32 26 41 24
  1 48 80 65 25 14 74 37 10 23 43]
we are doing training validation split
training loss = 517.1445922851562 100
val loss = 470.25634765625
training loss = 77.67716217041016 200
val loss = 94.63369750976562
training loss = 11.10306167602539 300
val loss = 14.457291603088379
training loss = 10.658422470092773 400
val loss = 13.860674858093262
training loss = 10.197306632995605 500
val loss = 13.226760864257812
training loss = 9.74178409576416 600
val loss = 12.590295791625977
training loss = 9.313455581665039 700
val loss = 11.980583190917969
training loss = 8.929487228393555 800
val loss = 11.422443389892578
training loss = 8.600868225097656 900
val loss = 10.933341026306152
training loss = 8.331282615661621 1000
val loss = 10.521660804748535
training loss = 8.117392539978027 1100
val loss = 10.18612289428711
training loss = 7.9507951736450195 1200
val loss = 9.918148040771484
training loss = 7.820901870727539 1300
val loss = 9.704803466796875
training loss = 7.717637062072754 1400
val loss = 9.533502578735352
training loss = 7.632961750030518 1500
val loss = 9.392900466918945
training loss = 7.561213493347168 1600
val loss = 9.275236129760742
training loss = 7.498719215393066 1700
val loss = 9.17434310913086
training loss = 7.443169116973877 1800
val loss = 9.086392402648926
training loss = 7.393099784851074 1900
val loss = 9.009032249450684
training loss = 7.3475213050842285 2000
val loss = 8.939868927001953
training loss = 7.30570650100708 2100
val loss = 8.87759780883789
training loss = 7.267102241516113 2200
val loss = 8.821115493774414
training loss = 7.231237888336182 2300
val loss = 8.769378662109375
training loss = 7.197714805603027 2400
val loss = 8.721807479858398
training loss = 7.166198253631592 2500
val loss = 8.677659034729004
training loss = 7.136388301849365 2600
val loss = 8.636363983154297
training loss = 7.108020782470703 2700
val loss = 8.597628593444824
training loss = 7.080873489379883 2800
val loss = 8.56080436706543
training loss = 7.054743766784668 2900
val loss = 8.525999069213867
training loss = 7.029451847076416 3000
val loss = 8.49239444732666
training loss = 7.004843711853027 3100
val loss = 8.460348129272461
training loss = 6.980790138244629 3200
val loss = 8.429190635681152
training loss = 6.957174777984619 3300
val loss = 8.399003982543945
training loss = 6.933920860290527 3400
val loss = 8.36937141418457
training loss = 6.910980701446533 3500
val loss = 8.340633392333984
training loss = 6.888352870941162 3600
val loss = 8.312850952148438
training loss = 6.866103649139404 3700
val loss = 8.285505294799805
training loss = 6.8443989753723145 3800
val loss = 8.259486198425293
training loss = 6.823512554168701 3900
val loss = 8.235240936279297
training loss = 6.8048295974731445 4000
val loss = 8.120647430419922
training loss = 6.786624431610107 4100
val loss = 8.187620162963867
training loss = 6.771876335144043 4200
val loss = 8.179758071899414
training loss = 6.760255813598633 4300
val loss = 8.100581169128418
training loss = 6.750722408294678 4400
val loss = 8.167322158813477
training loss = 6.744492530822754 4500
val loss = 8.04837417602539
training loss = 6.738236904144287 4600
val loss = 8.155667304992676
training loss = 6.733515739440918 4700
val loss = 8.134360313415527
training loss = 6.728740215301514 4800
val loss = 8.145599365234375
training loss = 6.722682476043701 4900
val loss = 8.124702453613281
training loss = 6.740577220916748 5000
val loss = 7.5870771408081055
training loss = 6.687974452972412 5100
val loss = 8.094274520874023
training loss = 6.582043647766113 5200
val loss = 7.999538421630859
training loss = 5.791324138641357 5300
val loss = 7.089402198791504
training loss = 4.278164863586426 5400
val loss = 5.464583396911621
training loss = 2.4660918712615967 5500
val loss = 3.3857715129852295
training loss = 2.238720178604126 5600
val loss = 2.7136166095733643
training loss = 2.2044501304626465 5700
val loss = 2.5859503746032715
training loss = 2.1821210384368896 5800
val loss = 2.477494239807129
training loss = 2.1667330265045166 5900
val loss = 2.4342799186706543
training loss = 2.157282829284668 6000
val loss = 2.3032922744750977
training loss = 2.146639585494995 6100
val loss = 2.349133014678955
training loss = 2.139660358428955 6200
val loss = 2.3100762367248535
training loss = 2.133831024169922 6300
val loss = 2.295182704925537
training loss = 2.128894805908203 6400
val loss = 2.285794258117676
training loss = 2.141653299331665 6500
val loss = 2.5365915298461914
training loss = 2.120615243911743 6600
val loss = 2.26173996925354
training loss = 2.1170310974121094 6700
val loss = 2.2510671615600586
training loss = 2.114424228668213 6800
val loss = 2.2952561378479004
training loss = 2.1105871200561523 6900
val loss = 2.2356953620910645
training loss = 2.109495162963867 7000
val loss = 2.150946617126465
training loss = 2.1046764850616455 7100
val loss = 2.2342681884765625
training loss = 2.101832389831543 7200
val loss = 2.2203686237335205
training loss = 2.100177764892578 7300
val loss = 2.154132604598999
training loss = 2.0962071418762207 7400
val loss = 2.213897228240967
training loss = 2.093353748321533 7500
val loss = 2.2095255851745605
training loss = 2.090616226196289 7600
val loss = 2.178770065307617
training loss = 2.0873658657073975 7700
val loss = 2.2046303749084473
training loss = 2.1683907508850098 7800
val loss = 1.7552998065948486
training loss = 2.080777645111084 7900
val loss = 2.196359634399414
training loss = 2.077167272567749 8000
val loss = 2.1985230445861816
training loss = 2.1009631156921387 8100
val loss = 1.9227042198181152
training loss = 2.0692081451416016 8200
val loss = 2.1975197792053223
training loss = 2.064873218536377 8300
val loss = 2.194204092025757
training loss = 2.059993028640747 8400
val loss = 2.1927077770233154
training loss = 2.0549657344818115 8500
val loss = 2.2038803100585938
training loss = 2.049685001373291 8600
val loss = 2.1931352615356445
training loss = 2.059621572494507 8700
val loss = 1.9855306148529053
training loss = 2.0387721061706543 8800
val loss = 2.1940393447875977
training loss = 2.0331742763519287 8900
val loss = 2.192121982574463
training loss = 2.028360605239868 9000
val loss = 2.2409629821777344
training loss = 2.02219295501709 9100
val loss = 2.1914381980895996
training loss = 2.017012119293213 9200
val loss = 2.2209720611572266
training loss = 2.01166033744812 9300
val loss = 2.1888232231140137
training loss = 2.0064332485198975 9400
val loss = 2.1881628036499023
training loss = 2.001725673675537 9500
val loss = 2.159425735473633
training loss = 1.99660325050354 9600
val loss = 2.186293125152588
training loss = 2.0146961212158203 9700
val loss = 2.4703469276428223
training loss = 1.9873114824295044 9800
val loss = 2.185455083847046
training loss = 1.9827684164047241 9900
val loss = 2.1811649799346924
training loss = 1.9785128831863403 10000
val loss = 2.1966702938079834
training loss = 1.9741791486740112 10100
val loss = 2.1782021522521973
training loss = 1.9731804132461548 10200
val loss = 2.082252025604248
training loss = 1.9660228490829468 10300
val loss = 2.1751387119293213
training loss = 1.966413974761963 10400
val loss = 2.290226459503174
training loss = 1.958276391029358 10500
val loss = 2.1640355587005615
training loss = 1.9544774293899536 10600
val loss = 2.1671886444091797
training loss = 2.137763738632202 10700
val loss = 1.6216518878936768
training loss = 1.9471553564071655 10800
val loss = 2.164824962615967
training loss = 1.9435276985168457 10900
val loss = 2.1603147983551025
training loss = 1.951518177986145 11000
val loss = 1.9910223484039307
training loss = 1.9364510774612427 11100
val loss = 2.154876708984375
training loss = 1.932906150817871 11200
val loss = 2.155637741088867
training loss = 1.9297001361846924 11300
val loss = 2.1371653079986572
training loss = 1.9263423681259155 11400
val loss = 2.148998498916626
training loss = 1.9521385431289673 11500
val loss = 1.899824619293213
training loss = 1.920094609260559 11600
val loss = 2.1424031257629395
training loss = 1.9170552492141724 11700
val loss = 2.1400418281555176
training loss = 1.9143520593643188 11800
val loss = 2.1257119178771973
training loss = 1.9115620851516724 11900
val loss = 2.1366147994995117
training loss = 1.9180548191070557 12000
val loss = 1.9919147491455078
training loss = 1.906585693359375 12100
val loss = 2.132664203643799
training loss = 1.9042847156524658 12200
val loss = 2.119894504547119
training loss = 1.9022040367126465 12300
val loss = 2.1209006309509277
training loss = 1.9002103805541992 12400
val loss = 2.1229748725891113
training loss = 1.9075700044631958 12500
val loss = 2.275484085083008
training loss = 1.896691918373108 12600
val loss = 2.118166446685791
training loss = 1.9698638916015625 12700
val loss = 1.7707667350769043
training loss = 1.8936735391616821 12800
val loss = 2.1172170639038086
training loss = 1.8922852277755737 12900
val loss = 2.1097800731658936
training loss = 1.8919018507003784 13000
val loss = 2.0646209716796875
reduced chi^2 level 2 = 1.8900519609451294
Constrained alpha: 1.8934388160705566
Constrained beta: 2.132117748260498
Constrained gamma: 15.622886657714844
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 870.4759,  831.4155,  913.0474,  930.3640, 1003.6615, 1033.1504,
        1101.3774, 1171.4984, 1073.8726, 1210.0692, 1238.6854, 1226.7025,
        1202.6144, 1174.1372, 1363.5486, 1456.8313, 1381.0992, 1456.4135,
        1549.6687, 1527.4735, 1554.8318, 1584.9879, 1542.7731, 1639.2979,
        1686.3043, 1768.5638, 1577.1116, 1714.7219, 1786.9471, 1685.0889,
        1688.9270, 1731.8978, 1714.8185, 1716.9547, 1651.4119, 1739.6046,
        1696.3005, 1633.6898, 1639.0326, 1561.1376, 1644.0986, 1557.3038,
        1547.6100, 1475.0083, 1347.1146, 1287.1238, 1207.9169, 1178.2137,
        1186.1785, 1141.0258, 1040.3242,  961.7214,  921.1036,  921.9935,
         861.8755,  812.0206,  822.5369,  725.7182,  641.6483,  508.1712,
         525.8668,  492.6948,  424.7036,  409.4637,  335.7774,  356.6983,
         309.5832,  271.9846,  195.2107,  184.0840,  153.5249,  147.3656,
         166.0663,  107.1268,  100.3220,   83.2491,   41.4676,   37.6032,
          35.3159,   44.9512,   23.1140,   36.4988,   34.7872])]
2766.9454677903923
3.3788576769154126 17.786456887345306 45.00962416704366
val isze = 8
idinces = [73 39 53 79 46 51 17 66 62 60 57 12 63 76 18 26  5 11 30 68 14 27 54 81
 77 50 20 56  4  2 25 35 48 75 42  8  3 40 65 44 24 69 23  6 22 34 19 72
 45 16 36 82 52 41  7 37 71 10 15  9 78 61 33 58 55 28 31 80 29 13 74 70
 38  0 21 67 47 64 32 59 49  1 43]
we are doing training validation split
training loss = 429.9148864746094 100
val loss = 477.562744140625
training loss = 39.43239212036133 200
val loss = 45.411861419677734
training loss = 9.234551429748535 300
val loss = 21.480695724487305
training loss = 9.01172161102295 400
val loss = 20.706279754638672
training loss = 8.787880897521973 500
val loss = 19.905359268188477
training loss = 8.574627876281738 600
val loss = 19.07683563232422
training loss = 8.380369186401367 700
val loss = 18.249942779541016
training loss = 8.209939956665039 800
val loss = 17.451353073120117
training loss = 8.064749717712402 900
val loss = 16.703123092651367
training loss = 7.943358898162842 1000
val loss = 16.021575927734375
training loss = 7.842444896697998 1100
val loss = 15.416543960571289
training loss = 7.757835388183594 1200
val loss = 14.891531944274902
training loss = 7.685405254364014 1300
val loss = 14.444351196289062
training loss = 7.621663570404053 1400
val loss = 14.068950653076172
training loss = 7.563955307006836 1500
val loss = 13.756454467773438
training loss = 7.510477066040039 1600
val loss = 13.497001647949219
training loss = 7.46005392074585 1700
val loss = 13.28065299987793
training loss = 7.411982536315918 1800
val loss = 13.09839916229248
training loss = 7.365835189819336 1900
val loss = 12.942301750183105
training loss = 7.321340560913086 2000
val loss = 12.805938720703125
training loss = 7.278318881988525 2100
val loss = 12.684341430664062
training loss = 7.236618995666504 2200
val loss = 12.573617935180664
training loss = 7.196116924285889 2300
val loss = 12.471036911010742
training loss = 7.156696319580078 2400
val loss = 12.374570846557617
training loss = 7.118250370025635 2500
val loss = 12.282882690429688
training loss = 7.080686092376709 2600
val loss = 12.194951057434082
training loss = 7.0439229011535645 2700
val loss = 12.110071182250977
training loss = 7.007884979248047 2800
val loss = 12.027877807617188
training loss = 6.97253942489624 2900
val loss = 11.947949409484863
training loss = 6.9378557205200195 3000
val loss = 11.870176315307617
training loss = 6.903861045837402 3100
val loss = 11.79434585571289
training loss = 6.87060546875 3200
val loss = 11.720714569091797
training loss = 6.838212966918945 3300
val loss = 11.649030685424805
training loss = 6.806862831115723 3400
val loss = 11.579805374145508
training loss = 6.776815891265869 3500
val loss = 11.513311386108398
training loss = 6.7483906745910645 3600
val loss = 11.449823379516602
training loss = 6.721957206726074 3700
val loss = 11.38990592956543
training loss = 6.697867393493652 3800
val loss = 11.333734512329102
training loss = 6.676347732543945 3900
val loss = 11.281209945678711
training loss = 6.657339096069336 4000
val loss = 11.231857299804688
training loss = 6.640214920043945 4100
val loss = 11.183903694152832
training loss = 6.623098850250244 4200
val loss = 11.132723808288574
training loss = 6.602975845336914 4300
val loss = 11.146890640258789
training loss = 6.556644439697266 4400
val loss = 10.944656372070312
training loss = 6.443816184997559 4500
val loss = 9.934203147888184
training loss = 5.632629871368408 4600
val loss = 8.440152168273926
training loss = 4.279907703399658 4700
val loss = 6.018037796020508
training loss = 2.6847269535064697 4800
val loss = 3.9329018592834473
training loss = 2.5218074321746826 4900
val loss = 3.8135581016540527
training loss = 2.4476559162139893 5000
val loss = 3.6821651458740234
training loss = 2.4002695083618164 5100
val loss = 3.4125423431396484
training loss = 2.3414206504821777 5200
val loss = 3.4869823455810547
training loss = 2.305053234100342 5300
val loss = 3.416958808898926
training loss = 2.2755632400512695 5400
val loss = 3.3718056678771973
training loss = 2.2498536109924316 5500
val loss = 3.299713134765625
training loss = 2.2276804447174072 5600
val loss = 3.2437727451324463
training loss = 2.2082324028015137 5700
val loss = 3.197420597076416
training loss = 2.2079505920410156 5800
val loss = 3.0082650184631348
training loss = 2.1751551628112793 5900
val loss = 3.1075782775878906
training loss = 2.4090523719787598 6000
val loss = 3.826195240020752
training loss = 2.14780330657959 6100
val loss = 3.02485728263855
training loss = 2.1356279850006104 6200
val loss = 2.994542121887207
training loss = 2.1248536109924316 6300
val loss = 2.950604200363159
training loss = 2.1144964694976807 6400
val loss = 2.9161829948425293
training loss = 2.106766700744629 6500
val loss = 2.8426523208618164
training loss = 2.096332550048828 6600
val loss = 2.8479771614074707
training loss = 2.1839606761932373 6700
val loss = 3.2156331539154053
training loss = 2.0807528495788574 6800
val loss = 2.7830746173858643
training loss = 2.073566436767578 6900
val loss = 2.752912998199463
training loss = 2.067645311355591 7000
val loss = 2.741145610809326
training loss = 2.0615735054016113 7100
val loss = 2.6960065364837646
training loss = 2.0990288257598877 7200
val loss = 2.901142120361328
training loss = 2.0521488189697266 7300
val loss = 2.646091938018799
training loss = 2.0481324195861816 7400
val loss = 2.629556894302368
training loss = 2.0451574325561523 7500
val loss = 2.6124043464660645
training loss = 2.0422041416168213 7600
val loss = 2.5848019123077393
training loss = 2.045076608657837 7700
val loss = 2.511221170425415
training loss = 2.0380523204803467 7800
val loss = 2.5543808937072754
training loss = 2.069714069366455 7900
val loss = 2.4135966300964355
training loss = 2.0352225303649902 8000
val loss = 2.527266025543213
training loss = 2.034130334854126 8100
val loss = 2.5123775005340576
training loss = 2.0333969593048096 8200
val loss = 2.5152318477630615
training loss = 2.0326404571533203 8300
val loss = 2.501317024230957
training loss = 2.043832302093506 8400
val loss = 2.594616174697876
training loss = 2.0317671298980713 8500
val loss = 2.488447666168213
training loss = 2.031399965286255 8600
val loss = 2.4826269149780273
training loss = 2.031593084335327 8700
val loss = 2.494497776031494
training loss = 2.0310111045837402 8800
val loss = 2.474031448364258
training loss = 2.0365066528320312 8900
val loss = 2.4220550060272217
training loss = 2.030812978744507 9000
val loss = 2.465092658996582
training loss = 2.030714273452759 9100
val loss = 2.461143970489502
training loss = 2.0308845043182373 9200
val loss = 2.4520912170410156
training loss = 2.0306506156921387 9300
val loss = 2.458554744720459
training loss = 2.037580966949463 9400
val loss = 2.396914482116699
training loss = 2.0306434631347656 9500
val loss = 2.4554524421691895
training loss = 2.052494764328003 9600
val loss = 2.351043462753296
training loss = 2.0306808948516846 9700
val loss = 2.448713779449463
training loss = 2.030639410018921 9800
val loss = 2.450389862060547
training loss = 2.0306925773620605 9900
val loss = 2.451359272003174
training loss = 2.0306572914123535 10000
val loss = 2.448103427886963
training loss = 2.040653944015503 10100
val loss = 2.3830924034118652
training loss = 2.030677556991577 10200
val loss = 2.445589303970337
training loss = 2.030653238296509 10300
val loss = 2.4451663494110107
training loss = 2.030705213546753 10400
val loss = 2.4484000205993652
training loss = 2.0306522846221924 10500
val loss = 2.4436604976654053
training loss = 2.0437252521514893 10600
val loss = 2.3622803688049316
training loss = 2.0306437015533447 10700
val loss = 2.4424638748168945
training loss = 2.0422065258026123 10800
val loss = 2.365490198135376
training loss = 2.0306320190429688 10900
val loss = 2.4424033164978027
training loss = 2.030590057373047 11000
val loss = 2.440748929977417
training loss = 2.0324454307556152 11100
val loss = 2.475257635116577
training loss = 2.030562400817871 11200
val loss = 2.4408631324768066
training loss = 2.0305399894714355 11300
val loss = 2.4364748001098633
training loss = 2.03056263923645 11400
val loss = 2.4447407722473145
training loss = 2.030485153198242 11500
val loss = 2.4386239051818848
training loss = 2.030546188354492 11600
val loss = 2.4457764625549316
training loss = 2.030442953109741 11700
val loss = 2.4387080669403076
training loss = 2.0303990840911865 11800
val loss = 2.4374961853027344
training loss = 2.0305228233337402 11900
val loss = 2.4306881427764893
training loss = 2.030348777770996 12000
val loss = 2.4372763633728027
training loss = 2.030543804168701 12100
val loss = 2.446570634841919
training loss = 2.030292510986328 12200
val loss = 2.437018871307373
training loss = 2.373591423034668 12300
val loss = 2.2410531044006348
training loss = 2.0302340984344482 12400
val loss = 2.4375696182250977
training loss = 2.030181407928467 12500
val loss = 2.4364941120147705
training loss = 2.030733346939087 12600
val loss = 2.41998291015625
training loss = 2.030111074447632 12700
val loss = 2.4370169639587402
training loss = 2.030090808868408 12800
val loss = 2.43194580078125
training loss = 2.0300540924072266 12900
val loss = 2.4402337074279785
training loss = 2.029975652694702 13000
val loss = 2.4365897178649902
training loss = 2.0299932956695557 13100
val loss = 2.4314188957214355
training loss = 2.0298871994018555 13200
val loss = 2.436412811279297
training loss = 2.029815912246704 13300
val loss = 2.4375650882720947
training loss = 2.029954671859741 13400
val loss = 2.449069023132324
training loss = 2.0296921730041504 13500
val loss = 2.4373786449432373
training loss = 2.043931007385254 13600
val loss = 2.3543176651000977
training loss = 2.0295424461364746 13700
val loss = 2.4377105236053467
training loss = 2.0824429988861084 13800
val loss = 2.296623468399048
training loss = 2.0293684005737305 13900
val loss = 2.4361422061920166
training loss = 2.0292112827301025 14000
val loss = 2.4388797283172607
training loss = 2.029467821121216 14100
val loss = 2.4266457557678223
training loss = 2.028919219970703 14200
val loss = 2.4411678314208984
training loss = 2.2097795009613037 14300
val loss = 2.239462375640869
training loss = 2.0285496711730957 14400
val loss = 2.4405736923217773
training loss = 2.028268337249756 14500
val loss = 2.443572759628296
training loss = 2.029452323913574 14600
val loss = 2.4797844886779785
training loss = 2.027700185775757 14700
val loss = 2.4457716941833496
training loss = 2.027364730834961 14800
val loss = 2.442326545715332
training loss = 2.027146339416504 14900
val loss = 2.4394407272338867
training loss = 2.0266482830047607 15000
val loss = 2.446444034576416
training loss = 2.026400089263916 15100
val loss = 2.4547016620635986
training loss = 2.0260071754455566 15200
val loss = 2.4462270736694336
training loss = 2.390638589859009 15300
val loss = 3.224808692932129
training loss = 2.0255091190338135 15400
val loss = 2.4441773891448975
training loss = 2.025268793106079 15500
val loss = 2.4417567253112793
training loss = 2.025249719619751 15600
val loss = 2.432349920272827
training loss = 2.0249812602996826 15700
val loss = 2.4394359588623047
training loss = 2.0248284339904785 15800
val loss = 2.4375510215759277
training loss = 2.024794340133667 15900
val loss = 2.4379372596740723
training loss = 2.0246505737304688 16000
val loss = 2.4363696575164795
training loss = 2.0246012210845947 16100
val loss = 2.427967071533203
training loss = 2.024500846862793 16200
val loss = 2.4343910217285156
training loss = 2.02437424659729 16300
val loss = 2.4335522651672363
training loss = 2.0302000045776367 16400
val loss = 2.5000853538513184
training loss = 2.024216651916504 16500
val loss = 2.4340109825134277
training loss = 2.0240914821624756 16600
val loss = 2.432169198989868
training loss = 2.02414870262146 16700
val loss = 2.4250879287719727
training loss = 2.0239038467407227 16800
val loss = 2.4320192337036133
training loss = 2.1929776668548584 16900
val loss = 2.2358226776123047
training loss = 2.023705244064331 17000
val loss = 2.4331068992614746
training loss = 2.023552179336548 17100
val loss = 2.4304044246673584
training loss = 2.0242745876312256 17200
val loss = 2.4559504985809326
training loss = 2.0233120918273926 17300
val loss = 2.4307844638824463
training loss = 2.1530001163482666 17400
val loss = 2.2378532886505127
training loss = 2.023090124130249 17500
val loss = 2.4322257041931152
training loss = 2.0229132175445557 17600
val loss = 2.430088520050049
training loss = 2.3614399433135986 17700
val loss = 3.1749229431152344
training loss = 2.022676944732666 17800
val loss = 2.433809518814087
training loss = 2.0224769115448 17900
val loss = 2.429356098175049
training loss = 2.2225699424743652 18000
val loss = 2.9707584381103516
training loss = 2.022207021713257 18100
val loss = 2.4300003051757812
training loss = 2.0220110416412354 18200
val loss = 2.428676128387451
training loss = 2.0242056846618652 18300
val loss = 2.4676778316497803
training loss = 2.0217058658599854 18400
val loss = 2.4285988807678223
training loss = 2.0215001106262207 18500
val loss = 2.427974224090576
training loss = 2.0218167304992676 18600
val loss = 2.4470410346984863
training loss = 2.021188497543335 18700
val loss = 2.4278039932250977
training loss = 2.021230459213257 18800
val loss = 2.4406960010528564
training loss = 2.0208964347839355 18900
val loss = 2.4335741996765137
training loss = 2.020629644393921 19000
val loss = 2.426696300506592
training loss = 2.022296190261841 19100
val loss = 2.466709613800049
training loss = 2.020263433456421 19200
val loss = 2.426025390625
training loss = 2.1076717376708984 19300
val loss = 2.748342752456665
training loss = 2.0199074745178223 19400
val loss = 2.4284884929656982
training loss = 2.019662618637085 19500
val loss = 2.425442934036255
training loss = 2.032771348953247 19600
val loss = 2.339137077331543
training loss = 2.019270420074463 19700
val loss = 2.4258620738983154
training loss = 2.0421128273010254 19800
val loss = 2.5699706077575684
training loss = 2.0188708305358887 19900
val loss = 2.428603172302246
training loss = 2.0185906887054443 20000
val loss = 2.424518346786499
training loss = 2.018535614013672 20100
val loss = 2.4167978763580322
training loss = 2.0181565284729004 20200
val loss = 2.4233198165893555
training loss = 2.0191237926483154 20300
val loss = 2.45235538482666
training loss = 2.0177221298217773 20400
val loss = 2.4241409301757812
training loss = 2.0174360275268555 20500
val loss = 2.4226531982421875
training loss = 2.017481565475464 20600
val loss = 2.411717414855957
training loss = 2.016967296600342 20700
val loss = 2.421872615814209
training loss = 2.0168750286102295 20800
val loss = 2.4269704818725586
training loss = 2.01648211479187 20900
val loss = 2.4227328300476074
training loss = 2.0161962509155273 21000
val loss = 2.416356325149536
training loss = 2.01602840423584 21100
val loss = 2.427851915359497
training loss = 2.0156514644622803 21200
val loss = 2.4203577041625977
training loss = 2.0156524181365967 21300
val loss = 2.436032772064209
training loss = 2.0151216983795166 21400
val loss = 2.420473098754883
training loss = 2.015096426010132 21500
val loss = 2.4045517444610596
training loss = 2.0146350860595703 21600
val loss = 2.413403034210205
training loss = 2.014213800430298 21700
val loss = 2.4189071655273438
training loss = 2.014904499053955 21800
val loss = 2.449230194091797
training loss = 2.013646364212036 21900
val loss = 2.4185070991516113
training loss = 2.0132710933685303 22000
val loss = 2.4181032180786133
training loss = 2.0138888359069824 22100
val loss = 2.395977020263672
training loss = 2.0126824378967285 22200
val loss = 2.4184060096740723
training loss = 2.01228928565979 22300
val loss = 2.416351795196533
training loss = 2.0122909545898438 22400
val loss = 2.433095932006836
training loss = 2.011603593826294 22500
val loss = 2.4176292419433594
training loss = 2.011770725250244 22600
val loss = 2.4373886585235596
training loss = 2.010890245437622 22700
val loss = 2.4172215461730957
training loss = 2.0141899585723877 22800
val loss = 2.476095676422119
training loss = 2.010181188583374 22900
val loss = 2.4196407794952393
training loss = 2.009716033935547 23000
val loss = 2.4172511100769043
training loss = 2.010326862335205 23100
val loss = 2.3925347328186035
training loss = 2.008920431137085 23200
val loss = 2.4179110527038574
training loss = 2.1124861240386963 23300
val loss = 2.229706048965454
training loss = 2.008103132247925 23400
val loss = 2.4199538230895996
training loss = 2.0075769424438477 23500
val loss = 2.4184093475341797
training loss = 2.007570266723633 23600
val loss = 2.4361636638641357
training loss = 2.006664991378784 23700
val loss = 2.4187512397766113
training loss = 2.0258948802948 23800
val loss = 2.5540413856506348
training loss = 2.005725860595703 23900
val loss = 2.42061710357666
training loss = 2.00512433052063 24000
val loss = 2.421006202697754
training loss = 2.004952907562256 24100
val loss = 2.4089179039001465
training loss = 2.004101276397705 24200
val loss = 2.4212965965270996
training loss = 2.053278923034668 24300
val loss = 2.6486687660217285
training loss = 2.003002643585205 24400
val loss = 2.4241416454315186
training loss = 2.0023105144500732 24500
val loss = 2.4213762283325195
training loss = 2.0018885135650635 24600
val loss = 2.418055534362793
training loss = 2.0010905265808105 24700
val loss = 2.425201892852783
training loss = 2.0086216926574707 24800
val loss = 2.3594517707824707
training loss = 1.9997758865356445 24900
val loss = 2.4274940490722656
training loss = 2.004140615463257 25000
val loss = 2.505206823348999
training loss = 1.9984346628189087 25100
val loss = 2.432316541671753
training loss = 1.9975863695144653 25200
val loss = 2.431786060333252
training loss = 2.0046002864837646 25300
val loss = 2.370480537414551
training loss = 1.9961084127426147 25400
val loss = 2.435903787612915
training loss = 1.9951965808868408 25500
val loss = 2.441342353820801
training loss = 1.9945110082626343 25600
val loss = 2.444833755493164
training loss = 1.9934988021850586 25700
val loss = 2.4415457248687744
training loss = 1.9946224689483643 25800
val loss = 2.4822657108306885
training loss = 1.9917871952056885 25900
val loss = 2.4463582038879395
training loss = 2.347644805908203 26000
val loss = 3.238943576812744
training loss = 1.989957571029663 26100
val loss = 2.45518159866333
training loss = 1.9888434410095215 26200
val loss = 2.4536404609680176
training loss = 1.9883002042770386 26300
val loss = 2.4711291790008545
training loss = 1.9869507551193237 26400
val loss = 2.4611802101135254
training loss = 2.141641139984131 26500
val loss = 2.9216766357421875
training loss = 1.9851053953170776 26600
val loss = 2.470440626144409
training loss = 1.9839516878128052 26700
val loss = 2.4716145992279053
training loss = 2.077723741531372 26800
val loss = 2.3050901889801025
training loss = 1.9819087982177734 26900
val loss = 2.480262517929077
training loss = 1.9859825372695923 27000
val loss = 2.5467162132263184
training loss = 1.979927659034729 27100
val loss = 2.4820873737335205
training loss = 1.9786920547485352 27200
val loss = 2.491542339324951
training loss = 1.9778741598129272 27300
val loss = 2.495512008666992
training loss = 1.9767159223556519 27400
val loss = 2.5005664825439453
training loss = 2.034261465072632 27500
val loss = 2.363736629486084
training loss = 1.9747909307479858 27600
val loss = 2.5111589431762695
training loss = 1.9736562967300415 27700
val loss = 2.512582778930664
reduced chi^2 level 2 = 1.9739876985549927
Constrained alpha: 1.790226936340332
Constrained beta: 2.5093274116516113
Constrained gamma: 13.543201446533203
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 808.7193,  888.6496,  927.4322,  975.2247,  978.3743,  979.7393,
        1124.2955, 1150.6224, 1149.6050, 1136.5950, 1246.0221, 1163.0627,
        1188.2153, 1284.6581, 1289.2266, 1482.2615, 1425.4606, 1425.0328,
        1436.2145, 1517.0917, 1568.1899, 1543.0540, 1580.9275, 1603.0658,
        1643.8015, 1730.3075, 1672.7640, 1640.1698, 1761.5059, 1719.4924,
        1717.7727, 1765.0071, 1695.5441, 1762.5225, 1723.3809, 1767.4023,
        1744.7462, 1689.8544, 1698.6428, 1657.4187, 1729.2583, 1582.7936,
        1513.8519, 1473.9012, 1359.2782, 1332.5764, 1279.4247, 1206.7290,
        1115.1935, 1144.1016, 1103.7969,  932.1729,  927.4836,  891.8488,
         831.3854,  905.1777,  853.8740,  716.4742,  590.6123,  552.9655,
         567.0452,  458.3553,  430.7429,  355.3386,  380.8170,  321.9055,
         261.6103,  267.8573,  212.3316,  156.6723,  165.5240,  136.9622,
         138.0978,   93.5991,   85.3758,   74.8463,   57.3168,   48.8091,
          24.8892,   52.1872,   20.5948,   35.0987,   46.7773])]
3013.8767913256315
1.3896825759563902 12.08713259131639 82.39176575152885
val isze = 8
idinces = [31  5  8 78 11 66 42 68 27 69 76 37 29  9 64 49 72  7 77 61 13  2 24 16
 59  0 50 19 43 23  4 36 12 81 58 47 48 82 67 38 55 14 73 80 10  1 75 79
 26 17  3 46 32 34 62 15 51 21 52  6 74 22 63 28 40 54 25 57 44 45 18 20
 53 33 65 70 60 39 35 30 41 71 56]
we are doing training validation split
training loss = 53.14242172241211 100
val loss = 61.09141540527344
training loss = 8.245321273803711 200
val loss = 4.961263656616211
training loss = 8.228655815124512 300
val loss = 5.135288238525391
training loss = 8.212923049926758 400
val loss = 5.143649101257324
training loss = 8.195103645324707 500
val loss = 5.151650905609131
training loss = 8.175468444824219 600
val loss = 5.1591315269470215
training loss = 8.154223442077637 700
val loss = 5.165738105773926
training loss = 8.131531715393066 800
val loss = 5.171026706695557
training loss = 8.107527732849121 900
val loss = 5.17463493347168
training loss = 8.082328796386719 1000
val loss = 5.176446914672852
training loss = 8.056041717529297 1100
val loss = 5.176231861114502
training loss = 8.028770446777344 1200
val loss = 5.1739349365234375
training loss = 8.00061321258545 1300
val loss = 5.169536113739014
training loss = 7.971663475036621 1400
val loss = 5.163100242614746
training loss = 7.942015171051025 1500
val loss = 5.154804229736328
training loss = 7.911754608154297 1600
val loss = 5.144798278808594
training loss = 7.880957126617432 1700
val loss = 5.1332807540893555
training loss = 7.849672317504883 1800
val loss = 5.120418548583984
training loss = 7.817923545837402 1900
val loss = 5.106500625610352
training loss = 7.785674095153809 2000
val loss = 5.09171199798584
training loss = 7.752798080444336 2100
val loss = 5.0762553215026855
training loss = 7.719021797180176 2200
val loss = 5.060303688049316
training loss = 7.683823585510254 2300
val loss = 5.044035911560059
training loss = 7.646247863769531 2400
val loss = 5.027558326721191
training loss = 7.604586601257324 2500
val loss = 5.011019229888916
training loss = 7.555759429931641 2600
val loss = 4.994419097900391
training loss = 7.494410037994385 2700
val loss = 4.977934837341309
training loss = 7.411803722381592 2800
val loss = 4.961364269256592
training loss = 7.295543670654297 2900
val loss = 4.942509174346924
training loss = 7.128758430480957 3000
val loss = 4.910400390625
training loss = 6.880034446716309 3100
val loss = 4.835962772369385
training loss = 6.478233337402344 3200
val loss = 4.670079708099365
training loss = 5.783024311065674 3300
val loss = 4.352742671966553
training loss = 4.621006965637207 3400
val loss = 3.836163282394409
training loss = 3.2643094062805176 3500
val loss = 3.2237637042999268
training loss = 2.678050994873047 3600
val loss = 2.8988096714019775
training loss = 2.608431577682495 3700
val loss = 2.8341331481933594
training loss = 2.597238779067993 3800
val loss = 2.807199001312256
training loss = 2.589367151260376 3900
val loss = 2.7850189208984375
training loss = 2.582336902618408 4000
val loss = 2.765894889831543
training loss = 2.5758907794952393 4100
val loss = 2.7494497299194336
training loss = 2.5699119567871094 4200
val loss = 2.735356330871582
training loss = 2.56430721282959 4300
val loss = 2.723379135131836
training loss = 2.5590052604675293 4400
val loss = 2.7132420539855957
training loss = 2.553938865661621 4500
val loss = 2.7046597003936768
training loss = 2.5490527153015137 4600
val loss = 2.697446584701538
training loss = 2.5442936420440674 4700
val loss = 2.691349744796753
training loss = 2.539628505706787 4800
val loss = 2.6911635398864746
training loss = 2.534985065460205 4900
val loss = 2.679849147796631
training loss = 2.5466763973236084 5000
val loss = 2.831989288330078
training loss = 2.5257067680358887 5100
val loss = 2.6698927879333496
training loss = 2.6822257041931152 5200
val loss = 2.353059768676758
training loss = 2.516066789627075 5300
val loss = 2.655233383178711
training loss = 2.510963201522827 5400
val loss = 2.6539549827575684
training loss = 2.506202220916748 5500
val loss = 2.6765496730804443
training loss = 2.4999425411224365 5600
val loss = 2.6433310508728027
training loss = 2.5477190017700195 5700
val loss = 2.417675018310547
training loss = 2.4872477054595947 5800
val loss = 2.633657455444336
training loss = 2.479862928390503 5900
val loss = 2.6262412071228027
training loss = 2.4807770252227783 6000
val loss = 2.5168187618255615
training loss = 2.4621286392211914 6100
val loss = 2.6122448444366455
training loss = 2.450666666030884 6200
val loss = 2.5839617252349854
training loss = 2.436756134033203 6300
val loss = 2.602182149887085
training loss = 2.4192845821380615 6400
val loss = 2.585820198059082
training loss = 2.4027507305145264 6500
val loss = 2.6512722969055176
training loss = 2.3768999576568604 6600
val loss = 2.5590362548828125
training loss = 2.5942542552948 6700
val loss = 2.1935315132141113
training loss = 2.335376739501953 6800
val loss = 2.5208709239959717
training loss = 2.3160910606384277 6900
val loss = 2.504215717315674
training loss = 2.298436164855957 7000
val loss = 2.5078275203704834
training loss = 2.2795395851135254 7100
val loss = 2.4565374851226807
training loss = 2.260113000869751 7200
val loss = 2.4260616302490234
training loss = 2.241393566131592 7300
val loss = 2.4156672954559326
training loss = 2.2218470573425293 7400
val loss = 2.3773226737976074
training loss = 2.2068803310394287 7500
val loss = 2.4241461753845215
training loss = 2.1848084926605225 7600
val loss = 2.324172258377075
training loss = 2.166407823562622 7700
val loss = 2.2964048385620117
training loss = 2.149989366531372 7800
val loss = 2.2472612857818604
training loss = 2.133619546890259 7900
val loss = 2.2414417266845703
training loss = 2.1225740909576416 8000
val loss = 2.2805283069610596
training loss = 2.1068146228790283 8100
val loss = 2.1929569244384766
training loss = 2.52520751953125 8200
val loss = 1.8424216508865356
training loss = 2.0864250659942627 8300
val loss = 2.1524763107299805
training loss = 2.078225612640381 8400
val loss = 2.133619785308838
training loss = 2.398275136947632 8500
val loss = 3.0232996940612793
training loss = 2.064910650253296 8600
val loss = 2.105058431625366
training loss = 2.0593767166137695 8700
val loss = 2.0929007530212402
training loss = 2.0567522048950195 8800
val loss = 2.035604476928711
training loss = 2.050374746322632 8900
val loss = 2.0751423835754395
training loss = 2.046334981918335 9000
val loss = 2.067683696746826
training loss = 2.044032335281372 9100
val loss = 2.023704767227173
training loss = 2.0392439365386963 9200
val loss = 2.056562900543213
training loss = 2.03843092918396 9300
val loss = 2.000913619995117
training loss = 2.0331082344055176 9400
val loss = 2.04207706451416
training loss = 2.030226230621338 9500
val loss = 2.0447001457214355
training loss = 2.027384042739868 9600
val loss = 2.042426347732544
training loss = 2.025070905685425 9700
val loss = 2.024301528930664
training loss = 2.022324562072754 9800
val loss = 2.0365281105041504
training loss = 2.420668601989746 9900
val loss = 3.051515817642212
training loss = 2.0175344944000244 10000
val loss = 2.0357422828674316
training loss = 2.0151777267456055 10100
val loss = 2.0306801795959473
training loss = 2.014056444168091 10200
val loss = 1.998060703277588
training loss = 2.010838031768799 10300
val loss = 2.02866530418396
training loss = 2.1717395782470703 10400
val loss = 2.5847034454345703
training loss = 2.006693124771118 10500
val loss = 2.0259804725646973
training loss = 2.004608631134033 10600
val loss = 2.0253026485443115
training loss = 2.002774477005005 10700
val loss = 2.017395496368408
training loss = 2.000746250152588 10800
val loss = 2.024477958679199
training loss = 2.002495527267456 10900
val loss = 2.082885265350342
training loss = 1.9969910383224487 11000
val loss = 2.021667718887329
training loss = 1.9950664043426514 11100
val loss = 2.02467942237854
training loss = 1.9933958053588867 11200
val loss = 2.0288166999816895
training loss = 1.9915629625320435 11300
val loss = 2.023186683654785
training loss = 1.995452880859375 11400
val loss = 1.9480608701705933
training loss = 1.9882316589355469 11500
val loss = 2.0221047401428223
training loss = 1.9865100383758545 11600
val loss = 2.0241055488586426
training loss = 1.9897160530090332 11700
val loss = 1.955750584602356
training loss = 1.9833482503890991 11800
val loss = 2.0246009826660156
training loss = 2.353337049484253 11900
val loss = 1.7165544033050537
training loss = 1.9803471565246582 12000
val loss = 2.0249104499816895
training loss = 1.978805422782898 12100
val loss = 2.0285658836364746
training loss = 1.9776235818862915 12200
val loss = 2.017508029937744
training loss = 1.9760905504226685 12300
val loss = 2.028826951980591
training loss = 1.9819649457931519 12400
val loss = 1.9461534023284912
training loss = 1.9736223220825195 12500
val loss = 2.0309741497039795
training loss = 1.9723330736160278 12600
val loss = 2.0356388092041016
training loss = 1.9713877439498901 12700
val loss = 2.0398669242858887
training loss = 1.9701892137527466 12800
val loss = 2.0334677696228027
training loss = 1.9834896326065063 12900
val loss = 1.9106998443603516
training loss = 1.9681928157806396 13000
val loss = 2.0372800827026367
training loss = 1.9671387672424316 13100
val loss = 2.0358693599700928
training loss = 1.9685779809951782 13200
val loss = 2.0900521278381348
training loss = 1.9655213356018066 13300
val loss = 2.0380806922912598
training loss = 1.9645990133285522 13400
val loss = 2.0381407737731934
training loss = 1.9727061986923218 13500
val loss = 1.9436426162719727
training loss = 1.9631543159484863 13600
val loss = 2.0398988723754883
training loss = 1.962327480316162 13700
val loss = 2.0398788452148438
training loss = 1.9639742374420166 13800
val loss = 1.9929120540618896
training loss = 1.961039662361145 13900
val loss = 2.0408172607421875
training loss = 1.9602807760238647 14000
val loss = 2.0406813621520996
training loss = 1.9603530168533325 14100
val loss = 2.0189027786254883
training loss = 1.9591927528381348 14200
val loss = 2.042121410369873
training loss = 1.9584949016571045 14300
val loss = 2.041245222091675
training loss = 1.9585055112838745 14400
val loss = 2.019071102142334
training loss = 1.957383632659912 14500
val loss = 2.041921615600586
training loss = 2.0363669395446777 14600
val loss = 1.8041408061981201
training loss = 1.956411600112915 14700
val loss = 2.0429673194885254
training loss = 1.9557784795761108 14800
val loss = 2.0420236587524414
training loss = 1.9554033279418945 14900
val loss = 2.0350422859191895
training loss = 1.95478355884552 15000
val loss = 2.0436463356018066
training loss = 1.954152226448059 15100
val loss = 2.04097056388855
training loss = 1.9538558721542358 15200
val loss = 2.049103021621704
training loss = 1.9532020092010498 15300
val loss = 2.041351318359375
training loss = 2.200742244720459 15400
val loss = 1.6956143379211426
training loss = 1.9522807598114014 15500
val loss = 2.0369856357574463
training loss = 1.9516431093215942 15600
val loss = 2.0400009155273438
training loss = 1.9535452127456665 15700
val loss = 2.0965423583984375
training loss = 1.950650691986084 15800
val loss = 2.0392024517059326
training loss = 1.9700086116790771 15900
val loss = 2.202967643737793
training loss = 1.949720859527588 16000
val loss = 2.0392990112304688
training loss = 1.949074387550354 16100
val loss = 2.038590908050537
training loss = 1.9486788511276245 16200
val loss = 2.0355379581451416
training loss = 1.9480485916137695 16300
val loss = 2.038517951965332
training loss = 2.160189390182495 16400
val loss = 1.6903434991836548
training loss = 1.947035789489746 16500
val loss = 2.0404441356658936
training loss = 1.9463226795196533 16600
val loss = 2.0363950729370117
training loss = 1.9469444751739502 16700
val loss = 2.001203775405884
training loss = 1.9452240467071533 16800
val loss = 2.0361998081207275
training loss = 1.9473059177398682 16900
val loss = 1.9836127758026123
training loss = 1.944072961807251 17000
val loss = 2.034818649291992
training loss = 1.943692684173584 17100
val loss = 2.0602316856384277
training loss = 1.9428828954696655 17200
val loss = 2.039081335067749
training loss = 1.9420539140701294 17300
val loss = 2.0361645221710205
training loss = 1.9416528940200806 17400
val loss = 2.0470638275146484
training loss = 1.9407981634140015 17500
val loss = 2.036470413208008
training loss = 1.9398952722549438 17600
val loss = 2.039820671081543
training loss = 1.9397006034851074 17700
val loss = 2.055091619491577
training loss = 1.9385631084442139 17800
val loss = 2.0379371643066406
training loss = 2.4175453186035156 17900
val loss = 3.2824487686157227
training loss = 1.937307357788086 18000
val loss = 2.0387330055236816
training loss = 1.9363374710083008 18100
val loss = 2.040382146835327
training loss = 1.9356905221939087 18200
val loss = 2.0189998149871826
training loss = 1.9348279237747192 18300
val loss = 2.034649133682251
training loss = 1.93373441696167 18400
val loss = 2.0447206497192383
training loss = 1.9332857131958008 18500
val loss = 2.0562736988067627
training loss = 1.9321480989456177 18600
val loss = 2.047090530395508
training loss = 1.9445468187332153 18700
val loss = 2.194790840148926
training loss = 1.9305058717727661 18800
val loss = 2.0508697032928467
training loss = 2.1085376739501953 18900
val loss = 2.7005715370178223
training loss = 1.9289474487304688 19000
val loss = 2.050208568572998
training loss = 1.9278621673583984 19100
val loss = 2.0550966262817383
training loss = 1.9277828931808472 19200
val loss = 2.033665657043457
training loss = 1.9263957738876343 19300
val loss = 2.058415651321411
training loss = 1.9261263608932495 19400
val loss = 2.0962343215942383
training loss = 1.9251365661621094 19500
val loss = 2.070985794067383
training loss = 1.924046277999878 19600
val loss = 2.062572479248047
training loss = 1.9230058193206787 19700
val loss = 2.067779064178467
training loss = 1.9227055311203003 19800
val loss = 2.075286626815796
training loss = 1.921614646911621 19900
val loss = 2.066272258758545
training loss = 2.0762784481048584 20000
val loss = 2.6789331436157227
training loss = 1.9202214479446411 20100
val loss = 2.067840099334717
training loss = 1.91921067237854 20200
val loss = 2.0688693523406982
training loss = 1.9196932315826416 20300
val loss = 2.033242702484131
training loss = 1.9187891483306885 20400
val loss = 2.072885036468506
training loss = 1.9190375804901123 20500
val loss = 2.077317237854004
training loss = 1.9193974733352661 20600
val loss = 2.0837481021881104
training loss = 1.919522762298584 20700
val loss = 2.0830750465393066
training loss = 1.9211410284042358 20800
val loss = 2.0369253158569336
training loss = 1.9198226928710938 20900
val loss = 2.07878041267395
training loss = 1.9198167324066162 21000
val loss = 2.085244655609131
training loss = 1.9302747249603271 21100
val loss = 2.2218329906463623
training loss = 1.9198702573776245 21200
val loss = 2.087066411972046
training loss = 2.0591371059417725 21300
val loss = 1.7540829181671143
training loss = 1.9198232889175415 21400
val loss = 2.0892581939697266
training loss = 1.9197032451629639 21500
val loss = 2.087669849395752
training loss = 1.9197996854782104 21600
val loss = 2.0984580516815186
training loss = 1.919562578201294 21700
val loss = 2.0876502990722656
training loss = 2.348358631134033 21800
val loss = 1.6633609533309937
training loss = 1.9193753004074097 21900
val loss = 2.092090606689453
training loss = 1.9192055463790894 22000
val loss = 2.0932962894439697
training loss = 1.9193209409713745 22100
val loss = 2.1043505668640137
training loss = 1.9189443588256836 22200
val loss = 2.0880236625671387
training loss = 1.92129647731781 22300
val loss = 2.029299020767212
training loss = 1.9186878204345703 22400
val loss = 2.0867648124694824
training loss = 2.326569080352783 22500
val loss = 3.2171735763549805
training loss = 1.9184106588363647 22600
val loss = 2.0839014053344727
training loss = 1.9182556867599487 22700
val loss = 2.0781126022338867
training loss = 1.9181820154190063 22800
val loss = 2.0951411724090576
training loss = 1.9179134368896484 22900
val loss = 2.086451530456543
training loss = 1.918033480644226 23000
val loss = 2.102334499359131
training loss = 1.9176136255264282 23100
val loss = 2.0846755504608154
training loss = 1.9238510131835938 23200
val loss = 2.1886024475097656
training loss = 1.917319416999817 23300
val loss = 2.0823159217834473
training loss = 1.917110800743103 23400
val loss = 2.081804037094116
training loss = 1.9170632362365723 23500
val loss = 2.0906128883361816
training loss = 1.9168121814727783 23600
val loss = 2.083789110183716
training loss = 1.9590626955032349 23700
val loss = 1.8682811260223389
training loss = 1.9165092706680298 23800
val loss = 2.083261013031006
training loss = 1.9226747751235962 23900
val loss = 2.185513734817505
training loss = 1.916239857673645 24000
val loss = 2.0882568359375
training loss = 1.9159916639328003 24100
val loss = 2.081829786300659
training loss = 1.9162263870239258 24200
val loss = 2.058915615081787
training loss = 1.9156959056854248 24300
val loss = 2.0806190967559814
training loss = 2.3842151165008545 24400
val loss = 1.6642544269561768
training loss = 1.915396809577942 24500
val loss = 2.0815236568450928
training loss = 1.9151843786239624 24600
val loss = 2.0775792598724365
training loss = 1.9153412580490112 24700
val loss = 2.0986597537994385
training loss = 1.9148708581924438 24800
val loss = 2.079796075820923
training loss = 1.9202324151992798 24900
val loss = 1.9922873973846436
reduced chi^2 level 2 = 1.914597988128662
Constrained alpha: 1.8655250072479248
Constrained beta: -0.0021491507068276405
Constrained gamma: 13.778167724609375
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 901.8527,  856.5600,  967.5355,  933.7633,  985.3403, 1023.4960,
        1121.8035, 1134.8945, 1163.5535, 1130.0323, 1244.1604, 1215.7350,
        1279.8169, 1238.9742, 1321.5095, 1401.2986, 1394.9534, 1430.7780,
        1506.9584, 1532.9237, 1583.5730, 1622.7629, 1607.7848, 1620.5121,
        1618.1520, 1735.4103, 1562.2147, 1767.7167, 1737.2029, 1701.3350,
        1606.8027, 1661.0428, 1710.7117, 1763.6661, 1723.6213, 1782.6639,
        1704.4380, 1600.7959, 1598.5674, 1618.5471, 1675.5067, 1500.5909,
        1523.2841, 1475.2988, 1288.1055, 1294.3711, 1316.9108, 1228.9097,
        1123.1477, 1222.2772, 1114.2887,  997.1614,  943.1226,  946.2550,
         894.0056,  906.4123,  787.5334,  729.8827,  575.5128,  510.7050,
         540.9746,  494.7906,  438.8752,  408.2597,  338.4644,  306.2781,
         300.5459,  261.7215,  199.2399,  164.3658,  151.3976,  155.2489,
         119.5109,  106.3556,   94.4073,   71.8859,   50.1834,   40.1022,
          28.7534,   46.6971,   21.5731,   44.2297,   39.5715])]
2862.2379403045074
3.543388524090531 4.288232326088554 69.84951601303926
val isze = 8
idinces = [51 61 43  8 36  4 16 66 55 27 13 35 23 39 72  3 74 32 77 53 10 45 52 22
 71 57 48 79 63  0 12 73 28  5 60 38 20 65 25  1 42 29 64 31 81 14 30 68
 44 75 58 47 76  6 49 18  9  7 62  2 17 69 11 82 34 46 24 59 50 70 80 15
 41 33 78 21 26 56 19 54 67 37 40]
we are doing training validation split
training loss = 53.84757995605469 100
val loss = 59.14872741699219
training loss = 29.43241310119629 200
val loss = 33.19151306152344
training loss = 18.468740463256836 300
val loss = 21.079303741455078
training loss = 12.957374572753906 400
val loss = 14.339138984680176
training loss = 9.996389389038086 500
val loss = 10.29121208190918
training loss = 8.371896743774414 600
val loss = 7.7985687255859375
training loss = 7.464376926422119 700
val loss = 6.230722904205322
training loss = 6.9503655433654785 800
val loss = 5.224961280822754
training loss = 6.656768321990967 900
val loss = 4.56904411315918
training loss = 6.487979888916016 1000
val loss = 4.135403633117676
training loss = 6.389902591705322 1100
val loss = 3.845369815826416
training loss = 6.331534385681152 1200
val loss = 3.649686336517334
training loss = 6.295088768005371 1300
val loss = 3.5167431831359863
training loss = 6.270453929901123 1400
val loss = 3.4261233806610107
training loss = 6.251980781555176 1500
val loss = 3.364302158355713
training loss = 6.236581325531006 1600
val loss = 3.3221993446350098
training loss = 6.222564220428467 1700
val loss = 3.2935941219329834
training loss = 6.20894718170166 1800
val loss = 3.27402400970459
training loss = 6.194993495941162 1900
val loss = 3.260357618331909
training loss = 6.179834365844727 2000
val loss = 3.2500782012939453
training loss = 6.161961555480957 2100
val loss = 3.2409520149230957
training loss = 6.137973308563232 2200
val loss = 3.2299671173095703
training loss = 6.098414897918701 2300
val loss = 3.2109947204589844
training loss = 6.009881973266602 2400
val loss = 3.162899971008301
training loss = 5.7309184074401855 2500
val loss = 2.998324394226074
training loss = 4.81953763961792 2600
val loss = 2.447460174560547
training loss = 3.259805917739868 2700
val loss = 1.4016635417938232
training loss = 2.496549606323242 2800
val loss = 0.9236774444580078
training loss = 2.4024126529693604 2900
val loss = 0.9959215521812439
training loss = 2.382758855819702 3000
val loss = 1.0734654664993286
training loss = 2.3708035945892334 3100
val loss = 1.1315844058990479
training loss = 2.361680030822754 3200
val loss = 1.1764559745788574
training loss = 2.3544583320617676 3300
val loss = 1.2119500637054443
training loss = 2.3482391834259033 3400
val loss = 1.2393438816070557
training loss = 2.3430771827697754 3500
val loss = 1.2601381540298462
training loss = 2.338430404663086 3600
val loss = 1.2759795188903809
training loss = 2.337418794631958 3700
val loss = 1.2953176498413086
training loss = 2.330397129058838 3800
val loss = 1.297966480255127
training loss = 2.369044542312622 3900
val loss = 1.336338758468628
training loss = 2.3237223625183105 4000
val loss = 1.3106545209884644
training loss = 2.320777654647827 4100
val loss = 1.3147852420806885
training loss = 2.3181278705596924 4200
val loss = 1.3181657791137695
training loss = 2.3156702518463135 4300
val loss = 1.3205301761627197
training loss = 2.3137049674987793 4400
val loss = 1.323113203048706
training loss = 2.3113927841186523 4500
val loss = 1.3235752582550049
training loss = 2.316300630569458 4600
val loss = 1.3252366781234741
training loss = 2.307743549346924 4700
val loss = 1.3251476287841797
training loss = 2.30615234375 4800
val loss = 1.325580358505249
training loss = 2.304624080657959 4900
val loss = 1.3256242275238037
training loss = 2.303267478942871 5000
val loss = 1.3258460760116577
training loss = 2.3030364513397217 5100
val loss = 1.3253631591796875
training loss = 2.300697088241577 5200
val loss = 1.324771523475647
training loss = 2.299588680267334 5300
val loss = 1.3248480558395386
training loss = 2.298854351043701 5400
val loss = 1.3227100372314453
training loss = 2.297412395477295 5500
val loss = 1.3235180377960205
training loss = 2.2994608879089355 5600
val loss = 1.3220793008804321
training loss = 2.2954139709472656 5700
val loss = 1.3223261833190918
training loss = 2.294492721557617 5800
val loss = 1.3213577270507812
training loss = 2.2935385704040527 5900
val loss = 1.3194098472595215
training loss = 2.292645215988159 6000
val loss = 1.319305181503296
training loss = 2.296891689300537 6100
val loss = 1.3160587549209595
training loss = 2.290868043899536 6200
val loss = 1.316884994506836
training loss = 2.29007887840271 6300
val loss = 1.3159458637237549
training loss = 2.289273738861084 6400
val loss = 1.3152822256088257
training loss = 2.288411855697632 6500
val loss = 1.3135299682617188
training loss = 2.2994136810302734 6600
val loss = 1.3139104843139648
training loss = 2.286799669265747 6700
val loss = 1.3110390901565552
training loss = 2.286064863204956 6800
val loss = 1.3100275993347168
training loss = 2.2854254245758057 6900
val loss = 1.3098788261413574
training loss = 2.284482002258301 7000
val loss = 1.3071038722991943
training loss = 2.2837722301483154 7100
val loss = 1.3061180114746094
training loss = 2.2864973545074463 7200
val loss = 1.3134912252426147
training loss = 2.2822675704956055 7300
val loss = 1.3034594058990479
training loss = 2.2815756797790527 7400
val loss = 1.3022806644439697
training loss = 2.2809736728668213 7500
val loss = 1.302284836769104
training loss = 2.2801146507263184 7600
val loss = 1.2997945547103882
training loss = 2.2829954624176025 7700
val loss = 1.2960718870162964
training loss = 2.2786765098571777 7800
val loss = 1.2971692085266113
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 844.2374,  861.7513,  928.5913,  928.4500,  967.8902, 1069.6184,
        1142.7269, 1142.4869, 1146.3379, 1102.1793, 1217.8412, 1143.2927,
        1204.4211, 1205.4777, 1380.6002, 1415.0741, 1471.3513, 1478.9581,
        1503.6198, 1574.8827, 1573.7312, 1614.7474, 1653.6661, 1694.3380,
        1627.7988, 1659.2971, 1581.9160, 1753.0994, 1703.2491, 1720.6566,
        1709.5315, 1737.4885, 1744.1287, 1690.3668, 1684.3334, 1750.9164,
        1671.3264, 1600.2795, 1636.4620, 1662.6495, 1669.3251, 1587.4169,
        1480.6316, 1550.3142, 1331.8550, 1313.3333, 1344.0216, 1260.9028,
        1193.2531, 1176.4783, 1070.3278,  974.5916,  936.9384,  906.6553,
         888.7614,  832.7150,  823.6765,  700.7919,  614.0065,  534.2251,
         558.5910,  476.8678,  430.0016,  392.3245,  331.1884,  321.2856,
         305.3313,  275.7222,  201.9137,  173.9703,  147.0489,  179.5353,
         158.0742,   86.8970,  102.7567,   71.4223,   58.7977,   36.2673,
          50.7193,   41.5829,   22.3628,   37.1651,   39.2575])]
2567.811435452454
3.5105138169786407 8.923288846498096 29.765189345456943
val isze = 8
idinces = [12 80 45 40 66 44 70  1 69  2 55 22 28 38 30 32 56 67 60 46  0 51 23 78
 26 31 74 25 35 82 11 18 19 63 14 13 27 20 75  5 37 16 42  9 29 17 24 77
 79 81 10 76 34 68  6 48 53 15 61  4 49 21  8 33  7 72 65 71 52 43 54  3
 39 59 64 41 47 57 36 58 73 62 50]
we are doing training validation split
training loss = 251.9672393798828 100
val loss = 325.67218017578125
training loss = 18.845090866088867 200
val loss = 28.457263946533203
training loss = 13.596165657043457 300
val loss = 19.220918655395508
training loss = 10.76448917388916 400
val loss = 14.031375885009766
training loss = 9.210906982421875 500
val loss = 11.107198715209961
training loss = 8.344572067260742 600
val loss = 9.442532539367676
training loss = 7.855534076690674 700
val loss = 8.486981391906738
training loss = 7.5768513679504395 800
val loss = 7.935151100158691
training loss = 7.4163031578063965 900
val loss = 7.614632606506348
training loss = 7.321976184844971 1000
val loss = 7.426864147186279
training loss = 7.264339447021484 1100
val loss = 7.314949989318848
training loss = 7.226597785949707 1200
val loss = 7.246122360229492
training loss = 7.199285507202148 1300
val loss = 7.201545238494873
training loss = 7.177156925201416 1400
val loss = 7.170513153076172
training loss = 7.157392978668213 1500
val loss = 7.146980285644531
training loss = 7.138525485992432 1600
val loss = 7.1276702880859375
training loss = 7.119833946228027 1700
val loss = 7.110711097717285
training loss = 7.100998878479004 1800
val loss = 7.095052719116211
training loss = 7.0819172859191895 1900
val loss = 7.080177307128906
training loss = 7.062640190124512 2000
val loss = 7.065765380859375
training loss = 7.043304443359375 2100
val loss = 7.051787853240967
training loss = 7.024130344390869 2200
val loss = 7.038114070892334
training loss = 7.005388259887695 2300
val loss = 7.025042533874512
training loss = 6.987288475036621 2400
val loss = 7.012744903564453
training loss = 6.969806671142578 2500
val loss = 7.0013580322265625
training loss = 6.952178478240967 2600
val loss = 6.9908366203308105
training loss = 6.9312639236450195 2700
val loss = 6.9807586669921875
training loss = 6.891192436218262 2800
val loss = 6.967188358306885
training loss = 6.560389995574951 2900
val loss = 6.861234664916992
training loss = 4.765344142913818 3000
val loss = 5.918008804321289
training loss = 2.7109527587890625 3100
val loss = 4.734372615814209
training loss = 1.980178713798523 3200
val loss = 4.460629463195801
training loss = 1.949155330657959 3300
val loss = 4.391434192657471
training loss = 1.9323161840438843 3400
val loss = 4.333343505859375
training loss = 1.919654130935669 3500
val loss = 4.258757591247559
training loss = 1.921359896659851 3600
val loss = 4.017019271850586
training loss = 1.900388240814209 3700
val loss = 4.216239929199219
training loss = 1.8937946557998657 3800
val loss = 4.2345781326293945
training loss = 1.8868534564971924 3900
val loss = 4.161992073059082
training loss = 1.8817973136901855 4000
val loss = 4.121795177459717
training loss = 1.8767824172973633 4100
val loss = 4.121255397796631
training loss = 1.8729655742645264 4200
val loss = 4.078583717346191
training loss = 1.8688883781433105 4300
val loss = 4.089503288269043
training loss = 1.8663579225540161 4400
val loss = 4.1281023025512695
training loss = 1.8624638319015503 4500
val loss = 4.0649518966674805
training loss = 1.8916743993759155 4600
val loss = 4.455759525299072
training loss = 1.8569751977920532 4700
val loss = 4.043693542480469
training loss = 1.8593018054962158 4800
val loss = 4.188424587249756
training loss = 1.8522677421569824 4900
val loss = 4.03114652633667
training loss = 1.8499573469161987 5000
val loss = 4.018362998962402
training loss = 1.848441243171692 5100
val loss = 4.047613620758057
training loss = 1.8460091352462769 5200
val loss = 4.0042619705200195
training loss = 1.8445488214492798 5300
val loss = 3.9674041271209717
training loss = 1.8425655364990234 5400
val loss = 3.9900426864624023
training loss = 1.8407601118087769 5500
val loss = 3.983487367630005
training loss = 1.8395143747329712 5600
val loss = 3.956153392791748
training loss = 1.8378208875656128 5700
val loss = 3.9729485511779785
training loss = 1.8434101343154907 5800
val loss = 4.1503586769104
training loss = 1.8351157903671265 5900
val loss = 3.9711127281188965
training loss = 1.833646297454834 6000
val loss = 3.957146644592285
training loss = 2.21649169921875 6100
val loss = 5.577970504760742
training loss = 1.8312166929244995 6200
val loss = 3.950850009918213
training loss = 1.829903244972229 6300
val loss = 3.942378520965576
training loss = 1.8605326414108276 6400
val loss = 3.593576192855835
training loss = 1.8276090621948242 6500
val loss = 3.9316110610961914
training loss = 1.8886150121688843 6600
val loss = 3.4736521244049072
training loss = 1.8254661560058594 6700
val loss = 3.925717830657959
training loss = 1.8243076801300049 6800
val loss = 3.923898220062256
training loss = 1.82368004322052 6900
val loss = 3.9432225227355957
training loss = 1.8223562240600586 7000
val loss = 3.912170886993408
training loss = 1.8217179775238037 7100
val loss = 3.8838467597961426
training loss = 1.8205499649047852 7200
val loss = 3.903162956237793
training loss = 1.951156497001648 7300
val loss = 4.757795333862305
training loss = 1.8188049793243408 7400
val loss = 3.8979644775390625
training loss = 1.8178259134292603 7500
val loss = 3.89294171333313
training loss = 1.8174160718917847 7600
val loss = 3.857907295227051
training loss = 1.8162789344787598 7700
val loss = 3.886212110519409
training loss = 1.9918441772460938 7800
val loss = 4.886536598205566
training loss = 1.8148161172866821 7900
val loss = 3.881732940673828
training loss = 1.813942551612854 8000
val loss = 3.876997470855713
training loss = 1.8135323524475098 8100
val loss = 3.8477416038513184
training loss = 1.8125362396240234 8200
val loss = 3.8697447776794434
training loss = 1.8853235244750977 8300
val loss = 4.467631816864014
training loss = 1.8112167119979858 8400
val loss = 3.8592629432678223
training loss = 1.810415506362915 8500
val loss = 3.8627846240997314
training loss = 1.810408115386963 8600
val loss = 3.8938345909118652
training loss = 1.809208631515503 8700
val loss = 3.8560266494750977
training loss = 1.898163080215454 8800
val loss = 3.3293509483337402
training loss = 1.8080224990844727 8900
val loss = 3.8456218242645264
training loss = 1.807370901107788 9000
val loss = 3.830091953277588
training loss = 1.8068958520889282 9100
val loss = 3.843919277191162
training loss = 1.806169033050537 9200
val loss = 3.843137264251709
training loss = 1.8077458143234253 9300
val loss = 3.7489523887634277
training loss = 1.805172324180603 9400
val loss = 3.8360390663146973
training loss = 1.8049629926681519 9500
val loss = 3.8796348571777344
training loss = 1.8042241334915161 9600
val loss = 3.8450205326080322
training loss = 1.8034932613372803 9700
val loss = 3.829702377319336
training loss = 1.804173231124878 9800
val loss = 3.762773036956787
training loss = 1.802544355392456 9900
val loss = 3.8254220485687256
training loss = 1.8172917366027832 10000
val loss = 4.069593906402588
training loss = 1.801677942276001 10100
val loss = 3.817251205444336
training loss = 1.8010616302490234 10200
val loss = 3.8187320232391357
training loss = 1.8010461330413818 10300
val loss = 3.8310723304748535
training loss = 1.8002963066101074 10400
val loss = 3.8142075538635254
training loss = 2.0353987216949463 10500
val loss = 3.074584484100342
training loss = 1.7995117902755737 10600
val loss = 3.8094282150268555
training loss = 1.7989264726638794 10700
val loss = 3.810063600540161
training loss = 1.7992653846740723 10800
val loss = 3.8448643684387207
training loss = 1.7981669902801514 10900
val loss = 3.80438232421875
training loss = 1.7983133792877197 11000
val loss = 3.820516347885132
training loss = 1.7974867820739746 11100
val loss = 3.79799485206604
training loss = 1.7969417572021484 11200
val loss = 3.8008790016174316
training loss = 1.7973124980926514 11300
val loss = 3.7530159950256348
training loss = 1.7963281869888306 11400
val loss = 3.7949283123016357
training loss = 2.303659200668335 11500
val loss = 5.6433610916137695
training loss = 1.7957589626312256 11600
val loss = 3.792755365371704
training loss = 1.7952258586883545 11700
val loss = 3.7908313274383545
training loss = 1.8075374364852905 11800
val loss = 3.575265407562256
training loss = 1.7946903705596924 11900
val loss = 3.784799814224243
training loss = 1.794189453125 12000
val loss = 3.7864267826080322
training loss = 1.794241189956665 12100
val loss = 3.777613878250122
training loss = 1.793668508529663 12200
val loss = 3.7818655967712402
training loss = 1.8795151710510254 12300
val loss = 3.2814831733703613
training loss = 1.7931400537490845 12400
val loss = 3.781269073486328
training loss = 1.7926424741744995 12500
val loss = 3.779360055923462
training loss = 1.7929638624191284 12600
val loss = 3.8040571212768555
training loss = 1.7921301126480103 12700
val loss = 3.7740097045898438
training loss = 1.8061071634292603 12800
val loss = 3.997673988342285
training loss = 1.7917875051498413 12900
val loss = 3.769268035888672
training loss = 1.7913118600845337 13000
val loss = 3.770251750946045
training loss = 1.7913742065429688 13100
val loss = 3.7617249488830566
training loss = 1.790877342224121 13200
val loss = 3.7656784057617188
training loss = 1.7904207706451416 13300
val loss = 3.763230085372925
training loss = 1.7907158136367798 13400
val loss = 3.7863895893096924
training loss = 1.7899960279464722 13500
val loss = 3.7622885704040527
training loss = 1.7901064157485962 13600
val loss = 3.754729747772217
training loss = 1.7895703315734863 13700
val loss = 3.7601208686828613
training loss = 1.791435956954956 13800
val loss = 3.8536667823791504
training loss = 1.7892662286758423 13900
val loss = 3.759227752685547
training loss = 1.7887754440307617 14000
val loss = 3.7559304237365723
training loss = 1.7967497110366821 14100
val loss = 3.58579683303833
training loss = 1.7884328365325928 14200
val loss = 3.751084089279175
training loss = 1.7879986763000488 14300
val loss = 3.750200033187866
training loss = 1.7882827520370483 14400
val loss = 3.764824867248535
training loss = 1.7876684665679932 14500
val loss = 3.7493886947631836
training loss = 1.7880326509475708 14600
val loss = 3.701894760131836
training loss = 1.7875627279281616 14700
val loss = 3.7476024627685547
training loss = 1.7870842218399048 14800
val loss = 3.745922565460205
training loss = 1.7877197265625 14900
val loss = 3.8088927268981934
training loss = 1.7868916988372803 15000
val loss = 3.745236396789551
training loss = 1.7864079475402832 15100
val loss = 3.74230694770813
training loss = 1.78887939453125 15200
val loss = 3.6454203128814697
training loss = 1.7861206531524658 15300
val loss = 3.7353906631469727
training loss = 1.7856972217559814 15400
val loss = 3.73876953125
training loss = 1.7864961624145508 15500
val loss = 3.690263271331787
training loss = 1.7855224609375 15600
val loss = 3.735609531402588
training loss = 1.7856091260910034 15700
val loss = 3.6954545974731445
training loss = 1.7853127717971802 15800
val loss = 3.716569423675537
training loss = 1.7848095893859863 15900
val loss = 3.7329254150390625
training loss = 1.7851213216781616 16000
val loss = 3.709373712539673
training loss = 1.7845337390899658 16100
val loss = 3.730616569519043
training loss = 1.813359260559082 16200
val loss = 3.4236931800842285
training loss = 1.7843247652053833 16300
val loss = 3.7228989601135254
training loss = 1.783896565437317 16400
val loss = 3.7299065589904785
training loss = 1.7841756343841553 16500
val loss = 3.7098498344421387
training loss = 1.7836426496505737 16600
val loss = 3.726066827774048
training loss = 1.8590328693389893 16700
val loss = 4.295427322387695
training loss = 1.7835851907730103 16800
val loss = 3.723451852798462
training loss = 1.78314208984375 16900
val loss = 3.723285675048828
training loss = 1.8279749155044556 17000
val loss = 3.3576133251190186
training loss = 1.783003330230713 17100
val loss = 3.716618061065674
training loss = 1.7825820446014404 17200
val loss = 3.721379518508911
training loss = 1.7833882570266724 17300
val loss = 3.670151710510254
training loss = 1.7823704481124878 17400
val loss = 3.717707633972168
training loss = 1.8484688997268677 17500
val loss = 4.247541904449463
training loss = 1.7823067903518677 17600
val loss = 3.711508274078369
training loss = 1.781867504119873 17700
val loss = 3.715874195098877
training loss = 1.8005660772323608 17800
val loss = 3.973341703414917
training loss = 1.7817802429199219 17900
val loss = 3.7118453979492188
training loss = 1.7813631296157837 18000
val loss = 3.7127718925476074
training loss = 1.7829335927963257 18100
val loss = 3.7683472633361816
training loss = 1.781233787536621 18200
val loss = 3.7091712951660156
training loss = 2.0783660411834717 18300
val loss = 2.9642348289489746
training loss = 1.7811529636383057 18400
val loss = 3.7008109092712402
training loss = 1.7806957960128784 18500
val loss = 3.708129644393921
training loss = 1.7845823764801025 18600
val loss = 3.589110851287842
training loss = 1.7805196046829224 18700
val loss = 3.7041056156158447
training loss = 1.8890975713729858 18800
val loss = 4.402890682220459
training loss = 1.780393123626709 18900
val loss = 3.701153039932251
training loss = 1.7799577713012695 19000
val loss = 3.7060041427612305
training loss = 1.7804921865463257 19100
val loss = 3.7224133014678955
training loss = 1.7797796726226807 19200
val loss = 3.7014424800872803
training loss = 1.8159440755844116 19300
val loss = 4.077233791351318
training loss = 1.7797983884811401 19400
val loss = 3.6944379806518555
training loss = 1.7793564796447754 19500
val loss = 3.6996965408325195
reduced chi^2 level 2 = 1.7842607498168945
Constrained alpha: 1.997322916984558
Constrained beta: 3.694438934326172
Constrained gamma: 13.667165756225586
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 880.9548,  856.7490,  912.0749,  933.6240, 1024.2992, 1084.9860,
        1079.8457, 1180.3188, 1119.2871, 1201.1259, 1212.1807, 1167.3141,
        1290.9238, 1227.2535, 1330.4208, 1451.3403, 1382.2656, 1500.9070,
        1564.1919, 1425.1740, 1590.1381, 1539.2737, 1629.2184, 1602.4424,
        1704.1073, 1609.9784, 1638.6345, 1678.9688, 1691.3483, 1655.8170,
        1702.8503, 1743.6112, 1780.5165, 1758.9238, 1725.5566, 1728.2352,
        1708.0487, 1644.3154, 1620.8628, 1589.4100, 1631.9628, 1576.0149,
        1529.5260, 1491.8883, 1406.4150, 1340.1766, 1318.8007, 1208.7218,
        1116.6558, 1218.2601, 1089.5582,  962.7870,  887.0828,  899.1307,
         917.2288,  903.2107,  826.4550,  663.3607,  640.4108,  534.1494,
         582.6997,  496.0479,  451.4151,  414.4805,  354.9943,  328.8649,
         296.7719,  240.8328,  211.6630,  173.3433,  168.0257,  156.1062,
         150.2905,   99.6410,   85.2785,   72.0838,   35.9171,   51.9180,
          29.9792,   38.8220,   18.6849,   39.9082,   30.3748])]
2660.6045336874695
2.657961650294898 5.799234447085473 60.27442375546757
val isze = 8
idinces = [57  8 25  7 67 64 26 63  0 29 78  2 58 22 73 20 18 49 56 28 17  4 33 50
 46 32 11 71 19 42  3  5 13 53 21 27 77 35 31 15 10 37 74 30 48 60 24 23
 44  6 51 47 12 76 16 36 45 34 75 65 38 81 40 72 59 66  9 79 43 80 55 41
  1 62 52 68 69 54 39 70 61 14 82]
we are doing training validation split
training loss = 23.27174186706543 100
val loss = 21.371929168701172
training loss = 17.163536071777344 200
val loss = 15.74478530883789
training loss = 13.10181713104248 300
val loss = 12.914291381835938
training loss = 10.51945686340332 400
val loss = 10.545303344726562
training loss = 8.901853561401367 500
val loss = 8.67462158203125
training loss = 7.883478164672852 600
val loss = 7.2412896156311035
training loss = 7.236059665679932 700
val loss = 6.158590316772461
training loss = 6.8203630447387695 800
val loss = 5.345566749572754
training loss = 6.550861835479736 900
val loss = 4.736207962036133
training loss = 6.37422513961792 1000
val loss = 4.27919340133667
training loss = 6.256705284118652 1100
val loss = 3.9360995292663574
training loss = 6.176705837249756 1200
val loss = 3.6781258583068848
training loss = 6.1203460693359375 1300
val loss = 3.4840259552001953
training loss = 6.078677654266357 1400
val loss = 3.337890625
training loss = 6.045993804931641 1500
val loss = 3.2279491424560547
training loss = 6.018690586090088 1600
val loss = 3.1454389095306396
training loss = 5.994531631469727 1700
val loss = 3.083681583404541
training loss = 5.972156047821045 1800
val loss = 3.037623167037964
training loss = 5.950753211975098 1900
val loss = 3.0034377574920654
training loss = 5.929838180541992 2000
val loss = 2.9780564308166504
training loss = 5.909079551696777 2100
val loss = 2.959275722503662
training loss = 5.888168811798096 2200
val loss = 2.9452948570251465
training loss = 5.866683006286621 2300
val loss = 2.934743881225586
training loss = 5.84385871887207 2400
val loss = 2.926595687866211
training loss = 5.818167686462402 2500
val loss = 2.91998291015625
training loss = 5.786199569702148 2600
val loss = 2.914259910583496
training loss = 5.739765167236328 2700
val loss = 2.909193277359009
training loss = 5.658496379852295 2800
val loss = 2.9066648483276367
training loss = 5.503455638885498 2900
val loss = 2.9136714935302734
training loss = 5.2595930099487305 3000
val loss = 2.912768840789795
training loss = 4.9464111328125 3100
val loss = 2.8142948150634766
training loss = 4.520301818847656 3200
val loss = 2.5856728553771973
training loss = 3.9202792644500732 3300
val loss = 2.221676826477051
training loss = 3.1649279594421387 3400
val loss = 1.6943411827087402
training loss = 2.5366456508636475 3500
val loss = 1.1949925422668457
training loss = 2.3099515438079834 3600
val loss = 1.0441197156906128
training loss = 2.260451555252075 3700
val loss = 1.0475444793701172
training loss = 2.2371561527252197 3800
val loss = 1.0571099519729614
training loss = 2.2180867195129395 3900
val loss = 1.0639357566833496
training loss = 2.2011568546295166 4000
val loss = 1.0707060098648071
training loss = 2.1859781742095947 4100
val loss = 1.0779386758804321
training loss = 2.1723453998565674 4200
val loss = 1.0854859352111816
training loss = 2.1600894927978516 4300
val loss = 1.0930988788604736
training loss = 2.149045944213867 4400
val loss = 1.1005717515945435
training loss = 2.139058828353882 4500
val loss = 1.1077396869659424
training loss = 2.129969596862793 4600
val loss = 1.1144734621047974
training loss = 2.1216330528259277 4700
val loss = 1.1207170486450195
training loss = 2.1139111518859863 4800
val loss = 1.1264102458953857
training loss = 2.106677770614624 4900
val loss = 1.1315556764602661
training loss = 2.099818468093872 5000
val loss = 1.1361887454986572
training loss = 2.0932295322418213 5100
val loss = 1.1403130292892456
training loss = 2.090003728866577 5200
val loss = 1.1312297582626343
training loss = 2.0807547569274902 5300
val loss = 1.1476292610168457
training loss = 2.0747554302215576 5400
val loss = 1.151132345199585
training loss = 2.068958282470703 5500
val loss = 1.154638648033142
training loss = 2.06370210647583 5600
val loss = 1.1518702507019043
training loss = 2.0575640201568604 5700
val loss = 1.161076307296753
training loss = 2.060215711593628 5800
val loss = 1.1995103359222412
training loss = 2.046461582183838 5900
val loss = 1.1680257320404053
training loss = 2.067235231399536 6000
val loss = 1.24442720413208
training loss = 2.0357279777526855 6100
val loss = 1.1750643253326416
training loss = 2.030466318130493 6200
val loss = 1.1779582500457764
training loss = 2.0525522232055664 6300
val loss = 1.2586086988449097
training loss = 2.020160436630249 6400
val loss = 1.1844719648361206
training loss = 2.0150680541992188 6500
val loss = 1.1879445314407349
training loss = 2.010948896408081 6600
val loss = 1.2021119594573975
training loss = 2.005208969116211 6700
val loss = 1.1946418285369873
training loss = 2.006740093231201 6800
val loss = 1.1763254404067993
training loss = 1.9956820011138916 6900
val loss = 1.2016937732696533
training loss = 1.9910016059875488 7000
val loss = 1.2047014236450195
training loss = 1.9875973463058472 7100
val loss = 1.197288990020752
training loss = 1.9820785522460938 7200
val loss = 1.211474895477295
training loss = 1.9776501655578613 7300
val loss = 1.2136635780334473
training loss = 1.9734677076339722 7400
val loss = 1.2208192348480225
training loss = 1.9692341089248657 7500
val loss = 1.2216463088989258
training loss = 1.9816396236419678 7600
val loss = 1.2895028591156006
training loss = 1.9612120389938354 7700
val loss = 1.2283399105072021
training loss = 1.9572538137435913 7800
val loss = 1.2321733236312866
training loss = 1.95374596118927 7900
val loss = 1.2291179895401
training loss = 1.9496930837631226 8000
val loss = 1.2393252849578857
training loss = 1.9512602090835571 8100
val loss = 1.2191048860549927
training loss = 1.9423582553863525 8200
val loss = 1.2468514442443848
training loss = 1.938693881034851 8300
val loss = 1.2509013414382935
training loss = 1.935412049293518 8400
val loss = 1.2616981267929077
training loss = 1.9316143989562988 8500
val loss = 1.2586039304733276
training loss = 1.9858297109603882 8600
val loss = 1.2110098600387573
reduced chi^2 level 2 = 1.9273747205734253
Constrained alpha: 1.817996859550476
Constrained beta: 3.263416290283203
Constrained gamma: 23.090103149414062
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 824.0231,  836.4152,  978.0181,  990.7189, 1026.7333, 1039.6953,
        1095.5116, 1155.1876, 1137.7136, 1178.6426, 1194.0369, 1207.0475,
        1263.3020, 1284.5334, 1323.3225, 1390.8555, 1392.9010, 1464.1884,
        1570.2172, 1543.8318, 1556.6426, 1510.9685, 1634.0405, 1580.1545,
        1625.4393, 1772.5791, 1561.0981, 1785.9274, 1720.0502, 1705.9845,
        1696.1768, 1755.6395, 1700.7716, 1722.5728, 1716.7091, 1763.2942,
        1624.3138, 1630.5125, 1641.7975, 1633.4524, 1590.2605, 1568.0093,
        1513.8521, 1467.2245, 1329.1163, 1368.1597, 1223.2174, 1207.0250,
        1159.7662, 1193.7747, 1001.9803,  978.2535,  914.6526,  922.5805,
         879.6569,  860.9255,  747.7725,  679.2803,  626.2479,  554.5775,
         576.2790,  468.3048,  417.1581,  362.9455,  346.2151,  339.5371,
         300.8060,  259.1268,  220.2227,  170.5075,  144.4762,  153.6630,
         142.5520,  112.8792,  105.3840,   66.9865,   54.1933,   40.2722,
          35.9772,   54.2919,   22.6079,   44.9606,   21.2187])]
2605.1684931919303
3.6082576871142313 2.858202171193389 98.1051938472198
val isze = 8
idinces = [68 12 45 14  5 56 75 40 52  3 25 24 28  9 31 49 44 50 61 67 60 39 53 64
  4  8 81 69 13 16 22 48 78 23 70 54 58 77 38 76 55 11  7 32 21  6 42 51
  1 66 33 34 30  0 35 80 65 71 36 47 19 72 27 10 29 26 82  2 37 63 41 18
 20 15 79 73 57 59 46 62 74 43 17]
we are doing training validation split
training loss = 78.15682220458984 100
val loss = 81.84874725341797
training loss = 31.089067459106445 200
val loss = 34.70250701904297
training loss = 15.708566665649414 300
val loss = 16.118391036987305
training loss = 9.99264907836914 400
val loss = 8.431869506835938
training loss = 7.661198139190674 500
val loss = 5.186580657958984
training loss = 6.45380973815918 600
val loss = 3.735795021057129
training loss = 5.363025188446045 700
val loss = 2.9023447036743164
training loss = 4.328775405883789 800
val loss = 2.2987356185913086
training loss = 3.5949065685272217 900
val loss = 1.7446365356445312
training loss = 3.1414718627929688 1000
val loss = 1.3715381622314453
training loss = 2.889984607696533 1100
val loss = 1.1576058864593506
training loss = 2.7503325939178467 1200
val loss = 1.0423803329467773
training loss = 2.661134719848633 1300
val loss = 0.9788076877593994
training loss = 2.593165397644043 1400
val loss = 0.9400429129600525
training loss = 2.5355496406555176 1500
val loss = 0.9131266474723816
training loss = 2.484740734100342 1600
val loss = 0.8923382759094238
training loss = 2.4395272731781006 1700
val loss = 0.8751667737960815
training loss = 2.399338960647583 1800
val loss = 0.8605251908302307
training loss = 2.36377215385437 1900
val loss = 0.847779393196106
training loss = 2.33245587348938 2000
val loss = 0.8366727828979492
training loss = 2.305023431777954 2100
val loss = 0.8269811868667603
training loss = 2.281113386154175 2200
val loss = 0.8184921741485596
training loss = 2.260366678237915 2300
val loss = 0.8111112117767334
training loss = 2.2424371242523193 2400
val loss = 0.8046857118606567
training loss = 2.226989507675171 2500
val loss = 0.7991136312484741
training loss = 2.2137179374694824 2600
val loss = 0.7943032383918762
training loss = 2.2023332118988037 2700
val loss = 0.7901278734207153
training loss = 2.192577600479126 2800
val loss = 0.7864995002746582
training loss = 2.184215545654297 2900
val loss = 0.7833527326583862
training loss = 2.1770365238189697 3000
val loss = 0.7805705070495605
training loss = 2.170860767364502 3100
val loss = 0.7781079411506653
training loss = 2.165529489517212 3200
val loss = 0.7758960723876953
training loss = 2.160900354385376 3300
val loss = 0.7738513350486755
training loss = 2.1568603515625 3400
val loss = 0.771954357624054
training loss = 2.1533048152923584 3500
val loss = 0.7701335549354553
training loss = 2.1501457691192627 3600
val loss = 0.768399178981781
training loss = 2.1477177143096924 3700
val loss = 0.7767701148986816
training loss = 2.1447980403900146 3800
val loss = 0.7650728225708008
training loss = 2.142573595046997 3900
val loss = 0.7677448391914368
training loss = 2.140416383743286 4000
val loss = 0.7618963718414307
training loss = 2.138533115386963 4100
val loss = 0.7574841380119324
training loss = 2.1367082595825195 4200
val loss = 0.7587344646453857
training loss = 2.1350553035736084 4300
val loss = 0.7549455165863037
training loss = 2.1334614753723145 4400
val loss = 0.7559487819671631
training loss = 2.132085084915161 4500
val loss = 0.7612694501876831
training loss = 2.1305108070373535 4600
val loss = 0.7528362274169922
training loss = 2.133493185043335 4700
val loss = 0.7920538783073425
training loss = 2.1276986598968506 4800
val loss = 0.7500754594802856
training loss = 2.138498067855835 4900
val loss = 0.7010632157325745
training loss = 2.1248531341552734 5000
val loss = 0.7470484972000122
training loss = 2.2344884872436523 5100
val loss = 1.0340818166732788
training loss = 2.121840000152588 5200
val loss = 0.7447506189346313
training loss = 2.12031888961792 5300
val loss = 0.7431437373161316
training loss = 2.1189143657684326 5400
val loss = 0.7330383062362671
training loss = 2.1168999671936035 5500
val loss = 0.74069744348526
training loss = 2.1152970790863037 5600
val loss = 0.7372295260429382
training loss = 2.1134045124053955 5700
val loss = 0.739417552947998
training loss = 2.1115875244140625 5800
val loss = 0.7364933490753174
training loss = 2.110206365585327 5900
val loss = 0.7453608512878418
training loss = 2.1080400943756104 6000
val loss = 0.735576868057251
training loss = 2.106261730194092 6100
val loss = 0.7320772409439087
training loss = 2.105060577392578 6200
val loss = 0.717736005783081
training loss = 2.102674961090088 6300
val loss = 0.7291012406349182
training loss = 2.124227285385132 6400
val loss = 0.6641150712966919
training loss = 2.0991361141204834 6500
val loss = 0.7266913056373596
training loss = 2.0974371433258057 6600
val loss = 0.7242716550827026
training loss = 2.095689535140991 6700
val loss = 0.7223392724990845
training loss = 2.093994379043579 6800
val loss = 0.7221582531929016
training loss = 2.094470977783203 6900
val loss = 0.6956645250320435
training loss = 2.090675115585327 7000
val loss = 0.7196477055549622
training loss = 2.089073657989502 7100
val loss = 0.7181771397590637
training loss = 2.0876119136810303 7200
val loss = 0.7234762907028198
training loss = 2.085888624191284 7300
val loss = 0.7148678302764893
training loss = 2.0860066413879395 7400
val loss = 0.69247967004776
training loss = 2.0828092098236084 7500
val loss = 0.7119370102882385
training loss = 2.2982168197631836 7600
val loss = 0.6691904067993164
training loss = 2.0798184871673584 7700
val loss = 0.7105936408042908
training loss = 2.0783538818359375 7800
val loss = 0.7087547779083252
training loss = 2.076984167098999 7900
val loss = 0.7117562294006348
training loss = 2.07546067237854 8000
val loss = 0.7046107053756714
training loss = 2.0769996643066406 8100
val loss = 0.6779708862304688
training loss = 2.072620391845703 8200
val loss = 0.7020764350891113
training loss = 2.0732834339141846 8300
val loss = 0.6759476065635681
training loss = 2.0698025226593018 8400
val loss = 0.6991413235664368
training loss = 2.068389415740967 8500
val loss = 0.6977021098136902
training loss = 2.0670204162597656 8600
val loss = 0.6920475959777832
training loss = 2.065523624420166 8700
val loss = 0.6945571899414062
training loss = 2.1197075843811035 8800
val loss = 0.6222589015960693
training loss = 2.0625314712524414 8900
val loss = 0.6907837390899658
training loss = 2.0609612464904785 9000
val loss = 0.6898058652877808
training loss = 2.059345006942749 9100
val loss = 0.6891496181488037
training loss = 2.0576860904693604 9200
val loss = 0.6861092448234558
training loss = 2.079707384109497 9300
val loss = 0.626643180847168
training loss = 2.054206132888794 9400
val loss = 0.6835676431655884
training loss = 2.052410840988159 9500
val loss = 0.6805387735366821
training loss = 2.0557024478912354 9600
val loss = 0.648475706577301
training loss = 2.048769235610962 9700
val loss = 0.6769037246704102
training loss = 2.0469086170196533 9800
val loss = 0.6754201650619507
training loss = 2.064300298690796 9900
val loss = 0.7607777118682861
training loss = 2.04307222366333 10000
val loss = 0.6727255582809448
training loss = 2.0410757064819336 10100
val loss = 0.6699860095977783
training loss = 2.039074420928955 10200
val loss = 0.672397255897522
training loss = 2.036935806274414 10300
val loss = 0.6674282550811768
training loss = 2.0386383533477783 10400
val loss = 0.640167772769928
training loss = 2.0325260162353516 10500
val loss = 0.6639368534088135
training loss = 2.0301520824432373 10600
val loss = 0.6619254350662231
training loss = 2.0277562141418457 10700
val loss = 0.6643292903900146
training loss = 2.025130033493042 10800
val loss = 0.6587607264518738
training loss = 2.0236220359802246 10900
val loss = 0.6748238205909729
training loss = 2.01967716217041 11000
val loss = 0.6545393466949463
training loss = 2.0167789459228516 11100
val loss = 0.6570375561714172
training loss = 2.013730049133301 11200
val loss = 0.6523447036743164
training loss = 2.0105655193328857 11300
val loss = 0.6479505300521851
training loss = 2.0096757411956787 11400
val loss = 0.6704883575439453
training loss = 2.0039260387420654 11500
val loss = 0.6426229476928711
training loss = 2.1038315296173096 11600
val loss = 0.5960171818733215
training loss = 1.996951699256897 11700
val loss = 0.636784553527832
training loss = 1.9932739734649658 11800
val loss = 0.6353693008422852
training loss = 1.9897733926773071 11900
val loss = 0.6358724236488342
training loss = 1.9860252141952515 12000
val loss = 0.629231333732605
training loss = 1.9867610931396484 12100
val loss = 0.6032003164291382
training loss = 1.978801965713501 12200
val loss = 0.623438835144043
training loss = 1.9750664234161377 12300
val loss = 0.6206227540969849
training loss = 1.9725606441497803 12400
val loss = 0.6322041749954224
training loss = 1.9679584503173828 12500
val loss = 0.6150458455085754
training loss = 1.9652271270751953 12600
val loss = 0.6259286403656006
training loss = 1.9609273672103882 12700
val loss = 0.6095752716064453
training loss = 1.9574278593063354 12800
val loss = 0.6068612933158875
training loss = 1.9708508253097534 12900
val loss = 0.671619713306427
training loss = 1.9507975578308105 13000
val loss = 0.6016333103179932
training loss = 1.9473600387573242 13100
val loss = 0.5995219945907593
training loss = 1.9441921710968018 13200
val loss = 0.5979716777801514
training loss = 1.940861463546753 13300
val loss = 0.5944252610206604
training loss = 1.9398820400238037 13400
val loss = 0.5770840644836426
training loss = 1.9345526695251465 13500
val loss = 0.5899718999862671
training loss = 1.9429916143417358 13600
val loss = 0.6342172622680664
training loss = 1.9283115863800049 13700
val loss = 0.5862365961074829
training loss = 1.9250870943069458 13800
val loss = 0.5865738391876221
training loss = 1.9222298860549927 13900
val loss = 0.5842771530151367
training loss = 1.9190946817398071 14000
val loss = 0.5800806283950806
training loss = 1.9184092283248901 14100
val loss = 0.5641227960586548
training loss = 1.9133466482162476 14200
val loss = 0.5767198801040649
training loss = 1.916702151298523 14300
val loss = 0.6055121421813965
training loss = 1.9078834056854248 14400
val loss = 0.5742748975753784
training loss = 1.9051942825317383 14500
val loss = 0.5689660310745239
training loss = 1.9028878211975098 14600
val loss = 0.573738694190979
training loss = 1.900377631187439 14700
val loss = 0.5699385404586792
training loss = 1.9109269380569458 14800
val loss = 0.618881344795227
training loss = 1.8959710597991943 14900
val loss = 0.5681648850440979
training loss = 1.8937760591506958 15000
val loss = 0.5677966475486755
training loss = 1.892045497894287 15100
val loss = 0.5635493993759155
training loss = 1.8900715112686157 15200
val loss = 0.565027117729187
training loss = 1.9038249254226685 15300
val loss = 0.5399497151374817
training loss = 1.8868376016616821 15400
val loss = 0.5636582374572754
training loss = 1.885246992111206 15500
val loss = 0.562475323677063
training loss = 1.8840950727462769 15600
val loss = 0.5664002895355225
training loss = 1.8825844526290894 15700
val loss = 0.5611845850944519
training loss = 1.8958607912063599 15800
val loss = 0.6136583089828491
training loss = 1.880354642868042 15900
val loss = 0.5599759817123413
training loss = 1.8792724609375 16000
val loss = 0.5581929087638855
training loss = 1.8785624504089355 16100
val loss = 0.5550626516342163
training loss = 1.8775181770324707 16200
val loss = 0.5577418804168701
training loss = 1.8774255514144897 16300
val loss = 0.5651893615722656
training loss = 1.8760555982589722 16400
val loss = 0.556684136390686
training loss = 1.9429126977920532 16500
val loss = 0.7007988691329956
training loss = 1.874811053276062 16600
val loss = 0.5559226274490356
training loss = 1.8741779327392578 16700
val loss = 0.5552976727485657
training loss = 1.8753662109375 16800
val loss = 0.5437387228012085
training loss = 1.8731343746185303 16900
val loss = 0.5545044541358948
training loss = 1.8765771389007568 17000
val loss = 0.5724323987960815
training loss = 1.8722050189971924 17100
val loss = 0.5547683835029602
training loss = 1.8717100620269775 17200
val loss = 0.5537538528442383
training loss = 1.8718318939208984 17300
val loss = 0.5481724143028259
training loss = 1.8709468841552734 17400
val loss = 0.5535426139831543
training loss = 1.870728850364685 17500
val loss = 0.5578798055648804
training loss = 1.8702441453933716 17600
val loss = 0.5539345145225525
training loss = 1.869853138923645 17700
val loss = 0.5530568361282349
training loss = 1.8731377124786377 17800
val loss = 0.5763217210769653
training loss = 1.86923348903656 17900
val loss = 0.5532704591751099
training loss = 1.868883490562439 18000
val loss = 0.5522966384887695
training loss = 1.8687947988510132 18100
val loss = 0.5494458675384521
training loss = 1.8683085441589355 18200
val loss = 0.5529499053955078
training loss = 1.8907173871994019 18300
val loss = 0.6216298341751099
training loss = 1.8677937984466553 18400
val loss = 0.5531145334243774
training loss = 1.8674794435501099 18500
val loss = 0.5530275702476501
training loss = 1.8770267963409424 18600
val loss = 0.5338517427444458
training loss = 1.8669617176055908 18700
val loss = 0.553118109703064
training loss = 2.136040449142456 18800
val loss = 0.6547775268554688
training loss = 1.8664783239364624 18900
val loss = 0.5526835918426514
training loss = 1.86616849899292 19000
val loss = 0.5536413192749023
training loss = 1.866776943206787 19100
val loss = 0.5635018944740295
training loss = 1.865696668624878 19200
val loss = 0.5541470646858215
training loss = 2.123117208480835 19300
val loss = 0.9494685530662537
training loss = 1.865234613418579 19400
val loss = 0.555382490158081
training loss = 1.8649386167526245 19500
val loss = 0.5547463297843933
training loss = 1.8651432991027832 19600
val loss = 0.561497688293457
training loss = 1.8644745349884033 19700
val loss = 0.5550411343574524
training loss = 1.867626428604126 19800
val loss = 0.5760184526443481
training loss = 1.8640226125717163 19900
val loss = 0.5558536052703857
training loss = 1.9468196630477905 20000
val loss = 0.7210766673088074
training loss = 1.8635835647583008 20100
val loss = 0.5554586052894592
training loss = 1.863284707069397 20200
val loss = 0.5563722252845764
training loss = 1.8635236024856567 20300
val loss = 0.5518195033073425
training loss = 1.862841010093689 20400
val loss = 0.5571715235710144
training loss = 1.91396164894104 20500
val loss = 0.6718629598617554
training loss = 1.8623759746551514 20600
val loss = 0.5580893158912659
training loss = 1.8860496282577515 20700
val loss = 0.6269789934158325
training loss = 1.8619335889816284 20800
val loss = 0.5573546886444092
training loss = 1.8616150617599487 20900
val loss = 0.5587146282196045
training loss = 1.864396572113037 21000
val loss = 0.5452429056167603
training loss = 1.8611613512039185 21100
val loss = 0.5595353245735168
training loss = 1.8608529567718506 21200
val loss = 0.5598248839378357
training loss = 1.8609851598739624 21300
val loss = 0.5556780695915222
training loss = 1.8603572845458984 21400
val loss = 0.5605818629264832
training loss = 1.9025630950927734 21500
val loss = 0.537328839302063
training loss = 1.859832763671875 21600
val loss = 0.5618162155151367
training loss = 1.8595021963119507 21700
val loss = 0.5611409544944763
training loss = 1.8594368696212769 21800
val loss = 0.5657821297645569
training loss = 1.8589608669281006 21900
val loss = 0.5623921155929565
training loss = 1.9188534021377563 22000
val loss = 0.5459031462669373
training loss = 1.8583797216415405 22100
val loss = 0.5637103319168091
training loss = 1.8584645986557007 22200
val loss = 0.5578301548957825
training loss = 1.8577808141708374 22300
val loss = 0.5643081665039062
training loss = 1.8574206829071045 22400
val loss = 0.5644262433052063
training loss = 1.8575563430786133 22500
val loss = 0.5713928937911987
training loss = 1.8567895889282227 22600
val loss = 0.5651877522468567
training loss = 1.8655991554260254 22700
val loss = 0.6029655933380127
training loss = 1.8561527729034424 22800
val loss = 0.5661171674728394
training loss = 1.8809646368026733 22900
val loss = 0.6364350318908691
training loss = 1.8555314540863037 23000
val loss = 0.5657275915145874
training loss = 1.8551349639892578 23100
val loss = 0.5672752261161804
training loss = 1.8550918102264404 23200
val loss = 0.5638285875320435
reduced chi^2 level 2 = 1.8545382022857666
Constrained alpha: 1.7741711139678955
Constrained beta: 3.869626522064209
Constrained gamma: 14.102052688598633
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 820.1541,  865.1537,  986.3035,  962.1713,  977.1426, 1058.7036,
        1063.7711, 1111.1652, 1081.2075, 1217.6495, 1190.2559, 1227.0981,
        1207.4020, 1246.3439, 1341.9314, 1443.0657, 1373.7721, 1430.8367,
        1506.8993, 1467.2632, 1611.3567, 1577.8761, 1579.9368, 1601.9718,
        1676.9423, 1680.1954, 1650.9799, 1735.7819, 1671.1003, 1735.2111,
        1617.0693, 1788.1027, 1677.9425, 1738.1757, 1650.2849, 1738.0137,
        1657.5575, 1585.9327, 1642.6484, 1641.3806, 1622.6696, 1580.0903,
        1457.1964, 1552.9944, 1380.0559, 1298.5780, 1220.1469, 1232.5112,
        1124.7891, 1237.3262, 1050.7644,  989.0555,  966.5112,  902.5942,
         821.2875,  861.1551,  852.9285,  710.9769,  587.8989,  523.9337,
         540.9968,  445.0119,  447.8369,  386.1685,  341.7342,  322.3030,
         307.4291,  254.2796,  193.1603,  183.1722,  159.5789,  156.9958,
         147.7021,   86.2737,  104.3781,   75.1087,   50.0119,   51.0134,
          32.8051,   47.1447,   12.8676,   28.6644,   28.9256])]
2819.051417528613
3.9447199062215677 0.1680718991461605 99.8009273631801
val isze = 8
idinces = [45  4 72 54 81 26 37 61 28  7 60 44 39 55 32 16 74 51  0 53 18 80  2 21
 71 10 43 49 13 62 19 11 57 64 77 20 34 82 69 67  5 56 75 58 15 27 76  1
 59  9 73 46 70 66 14 42 24 35 79 47 40 33  3 29 22 48 36 38 41 25 31 50
 17  8 52 30 23 78 68 65 12 63  6]
we are doing training validation split
training loss = 30.686389923095703 100
val loss = 26.307348251342773
training loss = 6.976069450378418 200
val loss = 8.879727363586426
training loss = 4.471365451812744 300
val loss = 5.817086219787598
training loss = 3.4678423404693604 400
val loss = 4.546136856079102
training loss = 3.0881526470184326 500
val loss = 3.9484715461730957
training loss = 2.7912566661834717 600
val loss = 3.3825268745422363
training loss = 2.563361406326294 700
val loss = 3.07222056388855
training loss = 2.412264108657837 800
val loss = 2.777796745300293
training loss = 2.312654972076416 900
val loss = 2.6439971923828125
training loss = 2.298962116241455 1000
val loss = 2.1271135807037354
training loss = 2.198814868927002 1100
val loss = 2.4494988918304443
training loss = 2.165583610534668 1200
val loss = 2.406216621398926
training loss = 2.141918420791626 1300
val loss = 2.364572525024414
training loss = 2.1234331130981445 1400
val loss = 2.3680641651153564
training loss = 2.1178572177886963 1500
val loss = 2.549783706665039
training loss = 2.096409320831299 1600
val loss = 2.367741584777832
training loss = 2.0857319831848145 1700
val loss = 2.360546588897705
training loss = 2.0764670372009277 1800
val loss = 2.387843608856201
training loss = 2.0681493282318115 1900
val loss = 2.393662691116333
training loss = 2.061530113220215 2000
val loss = 2.4570183753967285
training loss = 2.0541739463806152 2100
val loss = 2.41230845451355
training loss = 2.0479917526245117 2200
val loss = 2.4215030670166016
training loss = 2.0425899028778076 2300
val loss = 2.4512600898742676
training loss = 2.0373475551605225 2400
val loss = 2.4317173957824707
training loss = 2.032522439956665 2500
val loss = 2.4397425651550293
training loss = 2.0281689167022705 2600
val loss = 2.4367260932922363
training loss = 2.024047374725342 2700
val loss = 2.44118070602417
training loss = 2.0202629566192627 2800
val loss = 2.42718505859375
training loss = 2.0165727138519287 2900
val loss = 2.4454658031463623
training loss = 2.030001640319824 3000
val loss = 2.2298083305358887
training loss = 2.009878396987915 3100
val loss = 2.44492244720459
training loss = 2.0068447589874268 3200
val loss = 2.458937168121338
training loss = 2.0038561820983887 3300
val loss = 2.460139751434326
training loss = 2.0009984970092773 3400
val loss = 2.4438636302948
training loss = 2.3172812461853027 3500
val loss = 1.724789023399353
training loss = 1.9956278800964355 3600
val loss = 2.4386532306671143
training loss = 1.99311363697052 3700
val loss = 2.4386181831359863
training loss = 1.9912683963775635 3800
val loss = 2.3933258056640625
training loss = 1.98824143409729 3900
val loss = 2.435731887817383
training loss = 2.0012805461883545 4000
val loss = 2.2270209789276123
training loss = 1.9836280345916748 4100
val loss = 2.4307408332824707
training loss = 1.9910686016082764 4200
val loss = 2.2642641067504883
training loss = 1.979251503944397 4300
val loss = 2.434079885482788
training loss = 1.977145791053772 4400
val loss = 2.4224135875701904
training loss = 1.989406943321228 4500
val loss = 2.6420254707336426
training loss = 1.9730390310287476 4600
val loss = 2.418060779571533
training loss = 1.971093773841858 4700
val loss = 2.4135515689849854
training loss = 1.9694947004318237 4800
val loss = 2.377061128616333
training loss = 1.9672483205795288 4900
val loss = 2.4071154594421387
training loss = 1.9796240329742432 5000
val loss = 2.2074532508850098
training loss = 1.9635783433914185 5100
val loss = 2.397716522216797
training loss = 1.9618229866027832 5200
val loss = 2.3967747688293457
training loss = 1.960060715675354 5300
val loss = 2.392275333404541
training loss = 1.9583760499954224 5400
val loss = 2.3894248008728027
training loss = 2.3267383575439453 5500
val loss = 3.7792882919311523
training loss = 1.9550822973251343 5600
val loss = 2.3886756896972656
training loss = 1.9535006284713745 5700
val loss = 2.3793623447418213
training loss = 1.9790153503417969 5800
val loss = 2.110823392868042
training loss = 1.950425624847412 5900
val loss = 2.372227430343628
training loss = 1.9492554664611816 6000
val loss = 2.3387653827667236
training loss = 1.9475358724594116 6100
val loss = 2.3530173301696777
training loss = 1.9460793733596802 6200
val loss = 2.361147880554199
training loss = 1.952928066253662 6300
val loss = 2.2050957679748535
training loss = 1.9433419704437256 6400
val loss = 2.3531205654144287
training loss = 1.9584243297576904 6500
val loss = 2.589613199234009
training loss = 1.9407570362091064 6600
val loss = 2.338468551635742
training loss = 1.9394725561141968 6700
val loss = 2.3434948921203613
training loss = 1.9413074254989624 6800
val loss = 2.2459583282470703
training loss = 1.9370442628860474 6900
val loss = 2.335736036300659
training loss = 1.994585633277893 7000
val loss = 1.9610586166381836
training loss = 1.9347339868545532 7100
val loss = 2.324169635772705
training loss = 1.933587670326233 7200
val loss = 2.3268957138061523
training loss = 1.9330261945724487 7300
val loss = 2.2841508388519287
training loss = 1.9314295053482056 7400
val loss = 2.31976318359375
training loss = 1.9306625127792358 7500
val loss = 2.3481943607330322
training loss = 1.9294193983078003 7600
val loss = 2.32666277885437
training loss = 1.928348422050476 7700
val loss = 2.3108224868774414
training loss = 2.1952409744262695 7800
val loss = 3.447617530822754
training loss = 1.9264007806777954 7900
val loss = 2.29909086227417
training loss = 1.9254045486450195 8000
val loss = 2.3061463832855225
training loss = 1.924504041671753 8100
val loss = 2.305129051208496
training loss = 1.923536777496338 8200
val loss = 2.2969417572021484
training loss = 1.9259896278381348 8300
val loss = 2.196654796600342
training loss = 1.92172372341156 8400
val loss = 2.292025089263916
training loss = 2.155703067779541 8500
val loss = 1.651963710784912
training loss = 1.9199957847595215 8600
val loss = 2.289567708969116
training loss = 1.9190828800201416 8700
val loss = 2.284069776535034
training loss = 2.338460922241211 8800
val loss = 1.5240733623504639
training loss = 1.9174211025238037 8900
val loss = 2.285205602645874
training loss = 1.9165153503417969 9000
val loss = 2.2774064540863037
training loss = 2.1104211807250977 9100
val loss = 3.215256929397583
training loss = 1.9148533344268799 9200
val loss = 2.2682082653045654
training loss = 1.9139299392700195 9300
val loss = 2.2713027000427246
training loss = 1.9141188859939575 9400
val loss = 2.2172985076904297
training loss = 1.9122514724731445 9500
val loss = 2.2671732902526855
training loss = 2.1236727237701416 9600
val loss = 1.656014323234558
training loss = 1.9105892181396484 9700
val loss = 2.2630274295806885
training loss = 1.9096547365188599 9800
val loss = 2.261889934539795
training loss = 1.9089305400848389 9900
val loss = 2.2674241065979004
training loss = 1.907994270324707 10000
val loss = 2.25856876373291
training loss = 1.928841233253479 10100
val loss = 2.5309295654296875
training loss = 1.9063684940338135 10200
val loss = 2.2477452754974365
training loss = 1.905403733253479 10300
val loss = 2.252913236618042
training loss = 1.926437258720398 10400
val loss = 2.523942708969116
training loss = 1.9037858247756958 10500
val loss = 2.248405933380127
training loss = 1.9028348922729492 10600
val loss = 2.2487049102783203
training loss = 1.9021614789962769 10700
val loss = 2.2508225440979004
training loss = 1.901232123374939 10800
val loss = 2.244809865951538
training loss = 1.955214023590088 10900
val loss = 2.69191837310791
training loss = 1.8996955156326294 11000
val loss = 2.24422025680542
training loss = 1.898777961730957 11100
val loss = 2.240204095840454
training loss = 1.9000393152236938 11200
val loss = 2.3146982192993164
training loss = 1.8973348140716553 11300
val loss = 2.237983465194702
training loss = 1.8964431285858154 11400
val loss = 2.2401556968688965
training loss = 1.8959985971450806 11500
val loss = 2.226623773574829
training loss = 1.8951072692871094 11600
val loss = 2.2335517406463623
training loss = 2.200483798980713 11700
val loss = 1.5794179439544678
training loss = 1.893748164176941 11800
val loss = 2.2365002632141113
training loss = 1.8928585052490234 11900
val loss = 2.2298531532287598
training loss = 1.89449143409729 12000
val loss = 2.1589293479919434
training loss = 1.8916702270507812 12100
val loss = 2.227918863296509
training loss = 1.8907102346420288 12200
val loss = 2.2269914150238037
training loss = 1.8905305862426758 12300
val loss = 2.2540078163146973
training loss = 1.8891874551773071 12400
val loss = 2.223351001739502
training loss = 1.888709306716919 12500
val loss = 2.2397453784942627
training loss = 1.887346625328064 12600
val loss = 2.2190818786621094
training loss = 1.885882019996643 12700
val loss = 2.2263035774230957
training loss = 1.8852535486221313 12800
val loss = 2.2209272384643555
training loss = 1.883834719657898 12900
val loss = 2.212395668029785
training loss = 1.8947837352752686 13000
val loss = 2.404578685760498
training loss = 1.881898283958435 13100
val loss = 2.210134983062744
training loss = 1.880782961845398 13200
val loss = 2.2260196208953857
training loss = 1.8802366256713867 13300
val loss = 2.197003126144409
training loss = 1.8791217803955078 13400
val loss = 2.207813024520874
training loss = 1.8787569999694824 13500
val loss = 2.219682216644287
training loss = 1.8778841495513916 13600
val loss = 2.2097394466400146
training loss = 1.8770650625228882 13700
val loss = 2.2032976150512695
training loss = 1.8769193887710571 13800
val loss = 2.2135798931121826
training loss = 1.8762366771697998 13900
val loss = 2.211559534072876
training loss = 1.886340618133545 14000
val loss = 2.386491537094116
training loss = 1.8754746913909912 14100
val loss = 2.2082927227020264
training loss = 1.8748936653137207 14200
val loss = 2.216364860534668
training loss = 1.8759514093399048 14300
val loss = 2.1715540885925293
training loss = 1.874311923980713 14400
val loss = 2.220621109008789
training loss = 1.8768142461776733 14500
val loss = 2.309514045715332
training loss = 1.8737821578979492 14600
val loss = 2.2212095260620117
training loss = 1.8733450174331665 14700
val loss = 2.2254648208618164
training loss = 1.8733899593353271 14800
val loss = 2.227412223815918
training loss = 1.8729747533798218 14900
val loss = 2.2288639545440674
training loss = 1.8726383447647095 15000
val loss = 2.218364715576172
training loss = 1.8726776838302612 15100
val loss = 2.2482523918151855
training loss = 1.8722208738327026 15200
val loss = 2.2342276573181152
training loss = 1.880866289138794 15300
val loss = 2.1064414978027344
training loss = 1.8718926906585693 15400
val loss = 2.236328601837158
training loss = 1.872826337814331 15500
val loss = 2.2944204807281494
training loss = 1.871634602546692 15600
val loss = 2.2289273738861084
training loss = 1.8712040185928345 15700
val loss = 2.240938425064087
training loss = 1.8713513612747192 15800
val loss = 2.2544305324554443
training loss = 1.870965600013733 15900
val loss = 2.2436442375183105
training loss = 1.8721696138381958 16000
val loss = 2.1879849433898926
training loss = 1.87074613571167 16100
val loss = 2.2452712059020996
training loss = 1.8703914880752563 16200
val loss = 2.2472891807556152
training loss = 1.872769832611084 16300
val loss = 2.3281686305999756
training loss = 1.8701677322387695 16400
val loss = 2.2483818531036377
reduced chi^2 level 2 = 1.8700885772705078
Constrained alpha: 1.8905959129333496
Constrained beta: 0.9189833402633667
Constrained gamma: 16.11406898498535
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 879.4202,  830.5125,  951.4952,  955.0228, 1043.2262, 1016.8404,
        1094.2698, 1221.1317, 1132.6654, 1163.5094, 1182.6755, 1199.0623,
        1263.8315, 1288.5110, 1354.8275, 1399.8154, 1421.5150, 1460.2858,
        1614.7629, 1506.1588, 1634.4131, 1585.4927, 1580.7747, 1555.1682,
        1663.9518, 1707.9590, 1619.3740, 1740.6193, 1780.2638, 1630.3018,
        1652.2316, 1693.1821, 1714.4277, 1754.3469, 1718.7365, 1666.6082,
        1760.8293, 1595.5286, 1703.4646, 1615.8989, 1611.8927, 1561.2600,
        1506.0662, 1541.1328, 1350.8677, 1333.4534, 1299.4575, 1226.5996,
        1206.8495, 1257.9163, 1135.3427,  949.8539,  915.3817,  937.9031,
         924.2413,  874.0054,  850.2905,  674.2826,  571.6454,  517.3671,
         564.8323,  467.8258,  422.7896,  366.1901,  355.5615,  382.5629,
         301.1415,  272.9474,  201.1517,  165.8407,  157.3928,  149.5432,
         154.2599,  105.2452,   90.5457,   70.5068,   44.1708,   46.9294,
          49.7977,   38.4887,   21.8327,   45.2912,   31.8215])]
2982.6721449076767
3.0927086451451165 6.679298528894979 96.8403353940495
val isze = 8
idinces = [44 45 69 39 23 14  8 71 67 66 52 30 65 68 72  3 29 11 19 40 15 62 46 55
 10 32 48 56 13 57 41  6 47 20 12 82  9 22 80 26 74 70 28 50 17 77 73 21
 81 76 51 60 37 53 38 58 31 18  7  1 16  0 64 33 78  4 24 79 43 42 59 75
  5 27 61  2 63 34 25 54 36 49 35]
we are doing training validation split
training loss = 29.232223510742188 100
val loss = 27.726848602294922
training loss = 20.24738311767578 200
val loss = 16.652652740478516
training loss = 15.102356910705566 300
val loss = 10.262918472290039
training loss = 12.076013565063477 400
val loss = 6.480422019958496
training loss = 10.276482582092285 500
val loss = 4.32637357711792
training loss = 9.185262680053711 600
val loss = 3.13462233543396
training loss = 8.513497352600098 700
val loss = 2.505216360092163
training loss = 8.095511436462402 800
val loss = 2.200658082962036
training loss = 7.832996845245361 900
val loss = 2.079209804534912
training loss = 7.666128158569336 1000
val loss = 2.0560970306396484
training loss = 7.557937145233154 1100
val loss = 2.0811855792999268
training loss = 7.485428333282471 1200
val loss = 2.125247001647949
training loss = 7.4343342781066895 1300
val loss = 2.172044515609741
training loss = 7.39588737487793 1400
val loss = 2.2135813236236572
training loss = 7.364819526672363 1500
val loss = 2.246277332305908
training loss = 7.338040351867676 1600
val loss = 2.2695541381835938
training loss = 7.313818454742432 1700
val loss = 2.2841615676879883
training loss = 7.29123067855835 1800
val loss = 2.2915165424346924
training loss = 7.2698187828063965 1900
val loss = 2.293243646621704
training loss = 7.2493672370910645 2000
val loss = 2.2908360958099365
training loss = 7.229743003845215 2100
val loss = 2.2854928970336914
training loss = 7.210776329040527 2200
val loss = 2.278165340423584
training loss = 7.1921257972717285 2300
val loss = 2.269392728805542
training loss = 7.173020839691162 2400
val loss = 2.259479522705078
training loss = 7.151571750640869 2500
val loss = 2.2479264736175537
training loss = 7.122505187988281 2600
val loss = 2.232665538787842
training loss = 7.068047046661377 2700
val loss = 2.204874277114868
training loss = 6.9214911460876465 2800
val loss = 2.1263349056243896
training loss = 6.568546772003174 2900
val loss = 1.9517364501953125
training loss = 5.980353355407715 3000
val loss = 1.8077545166015625
training loss = 4.839828014373779 3100
val loss = 1.6778719425201416
training loss = 3.027933359146118 3200
val loss = 1.9321433305740356
training loss = 2.7257189750671387 3300
val loss = 2.472808837890625
training loss = 2.705561399459839 3400
val loss = 2.5946333408355713
training loss = 2.6908130645751953 3500
val loss = 2.67205810546875
training loss = 2.6790504455566406 3600
val loss = 2.726304769515991
training loss = 2.66969633102417 3700
val loss = 2.759258985519409
training loss = 2.6620230674743652 3800
val loss = 2.779196262359619
training loss = 2.6559014320373535 3900
val loss = 2.784050941467285
training loss = 2.6502861976623535 4000
val loss = 2.762014150619507
training loss = 2.6452813148498535 4100
val loss = 2.76877760887146
training loss = 2.640305280685425 4200
val loss = 2.7457356452941895
training loss = 2.6356585025787354 4300
val loss = 2.76841402053833
training loss = 2.630669116973877 4400
val loss = 2.711045026779175
training loss = 2.626549482345581 4500
val loss = 2.6333107948303223
training loss = 2.621426582336426 4600
val loss = 2.67305064201355
training loss = 2.617624521255493 4700
val loss = 2.5906076431274414
training loss = 2.612778902053833 4800
val loss = 2.6325342655181885
training loss = 2.620443105697632 4900
val loss = 2.345987558364868
training loss = 2.605163335800171 5000
val loss = 2.591644763946533
training loss = 2.6024341583251953 5100
val loss = 2.6362698078155518
training loss = 2.5990524291992188 5200
val loss = 2.552553653717041
training loss = 2.5965816974639893 5300
val loss = 2.5388214588165283
training loss = 2.594984292984009 5400
val loss = 2.4818036556243896
training loss = 2.593012809753418 5500
val loss = 2.513788938522339
training loss = 2.6056013107299805 5600
val loss = 2.217859983444214
training loss = 2.5908727645874023 5700
val loss = 2.4929163455963135
training loss = 2.7022006511688232 5800
val loss = 3.4866342544555664
training loss = 2.589695930480957 5900
val loss = 2.4905247688293457
training loss = 2.589346170425415 6000
val loss = 2.4747493267059326
training loss = 2.589186191558838 6100
val loss = 2.454496383666992
training loss = 2.588886260986328 6200
val loss = 2.4781036376953125
training loss = 2.5980679988861084 6300
val loss = 2.24149751663208
training loss = 2.588670253753662 6400
val loss = 2.476229667663574
training loss = 2.588642120361328 6500
val loss = 2.477182388305664
training loss = 2.588610887527466 6600
val loss = 2.475162982940674
training loss = 2.5885677337646484 6700
val loss = 2.4809939861297607
training loss = 2.693453073501587 6800
val loss = 3.4387924671173096
training loss = 2.5885837078094482 6900
val loss = 2.4770071506500244
training loss = 2.588624954223633 7000
val loss = 2.4739315509796143
training loss = 2.588712692260742 7100
val loss = 2.459299087524414
training loss = 2.588613748550415 7200
val loss = 2.4863381385803223
training loss = 2.588954210281372 7300
val loss = 2.5358645915985107
training loss = 2.5886189937591553 7400
val loss = 2.488659620285034
training loss = 2.588819980621338 7500
val loss = 2.457397699356079
training loss = 2.588653564453125 7600
val loss = 2.481064558029175
training loss = 2.5886402130126953 7700
val loss = 2.4923174381256104
training loss = 2.6046087741851807 7800
val loss = 2.1857123374938965
training loss = 2.5886266231536865 7900
val loss = 2.4920499324798584
training loss = 2.58864164352417 8000
val loss = 2.498680830001831
training loss = 2.588864803314209 8100
val loss = 2.456620693206787
training loss = 2.5886173248291016 8200
val loss = 2.4959828853607178
training loss = 2.58866024017334 8300
val loss = 2.479910373687744
training loss = 2.5885870456695557 8400
val loss = 2.496644973754883
training loss = 2.626169443130493 8500
val loss = 2.038555860519409
training loss = 2.588543176651001 8600
val loss = 2.5019888877868652
training loss = 2.588907480239868 8700
val loss = 2.549818754196167
training loss = 2.5884974002838135 8800
val loss = 2.4946887493133545
training loss = 2.588470458984375 8900
val loss = 2.5001702308654785
training loss = 2.591796875 9000
val loss = 2.655608654022217
training loss = 2.588388442993164 9100
val loss = 2.5005922317504883
training loss = 2.5884313583374023 9200
val loss = 2.4814417362213135
training loss = 2.588330030441284 9300
val loss = 2.5089821815490723
training loss = 2.588289976119995 9400
val loss = 2.5010688304901123
training loss = 2.5914742946624756 9500
val loss = 2.359233856201172
training loss = 2.5881991386413574 9600
val loss = 2.5001935958862305
training loss = 2.7374913692474365 9700
val loss = 3.679488182067871
training loss = 2.588116407394409 9800
val loss = 2.4989523887634277
training loss = 2.588432550430298 9900
val loss = 2.4539542198181152
training loss = 2.588026523590088 10000
val loss = 2.4950249195098877
training loss = 2.5879745483398438 10100
val loss = 2.501188039779663
training loss = 2.5906381607055664 10200
val loss = 2.640929698944092
training loss = 2.58787202835083 10300
val loss = 2.5024056434631348
training loss = 2.595797061920166 10400
val loss = 2.2800850868225098
training loss = 2.5877673625946045 10500
val loss = 2.5041372776031494
training loss = 2.8986165523529053 10600
val loss = 1.4083054065704346
training loss = 2.587662935256958 10700
val loss = 2.5109777450561523
training loss = 2.587599039077759 10800
val loss = 2.50104022026062
training loss = 2.587772846221924 10900
val loss = 2.463020086288452
training loss = 2.587477922439575 11000
val loss = 2.497606039047241
training loss = 2.587421417236328 11100
val loss = 2.5006110668182373
training loss = 2.5878958702087402 11200
val loss = 2.5616331100463867
training loss = 2.5873076915740967 11300
val loss = 2.5036842823028564
training loss = 2.58725905418396 11400
val loss = 2.4998247623443604
training loss = 2.587496757507324 11500
val loss = 2.4568681716918945
training loss = 2.5871329307556152 11600
val loss = 2.499624252319336
training loss = 2.5892012119293213 11700
val loss = 2.6213366985321045
training loss = 2.587029457092285 11800
val loss = 2.5075128078460693
training loss = 2.5869672298431396 11900
val loss = 2.498278856277466
training loss = 2.5920138359069824 12000
val loss = 2.691329002380371
training loss = 2.586841583251953 12100
val loss = 2.4975857734680176
training loss = 2.586796522140503 12200
val loss = 2.501446485519409
training loss = 2.58685564994812 12300
val loss = 2.4697728157043457
training loss = 2.586665630340576 12400
val loss = 2.49837589263916
training loss = 2.591052293777466 12500
val loss = 2.680356025695801
training loss = 2.5865559577941895 12600
val loss = 2.502044677734375
training loss = 2.5890161991119385 12700
val loss = 2.3700480461120605
training loss = 2.58644437789917 12800
val loss = 2.4914214611053467
training loss = 2.5863826274871826 12900
val loss = 2.496835231781006
training loss = 2.5868027210235596 13000
val loss = 2.4405102729797363
training loss = 2.5862653255462646 13100
val loss = 2.4949679374694824
training loss = 2.606700897216797 13200
val loss = 2.14858078956604
training loss = 2.5861473083496094 13300
val loss = 2.4969377517700195
training loss = 2.5873122215270996 13400
val loss = 2.586925983428955
training loss = 2.5860636234283447 13500
val loss = 2.481785774230957
training loss = 2.585981607437134 13600
val loss = 2.4934704303741455
training loss = 2.586294174194336 13700
val loss = 2.445791006088257
training loss = 2.585865020751953 13800
val loss = 2.4935004711151123
training loss = 2.5930163860321045 13900
val loss = 2.7228410243988037
training loss = 2.585753917694092 14000
val loss = 2.493622064590454
training loss = 2.6219584941864014 14100
val loss = 3.033820152282715
training loss = 2.5856516361236572 14200
val loss = 2.4986181259155273
training loss = 2.7829058170318604 14300
val loss = 3.8692855834960938
training loss = 2.5855484008789062 14400
val loss = 2.500758171081543
training loss = 2.5855154991149902 14500
val loss = 2.47886061668396
training loss = 2.5855233669281006 14600
val loss = 2.517016649246216
training loss = 2.585374355316162 14700
val loss = 2.490621566772461
training loss = 2.590327501296997 14800
val loss = 2.680278778076172
training loss = 2.5852673053741455 14900
val loss = 2.491283416748047
training loss = 2.585458278656006 15000
val loss = 2.5304298400878906
training loss = 2.585174322128296 15100
val loss = 2.496784210205078
training loss = 2.58512020111084 15200
val loss = 2.489006757736206
training loss = 2.585775375366211 15300
val loss = 2.4222443103790283
training loss = 2.5850119590759277 15400
val loss = 2.489391565322876
training loss = 2.585021495819092 15500
val loss = 2.5000689029693604
training loss = 2.5849180221557617 15600
val loss = 2.4828994274139404
training loss = 2.5848629474639893 15700
val loss = 2.4879252910614014
training loss = 2.5983550548553467 15800
val loss = 2.203683376312256
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 877.3365,  832.0776,  940.2173,  972.9445, 1008.7264, 1041.1521,
        1137.3009, 1122.2738, 1126.0669, 1127.6848, 1260.7589, 1173.2614,
        1299.2181, 1212.1892, 1318.5414, 1486.6239, 1452.3177, 1466.0770,
        1598.9515, 1500.3209, 1515.4486, 1533.1980, 1570.7936, 1540.6736,
        1617.1466, 1729.2887, 1657.5682, 1734.3762, 1673.6576, 1701.8314,
        1659.2220, 1711.5435, 1735.3724, 1721.0822, 1706.7578, 1795.5792,
        1709.9401, 1513.2244, 1551.2577, 1629.0587, 1629.7531, 1552.4570,
        1462.4326, 1550.7255, 1342.7321, 1345.3746, 1292.6405, 1254.2976,
        1119.3689, 1100.2455, 1085.3717, 1007.1521,  968.7626,  901.9919,
         782.0155,  838.1325,  800.3999,  707.4517,  601.2493,  535.5981,
         557.8502,  504.4042,  461.5659,  394.8301,  324.4407,  338.0779,
         296.9898,  260.5116,  224.0676,  163.1133,  165.9042,  143.3944,
         133.6887,  116.5659,   93.5295,   71.6306,   57.7224,   47.9136,
          34.4581,   44.3346,   23.1978,   51.0355,   38.6698])]
2725.524863091201
1.6644597518793742 19.667657104785178 41.145155233288655
val isze = 8
idinces = [21 78 17 40 49 68 13 43 77 62 28 60  1 44 34  4  7 20 31 25  9 81 66 70
  5 37 39  0 69 54 32 51 16 35 61 10 36 14 50 46 64  2 12 48 45 75  8 74
 23 73 82 22 80 29 67 41 53 11 18 30 58 65 33 27 57 42 52  6 55 71 47 63
 15 56 72 24  3 38 19 59 76 26 79]
we are doing training validation split
training loss = 294.9249267578125 100
val loss = 305.91827392578125
training loss = 53.643306732177734 200
val loss = 46.69524002075195
training loss = 14.402995109558105 300
val loss = 11.516281127929688
training loss = 13.808293342590332 400
val loss = 11.009552001953125
training loss = 13.155049324035645 500
val loss = 10.471389770507812
training loss = 12.454789161682129 600
val loss = 9.888445854187012
training loss = 11.730363845825195 700
val loss = 9.278965950012207
training loss = 11.00727367401123 800
val loss = 8.664361953735352
training loss = 10.31291675567627 900
val loss = 8.069009780883789
training loss = 9.67445182800293 1000
val loss = 7.518455505371094
training loss = 9.115224838256836 1100
val loss = 7.036060333251953
training loss = 8.650599479675293 1200
val loss = 6.638731002807617
training loss = 8.284767150878906 1300
val loss = 6.332974433898926
training loss = 8.01012134552002 1400
val loss = 6.113498687744141
training loss = 7.810145854949951 1500
val loss = 5.965378761291504
training loss = 7.664584159851074 1600
val loss = 5.868887901306152
training loss = 7.554588317871094 1700
val loss = 5.805094242095947
training loss = 7.465768337249756 1800
val loss = 5.759195327758789
training loss = 7.388838768005371 1900
val loss = 5.721663475036621
training loss = 7.318642616271973 2000
val loss = 5.687280654907227
training loss = 7.252721309661865 2100
val loss = 5.653846740722656
training loss = 7.190102577209473 2200
val loss = 5.620749473571777
training loss = 7.130481719970703 2300
val loss = 5.5879621505737305
training loss = 7.073785781860352 2400
val loss = 5.5558762550354
training loss = 7.019915580749512 2500
val loss = 5.524750709533691
training loss = 6.968653202056885 2600
val loss = 5.494765281677246
training loss = 6.91956090927124 2700
val loss = 5.465877532958984
training loss = 6.871906280517578 2800
val loss = 5.437834739685059
training loss = 6.824520111083984 2900
val loss = 5.4101433753967285
training loss = 6.775476932525635 3000
val loss = 5.381837368011475
training loss = 6.721324920654297 3100
val loss = 5.351159572601318
training loss = 6.654748439788818 3200
val loss = 5.314393043518066
training loss = 6.556253910064697 3300
val loss = 5.261897087097168
training loss = 6.359067916870117 3400
val loss = 5.162230491638184
training loss = 5.872802257537842 3500
val loss = 4.944714069366455
training loss = 4.999183654785156 3600
val loss = 4.554457664489746
training loss = 3.703505039215088 3700
val loss = 3.857481002807617
training loss = 2.445983648300171 3800
val loss = 3.134613275527954
training loss = 2.109933614730835 3900
val loss = 2.874454975128174
training loss = 2.078972578048706 4000
val loss = 2.8193016052246094
training loss = 2.0626327991485596 4100
val loss = 2.7891368865966797
training loss = 2.048719882965088 4200
val loss = 2.7658653259277344
training loss = 2.0362448692321777 4300
val loss = 2.7472310066223145
training loss = 2.0246500968933105 4400
val loss = 2.732294797897339
training loss = 2.0135481357574463 4500
val loss = 2.7203478813171387
training loss = 2.0371782779693604 4600
val loss = 2.78615403175354
training loss = 1.992618203163147 4700
val loss = 2.702155828475952
training loss = 2.0289413928985596 4800
val loss = 2.789151668548584
training loss = 1.972983479499817 4900
val loss = 2.689730644226074
training loss = 1.986918568611145 5000
val loss = 2.7456908226013184
training loss = 1.9535974264144897 5100
val loss = 2.6813833713531494
training loss = 1.9434560537338257 5200
val loss = 2.6800265312194824
training loss = 1.9335734844207764 5300
val loss = 2.6794118881225586
training loss = 1.9231457710266113 5400
val loss = 2.6782493591308594
training loss = 1.9305609464645386 5500
val loss = 2.7274398803710938
training loss = 1.9022576808929443 5600
val loss = 2.680123805999756
training loss = 1.8917053937911987 5700
val loss = 2.6827807426452637
training loss = 1.882298231124878 5800
val loss = 2.679502487182617
training loss = 1.872178554534912 5900
val loss = 2.685926914215088
training loss = 1.8638914823532104 6000
val loss = 2.698333263397217
training loss = 1.8532615900039673 6100
val loss = 2.6888248920440674
training loss = 1.8439327478408813 6200
val loss = 2.6874032020568848
training loss = 1.8359013795852661 6300
val loss = 2.6801767349243164
training loss = 1.8263880014419556 6400
val loss = 2.686906099319458
training loss = 1.8664941787719727 6500
val loss = 2.795168399810791
training loss = 1.809668779373169 6600
val loss = 2.6866440773010254
training loss = 1.8015387058258057 6700
val loss = 2.687617301940918
training loss = 1.7942101955413818 6800
val loss = 2.6842756271362305
training loss = 1.7868458032608032 6900
val loss = 2.687613010406494
training loss = 1.8126837015151978 7000
val loss = 2.7753963470458984
training loss = 1.7736856937408447 7100
val loss = 2.6892528533935547
training loss = 1.7676056623458862 7200
val loss = 2.689763069152832
training loss = 1.7672208547592163 7300
val loss = 2.7191696166992188
training loss = 1.7570772171020508 7400
val loss = 2.6916255950927734
training loss = 1.7523270845413208 7500
val loss = 2.6920337677001953
training loss = 1.748549461364746 7600
val loss = 2.6873626708984375
training loss = 1.744507074356079 7700
val loss = 2.693924903869629
training loss = 1.81709885597229 7800
val loss = 2.675640344619751
training loss = 1.7382299900054932 7900
val loss = 2.6954212188720703
training loss = 1.7356339693069458 8000
val loss = 2.69832181930542
training loss = 1.7333787679672241 8100
val loss = 2.6943092346191406
training loss = 1.7313674688339233 8200
val loss = 2.6964378356933594
training loss = 1.7366464138031006 8300
val loss = 2.672347068786621
training loss = 1.7281954288482666 8400
val loss = 2.696854829788208
training loss = 1.7268750667572021 8500
val loss = 2.6968929767608643
training loss = 1.7257801294326782 8600
val loss = 2.6991770267486572
training loss = 1.724778652191162 8700
val loss = 2.697305202484131
training loss = 1.845870852470398 8800
val loss = 2.686283588409424
training loss = 1.7231881618499756 8900
val loss = 2.698037624359131
training loss = 1.7225370407104492 9000
val loss = 2.697155475616455
training loss = 1.7640615701675415 9100
val loss = 2.653860092163086
training loss = 1.7214604616165161 9200
val loss = 2.6978182792663574
training loss = 1.7210125923156738 9300
val loss = 2.6967740058898926
training loss = 1.720608115196228 9400
val loss = 2.695098638534546
training loss = 1.7202612161636353 9500
val loss = 2.6967315673828125
training loss = 1.7199504375457764 9600
val loss = 2.696335792541504
training loss = 1.7196660041809082 9700
val loss = 2.694333791732788
training loss = 1.7194095849990845 9800
val loss = 2.6961770057678223
training loss = 1.71918785572052 9900
val loss = 2.6957826614379883
training loss = 1.7202478647232056 10000
val loss = 2.6818008422851562
training loss = 1.7187830209732056 10100
val loss = 2.6954431533813477
training loss = 1.7186471223831177 10200
val loss = 2.6927847862243652
training loss = 1.7185481786727905 10300
val loss = 2.6990294456481934
training loss = 1.7183074951171875 10400
val loss = 2.6947648525238037
training loss = 1.7267122268676758 10500
val loss = 2.742736339569092
training loss = 1.7180417776107788 10600
val loss = 2.694307327270508
training loss = 1.7214593887329102 10700
val loss = 2.7228736877441406
training loss = 1.7178150415420532 10800
val loss = 2.6947219371795654
training loss = 1.717716097831726 10900
val loss = 2.6937766075134277
training loss = 1.7196063995361328 11000
val loss = 2.6761035919189453
training loss = 1.7175244092941284 11100
val loss = 2.693419933319092
training loss = 1.73492431640625 11200
val loss = 2.6557164192199707
training loss = 1.717374563217163 11300
val loss = 2.6948587894439697
training loss = 1.7172856330871582 11400
val loss = 2.6930360794067383
training loss = 1.7210692167282104 11500
val loss = 2.669581413269043
training loss = 1.7171399593353271 11600
val loss = 2.692483901977539
training loss = 1.7178802490234375 11700
val loss = 2.7052178382873535
training loss = 1.717024803161621 11800
val loss = 2.6939096450805664
training loss = 1.716950535774231 11900
val loss = 2.6921820640563965
training loss = 1.843410849571228 12000
val loss = 2.9828455448150635
training loss = 1.7168371677398682 12100
val loss = 2.6931183338165283
training loss = 1.7167800664901733 12200
val loss = 2.691802978515625
training loss = 1.7426766157150269 12300
val loss = 2.649782180786133
training loss = 1.7166776657104492 12400
val loss = 2.690495014190674
training loss = 1.7166337966918945 12500
val loss = 2.6913726329803467
training loss = 1.7182735204696655 12600
val loss = 2.71016788482666
training loss = 1.7165387868881226 12700
val loss = 2.690934658050537
training loss = 1.9073165655136108 12800
val loss = 3.0816283226013184
training loss = 1.7164469957351685 12900
val loss = 2.6902401447296143
training loss = 1.7164084911346436 13000
val loss = 2.690805435180664
training loss = 1.7217175960540771 13100
val loss = 2.7283060550689697
training loss = 1.7163212299346924 13200
val loss = 2.6909594535827637
training loss = 1.7162877321243286 13300
val loss = 2.6899237632751465
training loss = 1.7162659168243408 13400
val loss = 2.692426919937134
training loss = 1.7162054777145386 13500
val loss = 2.6903584003448486
training loss = 1.7198103666305542 13600
val loss = 2.720553398132324
training loss = 1.7161263227462769 13700
val loss = 2.6898915767669678
training loss = 1.7160923480987549 13800
val loss = 2.690141201019287
training loss = 1.7169749736785889 13900
val loss = 2.6770238876342773
training loss = 1.7160128355026245 14000
val loss = 2.690194845199585
training loss = 1.7163033485412598 14100
val loss = 2.682255268096924
training loss = 1.715994119644165 14200
val loss = 2.686420440673828
training loss = 1.7159041166305542 14300
val loss = 2.6899707317352295
training loss = 1.716851830482483 14400
val loss = 2.7049269676208496
training loss = 1.71583092212677 14500
val loss = 2.689659595489502
training loss = 1.715801477432251 14600
val loss = 2.690425395965576
training loss = 1.715935468673706 14700
val loss = 2.6837522983551025
training loss = 1.7157237529754639 14800
val loss = 2.689940929412842
training loss = 1.7274322509765625 14900
val loss = 2.7439920902252197
training loss = 1.715651035308838 15000
val loss = 2.6901626586914062
training loss = 1.7156164646148682 15100
val loss = 2.689959764480591
training loss = 1.7158901691436768 15200
val loss = 2.6977553367614746
training loss = 1.7155418395996094 15300
val loss = 2.6900506019592285
training loss = 1.895078182220459 15400
val loss = 2.691126823425293
training loss = 1.715471863746643 15500
val loss = 2.68881893157959
training loss = 1.7154309749603271 15600
val loss = 2.6898648738861084
training loss = 1.715904951095581 15700
val loss = 2.7004590034484863
training loss = 1.7153515815734863 15800
val loss = 2.690099000930786
training loss = 1.8604716062545776 15900
val loss = 2.670135259628296
training loss = 1.715279221534729 16000
val loss = 2.6891849040985107
training loss = 1.7152341604232788 16100
val loss = 2.690286636352539
training loss = 1.7153334617614746 16200
val loss = 2.6841816902160645
training loss = 1.715153694152832 16300
val loss = 2.6904287338256836
training loss = 1.7151113748550415 16400
val loss = 2.690765857696533
training loss = 1.7154451608657837 16500
val loss = 2.6994731426239014
training loss = 1.7150276899337769 16600
val loss = 2.69085693359375
training loss = 1.7538172006607056 16700
val loss = 2.641878604888916
training loss = 1.714943528175354 16800
val loss = 2.6911849975585938
training loss = 1.7148998975753784 16900
val loss = 2.6900854110717773
training loss = 1.7149959802627563 17000
val loss = 2.696286916732788
training loss = 1.7148054838180542 17100
val loss = 2.691580057144165
training loss = 1.7390385866165161 17200
val loss = 2.6434693336486816
training loss = 1.7147141695022583 17300
val loss = 2.6916236877441406
training loss = 1.71466064453125 17400
val loss = 2.6918458938598633
training loss = 1.714641809463501 17500
val loss = 2.6899077892303467
training loss = 1.714565634727478 17600
val loss = 2.6923394203186035
training loss = 1.7343260049819946 17700
val loss = 2.777745485305786
training loss = 1.7144674062728882 17800
val loss = 2.6921448707580566
training loss = 1.714407205581665 17900
val loss = 2.6929996013641357
training loss = 1.7172670364379883 18000
val loss = 2.6704494953155518
training loss = 1.7143019437789917 18100
val loss = 2.69350528717041
training loss = 2.066722869873047 18200
val loss = 2.8023681640625
training loss = 1.7141941785812378 18300
val loss = 2.693781852722168
training loss = 1.7141276597976685 18400
val loss = 2.6943840980529785
training loss = 1.71435546875 18500
val loss = 2.6864161491394043
training loss = 1.7140125036239624 18600
val loss = 2.69500732421875
training loss = 1.8523975610733032 18700
val loss = 2.6675362586975098
training loss = 1.7138956785202026 18800
val loss = 2.6961185932159424
training loss = 1.713827133178711 18900
val loss = 2.6968469619750977
training loss = 1.7138253450393677 19000
val loss = 2.69897723197937
training loss = 1.7136943340301514 19100
val loss = 2.696533203125
training loss = 1.7549443244934082 19200
val loss = 2.644181728363037
training loss = 1.7135738134384155 19300
val loss = 2.6977343559265137
training loss = 1.7134931087493896 19400
val loss = 2.697601318359375
training loss = 1.7248203754425049 19500
val loss = 2.6612844467163086
training loss = 1.7133595943450928 19600
val loss = 2.698716163635254
training loss = 1.7136576175689697 19700
val loss = 2.69008469581604
training loss = 1.713226318359375 19800
val loss = 2.7000784873962402
training loss = 1.7131329774856567 19900
val loss = 2.699657917022705
training loss = 1.713159203529358 20000
val loss = 2.702042579650879
training loss = 1.7129954099655151 20100
val loss = 2.7007620334625244
training loss = 1.712904930114746 20200
val loss = 2.700756788253784
training loss = 1.7129437923431396 20300
val loss = 2.7056188583374023
training loss = 1.7127560377120972 20400
val loss = 2.702028751373291
training loss = 1.7257349491119385 20500
val loss = 2.6552441120147705
training loss = 1.7126200199127197 20600
val loss = 2.7036936283111572
training loss = 1.7125203609466553 20700
val loss = 2.7035088539123535
training loss = 1.720941185951233 20800
val loss = 2.755186080932617
training loss = 1.7123692035675049 20900
val loss = 2.703972816467285
training loss = 1.7122868299484253 21000
val loss = 2.7067484855651855
training loss = 1.712260365486145 21100
val loss = 2.701849937438965
reduced chi^2 level 2 = 1.7121566534042358
Constrained alpha: 1.8897831439971924
Constrained beta: 2.1397063732147217
Constrained gamma: 13.973531723022461
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 829.2401,  906.5712,  962.3231,  986.2005, 1006.3302, 1055.3705,
        1166.1282, 1099.1384, 1099.3179, 1159.5942, 1182.8373, 1193.0842,
        1259.5770, 1263.1798, 1350.1477, 1414.9197, 1401.2803, 1408.7289,
        1524.2820, 1535.8032, 1621.3838, 1511.3395, 1643.3542, 1687.0055,
        1671.2362, 1640.1079, 1636.1316, 1669.1682, 1738.0630, 1678.3107,
        1650.7477, 1743.2148, 1696.2726, 1724.7670, 1724.9231, 1759.6306,
        1620.7214, 1612.3165, 1596.1351, 1657.4686, 1617.2397, 1513.3115,
        1552.1355, 1517.8363, 1300.6765, 1266.7548, 1288.6234, 1225.3052,
        1178.8888, 1144.6149, 1093.2067, 1053.3872,  883.9000,  922.2488,
         844.0308,  861.5438,  811.8947,  731.8348,  623.5908,  524.8939,
         539.4180,  475.4209,  441.1192,  376.8390,  363.1006,  365.0508,
         288.3435,  300.5200,  208.1180,  145.2956,  180.5450,  165.5891,
         134.5063,  110.5485,  101.4710,   59.7925,   46.4617,   39.6624,
          31.5378,   40.9605,   17.9849,   35.4657,   27.2823])]
2628.2554749383553
4.135297328043705 19.941406433105 35.303966152186796
val isze = 8
idinces = [75 33 63 36 67 70 30 31 18 47 80 76 23 45 19 28 49 39 27 64 16 62 10 42
 17  8 48 32 73 50  5 41 77 82 26 24 12 66 34 72 65 59 25  0 60 46 22 43
  2 21 68 37 53  9 54 44 74  7 71 61 52  3 81 13 20 57 35 11 58  6 29 38
  4 14 40 69 51 78 55  1 15 56 79]
we are doing training validation split
training loss = 554.3781127929688 100
val loss = 660.331298828125
training loss = 82.51168060302734 200
val loss = 86.02932739257812
training loss = 10.899508476257324 300
val loss = 22.467071533203125
training loss = 10.296808242797852 400
val loss = 20.934661865234375
training loss = 9.666102409362793 500
val loss = 18.967317581176758
training loss = 8.998067855834961 600
val loss = 16.811660766601562
training loss = 8.094534873962402 700
val loss = 14.00040054321289
training loss = 7.32161808013916 800
val loss = 11.260472297668457
training loss = 6.951977729797363 900
val loss = 9.859617233276367
training loss = 6.66636323928833 1000
val loss = 8.768211364746094
training loss = 6.435986518859863 1100
val loss = 7.899169921875
training loss = 6.245845317840576 1200
val loss = 7.218280792236328
training loss = 6.082392692565918 1300
val loss = 6.682194232940674
training loss = 5.935047149658203 1400
val loss = 6.249965667724609
training loss = 5.7959489822387695 1500
val loss = 5.888043403625488
training loss = 5.659087657928467 1600
val loss = 5.571307182312012
training loss = 5.519479751586914 1700
val loss = 5.280973434448242
training loss = 5.372686862945557 1800
val loss = 5.003877639770508
training loss = 5.214618682861328 1900
val loss = 4.731142044067383
training loss = 5.041645050048828 2000
val loss = 4.457468032836914
training loss = 4.850864887237549 2100
val loss = 4.18137788772583
training loss = 4.640446186065674 2200
val loss = 3.9051737785339355
training loss = 4.409620761871338 2300
val loss = 3.633577346801758
training loss = 4.157873630523682 2400
val loss = 3.372492790222168
training loss = 3.8826820850372314 2500
val loss = 3.1275923252105713
training loss = 3.575728416442871 2600
val loss = 2.9055233001708984
training loss = 3.222539186477661 2700
val loss = 2.725778102874756
training loss = 2.8363754749298096 2800
val loss = 2.658351421356201
training loss = 2.5336499214172363 2900
val loss = 2.7620701789855957
training loss = 2.3854928016662598 3000
val loss = 2.8519506454467773
training loss = 2.3162591457366943 3100
val loss = 2.8224146366119385
training loss = 2.277204751968384 3200
val loss = 2.7281227111816406
training loss = 2.2499470710754395 3300
val loss = 2.6730384826660156
training loss = 2.2309858798980713 3400
val loss = 2.604020118713379
training loss = 2.215207576751709 3500
val loss = 2.5635390281677246
training loss = 2.202279806137085 3600
val loss = 2.5273852348327637
training loss = 2.190624952316284 3700
val loss = 2.4913175106048584
training loss = 2.18196964263916 3800
val loss = 2.4531002044677734
training loss = 2.170119285583496 3900
val loss = 2.4423437118530273
training loss = 2.1629931926727295 4000
val loss = 2.4140965938568115
training loss = 2.1519718170166016 4100
val loss = 2.4082913398742676
training loss = 2.1454825401306152 4200
val loss = 2.3875532150268555
training loss = 2.135704278945923 4300
val loss = 2.383513927459717
training loss = 2.1348776817321777 4400
val loss = 2.394568681716919
training loss = 2.121051073074341 4500
val loss = 2.3645355701446533
training loss = 2.121572494506836 4600
val loss = 2.3447020053863525
training loss = 2.107919454574585 4700
val loss = 2.348405599594116
training loss = 2.1018965244293213 4800
val loss = 2.340447425842285
training loss = 2.1854088306427 4900
val loss = 2.364225387573242
training loss = 2.090301275253296 5000
val loss = 2.325702667236328
training loss = 2.084810733795166 5100
val loss = 2.317506790161133
training loss = 2.079948663711548 5200
val loss = 2.3089089393615723
training loss = 2.074554443359375 5300
val loss = 2.303621768951416
training loss = 2.094984292984009 5400
val loss = 2.3366174697875977
training loss = 2.064899444580078 5500
val loss = 2.289862871170044
training loss = 2.060195207595825 5600
val loss = 2.2822937965393066
training loss = 2.055636405944824 5700
val loss = 2.275810956954956
training loss = 2.051199197769165 5800
val loss = 2.268601894378662
training loss = 2.047961473464966 5900
val loss = 2.264643669128418
training loss = 2.0425264835357666 6000
val loss = 2.2551565170288086
training loss = 2.0382156372070312 6100
val loss = 2.2482922077178955
training loss = 2.0341508388519287 6200
val loss = 2.2416324615478516
training loss = 2.0300610065460205 6300
val loss = 2.2353694438934326
training loss = 2.058087110519409 6400
val loss = 2.2500436305999756
training loss = 2.0219287872314453 6500
val loss = 2.2225096225738525
training loss = 2.0179121494293213 6600
val loss = 2.2158334255218506
training loss = 2.013918876647949 6700
val loss = 2.209366798400879
training loss = 2.010067939758301 6800
val loss = 2.203282594680786
training loss = 2.043628454208374 6900
val loss = 2.2310595512390137
training loss = 2.0023951530456543 7000
val loss = 2.190843105316162
training loss = 1.9986528158187866 7100
val loss = 2.1846327781677246
training loss = 1.9952484369277954 7200
val loss = 2.178464412689209
training loss = 1.991276502609253 7300
val loss = 2.172311305999756
training loss = 1.9876320362091064 7400
val loss = 2.1660757064819336
training loss = 1.9841296672821045 7500
val loss = 2.1599907875061035
training loss = 1.9806774854660034 7600
val loss = 2.153870105743408
training loss = 1.9972361326217651 7700
val loss = 2.1746668815612793
training loss = 1.9739822149276733 7800
val loss = 2.1415512561798096
training loss = 1.9707739353179932 7900
val loss = 2.135551691055298
training loss = 1.9677716493606567 8000
val loss = 2.1283328533172607
training loss = 1.9647396802902222 8100
val loss = 2.1232285499572754
training loss = 1.9626660346984863 8200
val loss = 2.120009422302246
training loss = 1.959306240081787 8300
val loss = 2.111475944519043
training loss = 1.9567928314208984 8400
val loss = 2.1060750484466553
training loss = 1.9545278549194336 8500
val loss = 2.0989720821380615
training loss = 1.95229172706604 8600
val loss = 2.095149517059326
training loss = 1.9503426551818848 8700
val loss = 2.0883278846740723
training loss = 1.9484730958938599 8800
val loss = 2.0853676795959473
training loss = 1.9857378005981445 8900
val loss = 2.0858638286590576
training loss = 1.9453296661376953 9000
val loss = 2.0768823623657227
training loss = 1.9440104961395264 9100
val loss = 2.0728867053985596
training loss = 1.9428943395614624 9200
val loss = 2.0677080154418945
training loss = 1.9417974948883057 9300
val loss = 2.0665953159332275
training loss = 2.014508008956909 9400
val loss = 2.176978826522827
training loss = 1.9400745630264282 9500
val loss = 2.061568260192871
training loss = 1.9393831491470337 9600
val loss = 2.0585825443267822
training loss = 1.9387476444244385 9700
val loss = 2.055849313735962
training loss = 1.9382283687591553 9800
val loss = 2.0550005435943604
training loss = 1.9817615747451782 9900
val loss = 2.1340742111206055
training loss = 1.937345027923584 10000
val loss = 2.0518908500671387
training loss = 1.9370197057724 10100
val loss = 2.0506701469421387
training loss = 1.9371503591537476 10200
val loss = 2.054180383682251
training loss = 1.9364591836929321 10300
val loss = 2.0487308502197266
training loss = 1.9362467527389526 10400
val loss = 2.047687292098999
training loss = 1.938114881515503 10500
val loss = 2.038224935531616
training loss = 1.93586266040802 10600
val loss = 2.0459048748016357
training loss = 1.935734510421753 10700
val loss = 2.045552968978882
training loss = 1.9358317852020264 10800
val loss = 2.0413358211517334
training loss = 1.9354760646820068 10900
val loss = 2.0443928241729736
training loss = 2.025015115737915 11000
val loss = 2.0613698959350586
training loss = 1.9352763891220093 11100
val loss = 2.043536424636841
training loss = 1.935212254524231 11200
val loss = 2.0433149337768555
training loss = 1.9351998567581177 11300
val loss = 2.044370412826538
training loss = 1.9350559711456299 11400
val loss = 2.042234420776367
training loss = 2.0093791484832764 11500
val loss = 2.1668832302093506
training loss = 1.9349263906478882 11600
val loss = 2.0410637855529785
training loss = 1.9348852634429932 11700
val loss = 2.0411880016326904
training loss = 1.9353032112121582 11800
val loss = 2.0456137657165527
training loss = 1.9347774982452393 11900
val loss = 2.0405144691467285
training loss = 1.9719232320785522 12000
val loss = 2.0318164825439453
training loss = 1.9346704483032227 12100
val loss = 2.039320707321167
training loss = 1.9346367120742798 12200
val loss = 2.0394864082336426
training loss = 1.9379916191101074 12300
val loss = 2.0290892124176025
training loss = 1.9345446825027466 12400
val loss = 2.0387375354766846
training loss = 1.952499270439148 12500
val loss = 2.0835347175598145
training loss = 1.9344524145126343 12600
val loss = 2.037890672683716
training loss = 1.9348130226135254 12700
val loss = 2.0338950157165527
training loss = 1.9343900680541992 12800
val loss = 2.0361363887786865
training loss = 1.9343266487121582 12900
val loss = 2.03702449798584
training loss = 1.9376657009124756 13000
val loss = 2.0518834590911865
training loss = 1.934232234954834 13100
val loss = 2.0362462997436523
training loss = 1.9342041015625 13200
val loss = 2.0355167388916016
training loss = 1.9341577291488647 13300
val loss = 2.0346639156341553
training loss = 1.9341003894805908 13400
val loss = 2.035317897796631
training loss = 1.9889317750930786 13500
val loss = 2.0342679023742676
training loss = 1.9340088367462158 13600
val loss = 2.0342836380004883
training loss = 1.933964490890503 13700
val loss = 2.034329414367676
training loss = 1.933977484703064 13800
val loss = 2.0323686599731445
training loss = 1.9338617324829102 13900
val loss = 2.033663272857666
training loss = 1.9338899850845337 14000
val loss = 2.0316548347473145
training loss = 1.9338033199310303 14100
val loss = 2.03469181060791
training loss = 1.9337103366851807 14200
val loss = 2.0327775478363037
training loss = 1.9383140802383423 14300
val loss = 2.051530361175537
training loss = 1.9335970878601074 14400
val loss = 2.0322494506835938
training loss = 1.9335476160049438 14500
val loss = 2.0321247577667236
training loss = 1.933632493019104 14600
val loss = 2.0292468070983887
training loss = 1.933424949645996 14700
val loss = 2.031259298324585
training loss = 1.9344072341918945 14800
val loss = 2.039219617843628
training loss = 1.933295488357544 14900
val loss = 2.030789613723755
training loss = 2.003286123275757 15000
val loss = 2.151583671569824
training loss = 1.9331551790237427 15100
val loss = 2.0305163860321045
training loss = 1.9330857992172241 15200
val loss = 2.029883861541748
training loss = 1.9338666200637817 15300
val loss = 2.037184476852417
training loss = 1.9329280853271484 15400
val loss = 2.029341697692871
training loss = 1.9328488111495972 15500
val loss = 2.0290513038635254
training loss = 1.9327967166900635 15600
val loss = 2.0277111530303955
training loss = 1.9326717853546143 15700
val loss = 2.028554916381836
training loss = 1.9325803518295288 15800
val loss = 2.0279452800750732
training loss = 1.9324768781661987 15900
val loss = 2.0276570320129395
training loss = 1.9323753118515015 16000
val loss = 2.027733087539673
training loss = 1.9324955940246582 16100
val loss = 2.031049966812134
training loss = 1.9321458339691162 16200
val loss = 2.027663469314575
training loss = 1.9320255517959595 16300
val loss = 2.026901960372925
training loss = 1.9318890571594238 16400
val loss = 2.0273592472076416
training loss = 1.9317574501037598 16500
val loss = 2.0263638496398926
training loss = 1.9329643249511719 16600
val loss = 2.019134044647217
training loss = 1.931478500366211 16700
val loss = 2.024765729904175
training loss = 1.931289553642273 16800
val loss = 2.0254409313201904
training loss = 1.9338738918304443 16900
val loss = 2.039576530456543
training loss = 1.9309273958206177 17000
val loss = 2.024954080581665
training loss = 1.930722713470459 17100
val loss = 2.0247223377227783
training loss = 1.930625081062317 17200
val loss = 2.0270800590515137
training loss = 1.9302726984024048 17300
val loss = 2.023895740509033
training loss = 1.9432162046432495 17400
val loss = 2.0094971656799316
training loss = 1.9297378063201904 17500
val loss = 2.0230419635772705
training loss = 2.0009562969207764 17600
val loss = 2.0258092880249023
training loss = 1.9291096925735474 17700
val loss = 2.023247718811035
training loss = 1.9287447929382324 17800
val loss = 2.0220868587493896
training loss = 1.9352848529815674 17900
val loss = 2.0473618507385254
training loss = 1.9278947114944458 18000
val loss = 2.0213332176208496
training loss = 1.9961847066879272 18100
val loss = 2.1433842182159424
training loss = 1.9268536567687988 18200
val loss = 2.021415948867798
training loss = 1.9262244701385498 18300
val loss = 2.019972324371338
training loss = 1.9281437397003174 18400
val loss = 2.0096757411956787
training loss = 1.9246881008148193 18500
val loss = 2.019671678543091
training loss = 1.9237195253372192 18600
val loss = 2.0185441970825195
training loss = 1.9233806133270264 18700
val loss = 2.025852918624878
training loss = 1.9213308095932007 18800
val loss = 2.0179834365844727
training loss = 1.93653404712677 18900
val loss = 2.000331401824951
training loss = 1.9179950952529907 19000
val loss = 2.0175933837890625
training loss = 1.9158358573913574 19100
val loss = 2.0188446044921875
training loss = 1.9135380983352661 19200
val loss = 2.0184547901153564
training loss = 1.9108635187149048 19300
val loss = 2.022552490234375
training loss = 1.907931923866272 19400
val loss = 2.0279481410980225
training loss = 1.9048659801483154 19500
val loss = 2.030343532562256
training loss = 1.9016081094741821 19600
val loss = 2.033869504928589
training loss = 1.9042683839797974 19700
val loss = 2.021289825439453
training loss = 1.8947538137435913 19800
val loss = 2.0425684452056885
training loss = 1.8912036418914795 19900
val loss = 2.042811870574951
training loss = 1.8871781826019287 20000
val loss = 2.051103115081787
training loss = 1.883073329925537 20100
val loss = 2.0566463470458984
training loss = 1.8791931867599487 20200
val loss = 2.0677285194396973
training loss = 1.8745485544204712 20300
val loss = 2.067681074142456
training loss = 1.993167757987976 20400
val loss = 2.0675811767578125
training loss = 1.8652608394622803 20500
val loss = 2.080824375152588
training loss = 1.860277533531189 20600
val loss = 2.0894854068756104
training loss = 1.8556538820266724 20700
val loss = 2.0916121006011963
training loss = 1.850250244140625 20800
val loss = 2.106937885284424
training loss = 1.8455005884170532 20900
val loss = 2.124382734298706
training loss = 1.839897871017456 21000
val loss = 2.1271350383758545
training loss = 1.9347034692764282 21100
val loss = 2.1038970947265625
training loss = 1.8294570446014404 21200
val loss = 2.1485471725463867
training loss = 1.82411527633667 21300
val loss = 2.162984609603882
training loss = 1.8194515705108643 21400
val loss = 2.1824166774749756
training loss = 1.8141332864761353 21500
val loss = 2.1879870891571045
training loss = 1.8771007061004639 21600
val loss = 2.3805439472198486
training loss = 1.8046385049819946 21700
val loss = 2.2135348320007324
training loss = 1.7999404668807983 21800
val loss = 2.227461099624634
training loss = 1.7960474491119385 21900
val loss = 2.2454185485839844
training loss = 1.7917609214782715 22000
val loss = 2.2538299560546875
training loss = 1.7945958375930786 22100
val loss = 2.310123920440674
training loss = 1.7845959663391113 22200
val loss = 2.2783970832824707
training loss = 1.7811967134475708 22300
val loss = 2.2875242233276367
training loss = 1.7784616947174072 22400
val loss = 2.30277943611145
training loss = 1.7756067514419556 22500
val loss = 2.3133745193481445
training loss = 1.7800116539001465 22600
val loss = 2.3719112873077393
training loss = 1.7710412740707397 22700
val loss = 2.3334174156188965
training loss = 1.7715867757797241 22800
val loss = 2.3750686645507812
training loss = 1.767399549484253 22900
val loss = 2.353170871734619
training loss = 1.76570725440979 23000
val loss = 2.3605189323425293
training loss = 1.766625165939331 23100
val loss = 2.3934645652770996
training loss = 1.763201117515564 23200
val loss = 2.3752028942108154
training loss = 1.807726502418518 23300
val loss = 2.5494120121002197
training loss = 1.7613060474395752 23400
val loss = 2.3864777088165283
training loss = 1.7604472637176514 23500
val loss = 2.394630193710327
training loss = 1.7601306438446045 23600
val loss = 2.402132272720337
training loss = 1.7594047784805298 23700
val loss = 2.402991533279419
training loss = 1.7588868141174316 23800
val loss = 2.4087042808532715
training loss = 1.7591497898101807 23900
val loss = 2.4227237701416016
reduced chi^2 level 2 = 1.7591497898101807
Constrained alpha: 1.7927403450012207
Constrained beta: 4.082422256469727
Constrained gamma: 12.935857772827148
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 862.7782,  859.9592,  914.7548,  982.1820, 1015.7998, 1032.7885,
        1133.3491, 1189.9701, 1223.2704, 1169.1450, 1238.6823, 1169.2833,
        1201.0797, 1307.4592, 1305.4502, 1413.2003, 1434.9037, 1466.3240,
        1567.8507, 1531.0703, 1598.6509, 1527.4840, 1565.3953, 1604.7717,
        1606.2958, 1728.8368, 1585.6245, 1704.3773, 1663.0758, 1770.8142,
        1690.2585, 1623.3086, 1700.6213, 1779.4187, 1686.1636, 1714.8929,
        1640.9137, 1653.7069, 1672.1738, 1593.3969, 1573.7988, 1609.2043,
        1433.7675, 1537.5175, 1352.9403, 1360.1700, 1293.0240, 1221.2852,
        1163.7281, 1132.6127, 1094.0404,  990.1056,  919.1332,  907.7307,
         875.7707,  908.5312,  824.4061,  674.8600,  622.4441,  528.3727,
         523.4935,  496.6205,  487.4946,  428.0225,  388.6835,  340.8682,
         332.2009,  239.0638,  216.4355,  158.7202,  165.2755,  146.9652,
         139.9427,  110.8981,   84.5587,   65.4868,   48.7611,   45.3198,
          31.6904,   42.6600,   19.8173,   37.4453,   23.7861])]
2719.179880508937
0.2715190964097275 2.061856488573728 46.43835914943201
val isze = 8
idinces = [ 5 41 66 54 67 36 80 46 71 13 68 25 77 11 82 12 50 39 64 55 30  6 65 26
 16 78 27 45 44 57 17  9 56 29 18 51 47 69  7 73 22 52 38 42 74  3 61 60
 48 81 76 53  1 70  2 10 28 24 32 62 58 79  8 23 59 43 49 75 34 19  4  0
 63 20 33 15 31 35 37 72 40 21 14]
we are doing training validation split
training loss = 22.66441535949707 100
val loss = 28.88249397277832
training loss = 13.668452262878418 200
val loss = 16.666263580322266
training loss = 8.908299446105957 300
val loss = 10.187236785888672
training loss = 6.408926963806152 400
val loss = 6.58267879486084
training loss = 5.288844585418701 500
val loss = 4.887081623077393
training loss = 4.7690110206604 600
val loss = 4.171684741973877
training loss = 4.362308025360107 700
val loss = 3.7914865016937256
training loss = 3.600187301635742 800
val loss = 3.363710880279541
training loss = 3.005235195159912 900
val loss = 3.049384117126465
training loss = 2.7475125789642334 1000
val loss = 2.8420605659484863
training loss = 2.588057279586792 1100
val loss = 2.5429205894470215
training loss = 2.454685926437378 1200
val loss = 2.358367919921875
training loss = 2.392094850540161 1300
val loss = 2.32331919670105
training loss = 2.2832374572753906 1400
val loss = 2.193432331085205
training loss = 2.23262357711792 1500
val loss = 2.1509294509887695
training loss = 2.194355010986328 1600
val loss = 2.1127357482910156
training loss = 2.165851354598999 1700
val loss = 2.0864343643188477
training loss = 2.1371772289276123 1800
val loss = 2.0428600311279297
training loss = 2.163492202758789 1900
val loss = 2.01507306098938
training loss = 2.0926737785339355 2000
val loss = 1.9741684198379517
training loss = 2.156832456588745 2100
val loss = 2.061403751373291
training loss = 2.0578479766845703 2200
val loss = 1.9189398288726807
training loss = 2.0436885356903076 2300
val loss = 1.8972474336624146
training loss = 2.0314691066741943 2400
val loss = 1.8830244541168213
training loss = 2.020822286605835 2500
val loss = 1.8698461055755615
training loss = 2.0118086338043213 2600
val loss = 1.8592143058776855
training loss = 2.0034611225128174 2700
val loss = 1.85735285282135
training loss = 1.9963017702102661 2800
val loss = 1.8569340705871582
training loss = 1.9894869327545166 2900
val loss = 1.8562216758728027
training loss = 1.982981562614441 3000
val loss = 1.8618569374084473
training loss = 1.97650945186615 3100
val loss = 1.8655236959457397
training loss = 1.9696913957595825 3200
val loss = 1.8731920719146729
training loss = 1.9996415376663208 3300
val loss = 1.862328290939331
training loss = 1.9568973779678345 3400
val loss = 1.8728697299957275
training loss = 1.9517009258270264 3500
val loss = 1.8744028806686401
training loss = 1.9457528591156006 3600
val loss = 1.8629348278045654
training loss = 1.9408608675003052 3700
val loss = 1.858494520187378
training loss = 1.9382658004760742 3800
val loss = 1.8451746702194214
training loss = 1.9323761463165283 3900
val loss = 1.847982406616211
training loss = 2.1954314708709717 4000
val loss = 1.9471100568771362
training loss = 1.9253216981887817 4100
val loss = 1.8386189937591553
training loss = 1.9222420454025269 4200
val loss = 1.8333048820495605
training loss = 1.9202344417572021 4300
val loss = 1.8356215953826904
training loss = 1.9169269800186157 4400
val loss = 1.824809193611145
training loss = 1.9145970344543457 4500
val loss = 1.8204739093780518
training loss = 1.9129061698913574 4600
val loss = 1.812165379524231
training loss = 1.9106035232543945 4700
val loss = 1.8131744861602783
training loss = 1.9088729619979858 4800
val loss = 1.8099428415298462
training loss = 1.9073230028152466 4900
val loss = 1.8073161840438843
training loss = 1.905907392501831 5000
val loss = 1.8032355308532715
training loss = 1.9051225185394287 5100
val loss = 1.7966231107711792
training loss = 1.903507947921753 5200
val loss = 1.7973965406417847
training loss = 1.90249764919281 5300
val loss = 1.7948945760726929
training loss = 1.9016613960266113 5400
val loss = 1.7947686910629272
training loss = 1.900797963142395 5500
val loss = 1.7905158996582031
training loss = 1.9000911712646484 5600
val loss = 1.788386344909668
training loss = 1.8998185396194458 5700
val loss = 1.79130220413208
training loss = 1.8989026546478271 5800
val loss = 1.784515380859375
training loss = 2.2163314819335938 5900
val loss = 2.1675033569335938
training loss = 1.8979811668395996 6000
val loss = 1.7807986736297607
training loss = 1.897627353668213 6100
val loss = 1.779994249343872
training loss = 1.8973665237426758 6200
val loss = 1.7755322456359863
training loss = 1.8970091342926025 6300
val loss = 1.7761805057525635
training loss = 1.8974969387054443 6400
val loss = 1.7690826654434204
training loss = 1.8965576887130737 6500
val loss = 1.7732189893722534
training loss = 2.0107736587524414 6600
val loss = 1.9427800178527832
training loss = 1.8962301015853882 6700
val loss = 1.7710951566696167
training loss = 1.8961068391799927 6800
val loss = 1.7692850828170776
training loss = 1.9008333683013916 6900
val loss = 1.7553493976593018
training loss = 1.8959027528762817 7000
val loss = 1.7669758796691895
training loss = 1.8959951400756836 7100
val loss = 1.7623802423477173
training loss = 1.89576256275177 7200
val loss = 1.7652363777160645
training loss = 1.8956854343414307 7300
val loss = 1.7632004022598267
training loss = 1.9004855155944824 7400
val loss = 1.7841119766235352
training loss = 1.8955599069595337 7500
val loss = 1.7610089778900146
training loss = 1.8956643342971802 7600
val loss = 1.763249397277832
training loss = 1.895426869392395 7700
val loss = 1.7583329677581787
training loss = 1.8953399658203125 7800
val loss = 1.7578903436660767
training loss = 2.273097038269043 7900
val loss = 1.9145393371582031
training loss = 1.8951483964920044 8000
val loss = 1.7570136785507202
training loss = 1.8950010538101196 8100
val loss = 1.7550249099731445
training loss = 1.9027729034423828 8200
val loss = 1.7832658290863037
training loss = 1.8946808576583862 8300
val loss = 1.753329873085022
training loss = 1.9216642379760742 8400
val loss = 1.817536473274231
training loss = 1.894277811050415 8500
val loss = 1.752624750137329
training loss = 1.8939974308013916 8600
val loss = 1.7505550384521484
training loss = 1.8939545154571533 8700
val loss = 1.7541835308074951
training loss = 1.893433928489685 8800
val loss = 1.7491836547851562
training loss = 1.960873007774353 8900
val loss = 1.8700579404830933
training loss = 1.8927819728851318 9000
val loss = 1.7482240200042725
training loss = 1.8923983573913574 9100
val loss = 1.7468112707138062
training loss = 1.91193687915802 9200
val loss = 1.8000893592834473
training loss = 1.8916242122650146 9300
val loss = 1.7450153827667236
training loss = 1.8911943435668945 9400
val loss = 1.744520664215088
training loss = 1.890876054763794 9500
val loss = 1.7461025714874268
training loss = 1.8903565406799316 9600
val loss = 1.7429230213165283
training loss = 2.2525548934936523 9700
val loss = 1.8806113004684448
training loss = 1.8895374536514282 9800
val loss = 1.742002010345459
training loss = 1.8891280889511108 9900
val loss = 1.7408223152160645
training loss = 1.9052234888076782 10000
val loss = 1.7904584407806396
training loss = 1.8884236812591553 10100
val loss = 1.7398531436920166
training loss = 1.8880865573883057 10200
val loss = 1.7389168739318848
training loss = 1.8929848670959473 10300
val loss = 1.7216066122055054
training loss = 1.8875149488449097 10400
val loss = 1.7375006675720215
training loss = 1.8872371912002563 10500
val loss = 1.7371032238006592
training loss = 1.891666054725647 10600
val loss = 1.759385585784912
training loss = 1.8868449926376343 10700
val loss = 1.7362006902694702
training loss = 1.8866976499557495 10800
val loss = 1.7343969345092773
training loss = 1.8866868019104004 10900
val loss = 1.7385520935058594
training loss = 1.8864308595657349 11000
val loss = 1.7349433898925781
training loss = 1.89356529712677 11100
val loss = 1.7648916244506836
training loss = 1.8862347602844238 11200
val loss = 1.7342547178268433
training loss = 1.8861428499221802 11300
val loss = 1.7340407371520996
training loss = 1.8862724304199219 11400
val loss = 1.7300559282302856
training loss = 1.8860046863555908 11500
val loss = 1.7332870960235596
training loss = 1.8883965015411377 11600
val loss = 1.7208502292633057
training loss = 1.8859002590179443 11700
val loss = 1.731971025466919
training loss = 1.8858333826065063 11800
val loss = 1.7323448657989502
training loss = 1.887136697769165 11900
val loss = 1.7434654235839844
training loss = 1.8857524394989014 12000
val loss = 1.7317893505096436
training loss = 1.8857043981552124 12100
val loss = 1.731541633605957
training loss = 1.8863331079483032 12200
val loss = 1.7391142845153809
training loss = 1.8856360912322998 12300
val loss = 1.7312469482421875
training loss = 1.9214810132980347 12400
val loss = 1.814730167388916
training loss = 1.8855750560760498 12500
val loss = 1.7310175895690918
training loss = 1.8855375051498413 12600
val loss = 1.730168104171753
training loss = 1.885733723640442 12700
val loss = 1.7265123128890991
training loss = 1.8854804039001465 12800
val loss = 1.7302430868148804
training loss = 1.9299184083938599 12900
val loss = 1.8281067609786987
training loss = 1.8854234218597412 13000
val loss = 1.7299476861953735
training loss = 1.885390281677246 13100
val loss = 1.729292869567871
training loss = 1.8855353593826294 13200
val loss = 1.7261254787445068
training loss = 1.885331630706787 13300
val loss = 1.729529857635498
training loss = 1.899214267730713 13400
val loss = 1.7083501815795898
training loss = 1.8852742910385132 13500
val loss = 1.7292790412902832
training loss = 1.9134639501571655 13600
val loss = 1.7044217586517334
training loss = 1.885217547416687 13700
val loss = 1.72909677028656
training loss = 1.8851807117462158 13800
val loss = 1.7290278673171997
training loss = 1.8852152824401855 13900
val loss = 1.7311878204345703
training loss = 1.8851208686828613 14000
val loss = 1.7288455963134766
training loss = 1.9701217412948608 14100
val loss = 1.8806675672531128
training loss = 1.8850667476654053 14200
val loss = 1.7292237281799316
training loss = 1.8850232362747192 14300
val loss = 1.7285685539245605
training loss = 1.8853965997695923 14400
val loss = 1.7347264289855957
training loss = 1.8849613666534424 14500
val loss = 1.7284225225448608
training loss = 1.8849949836730957 14600
val loss = 1.7258214950561523
training loss = 1.884962558746338 14700
val loss = 1.7259716987609863
training loss = 1.8848539590835571 14800
val loss = 1.7283072471618652
training loss = 1.9045403003692627 14900
val loss = 1.7031745910644531
training loss = 1.8847856521606445 15000
val loss = 1.7283798456192017
training loss = 1.8847382068634033 15100
val loss = 1.7281038761138916
training loss = 1.8854343891143799 15200
val loss = 1.7208635807037354
training loss = 1.8846622705459595 15300
val loss = 1.7282397747039795
training loss = 1.9411646127700806 15400
val loss = 1.8425884246826172
training loss = 1.8845833539962769 15500
val loss = 1.7284495830535889
training loss = 1.88453209400177 15600
val loss = 1.7279484272003174
training loss = 1.884735107421875 15700
val loss = 1.7239264249801636
training loss = 1.8844465017318726 15800
val loss = 1.7281564474105835
training loss = 1.9164831638336182 15900
val loss = 1.8078817129135132
training loss = 1.884384036064148 16000
val loss = 1.7296448945999146
training loss = 1.8843008279800415 16100
val loss = 1.728135108947754
training loss = 1.888664960861206 16200
val loss = 1.7123422622680664
training loss = 1.8842039108276367 16300
val loss = 1.7281477451324463
training loss = 1.9717907905578613 16400
val loss = 1.7133924961090088
training loss = 1.8841065168380737 16500
val loss = 1.7280962467193604
training loss = 1.8840421438217163 16600
val loss = 1.7283695936203003
training loss = 1.8845856189727783 16700
val loss = 1.7359213829040527
training loss = 1.8839346170425415 16800
val loss = 1.7283921241760254
training loss = 2.048470973968506 16900
val loss = 1.987967610359192
training loss = 1.883830189704895 17000
val loss = 1.7280011177062988
training loss = 1.8837556838989258 17100
val loss = 1.728588342666626
training loss = 1.8841801881790161 17200
val loss = 1.7356611490249634
training loss = 1.8836393356323242 17300
val loss = 1.728834629058838
training loss = 2.2212796211242676 17400
val loss = 2.187406539916992
training loss = 1.8835195302963257 17500
val loss = 1.728327751159668
training loss = 1.8834383487701416 17600
val loss = 1.7289462089538574
training loss = 1.8836591243743896 17700
val loss = 1.724306344985962
training loss = 1.883315086364746 17800
val loss = 1.7290873527526855
training loss = 1.8832333087921143 17900
val loss = 1.7292827367782593
training loss = 1.8835880756378174 18000
val loss = 1.7359223365783691
training loss = 1.883098840713501 18100
val loss = 1.7293598651885986
training loss = 1.9011446237564087 18200
val loss = 1.7003682851791382
training loss = 1.8829630613327026 18300
val loss = 1.729074239730835
training loss = 1.8828719854354858 18400
val loss = 1.7296696901321411
training loss = 1.882853388786316 18500
val loss = 1.731509804725647
training loss = 1.882725477218628 18600
val loss = 1.7300548553466797
training loss = 2.1969618797302246 18700
val loss = 1.8240786790847778
training loss = 1.8825795650482178 18800
val loss = 1.7293307781219482
training loss = 1.8824760913848877 18900
val loss = 1.7302402257919312
training loss = 1.8824292421340942 19000
val loss = 1.7293452024459839
training loss = 1.8823198080062866 19100
val loss = 1.730520486831665
training loss = 1.8876794576644897 19200
val loss = 1.7123992443084717
training loss = 1.8821594715118408 19300
val loss = 1.7306129932403564
training loss = 1.8820549249649048 19400
val loss = 1.7310073375701904
training loss = 1.890828251838684 19500
val loss = 1.7685720920562744
training loss = 1.881881594657898 19600
val loss = 1.7311793565750122
training loss = 2.205685615539551 19700
val loss = 2.1793434619903564
training loss = 1.8817098140716553 19800
val loss = 1.7310614585876465
training loss = 1.8815951347351074 19900
val loss = 1.7318148612976074
training loss = 1.8821732997894287 20000
val loss = 1.7404141426086426
training loss = 1.881410002708435 20100
val loss = 1.7319717407226562
training loss = 1.8824398517608643 20200
val loss = 1.7425720691680908
training loss = 1.8812220096588135 20300
val loss = 1.7329151630401611
training loss = 1.8881181478500366 20400
val loss = 1.7648167610168457
training loss = 1.881026268005371 20500
val loss = 1.7331345081329346
training loss = 1.8809027671813965 20600
val loss = 1.7329020500183105
training loss = 1.894792079925537 20700
val loss = 1.782766580581665
training loss = 1.8807231187820435 20800
val loss = 1.7333755493164062
training loss = 1.880599856376648 20900
val loss = 1.7335096597671509
training loss = 2.2571866512298584 21000
val loss = 1.8591896295547485
training loss = 1.880401611328125 21100
val loss = 1.7340567111968994
training loss = 1.8802708387374878 21200
val loss = 1.7340657711029053
training loss = 1.8807919025421143 21300
val loss = 1.7279746532440186
training loss = 1.88006591796875 21400
val loss = 1.7343289852142334
training loss = 1.8799340724945068 21500
val loss = 1.7350918054580688
training loss = 1.8800804615020752 21600
val loss = 1.739881992340088
training loss = 1.8797160387039185 21700
val loss = 1.735130786895752
training loss = 1.881528377532959 21800
val loss = 1.7243903875350952
training loss = 1.879502773284912 21900
val loss = 1.7352797985076904
training loss = 1.8793604373931885 22000
val loss = 1.7359156608581543
training loss = 1.8795746564865112 22100
val loss = 1.7305994033813477
training loss = 1.8791356086730957 22200
val loss = 1.7361373901367188
training loss = 1.8832920789718628 22300
val loss = 1.7623660564422607
training loss = 1.8789067268371582 22400
val loss = 1.7367010116577148
training loss = 1.8825219869613647 22500
val loss = 1.719927430152893
training loss = 1.8786861896514893 22600
val loss = 1.7358639240264893
training loss = 1.8785254955291748 22700
val loss = 1.7372742891311646
training loss = 1.8792928457260132 22800
val loss = 1.7476916313171387
training loss = 1.8782941102981567 22900
val loss = 1.7376251220703125
reduced chi^2 level 2 = 1.878216028213501
Constrained alpha: 1.9361211061477661
Constrained beta: 2.31866455078125
Constrained gamma: 13.431536674499512
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 869.4916,  899.8196,  929.2844,  974.0912, 1020.9342, 1023.6408,
        1116.5481, 1159.5216, 1194.0784, 1224.6656, 1161.5555, 1201.7651,
        1231.4043, 1273.1167, 1289.2347, 1465.9922, 1407.2805, 1414.3962,
        1552.4412, 1492.4838, 1583.8671, 1463.9224, 1600.6116, 1611.9260,
        1731.1873, 1710.6798, 1594.0100, 1720.2501, 1674.2662, 1587.3909,
        1670.7825, 1735.0972, 1621.0919, 1692.3477, 1716.8402, 1706.0145,
        1662.8822, 1544.7476, 1607.5315, 1595.1881, 1599.1516, 1555.6127,
        1453.7434, 1501.7520, 1378.9031, 1320.6842, 1258.8800, 1250.4009,
        1093.7058, 1181.6521, 1034.1267, 1049.1724,  962.0403,  939.0164,
         882.9554,  871.8188,  870.3398,  691.4484,  619.0562,  513.5792,
         513.8789,  472.5717,  447.5088,  411.2288,  375.2350,  337.5382,
         287.0154,  253.0781,  222.9560,  188.3447,  154.5862,  156.6361,
         151.4667,   98.1347,   90.4799,   58.7853,   46.9542,   42.4049,
          36.7079,   34.3980,   29.4756,   40.0499,   34.8376])]
2754.1566813420013
3.049564349570258 15.783755705872322 75.85323435031466
val isze = 8
idinces = [41 37 34 38 70 48  3  2 15 49 26 58 55 61 35 44 45 69 16 78 51  4 28 56
 52  8 29 19 65 76 25 43 54 30 71  6  7 46 66 64 47 36 67 17  5 72 50 23
 62 10 12 42 57 73 31 11 60 18 14  9 63 53  0 27 32 33 21 24  1 77 79 82
 74 75 59 39 81 20 13 68 22 80 40]
we are doing training validation split
training loss = 303.3096923828125 100
val loss = 461.1264953613281
training loss = 10.340766906738281 200
val loss = 7.377877235412598
training loss = 7.532614707946777 300
val loss = 5.872916221618652
training loss = 7.261054992675781 400
val loss = 5.637911796569824
training loss = 6.994685173034668 500
val loss = 5.437198638916016
training loss = 6.749025821685791 600
val loss = 5.290371894836426
training loss = 6.534375190734863 700
val loss = 5.205850601196289
training loss = 6.355504989624023 800
val loss = 5.181889533996582
training loss = 6.212032318115234 900
val loss = 5.207685947418213
training loss = 6.099687099456787 1000
val loss = 5.267122268676758
training loss = 6.012004852294922 1100
val loss = 5.342563152313232
training loss = 5.9421162605285645 1200
val loss = 5.418558120727539
training loss = 5.883996486663818 1300
val loss = 5.484255313873291
training loss = 5.8330769538879395 1400
val loss = 5.533985137939453
training loss = 5.786270618438721 1500
val loss = 5.566137790679932
training loss = 5.741697788238525 1600
val loss = 5.5820536613464355
training loss = 5.698322296142578 1700
val loss = 5.584397315979004
training loss = 5.65561056137085 1800
val loss = 5.576328277587891
training loss = 5.613308429718018 1900
val loss = 5.5607829093933105
training loss = 5.571282386779785 2000
val loss = 5.540087699890137
training loss = 5.529457092285156 2100
val loss = 5.516087055206299
training loss = 5.487778663635254 2200
val loss = 5.490076065063477
training loss = 5.446197986602783 2300
val loss = 5.462884426116943
training loss = 5.404670238494873 2400
val loss = 5.435094833374023
training loss = 5.363134860992432 2500
val loss = 5.407022476196289
training loss = 5.321495056152344 2600
val loss = 5.378967761993408
training loss = 5.279542446136475 2700
val loss = 5.351062774658203
training loss = 5.236764430999756 2800
val loss = 5.3232927322387695
training loss = 5.191812515258789 2900
val loss = 5.295258522033691
training loss = 5.140970706939697 3000
val loss = 5.266119956970215
training loss = 5.072508335113525 3100
val loss = 5.234424591064453
training loss = 4.942653179168701 3200
val loss = 5.192675590515137
training loss = 4.634716033935547 3300
val loss = 5.089749336242676
training loss = 4.003578186035156 3400
val loss = 4.634832382202148
training loss = 3.1565487384796143 3500
val loss = 3.434927225112915
training loss = 1.9554883241653442 3600
val loss = 3.24524188041687
training loss = 1.9263802766799927 3700
val loss = 3.2487738132476807
training loss = 1.921958088874817 3800
val loss = 3.2422916889190674
training loss = 1.9189656972885132 3900
val loss = 3.2354702949523926
training loss = 1.9163731336593628 4000
val loss = 3.229093074798584
training loss = 1.9140797853469849 4100
val loss = 3.2188560962677
training loss = 1.9119046926498413 4200
val loss = 3.2191569805145264
training loss = 1.9116731882095337 4300
val loss = 3.131960868835449
training loss = 1.9077075719833374 4400
val loss = 3.2099056243896484
training loss = 1.9071033000946045 4500
val loss = 3.286315441131592
training loss = 1.9033887386322021 4600
val loss = 3.201478958129883
training loss = 1.9034523963928223 4700
val loss = 3.105372905731201
training loss = 1.8986124992370605 4800
val loss = 3.190178871154785
training loss = 1.9612370729446411 4900
val loss = 2.7663791179656982
training loss = 1.8930994272232056 5000
val loss = 3.178234100341797
training loss = 1.9861397743225098 5100
val loss = 2.684134006500244
training loss = 1.8866827487945557 5200
val loss = 3.159510612487793
training loss = 2.032318592071533 5300
val loss = 4.104821681976318
training loss = 1.8793339729309082 5400
val loss = 3.1361398696899414
training loss = 1.8809151649475098 5500
val loss = 3.2837579250335693
training loss = 1.8711748123168945 5600
val loss = 3.1099445819854736
training loss = 1.8668233156204224 5700
val loss = 3.084277868270874
training loss = 1.8626457452774048 5800
val loss = 3.0971591472625732
training loss = 1.8581525087356567 5900
val loss = 3.060483932495117
training loss = 1.8542509078979492 6000
val loss = 3.008091926574707
training loss = 1.8497636318206787 6100
val loss = 3.019646167755127
training loss = 2.045708179473877 6200
val loss = 2.3939337730407715
training loss = 1.8421270847320557 6300
val loss = 2.9789390563964844
training loss = 1.8385494947433472 6400
val loss = 2.9570319652557373
training loss = 1.846710205078125 6500
val loss = 3.152087688446045
training loss = 1.8324947357177734 6600
val loss = 2.916137456893921
training loss = 1.8297473192214966 6700
val loss = 2.898482322692871
training loss = 1.8279236555099487 6800
val loss = 2.8456318378448486
training loss = 1.8253809213638306 6900
val loss = 2.865513563156128
training loss = 2.1921849250793457 7000
val loss = 2.183631658554077
training loss = 1.8219273090362549 7100
val loss = 2.8316988945007324
training loss = 1.8204230070114136 7200
val loss = 2.827242851257324
training loss = 1.8194665908813477 7300
val loss = 2.8397226333618164
training loss = 1.8181707859039307 7400
val loss = 2.8031439781188965
training loss = 1.8267476558685303 7500
val loss = 2.9893798828125
training loss = 1.8165581226348877 7600
val loss = 2.7864437103271484
training loss = 1.8183138370513916 7700
val loss = 2.8754725456237793
training loss = 1.8154457807540894 7800
val loss = 2.7694051265716553
training loss = 1.8149782419204712 7900
val loss = 2.7664341926574707
training loss = 1.8149683475494385 8000
val loss = 2.7964630126953125
training loss = 1.814446210861206 8100
val loss = 2.7629194259643555
training loss = 1.8147554397583008 8200
val loss = 2.7994892597198486
training loss = 1.8142094612121582 8300
val loss = 2.7583348751068115
training loss = 1.9054725170135498 8400
val loss = 2.313910961151123
training loss = 1.8142110109329224 8500
val loss = 2.758770704269409
training loss = 1.8142118453979492 8600
val loss = 2.754908323287964
training loss = 1.8147622346878052 8700
val loss = 2.794124126434326
training loss = 1.8144303560256958 8800
val loss = 2.7551512718200684
training loss = 1.8820973634719849 8900
val loss = 3.3321118354797363
training loss = 1.814747929573059 9000
val loss = 2.7576704025268555
training loss = 1.8149462938308716 9100
val loss = 2.744173526763916
training loss = 1.8152278661727905 9200
val loss = 2.775503635406494
training loss = 1.8153129816055298 9300
val loss = 2.760587453842163
training loss = 1.8716565370559692 9400
val loss = 3.2789745330810547
training loss = 1.8157894611358643 9500
val loss = 2.757366180419922
training loss = 1.815955400466919 9600
val loss = 2.766083240509033
training loss = 1.8175923824310303 9700
val loss = 2.702098846435547
training loss = 1.8164339065551758 9800
val loss = 2.770787239074707
training loss = 1.8721598386764526 9900
val loss = 3.2963030338287354
training loss = 1.816939353942871 10000
val loss = 2.769784450531006
training loss = 1.8171178102493286 10100
val loss = 2.7765307426452637
training loss = 1.8190951347351074 10200
val loss = 2.7068839073181152
training loss = 1.8175983428955078 10300
val loss = 2.780452251434326
training loss = 1.8392549753189087 10400
val loss = 3.0886735916137695
training loss = 1.8180943727493286 10500
val loss = 2.7792487144470215
training loss = 1.8182567358016968 10600
val loss = 2.7874419689178467
training loss = 1.8185787200927734 10700
val loss = 2.7807865142822266
training loss = 1.8187108039855957 10800
val loss = 2.793104648590088
training loss = 1.840079426765442 10900
val loss = 3.106187343597412
training loss = 1.8191583156585693 11000
val loss = 2.793736219406128
training loss = 1.8193306922912598 11100
val loss = 2.7971651554107666
training loss = 1.8197686672210693 11200
val loss = 2.780911922454834
training loss = 1.8197674751281738 11300
val loss = 2.8036837577819824
training loss = 1.8199303150177002 11400
val loss = 2.803884506225586
training loss = 1.8201969861984253 11500
val loss = 2.8090789318084717
training loss = 1.8203415870666504 11600
val loss = 2.8100061416625977
training loss = 1.8542499542236328 11700
val loss = 3.2140369415283203
training loss = 1.820743203163147 11800
val loss = 2.8113019466400146
training loss = 1.8210759162902832 11900
val loss = 2.789787769317627
training loss = 1.8212037086486816 12000
val loss = 2.8357903957366943
training loss = 1.8212661743164062 12100
val loss = 2.81951904296875
training loss = 1.8233073949813843 12200
val loss = 2.7463126182556152
training loss = 1.8216476440429688 12300
val loss = 2.824039936065674
training loss = 1.8217875957489014 12400
val loss = 2.8303353786468506
training loss = 1.8220418691635132 12500
val loss = 2.8400557041168213
training loss = 1.8221325874328613 12600
val loss = 2.8293747901916504
training loss = 1.8256689310073853 12700
val loss = 2.7276439666748047
training loss = 1.8224825859069824 12800
val loss = 2.8335025310516357
training loss = 1.8283203840255737 12900
val loss = 2.6958541870117188
training loss = 1.8228508234024048 13000
val loss = 2.8273866176605225
training loss = 1.8229329586029053 13100
val loss = 2.8384482860565186
training loss = 1.8253705501556396 13200
val loss = 2.754269599914551
training loss = 1.8232650756835938 13300
val loss = 2.8430895805358887
training loss = 1.8234065771102905 13400
val loss = 2.854401111602783
training loss = 1.823638916015625 13500
val loss = 2.859330177307129
training loss = 1.8236949443817139 13600
val loss = 2.847322702407837
training loss = 1.8304946422576904 13700
val loss = 3.0229530334472656
training loss = 1.823999047279358 13800
val loss = 2.8539581298828125
training loss = 1.8240987062454224 13900
val loss = 2.853323459625244
training loss = 1.8245898485183716 14000
val loss = 2.8242130279541016
training loss = 1.8244011402130127 14100
val loss = 2.8554465770721436
reduced chi^2 level 2 = 1.8244173526763916
Constrained alpha: 2.0605735778808594
Constrained beta: 3.05704927444458
Constrained gamma: 16.78619384765625
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 904.9266,  860.2782,  928.9444,  949.7531,  999.4272, 1047.6403,
        1106.8940, 1132.3422, 1130.7075, 1142.8326, 1204.4650, 1242.1423,
        1300.2352, 1267.4788, 1339.2257, 1397.0665, 1397.5820, 1443.2008,
        1523.1694, 1460.7599, 1583.4465, 1605.5938, 1579.4227, 1645.7346,
        1697.0115, 1710.0397, 1687.5740, 1720.3154, 1730.4991, 1746.1868,
        1780.4865, 1707.9347, 1670.6133, 1772.1472, 1765.0686, 1726.0768,
        1622.7233, 1600.6133, 1657.8518, 1639.0504, 1579.8652, 1505.4904,
        1504.3474, 1520.2440, 1378.1479, 1322.2401, 1295.7009, 1240.6743,
        1106.7216, 1172.7152, 1058.2098,  977.2487,  965.9369,  963.1215,
         893.5617,  884.3953,  827.9374,  713.9338,  632.6194,  568.7714,
         588.2505,  473.3695,  384.2812,  403.2719,  354.7521,  339.6063,
         262.1790,  269.3939,  218.4903,  183.2135,  169.2393,  147.7499,
         151.1385,  100.5342,  102.8316,   60.7110,   55.4827,   52.1687,
          33.4664,   43.6751,   14.3202,   47.4958,   37.3552])]
2174.3518005165515
4.437217276056674 12.313434764062453 48.4448611321117
val isze = 8
idinces = [ 6  3 18 17 44 65 55 47 46 51 80  0  5 63 41 22  4 31 60 70 50 15 24 10
 14 11 56 64 68 71 12 48 72 73 39 32  1 67 76  9 82 23 57 43 34 53 45 62
  7 27 74 21 26 35 37 81 49  8 69 20 16 42 28 75 38 33 77 25 30 78 54 19
 13 79 61 52 66 29 36  2 59 40 58]
we are doing training validation split
training loss = 10.943575859069824 100
val loss = 6.40520715713501
training loss = 8.60308837890625 200
val loss = 3.6884939670562744
training loss = 8.103050231933594 300
val loss = 3.1653685569763184
training loss = 7.66293478012085 400
val loss = 2.7288811206817627
training loss = 7.329077243804932 500
val loss = 2.424856662750244
training loss = 7.105239391326904 600
val loss = 2.2495954036712646
training loss = 6.96555757522583 700
val loss = 2.166668653488159
training loss = 6.876707553863525 800
val loss = 2.134138345718384
training loss = 6.813683032989502 900
val loss = 2.122199058532715
training loss = 6.762905597686768 1000
val loss = 2.1161599159240723
training loss = 6.718469142913818 1100
val loss = 2.110631227493286
training loss = 6.6780924797058105 1200
val loss = 2.104511260986328
training loss = 6.6408467292785645 1300
val loss = 2.097933769226074
training loss = 6.606203556060791 1400
val loss = 2.0911803245544434
training loss = 6.57377290725708 1500
val loss = 2.0846199989318848
training loss = 6.543188571929932 1600
val loss = 2.078317642211914
training loss = 6.514129161834717 1700
val loss = 2.0721089839935303
training loss = 6.4863057136535645 1800
val loss = 2.0662841796875
training loss = 6.459475517272949 1900
val loss = 2.0605502128601074
training loss = 6.4334588050842285 2000
val loss = 2.0551161766052246
training loss = 6.408350467681885 2100
val loss = 2.033799648284912
training loss = 6.3836212158203125 2200
val loss = 2.045060634613037
training loss = 6.383739471435547 2300
val loss = 1.883782148361206
training loss = 6.33726167678833 2400
val loss = 2.0356149673461914
training loss = 6.320491790771484 2500
val loss = 2.1185808181762695
training loss = 6.296154975891113 2600
val loss = 2.0287222862243652
training loss = 6.277589797973633 2700
val loss = 2.0227432250976562
training loss = 6.259959697723389 2800
val loss = 2.0124409198760986
training loss = 6.240084648132324 2900
val loss = 2.0214662551879883
training loss = 6.289676189422607 3000
val loss = 1.77351975440979
training loss = 6.133945465087891 3100
val loss = 2.007141351699829
training loss = 5.897993087768555 3200
val loss = 1.9801526069641113
training loss = 5.427093982696533 3300
val loss = 1.894040584564209
training loss = 4.405678749084473 3400
val loss = 1.638857364654541
training loss = 2.607069253921509 3500
val loss = 1.6309144496917725
training loss = 1.8413877487182617 3600
val loss = 2.165853500366211
training loss = 1.802049160003662 3700
val loss = 2.2541823387145996
training loss = 1.779006838798523 3800
val loss = 2.2391672134399414
training loss = 1.7580546140670776 3900
val loss = 2.215775966644287
training loss = 1.731370449066162 4000
val loss = 2.1772589683532715
training loss = 1.703798532485962 4100
val loss = 2.0960516929626465
training loss = 1.6625657081604004 4200
val loss = 2.061326742172241
training loss = 1.6395723819732666 4300
val loss = 2.040494918823242
training loss = 1.6211869716644287 4400
val loss = 2.0190229415893555
training loss = 1.6039119958877563 4500
val loss = 1.9938528537750244
training loss = 1.600400686264038 4600
val loss = 1.9410638809204102
training loss = 1.5717202425003052 4700
val loss = 1.9446218013763428
training loss = 1.555998682975769 4800
val loss = 1.9181180000305176
training loss = 1.541491985321045 4900
val loss = 1.9018151760101318
training loss = 1.5260299444198608 5000
val loss = 1.8640856742858887
training loss = 1.5842124223709106 5100
val loss = 1.792826533317566
training loss = 1.4974595308303833 5200
val loss = 1.8081471920013428
training loss = 1.4837032556533813 5300
val loss = 1.7796921730041504
training loss = 1.4783613681793213 5400
val loss = 1.8031041622161865
training loss = 1.4587769508361816 5500
val loss = 1.7248249053955078
training loss = 1.4470410346984863 5600
val loss = 1.6976888179779053
training loss = 1.4365164041519165 5700
val loss = 1.6660524606704712
training loss = 1.426596760749817 5800
val loss = 1.6471705436706543
training loss = 1.4838372468948364 5900
val loss = 1.8452939987182617
training loss = 1.4096629619598389 6000
val loss = 1.6046278476715088
training loss = 1.4022685289382935 6100
val loss = 1.581038236618042
training loss = 1.4282336235046387 6200
val loss = 1.7124061584472656
training loss = 1.390204668045044 6300
val loss = 1.5455238819122314
training loss = 1.3849818706512451 6400
val loss = 1.5284130573272705
training loss = 1.381797432899475 6500
val loss = 1.535912036895752
training loss = 1.3767650127410889 6600
val loss = 1.5014432668685913
training loss = 1.3732267618179321 6700
val loss = 1.4830479621887207
training loss = 1.3703560829162598 6800
val loss = 1.4727330207824707
training loss = 1.3675010204315186 6900
val loss = 1.468711256980896
training loss = 1.3676691055297852 7000
val loss = 1.4978899955749512
training loss = 1.363004207611084 7100
val loss = 1.452155351638794
training loss = 1.411548376083374 7200
val loss = 1.669081449508667
training loss = 1.3594255447387695 7300
val loss = 1.438178300857544
training loss = 1.3588722944259644 7400
val loss = 1.4571770429611206
training loss = 1.3565753698349 7500
val loss = 1.4324023723602295
training loss = 1.3552286624908447 7600
val loss = 1.422898769378662
training loss = 1.3555335998535156 7700
val loss = 1.4469373226165771
training loss = 1.353086233139038 7800
val loss = 1.4153361320495605
training loss = 1.4050240516662598 7900
val loss = 1.2967383861541748
training loss = 1.351288080215454 8000
val loss = 1.4090914726257324
training loss = 1.3504347801208496 8100
val loss = 1.4054503440856934
training loss = 1.3502870798110962 8200
val loss = 1.3861274719238281
training loss = 1.3490393161773682 8300
val loss = 1.400896430015564
training loss = 1.3668098449707031 8400
val loss = 1.3133118152618408
training loss = 1.3478723764419556 8500
val loss = 1.3929351568222046
training loss = 1.3472487926483154 8600
val loss = 1.3948270082473755
training loss = 1.3500357866287231 8700
val loss = 1.439814567565918
training loss = 1.3462731838226318 8800
val loss = 1.3914616107940674
training loss = 1.3706310987472534 8900
val loss = 1.2964386940002441
training loss = 1.3454275131225586 9000
val loss = 1.388223648071289
training loss = 1.3449848890304565 9100
val loss = 1.387894630432129
training loss = 1.3447673320770264 9200
val loss = 1.3798727989196777
training loss = 1.344282865524292 9300
val loss = 1.3854347467422485
training loss = 1.3457655906677246 9400
val loss = 1.3580689430236816
training loss = 1.3436663150787354 9500
val loss = 1.3828556537628174
training loss = 1.343327522277832 9600
val loss = 1.3833221197128296
training loss = 1.3433618545532227 9700
val loss = 1.394348382949829
training loss = 1.3427989482879639 9800
val loss = 1.3810369968414307
training loss = 1.4399610757827759 9900
val loss = 1.7444641590118408
training loss = 1.3423326015472412 10000
val loss = 1.3787832260131836
training loss = 1.3420534133911133 10100
val loss = 1.3790737390518188
training loss = 1.342957615852356 10200
val loss = 1.3535038232803345
training loss = 1.3416365385055542 10300
val loss = 1.3776698112487793
training loss = 1.3638614416122437 10400
val loss = 1.2887994050979614
training loss = 1.341252088546753 10500
val loss = 1.378379225730896
training loss = 1.341073989868164 10600
val loss = 1.3695484399795532
training loss = 1.3408926725387573 10700
val loss = 1.3772231340408325
training loss = 1.3406567573547363 10800
val loss = 1.3748624324798584
training loss = 1.3417631387710571 10900
val loss = 1.3474555015563965
training loss = 1.3403193950653076 11000
val loss = 1.3745206594467163
training loss = 1.3441253900527954 11100
val loss = 1.4282724857330322
training loss = 1.3400059938430786 11200
val loss = 1.3735926151275635
training loss = 1.363383412361145 11300
val loss = 1.5219979286193848
training loss = 1.3397209644317627 11400
val loss = 1.3716354370117188
training loss = 1.3395224809646606 11500
val loss = 1.3725051879882812
training loss = 1.3438386917114258 11600
val loss = 1.4297702312469482
training loss = 1.339242935180664 11700
val loss = 1.3721801042556763
training loss = 1.340133547782898 11800
val loss = 1.399046778678894
training loss = 1.3390237092971802 11900
val loss = 1.3667044639587402
training loss = 1.3387893438339233 12000
val loss = 1.3715691566467285
training loss = 1.3474432229995728 12100
val loss = 1.309215784072876
training loss = 1.3385415077209473 12200
val loss = 1.3734081983566284
training loss = 1.3383448123931885 12300
val loss = 1.3717100620269775
training loss = 1.3386623859405518 12400
val loss = 1.3897539377212524
training loss = 1.3380999565124512 12500
val loss = 1.3776450157165527
training loss = 1.337834358215332 12600
val loss = 1.3718372583389282
training loss = 1.3436999320983887 12700
val loss = 1.3151941299438477
training loss = 1.337453007698059 12800
val loss = 1.3724561929702759
training loss = 1.3652201890945435 12900
val loss = 1.2684636116027832
training loss = 1.3369853496551514 13000
val loss = 1.3734005689620972
training loss = 1.336665391921997 13100
val loss = 1.373647689819336
training loss = 1.3367830514907837 13200
val loss = 1.3595422506332397
training loss = 1.3359476327896118 13300
val loss = 1.376017689704895
training loss = 1.3358721733093262 13400
val loss = 1.3632841110229492
training loss = 1.3350032567977905 13500
val loss = 1.3789803981781006
training loss = 1.334348201751709 13600
val loss = 1.3792389631271362
training loss = 1.3338886499404907 13700
val loss = 1.3930351734161377
training loss = 1.3328752517700195 13800
val loss = 1.38114595413208
training loss = 1.3319485187530518 13900
val loss = 1.3798528909683228
training loss = 1.3361961841583252 14000
val loss = 1.440101146697998
training loss = 1.3302184343338013 14100
val loss = 1.3776378631591797
training loss = 1.3293143510818481 14200
val loss = 1.3755711317062378
training loss = 1.3287307024002075 14300
val loss = 1.3788652420043945
training loss = 1.327945590019226 14400
val loss = 1.369974136352539
training loss = 1.3584297895431519 14500
val loss = 1.54902184009552
training loss = 1.3269450664520264 14600
val loss = 1.366465449333191
training loss = 1.3264349699020386 14700
val loss = 1.3626779317855835
training loss = 1.3265717029571533 14800
val loss = 1.3449897766113281
training loss = 1.3256672620773315 14900
val loss = 1.3596850633621216
training loss = 1.3252307176589966 15000
val loss = 1.356358528137207
training loss = 1.3251430988311768 15100
val loss = 1.346060037612915
training loss = 1.3245232105255127 15200
val loss = 1.355301856994629
training loss = 1.3240787982940674 15300
val loss = 1.3529729843139648
training loss = 1.323979377746582 15400
val loss = 1.3425887823104858
training loss = 1.3233058452606201 15500
val loss = 1.3528618812561035
training loss = 1.3258126974105835 15600
val loss = 1.3985934257507324
training loss = 1.3224766254425049 15700
val loss = 1.3564844131469727
training loss = 1.3219088315963745 15800
val loss = 1.3507517576217651
training loss = 1.321656584739685 15900
val loss = 1.3607063293457031
training loss = 1.3209354877471924 16000
val loss = 1.349624752998352
training loss = 1.3449351787567139 16100
val loss = 1.2535765171051025
training loss = 1.319906234741211 16200
val loss = 1.3494117259979248
training loss = 1.3193246126174927 16300
val loss = 1.3519164323806763
training loss = 1.3189460039138794 16400
val loss = 1.3561855554580688
training loss = 1.3181970119476318 16500
val loss = 1.346671462059021
training loss = 1.397729516029358 16600
val loss = 1.6695109605789185
training loss = 1.3170337677001953 16700
val loss = 1.344639778137207
training loss = 1.3163492679595947 16800
val loss = 1.344958782196045
training loss = 1.3191022872924805 16900
val loss = 1.3010085821151733
training loss = 1.3151285648345947 17000
val loss = 1.344059705734253
training loss = 1.3143997192382812 17100
val loss = 1.3433575630187988
training loss = 1.3141905069351196 17200
val loss = 1.3277820348739624
training loss = 1.3130664825439453 17300
val loss = 1.3421626091003418
training loss = 1.3323057889938354 17400
val loss = 1.2562105655670166
training loss = 1.3116782903671265 17500
val loss = 1.342198371887207
training loss = 1.3108676671981812 17600
val loss = 1.3393558263778687
training loss = 1.3106029033660889 17700
val loss = 1.356056809425354
training loss = 1.309395432472229 17800
val loss = 1.338568091392517
training loss = 1.3668566942214966 17900
val loss = 1.598616123199463
training loss = 1.3078784942626953 18000
val loss = 1.3365678787231445
training loss = 1.3069963455200195 18100
val loss = 1.3363878726959229
training loss = 1.3067126274108887 18200
val loss = 1.3204389810562134
training loss = 1.3054437637329102 18300
val loss = 1.3351538181304932
training loss = 1.3045148849487305 18400
val loss = 1.3326927423477173
training loss = 1.3037878274917603 18500
val loss = 1.3361706733703613
training loss = 1.3028373718261719 18600
val loss = 1.3328664302825928
training loss = 1.3235162496566772 18700
val loss = 1.476689100265503
training loss = 1.3010921478271484 18800
val loss = 1.3318984508514404
training loss = 1.3001350164413452 18900
val loss = 1.3362319469451904
training loss = 1.2993113994598389 19000
val loss = 1.3303273916244507
training loss = 1.2982937097549438 19100
val loss = 1.3288612365722656
training loss = 1.3102781772613525 19200
val loss = 1.2517821788787842
training loss = 1.2964119911193848 19300
val loss = 1.3264355659484863
training loss = 1.2954820394515991 19400
val loss = 1.3155207633972168
training loss = 1.294559121131897 19500
val loss = 1.3184640407562256
training loss = 1.293372392654419 19600
val loss = 1.3246742486953735
training loss = 1.2930684089660645 19700
val loss = 1.343725562095642
training loss = 1.2913920879364014 19800
val loss = 1.3236117362976074
training loss = 1.2903831005096436 19900
val loss = 1.3324759006500244
training loss = 1.2893705368041992 20000
val loss = 1.3150129318237305
training loss = 1.2881178855895996 20100
val loss = 1.320623755455017
training loss = 1.287353515625 20200
val loss = 1.3095306158065796
training loss = 1.2860050201416016 20300
val loss = 1.3194186687469482
training loss = 1.7574310302734375 20400
val loss = 1.245910882949829
training loss = 1.283844232559204 20500
val loss = 1.3189129829406738
training loss = 1.2825894355773926 20600
val loss = 1.3163915872573853
training loss = 1.2823885679244995 20700
val loss = 1.2917085886001587
training loss = 1.2803689241409302 20800
val loss = 1.3155832290649414
training loss = 1.2796659469604492 20900
val loss = 1.292884111404419
training loss = 1.2781836986541748 21000
val loss = 1.3072150945663452
training loss = 1.276826024055481 21100
val loss = 1.3126142024993896
training loss = 1.2764699459075928 21200
val loss = 1.2970259189605713
training loss = 1.27454674243927 21300
val loss = 1.3111326694488525
training loss = 1.2732079029083252 21400
val loss = 1.3104419708251953
training loss = 1.2722854614257812 21500
val loss = 1.3005733489990234
training loss = 1.2708388566970825 21600
val loss = 1.3094382286071777
training loss = 1.2968214750289917 21700
val loss = 1.1954275369644165
training loss = 1.2685422897338867 21800
val loss = 1.307938814163208
training loss = 1.2671934366226196 21900
val loss = 1.3071811199188232
training loss = 1.2663367986679077 22000
val loss = 1.3200795650482178
training loss = 1.2649202346801758 22100
val loss = 1.3073303699493408
training loss = 1.26356840133667 22200
val loss = 1.3057926893234253
training loss = 1.2627042531967163 22300
val loss = 1.3144991397857666
training loss = 1.2613388299942017 22400
val loss = 1.3056700229644775
training loss = 1.2600678205490112 22500
val loss = 1.2970908880233765
training loss = 1.2591853141784668 22600
val loss = 1.295711874961853
training loss = 1.2577894926071167 22700
val loss = 1.3043348789215088
training loss = 1.264719009399414 22800
val loss = 1.3952581882476807
training loss = 1.2557625770568848 22900
val loss = 1.3040788173675537
training loss = 1.2545416355133057 23000
val loss = 1.3042250871658325
training loss = 1.2627062797546387 23100
val loss = 1.2267200946807861
training loss = 1.2525438070297241 23200
val loss = 1.3048101663589478
training loss = 1.251903772354126 23300
val loss = 1.326061725616455
training loss = 1.2507343292236328 23400
val loss = 1.296525001525879
training loss = 1.2495654821395874 23500
val loss = 1.304971694946289
training loss = 1.2596460580825806 23600
val loss = 1.2190746068954468
training loss = 1.2478408813476562 23700
val loss = 1.3066201210021973
training loss = 1.2520012855529785 23800
val loss = 1.37931489944458
training loss = 1.2463303804397583 23900
val loss = 1.306553840637207
training loss = 1.2454254627227783 24000
val loss = 1.3073574304580688
training loss = 1.6646555662155151 24100
val loss = 1.1345291137695312
training loss = 1.2441027164459229 24200
val loss = 1.306896448135376
training loss = 1.2432963848114014 24300
val loss = 1.3088610172271729
training loss = 1.2699086666107178 24400
val loss = 1.1808613538742065
training loss = 1.2420953512191772 24500
val loss = 1.3117092847824097
training loss = 1.2703511714935303 24600
val loss = 1.1759780645370483
training loss = 1.2410519123077393 24700
val loss = 1.3180829286575317
training loss = 1.240371823310852 24800
val loss = 1.3132126331329346
training loss = 1.240045428276062 24900
val loss = 1.3130807876586914
training loss = 1.2394673824310303 25000
val loss = 1.3140053749084473
training loss = 1.2399711608886719 25100
val loss = 1.2878484725952148
training loss = 1.2386938333511353 25200
val loss = 1.3154692649841309
training loss = 1.2475132942199707 25300
val loss = 1.4216095209121704
training loss = 1.2380850315093994 25400
val loss = 1.3171758651733398
training loss = 1.2376458644866943 25500
val loss = 1.3184409141540527
training loss = 1.6953721046447754 25600
val loss = 1.132127046585083
training loss = 1.2371221780776978 25700
val loss = 1.3225096464157104
training loss = 1.236741304397583 25800
val loss = 1.3211113214492798
training loss = 1.2367143630981445 25900
val loss = 1.331621766090393
training loss = 1.2363001108169556 26000
val loss = 1.3232321739196777
training loss = 1.5409868955612183 26100
val loss = 1.0971167087554932
training loss = 1.2359577417373657 26200
val loss = 1.3220446109771729
training loss = 1.2356559038162231 26300
val loss = 1.3251869678497314
training loss = 1.2371433973312378 26400
val loss = 1.3657644987106323
training loss = 1.2353999614715576 26500
val loss = 1.3277753591537476
training loss = 1.2351499795913696 26600
val loss = 1.3288733959197998
training loss = 1.2355735301971436 26700
val loss = 1.3087743520736694
training loss = 1.2349222898483276 26800
val loss = 1.3296151161193848
training loss = 1.248266577720642 26900
val loss = 1.2260321378707886
training loss = 1.2347549200057983 27000
val loss = 1.331832766532898
training loss = 1.2802420854568481 27100
val loss = 1.1610361337661743
training loss = 1.234666347503662 27200
val loss = 1.3279402256011963
training loss = 1.234451174736023 27300
val loss = 1.3331444263458252
training loss = 1.237806797027588 27400
val loss = 1.279807686805725
training loss = 1.2343662977218628 27500
val loss = 1.3352606296539307
training loss = 1.2354047298431396 27600
val loss = 1.370248556137085
training loss = 1.2343107461929321 27700
val loss = 1.3358793258666992
training loss = 1.2341701984405518 27800
val loss = 1.3314719200134277
training loss = 1.2345516681671143 27900
val loss = 1.3241726160049438
training loss = 1.234213948249817 28000
val loss = 1.338493824005127
training loss = 1.2343024015426636 28100
val loss = 1.323365569114685
training loss = 1.2343173027038574 28200
val loss = 1.351509690284729
training loss = 1.2340513467788696 28300
val loss = 1.3410072326660156
training loss = 1.2348456382751465 28400
val loss = 1.3153958320617676
training loss = 1.2340346574783325 28500
val loss = 1.3421258926391602
training loss = 1.2470526695251465 28600
val loss = 1.2366383075714111
training loss = 1.234048843383789 28700
val loss = 1.3432375192642212
training loss = 1.4462040662765503 28800
val loss = 2.049436092376709
training loss = 1.2340680360794067 28900
val loss = 1.3450002670288086
training loss = 1.2339314222335815 29000
val loss = 1.345025897026062
training loss = 1.2372398376464844 29100
val loss = 1.406703233718872
training loss = 1.2340397834777832 29200
val loss = 1.3460397720336914
training loss = 1.2339026927947998 29300
val loss = 1.3466997146606445
training loss = 1.234096646308899 29400
val loss = 1.3505654335021973
training loss = 1.2339413166046143 29500
val loss = 1.347808837890625
training loss = 1.9697811603546143 29600
val loss = 1.220263123512268
training loss = 1.2340366840362549 29700
val loss = 1.345198154449463
training loss = 1.2338776588439941 29800
val loss = 1.3490371704101562
training loss = 1.254417061805725 29900
val loss = 1.5219452381134033
training loss = 1.2338987588882446 30000
val loss = 1.3489038944244385
reduced chi^2 level 2 = 1.233897089958191
Constrained alpha: 1.850272536277771
Constrained beta: 1.2580162286758423
Constrained gamma: 12.78617000579834
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 780.5616,  842.2511,  953.0565,  969.8277,  986.0197, 1058.6218,
        1076.5587, 1083.2535, 1152.4691, 1141.8387, 1157.6886, 1209.3274,
        1230.1277, 1237.8654, 1362.2559, 1370.6237, 1396.8922, 1444.2681,
        1569.4194, 1524.5148, 1566.9677, 1559.4753, 1567.8289, 1565.3625,
        1645.4358, 1714.0739, 1603.5684, 1719.2501, 1782.4496, 1737.2073,
        1695.9802, 1744.0023, 1729.0123, 1756.6899, 1744.6135, 1751.6990,
        1654.6752, 1631.3082, 1659.1807, 1588.1865, 1634.3059, 1572.2953,
        1516.0459, 1525.2976, 1417.8445, 1373.9639, 1248.2620, 1244.4821,
        1160.9070, 1178.5029, 1107.3940,  979.4725,  964.2623,  896.0189,
         936.9588,  874.1896,  826.4552,  680.9885,  611.4236,  532.6429,
         554.4354,  468.6228,  437.6317,  404.9507,  356.5810,  327.8700,
         280.3399,  266.3587,  204.0059,  160.0278,  175.1944,  135.0037,
         162.2613,  108.2910,   92.9888,   58.0477,   61.2791,   45.6078,
          40.8900,   41.0723,   17.5392,   37.1446,   35.6031])]
2437.229116230307
2.636799942322244 0.42076324475551985 52.92144857528288
val isze = 8
idinces = [ 4 22 58 24 76  8 56 38 73 52 34  3 27 79  5 71 55 74  6 23 78 43 59 25
 70 44 69 54 13 33 50 18 57 41  2 37 46 77 49 40 29 80 60 19 81 35  1 20
 48 17  0  7 28 72 47 10 45 63 65 39  9 68 53 61 51 16 31 66 36 30 64 62
 26 67 42 82 11 21 75 15 12 14 32]
we are doing training validation split
training loss = 93.14401245117188 100
val loss = 79.70394134521484
training loss = 30.400379180908203 200
val loss = 26.534950256347656
training loss = 8.721423149108887 300
val loss = 7.915527820587158
training loss = 5.09879732131958 400
val loss = 3.980038642883301
training loss = 3.6544182300567627 500
val loss = 2.297860622406006
training loss = 2.9961349964141846 600
val loss = 1.7299890518188477
training loss = 2.6441519260406494 700
val loss = 1.4425934553146362
training loss = 2.3745784759521484 800
val loss = 1.4903812408447266
training loss = 2.27718186378479 900
val loss = 1.3776445388793945
training loss = 2.128685235977173 1000
val loss = 1.5877043008804321
training loss = 2.054469347000122 1100
val loss = 1.6610034704208374
training loss = 1.9951705932617188 1200
val loss = 1.7176611423492432
training loss = 1.945268988609314 1300
val loss = 1.7481476068496704
training loss = 1.905362844467163 1400
val loss = 1.763747215270996
training loss = 1.8721116781234741 1500
val loss = 1.7836005687713623
training loss = 1.9878361225128174 1600
val loss = 2.290308952331543
training loss = 1.820493221282959 1700
val loss = 1.7897638082504272
training loss = 1.7994306087493896 1800
val loss = 1.7910422086715698
training loss = 1.7809261083602905 1900
val loss = 1.7938587665557861
training loss = 1.7634193897247314 2000
val loss = 1.7774803638458252
training loss = 1.7474247217178345 2100
val loss = 1.7791610956192017
training loss = 1.7320585250854492 2200
val loss = 1.7639472484588623
training loss = 1.7671105861663818 2300
val loss = 1.61171293258667
training loss = 1.7032054662704468 2400
val loss = 1.750679850578308
training loss = 1.6897145509719849 2500
val loss = 1.7409714460372925
training loss = 1.679111123085022 2600
val loss = 1.7716803550720215
training loss = 1.6654750108718872 2700
val loss = 1.7276358604431152
training loss = 1.7313954830169678 2800
val loss = 2.0134499073028564
training loss = 1.6442844867706299 2900
val loss = 1.7102527618408203
training loss = 1.6345744132995605 3000
val loss = 1.7055730819702148
training loss = 1.6274033784866333 3100
val loss = 1.731186866760254
training loss = 1.6167948246002197 3200
val loss = 1.6889829635620117
training loss = 1.6084030866622925 3300
val loss = 1.6815744638442993
training loss = 1.60061514377594 3400
val loss = 1.6787961721420288
training loss = 1.592922568321228 3500
val loss = 1.6624318361282349
training loss = 1.7986373901367188 3600
val loss = 2.170466899871826
training loss = 1.578640341758728 3700
val loss = 1.6448756456375122
training loss = 1.5718783140182495 3800
val loss = 1.6335567235946655
training loss = 1.5711040496826172 3900
val loss = 1.5816935300827026
training loss = 1.5593618154525757 4000
val loss = 1.6144274473190308
training loss = 1.5533853769302368 4100
val loss = 1.60418701171875
training loss = 1.5478140115737915 4200
val loss = 1.5998131036758423
training loss = 1.5422440767288208 4300
val loss = 1.584523320198059
training loss = 1.639293909072876 4400
val loss = 1.4939700365066528
training loss = 1.531977653503418 4500
val loss = 1.5658951997756958
training loss = 1.52731454372406 4600
val loss = 1.5457860231399536
training loss = 1.522598385810852 4700
val loss = 1.539609670639038
training loss = 1.5180027484893799 4800
val loss = 1.5348230600357056
training loss = 1.5167615413665771 4900
val loss = 1.5006014108657837
training loss = 1.5097914934158325 5000
val loss = 1.5159045457839966
training loss = 1.505788803100586 5100
val loss = 1.5028066635131836
training loss = 1.502347707748413 5200
val loss = 1.5027709007263184
training loss = 1.4984772205352783 5300
val loss = 1.487056851387024
training loss = 1.5417331457138062 5400
val loss = 1.4244446754455566
training loss = 1.4917726516723633 5500
val loss = 1.4682016372680664
training loss = 1.4884300231933594 5600
val loss = 1.4586315155029297
training loss = 1.4863954782485962 5700
val loss = 1.437669277191162
training loss = 1.4824316501617432 5800
val loss = 1.4406098127365112
training loss = 1.4983314275741577 5900
val loss = 1.50226628780365
reduced chi^2 level 2 = 1.4841539859771729
Constrained alpha: 1.8456677198410034
Constrained beta: 1.6547760963439941
Constrained gamma: 29.637516021728516
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 850.2985,  875.1876,  949.5547,  924.8657, 1006.1911, 1091.2045,
        1087.5497, 1163.8815, 1166.7073, 1161.8651, 1186.7551, 1170.4933,
        1231.5947, 1246.0942, 1323.8165, 1374.2977, 1436.2278, 1448.0978,
        1526.3623, 1466.8153, 1720.5702, 1590.1506, 1644.0156, 1653.8604,
        1639.5302, 1725.0320, 1663.6499, 1723.2538, 1790.1831, 1656.0314,
        1653.2903, 1704.7926, 1660.7108, 1769.7307, 1696.7783, 1729.1749,
        1658.1248, 1505.6021, 1645.5326, 1609.2136, 1633.9741, 1648.8977,
        1457.8328, 1468.1893, 1395.7014, 1311.3007, 1326.0063, 1223.4480,
        1113.6294, 1150.5460, 1041.2123,  932.0209,  974.2414,  973.3138,
         853.0400,  887.2740,  810.2072,  677.7736,  607.9604,  587.7637,
         531.8718,  448.7827,  468.8430,  369.3139,  370.2294,  365.3306,
         258.5871,  248.7097,  205.5738,  152.3351,  124.7567,  134.3472,
         159.4257,  102.6318,  103.0373,   63.2087,   53.5672,   35.2941,
          38.7662,   50.4140,   24.7759,   39.3824,   34.9455])]
2807.13615426871
2.7909059520097297 13.02193070928367 69.08779853008038
val isze = 8
idinces = [37 78 65 38 48  7 45 82 42 32 26 11  8 57 52 73 75 24 46 54 12 55  5 35
 56 36 21 31 81 16 41 22 39 74 47 50 63  9 34 13 27 80 40 61  6 69 15  4
 43 10 77 25 53 66  2 51 44 17 67 58 70 29 20 33 68 76 18 30 79 14 62 19
 60 64 72 59  3 71 28  0  1 49 23]
we are doing training validation split
training loss = 212.7864227294922 100
val loss = 222.82989501953125
training loss = 7.9357991218566895 200
val loss = 8.740347862243652
training loss = 7.776679992675781 300
val loss = 8.968780517578125
training loss = 7.666174411773682 400
val loss = 9.214025497436523
training loss = 7.569487571716309 500
val loss = 9.45168685913086
training loss = 7.487020969390869 600
val loss = 9.66973876953125
training loss = 7.417011737823486 700
val loss = 9.860624313354492
training loss = 7.3569793701171875 800
val loss = 10.02025032043457
training loss = 7.304471015930176 900
val loss = 10.14730167388916
training loss = 7.257410049438477 1000
val loss = 10.242552757263184
training loss = 7.2141804695129395 1100
val loss = 10.308435440063477
training loss = 7.173628330230713 1200
val loss = 10.348139762878418
training loss = 7.134962558746338 1300
val loss = 10.365621566772461
training loss = 7.097679138183594 1400
val loss = 10.3648099899292
training loss = 7.061466693878174 1500
val loss = 10.349601745605469
training loss = 7.026130676269531 1600
val loss = 10.323486328125
training loss = 6.9915690422058105 1700
val loss = 10.289484024047852
training loss = 6.9577107429504395 1800
val loss = 10.250075340270996
training loss = 6.924499034881592 1900
val loss = 10.207275390625
training loss = 6.891895294189453 2000
val loss = 10.16254997253418
training loss = 6.859860420227051 2100
val loss = 10.116987228393555
training loss = 6.8283610343933105 2200
val loss = 10.071240425109863
training loss = 6.797370910644531 2300
val loss = 10.025819778442383
training loss = 6.766860485076904 2400
val loss = 9.980998992919922
training loss = 6.736817359924316 2500
val loss = 9.936904907226562
training loss = 6.707242012023926 2600
val loss = 9.893569946289062
training loss = 6.678160190582275 2700
val loss = 9.850992202758789
training loss = 6.649608135223389 2800
val loss = 9.809351921081543
training loss = 6.621667861938477 2900
val loss = 9.768472671508789
training loss = 6.594462871551514 3000
val loss = 9.728548049926758
training loss = 6.5681538581848145 3100
val loss = 9.68960952758789
training loss = 6.542963027954102 3200
val loss = 9.651878356933594
training loss = 6.519153594970703 3300
val loss = 9.615306854248047
training loss = 6.497008800506592 3400
val loss = 9.580120086669922
training loss = 6.476825714111328 3500
val loss = 9.546409606933594
training loss = 6.458836555480957 3600
val loss = 9.51412582397461
training loss = 6.443182468414307 3700
val loss = 9.483369827270508
training loss = 6.429854393005371 3800
val loss = 9.453977584838867
training loss = 6.418679714202881 3900
val loss = 9.425561904907227
training loss = 6.430898189544678 4000
val loss = 9.22679328918457
training loss = 6.401642799377441 4100
val loss = 9.367476463317871
training loss = 6.405163288116455 4200
val loss = 9.222793579101562
training loss = 6.388288497924805 4300
val loss = 9.316064834594727
training loss = 6.404792308807373 4400
val loss = 9.511556625366211
training loss = 6.367300987243652 4500
val loss = 9.249820709228516
training loss = 6.45179557800293 4600
val loss = 9.709833145141602
training loss = 6.100801944732666 4700
val loss = 8.641294479370117
training loss = 5.542694568634033 4800
val loss = 7.873610496520996
training loss = 4.466741561889648 4900
val loss = 6.683879852294922
training loss = 2.870161294937134 5000
val loss = 5.314846515655518
training loss = 2.4871602058410645 5100
val loss = 6.229541778564453
training loss = 2.548970937728882 5200
val loss = 5.858318328857422
training loss = 2.468440532684326 5300
val loss = 6.343497276306152
training loss = 2.462672472000122 5400
val loss = 6.392376899719238
training loss = 2.4564805030822754 5500
val loss = 6.327179908752441
training loss = 2.4515528678894043 5600
val loss = 6.327543258666992
training loss = 2.449051856994629 5700
val loss = 6.415931224822998
training loss = 2.443160057067871 5800
val loss = 6.320368766784668
training loss = 2.4409146308898926 5900
val loss = 6.3938751220703125
training loss = 2.4361369609832764 6000
val loss = 6.319638729095459
training loss = 2.433032989501953 6100
val loss = 6.309629917144775
training loss = 2.4302477836608887 6200
val loss = 6.278507709503174
training loss = 2.4273571968078613 6300
val loss = 6.304128646850586
training loss = 2.5179173946380615 6400
val loss = 5.77935791015625
training loss = 2.422253131866455 6500
val loss = 6.296681880950928
training loss = 2.4198286533355713 6600
val loss = 6.299023151397705
training loss = 2.417804718017578 6700
val loss = 6.262848854064941
training loss = 2.4152631759643555 6800
val loss = 6.294454097747803
training loss = 2.4135589599609375 6900
val loss = 6.245738983154297
training loss = 2.410935878753662 7000
val loss = 6.291146278381348
training loss = 2.408731698989868 7100
val loss = 6.2936110496521
training loss = 2.406881332397461 7200
val loss = 6.265214920043945
training loss = 2.404660224914551 7300
val loss = 6.287857532501221
training loss = 2.4568471908569336 7400
val loss = 5.8767924308776855
training loss = 2.400691270828247 7500
val loss = 6.280595302581787
training loss = 2.3986244201660156 7600
val loss = 6.283480167388916
training loss = 2.401862859725952 7700
val loss = 6.136526107788086
training loss = 2.3949124813079834 7800
val loss = 6.279038429260254
training loss = 2.3929593563079834 7900
val loss = 6.279170036315918
training loss = 2.4147095680236816 8000
val loss = 6.00130033493042
training loss = 2.389224052429199 8100
val loss = 6.278706073760986
training loss = 2.3872616291046143 8200
val loss = 6.275415420532227
training loss = 2.3873536586761475 8300
val loss = 6.355398178100586
training loss = 2.3836779594421387 8400
val loss = 6.271059036254883
training loss = 2.3817248344421387 8500
val loss = 6.270633697509766
training loss = 2.3807218074798584 8600
val loss = 6.318290710449219
training loss = 2.3781356811523438 8700
val loss = 6.266392230987549
training loss = 2.3761210441589355 8800
val loss = 6.265603542327881
training loss = 2.3743700981140137 8900
val loss = 6.246121883392334
training loss = 2.3722641468048096 9000
val loss = 6.26015567779541
training loss = 2.3700337409973145 9100
val loss = 6.245555877685547
training loss = 2.367948532104492 9200
val loss = 6.270003318786621
training loss = 2.3652851581573486 9300
val loss = 6.2495903968811035
training loss = 2.417978525161743 9400
val loss = 5.839941024780273
training loss = 2.3591678142547607 9500
val loss = 6.234139442443848
training loss = 2.3551673889160156 9600
val loss = 6.233022689819336
training loss = 2.3510096073150635 9700
val loss = 6.202237129211426
training loss = 2.3459084033966064 9800
val loss = 6.214588165283203
training loss = 2.3410096168518066 9900
val loss = 6.237036228179932
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 807.6883,  890.7050,  920.7493,  956.7181, 1009.8647, 1114.2720,
        1106.9897, 1176.4841, 1150.3538, 1149.4285, 1222.3107, 1210.7926,
        1260.4958, 1309.4386, 1319.4606, 1403.0634, 1419.1578, 1444.1305,
        1526.9155, 1474.3503, 1557.8407, 1554.6584, 1667.6575, 1556.8628,
        1577.4601, 1742.9651, 1621.0427, 1799.5377, 1750.9686, 1733.7662,
        1641.1034, 1648.5906, 1698.0504, 1742.1792, 1771.0504, 1806.1052,
        1700.5623, 1642.9055, 1638.3467, 1579.9200, 1659.6880, 1538.6844,
        1558.1667, 1527.9594, 1301.9855, 1314.3510, 1320.0963, 1196.9475,
        1173.3428, 1196.0950, 1079.2180, 1039.7031,  958.9487,  909.3395,
         872.4921,  836.8558,  810.0271,  747.7689,  575.3824,  565.8448,
         561.5043,  501.5606,  460.2287,  392.0934,  383.2480,  351.9709,
         294.1259,  216.9668,  197.4644,  137.0402,  167.5695,  153.2224,
         131.6729,  106.5793,  102.5892,   62.4229,   58.8086,   41.1593,
          34.1134,   40.7300,   21.5825,   40.7630,   33.8592])]
2744.647743854818
0.8924352847691447 10.585853914129618 94.85014810675304
val isze = 8
idinces = [59 36 54 32 65 48 71 77 49  7 42 19 38 78 17  0 63 20 27 43 12 11 35 10
  1 60 76 34 21 79 70  4 52 28  6 68  5 53 41 24  9 61 73 37 13 75 81  8
 33 58  3 47  2 14 69 50 64 18 82 46 74 29 16 72 80 57 15 51 45 26 39 62
 30 55 67 66 40 56 22 44 25 23 31]
we are doing training validation split
training loss = 10.89979076385498 100
val loss = 15.349981307983398
training loss = 6.616527080535889 200
val loss = 8.720765113830566
training loss = 6.555857181549072 300
val loss = 8.524547576904297
training loss = 6.490477085113525 400
val loss = 8.30710506439209
training loss = 6.4246039390563965 500
val loss = 8.077078819274902
training loss = 6.361483573913574 600
val loss = 7.843677997589111
training loss = 6.303414821624756 700
val loss = 7.614564418792725
training loss = 6.251792907714844 800
val loss = 7.395839691162109
training loss = 6.2071638107299805 900
val loss = 7.192066669464111
training loss = 6.169357776641846 1000
val loss = 7.0063371658325195
training loss = 6.137657165527344 1100
val loss = 6.840272426605225
training loss = 6.1110100746154785 1200
val loss = 6.694258689880371
training loss = 6.088236331939697 1300
val loss = 6.567651748657227
training loss = 6.068195343017578 1400
val loss = 6.459053993225098
training loss = 6.0499114990234375 1500
val loss = 6.366564750671387
training loss = 6.032614707946777 1600
val loss = 6.2880353927612305
training loss = 6.015766143798828 1700
val loss = 6.221247673034668
training loss = 5.999009609222412 1800
val loss = 6.164044380187988
training loss = 5.98214054107666 1900
val loss = 6.114445686340332
training loss = 5.965051174163818 2000
val loss = 6.070651531219482
training loss = 5.947680950164795 2100
val loss = 6.0311970710754395
training loss = 5.929997444152832 2200
val loss = 5.994746685028076
training loss = 5.911952018737793 2300
val loss = 5.960263252258301
training loss = 5.893459796905518 2400
val loss = 5.9267897605896
training loss = 5.874378681182861 2500
val loss = 5.893520832061768
training loss = 5.854474067687988 2600
val loss = 5.859608173370361
training loss = 5.833348751068115 2700
val loss = 5.824129104614258
training loss = 5.810359001159668 2800
val loss = 5.785791873931885
training loss = 5.784392356872559 2900
val loss = 5.742590427398682
training loss = 5.753445148468018 3000
val loss = 5.69101095199585
training loss = 5.713594913482666 3100
val loss = 5.624192237854004
training loss = 5.6564717292785645 3200
val loss = 5.527118682861328
training loss = 5.562593936920166 3300
val loss = 5.364043235778809
training loss = 5.3841962814331055 3400
val loss = 5.045408725738525
training loss = 5.023597717285156 3500
val loss = 4.387600898742676
training loss = 4.442686557769775 3600
val loss = 3.4039947986602783
training loss = 3.7186784744262695 3700
val loss = 2.4904556274414062
training loss = 2.9188623428344727 3800
val loss = 1.7380752563476562
training loss = 2.381727457046509 3900
val loss = 1.5312005281448364
training loss = 2.2526164054870605 4000
val loss = 1.6887376308441162
training loss = 2.2381110191345215 4100
val loss = 1.7533563375473022
training loss = 2.233264923095703 4200
val loss = 1.753908395767212
training loss = 2.229519844055176 4300
val loss = 1.743033766746521
training loss = 2.2262957096099854 4400
val loss = 1.731948733329773
training loss = 2.223473072052002 4500
val loss = 1.7222260236740112
training loss = 2.2209928035736084 4600
val loss = 1.713909387588501
training loss = 2.218806028366089 4700
val loss = 1.7068941593170166
training loss = 2.2168734073638916 4800
val loss = 1.7010078430175781
training loss = 2.215160369873047 4900
val loss = 1.6961358785629272
training loss = 2.213637351989746 5000
val loss = 1.6921881437301636
training loss = 2.221341133117676 5100
val loss = 1.6635797023773193
training loss = 2.210557460784912 5200
val loss = 1.685009241104126
training loss = 2.2092761993408203 5300
val loss = 1.6813576221466064
training loss = 2.2078094482421875 5400
val loss = 1.6798404455184937
training loss = 2.2064883708953857 5500
val loss = 1.675823450088501
training loss = 2.2092504501342773 5600
val loss = 1.6933681964874268
training loss = 2.203843116760254 5700
val loss = 1.6696065664291382
training loss = 2.4908130168914795 5800
val loss = 1.717045783996582
training loss = 2.20133638381958 5900
val loss = 1.6646056175231934
training loss = 2.20021653175354 6000
val loss = 1.6621146202087402
training loss = 2.1993789672851562 6100
val loss = 1.6515190601348877
training loss = 2.197774648666382 6200
val loss = 1.6565028429031372
training loss = 2.196800947189331 6300
val loss = 1.657233715057373
training loss = 2.195498466491699 6400
val loss = 1.6530718803405762
training loss = 2.1943485736846924 6500
val loss = 1.6492040157318115
training loss = 2.1933281421661377 6600
val loss = 1.6477429866790771
training loss = 2.1921842098236084 6700
val loss = 1.64149808883667
training loss = 2.1910531520843506 6800
val loss = 1.6428077220916748
training loss = 2.1909196376800537 6900
val loss = 1.632765293121338
training loss = 2.1887118816375732 7000
val loss = 1.638319730758667
training loss = 2.18757700920105 7100
val loss = 1.636533498764038
training loss = 2.1896650791168213 7200
val loss = 1.6164157390594482
training loss = 2.1849875450134277 7300
val loss = 1.6318676471710205
training loss = 2.1836559772491455 7400
val loss = 1.6295509338378906
training loss = 2.182140350341797 7500
val loss = 1.6285138130187988
training loss = 2.1805286407470703 7600
val loss = 1.6249678134918213
training loss = 2.178819417953491 7700
val loss = 1.6238675117492676
training loss = 2.1768202781677246 7800
val loss = 1.619016170501709
training loss = 2.1747384071350098 7900
val loss = 1.6172044277191162
training loss = 2.1732430458068848 8000
val loss = 1.62312912940979
training loss = 2.1698083877563477 8100
val loss = 1.6116639375686646
training loss = 2.294296979904175 8200
val loss = 1.8131990432739258
training loss = 2.1639115810394287 8300
val loss = 1.6049864292144775
training loss = 2.1606733798980713 8400
val loss = 1.6038215160369873
training loss = 2.2829723358154297 8500
val loss = 1.804146409034729
training loss = 2.1533586978912354 8600
val loss = 1.5980219841003418
training loss = 2.1493582725524902 8700
val loss = 1.596902847290039
training loss = 2.1453540325164795 8800
val loss = 1.597070336341858
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 844.9582,  942.3591,  993.9424,  950.0673,  956.9609,  987.5892,
        1114.4022, 1167.1907, 1166.2654, 1157.2596, 1269.1577, 1203.7604,
        1266.1716, 1246.3829, 1341.0405, 1370.8066, 1359.0787, 1473.9786,
        1519.4607, 1532.4497, 1548.6709, 1631.3944, 1620.3732, 1650.4557,
        1576.9540, 1733.9323, 1665.1454, 1682.9066, 1775.9153, 1742.3832,
        1616.6211, 1744.1754, 1745.7690, 1726.6055, 1728.5868, 1744.7954,
        1683.6891, 1596.9648, 1634.2952, 1650.3878, 1546.2791, 1544.0796,
        1518.8993, 1524.7717, 1316.0181, 1328.9803, 1247.8451, 1243.4609,
        1152.9248, 1144.5928, 1068.2622, 1027.5902,  963.9381,  905.8046,
         875.5134,  871.2216,  835.7455,  679.5408,  619.7004,  505.3662,
         552.9171,  438.9823,  460.0653,  427.3357,  382.2378,  374.3900,
         259.8865,  268.2957,  185.3217,  164.6225,  182.0524,  156.0965,
         141.4015,  122.6509,   78.0893,   65.3778,   57.1609,   32.6995,
          19.1002,   52.4813,   29.9505,   38.5356,   44.7086])]
2629.526448789891
3.5170999667302305 9.15836758363561 65.74555521322453
val isze = 8
idinces = [48 54 18 72 77 60 81 66  9 76 78 41 40 24 13 34 53 17  5 12 16 28 64 57
 11 67 39 69 52 50 30 46 42 79 33  3 35 71  8  1 21 32 38 58 47 20 82 36
 23 14 75 19  7 45 29 44 65  2 31 51 59 27 37 73  6  4 49 63  0 61 74 15
 26 22 68 43 80 62 55 70 56 10 25]
we are doing training validation split
training loss = 111.13621520996094 100
val loss = 75.14498901367188
training loss = 13.468852996826172 200
val loss = 6.690767288208008
training loss = 10.917980194091797 300
val loss = 6.463813304901123
training loss = 9.270415306091309 400
val loss = 6.559125900268555
training loss = 8.24135971069336 500
val loss = 6.894103050231934
training loss = 7.611125469207764 600
val loss = 7.318175315856934
training loss = 7.230550289154053 700
val loss = 7.7465057373046875
training loss = 7.003058433532715 800
val loss = 8.135126113891602
training loss = 6.867637634277344 900
val loss = 8.464513778686523
training loss = 6.786409378051758 1000
val loss = 8.72940731048584
training loss = 6.736282825469971 1100
val loss = 8.93253231048584
training loss = 6.703479766845703 1200
val loss = 9.080686569213867
training loss = 6.679989814758301 1300
val loss = 9.182519912719727
training loss = 6.661310195922852 1400
val loss = 9.246947288513184
training loss = 6.644998550415039 1500
val loss = 9.282336235046387
training loss = 6.629789352416992 1600
val loss = 9.295961380004883
training loss = 6.615050315856934 1700
val loss = 9.293893814086914
training loss = 6.600479602813721 1800
val loss = 9.280923843383789
training loss = 6.585945129394531 1900
val loss = 9.260612487792969
training loss = 6.571381092071533 2000
val loss = 9.23556137084961
training loss = 6.556759357452393 2100
val loss = 9.207474708557129
training loss = 6.5420708656311035 2200
val loss = 9.177639961242676
training loss = 6.527308940887451 2300
val loss = 9.146690368652344
training loss = 6.512469291687012 2400
val loss = 9.115129470825195
training loss = 6.497551441192627 2500
val loss = 9.0831298828125
training loss = 6.482562065124512 2600
val loss = 9.050869941711426
training loss = 6.467512130737305 2700
val loss = 9.018394470214844
training loss = 6.452418327331543 2800
val loss = 8.985811233520508
training loss = 6.43731689453125 2900
val loss = 8.95314884185791
training loss = 6.422259330749512 3000
val loss = 8.920516967773438
training loss = 6.407316207885742 3100
val loss = 8.887948036193848
training loss = 6.392607688903809 3200
val loss = 8.855744361877441
training loss = 6.3782854080200195 3300
val loss = 8.824039459228516
training loss = 6.364551067352295 3400
val loss = 8.793256759643555
training loss = 6.3516621589660645 3500
val loss = 8.763628005981445
training loss = 6.3398966789245605 3600
val loss = 8.735624313354492
training loss = 6.329860687255859 3700
val loss = 8.705946922302246
training loss = 6.3214569091796875 3800
val loss = 8.683309555053711
training loss = 6.319120407104492 3900
val loss = 8.617539405822754
training loss = 6.309647560119629 4000
val loss = 8.642549514770508
training loss = 6.307550430297852 4100
val loss = 8.65786361694336
training loss = 6.303127765655518 4200
val loss = 8.612556457519531
training loss = 6.301421642303467 4300
val loss = 8.615410804748535
training loss = 6.299564838409424 4400
val loss = 8.590709686279297
training loss = 6.298480033874512 4500
val loss = 8.574396133422852
training loss = 6.297055721282959 4600
val loss = 8.575665473937988
training loss = 6.309218883514404 4700
val loss = 8.49538803100586
training loss = 6.294400691986084 4800
val loss = 8.564725875854492
training loss = 6.292642116546631 4900
val loss = 8.555910110473633
training loss = 6.293883800506592 5000
val loss = 8.51331901550293
training loss = 6.286259174346924 5100
val loss = 8.538826942443848
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 861.3787,  850.1006,  963.8118,  954.2406,  996.5031, 1069.5398,
        1085.7100, 1098.3719, 1166.7990, 1116.7473, 1193.5233, 1251.9292,
        1249.0597, 1272.3203, 1309.6603, 1381.4799, 1432.1718, 1456.0166,
        1549.9562, 1541.8759, 1573.2357, 1543.6323, 1575.9500, 1559.6919,
        1753.4924, 1742.9363, 1608.3177, 1719.8536, 1783.6439, 1637.7844,
        1666.4091, 1785.9683, 1688.6785, 1703.5393, 1703.5822, 1726.5409,
        1733.6881, 1557.7961, 1660.1262, 1561.3542, 1613.6697, 1543.4349,
        1438.4235, 1484.4420, 1324.0471, 1358.3712, 1355.1110, 1246.6180,
        1164.5309, 1209.4432, 1090.0751, 1001.9217, 1012.4205,  895.9316,
         892.4507,  856.0793,  830.6523,  715.8690,  572.6116,  516.9611,
         531.3641,  472.8512,  479.2283,  411.9707,  352.4448,  347.5747,
         262.1320,  276.9584,  218.8493,  180.2917,  147.0377,  136.8977,
         148.7837,  111.7342,   87.5624,   59.8346,   57.0935,   49.2383,
          30.1857,   46.5385,   20.0477,   29.1443,   41.5997])]
2607.9720292934417
0.5931114713459518 4.257795698505307 67.71055815944739
val isze = 8
idinces = [67 17 75 39 63 31 79  4 25 74  3 32 56 78 11  2 70 19 22 61 72 48 16 38
 58 80 14 23 77 26 46 65  1 55 50 76 42 57 60 66 10 49 12 59 41 28 54 20
 34 35 40 69 68 37 82 33 29  6 15 64  8  7 47 62 45 71 43 51  0 44 13 21
 24 52 73 53 18 30  5 27 81 36  9]
we are doing training validation split
training loss = 20.467477798461914 100
val loss = 20.47786521911621
training loss = 16.119049072265625 200
val loss = 17.457122802734375
training loss = 13.432934761047363 300
val loss = 14.565709114074707
training loss = 11.289015769958496 400
val loss = 12.341998100280762
training loss = 9.655410766601562 500
val loss = 10.739027976989746
training loss = 8.436205863952637 600
val loss = 9.631819725036621
training loss = 7.535971641540527 700
val loss = 8.896829605102539
training loss = 6.875805854797363 800
val loss = 8.432382583618164
training loss = 6.394413948059082 900
val loss = 8.160116195678711
training loss = 6.045266151428223 1000
val loss = 8.021268844604492
training loss = 5.793476581573486 1100
val loss = 7.972562789916992
training loss = 5.61314058303833 1200
val loss = 7.982466220855713
training loss = 5.484971046447754 1300
val loss = 8.027801513671875
training loss = 5.394354343414307 1400
val loss = 8.091806411743164
training loss = 5.330225944519043 1500
val loss = 8.162320137023926
training loss = 5.284307956695557 1600
val loss = 8.23106575012207
training loss = 5.250448703765869 1700
val loss = 8.292404174804688
training loss = 5.224050521850586 1800
val loss = 8.342653274536133
training loss = 5.201615333557129 1900
val loss = 8.379349708557129
training loss = 5.180383205413818 2000
val loss = 8.40057373046875
training loss = 5.158020973205566 2100
val loss = 8.404824256896973
training loss = 5.132404804229736 2200
val loss = 8.390259742736816
training loss = 5.101500988006592 2300
val loss = 8.355005264282227
training loss = 5.063392639160156 2400
val loss = 8.297210693359375
training loss = 5.016463279724121 2500
val loss = 8.216205596923828
training loss = 4.959562301635742 2600
val loss = 8.113639831542969
training loss = 4.89165735244751 2700
val loss = 7.993036270141602
training loss = 4.810874938964844 2800
val loss = 7.856479644775391
training loss = 4.713572978973389 2900
val loss = 7.701005935668945
training loss = 4.593937873840332 3000
val loss = 7.516502857208252
training loss = 4.4436750411987305 3100
val loss = 7.286433696746826
training loss = 4.251217365264893 3200
val loss = 6.989057540893555
training loss = 3.999386787414551 3300
val loss = 6.596347808837891
training loss = 3.6598618030548096 3400
val loss = 6.068663597106934
training loss = 3.18989896774292 3500
val loss = 5.344614028930664
training loss = 2.6139965057373047 3600
val loss = 4.4996466636657715
training loss = 2.1240503787994385 3700
val loss = 3.5450901985168457
training loss = 1.896311640739441 3800
val loss = 3.075141668319702
training loss = 1.8389836549758911 3900
val loss = 2.91033673286438
training loss = 1.8261491060256958 4000
val loss = 2.85603928565979
training loss = 1.8525562286376953 4100
val loss = 2.9334936141967773
training loss = 1.822357177734375 4200
val loss = 2.827584743499756
training loss = 1.821958065032959 4300
val loss = 2.8220534324645996
training loss = 1.822670340538025 4400
val loss = 2.802445888519287
training loss = 1.8207882642745972 4500
val loss = 2.8095784187316895
training loss = 1.8687565326690674 4600
val loss = 2.9426159858703613
training loss = 1.8178670406341553 4700
val loss = 2.7912163734436035
training loss = 1.8154296875 4800
val loss = 2.783374309539795
training loss = 1.8122600317001343 4900
val loss = 2.7643957138061523
training loss = 1.8081555366516113 5000
val loss = 2.7529988288879395
training loss = 1.8052834272384644 5100
val loss = 2.7588062286376953
training loss = 1.7979429960250854 5200
val loss = 2.7205893993377686
training loss = 1.8429179191589355 5300
val loss = 2.637834310531616
training loss = 1.7856205701828003 5400
val loss = 2.685948371887207
training loss = 1.7790495157241821 5500
val loss = 2.6690845489501953
training loss = 1.7727807760238647 5600
val loss = 2.654818058013916
training loss = 1.766648769378662 5700
val loss = 2.638399124145508
training loss = 1.7881160974502563 5800
val loss = 2.564534902572632
training loss = 1.7560455799102783 5900
val loss = 2.615609645843506
training loss = 1.7513052225112915 6000
val loss = 2.60205340385437
training loss = 1.7478317022323608 6100
val loss = 2.607445240020752
training loss = 1.7432512044906616 6200
val loss = 2.5816855430603027
training loss = 1.7396464347839355 6300
val loss = 2.5737550258636475
training loss = 1.7367336750030518 6400
val loss = 2.5588696002960205
training loss = 1.7335317134857178 6500
val loss = 2.5585813522338867
training loss = 1.7934339046478271 6600
val loss = 2.7362992763519287
training loss = 1.7283748388290405 6700
val loss = 2.5456411838531494
training loss = 1.7259949445724487 6800
val loss = 2.5380706787109375
training loss = 1.7243499755859375 6900
val loss = 2.523726463317871
training loss = 1.7220534086227417 7000
val loss = 2.5275325775146484
training loss = 1.797524094581604 7100
val loss = 2.436577320098877
training loss = 1.7186756134033203 7200
val loss = 2.5166749954223633
training loss = 1.717093586921692 7300
val loss = 2.51344895362854
training loss = 1.7162903547286987 7400
val loss = 2.5218558311462402
training loss = 1.7144792079925537 7500
val loss = 2.506382465362549
training loss = 1.713261365890503 7600
val loss = 2.505458354949951
training loss = 1.7123087644577026 7700
val loss = 2.4949283599853516
training loss = 1.7111378908157349 7800
val loss = 2.4969966411590576
training loss = 1.7332054376602173 7900
val loss = 2.593651533126831
training loss = 1.7093505859375 8000
val loss = 2.492487907409668
training loss = 1.7084684371948242 8100
val loss = 2.4902830123901367
training loss = 1.7095123529434204 8200
val loss = 2.46848726272583
training loss = 1.7069755792617798 8300
val loss = 2.487325668334961
training loss = 1.8678780794143677 8400
val loss = 2.4048867225646973
training loss = 1.7056677341461182 8500
val loss = 2.4861645698547363
training loss = 1.7049968242645264 8600
val loss = 2.483643054962158
training loss = 1.7065397500991821 8700
val loss = 2.5102062225341797
training loss = 1.7038679122924805 8800
val loss = 2.4826083183288574
training loss = 1.758976936340332 8900
val loss = 2.6547513008117676
training loss = 1.7028343677520752 9000
val loss = 2.481375217437744
training loss = 1.7022995948791504 9100
val loss = 2.481241226196289
training loss = 1.7024579048156738 9200
val loss = 2.494176149368286
training loss = 1.701393723487854 9300
val loss = 2.4817862510681152
training loss = 1.700911521911621 9400
val loss = 2.481546401977539
training loss = 1.7009482383728027 9500
val loss = 2.4711904525756836
training loss = 1.7000845670700073 9600
val loss = 2.482372522354126
training loss = 1.9933594465255737 9700
val loss = 3.015181303024292
training loss = 1.6993048191070557 9800
val loss = 2.485081911087036
training loss = 1.698870301246643 9900
val loss = 2.4842538833618164
training loss = 1.7018431425094604 10000
val loss = 2.457965850830078
training loss = 1.6981490850448608 10100
val loss = 2.486661195755005
training loss = 1.697738766670227 10200
val loss = 2.4869842529296875
training loss = 1.6974420547485352 10300
val loss = 2.4912800788879395
training loss = 1.6970099210739136 10400
val loss = 2.4894633293151855
training loss = 1.7065422534942627 10500
val loss = 2.4443750381469727
training loss = 1.6962863206863403 10600
val loss = 2.492239475250244
training loss = 1.6958732604980469 10700
val loss = 2.4918949604034424
training loss = 1.6956515312194824 10800
val loss = 2.5014185905456543
training loss = 1.6950947046279907 10900
val loss = 2.49670672416687
training loss = 1.7160195112228394 11000
val loss = 2.595059633255005
training loss = 1.6942604780197144 11100
val loss = 2.499398708343506
training loss = 1.6937845945358276 11200
val loss = 2.503767728805542
training loss = 1.693332552909851 11300
val loss = 2.501978874206543
training loss = 1.692749261856079 11400
val loss = 2.502577304840088
training loss = 1.7172375917434692 11500
val loss = 2.433197498321533
training loss = 1.6915959119796753 11600
val loss = 2.5040061473846436
training loss = 1.6908456087112427 11700
val loss = 2.5022904872894287
training loss = 1.693310260772705 11800
val loss = 2.5356125831604004
training loss = 1.6893551349639893 11900
val loss = 2.499152660369873
training loss = 1.6883869171142578 12000
val loss = 2.495285987854004
training loss = 1.6879528760910034 12100
val loss = 2.481635808944702
training loss = 1.6865438222885132 12200
val loss = 2.486299514770508
training loss = 1.6854150295257568 12300
val loss = 2.4789137840270996
training loss = 1.6850308179855347 12400
val loss = 2.484759569168091
training loss = 1.683388352394104 12500
val loss = 2.4633493423461914
training loss = 1.7202050685882568 12600
val loss = 2.587775468826294
training loss = 1.681628942489624 12700
val loss = 2.446866512298584
training loss = 1.6807588338851929 12800
val loss = 2.438711166381836
training loss = 1.6812341213226318 12900
val loss = 2.451076030731201
training loss = 1.6795263290405273 13000
val loss = 2.4257872104644775
training loss = 2.1304972171783447 13100
val loss = 3.1431689262390137
training loss = 1.6785780191421509 13200
val loss = 2.4153482913970947
training loss = 1.6780906915664673 13300
val loss = 2.410384178161621
training loss = 1.6817952394485474 13400
val loss = 2.4455137252807617
training loss = 1.6773704290390015 13500
val loss = 2.403754234313965
training loss = 1.738562822341919 13600
val loss = 2.31575870513916
training loss = 1.6767204999923706 13700
val loss = 2.398564100265503
training loss = 1.6763745546340942 13800
val loss = 2.3981425762176514
training loss = 1.6761789321899414 13900
val loss = 2.3904237747192383
training loss = 1.6757603883743286 14000
val loss = 2.391723394393921
training loss = 1.6897757053375244 14100
val loss = 2.464986801147461
training loss = 1.675181269645691 14200
val loss = 2.388235092163086
training loss = 1.6941546201705933 14300
val loss = 2.3250932693481445
training loss = 1.6746342182159424 14400
val loss = 2.388120174407959
training loss = 1.6742527484893799 14500
val loss = 2.382622241973877
training loss = 1.6759482622146606 14600
val loss = 2.359600067138672
training loss = 1.6736940145492554 14700
val loss = 2.380012035369873
training loss = 1.6787941455841064 14800
val loss = 2.4215564727783203
training loss = 1.6731210947036743 14900
val loss = 2.3769073486328125
training loss = 1.6727662086486816 15000
val loss = 2.374375104904175
training loss = 1.672835111618042 15100
val loss = 2.364724636077881
training loss = 1.6721891164779663 15200
val loss = 2.371392250061035
training loss = 1.684140920639038 15300
val loss = 2.3226819038391113
training loss = 1.671612024307251 15400
val loss = 2.3677520751953125
training loss = 1.6713776588439941 15500
val loss = 2.3717923164367676
training loss = 1.6710718870162964 15600
val loss = 2.3615736961364746
training loss = 1.6706653833389282 15700
val loss = 2.362703561782837
training loss = 1.6714345216751099 15800
val loss = 2.3457963466644287
training loss = 1.6700878143310547 15900
val loss = 2.359924077987671
training loss = 1.7669973373413086 16000
val loss = 2.6121809482574463
training loss = 1.6695153713226318 16100
val loss = 2.356688976287842
training loss = 1.6691519021987915 16200
val loss = 2.353982448577881
training loss = 1.669866681098938 16300
val loss = 2.3711585998535156
training loss = 1.6685616970062256 16400
val loss = 2.351259231567383
training loss = 1.6683176755905151 16500
val loss = 2.3497214317321777
training loss = 1.6679699420928955 16600
val loss = 2.348594903945923
training loss = 1.8040125370025635 16700
val loss = 2.252176284790039
training loss = 1.6673810482025146 16800
val loss = 2.344695806503296
training loss = 1.667013168334961 16900
val loss = 2.342451810836792
training loss = 1.6668072938919067 17000
val loss = 2.341444492340088
training loss = 1.6664447784423828 17100
val loss = 2.3399367332458496
training loss = 1.6663190126419067 17200
val loss = 2.328599214553833
training loss = 1.6658633947372437 17300
val loss = 2.337665557861328
training loss = 1.6654942035675049 17400
val loss = 2.3345816135406494
training loss = 1.6662168502807617 17500
val loss = 2.351180076599121
training loss = 1.664905071258545 17600
val loss = 2.3315820693969727
training loss = 1.664565086364746 17700
val loss = 2.3258323669433594
training loss = 1.6643613576889038 17800
val loss = 2.3333444595336914
training loss = 1.6639326810836792 17900
val loss = 2.3266825675964355
training loss = 1.6684575080871582 18000
val loss = 2.3676915168762207
training loss = 1.6633189916610718 18100
val loss = 2.3238625526428223
training loss = 1.670668601989746 18200
val loss = 2.3733596801757812
training loss = 1.6627259254455566 18300
val loss = 2.3227438926696777
training loss = 1.6623404026031494 18400
val loss = 2.3195648193359375
training loss = 1.662256121635437 18500
val loss = 2.3261618614196777
training loss = 1.6617209911346436 18600
val loss = 2.316678047180176
training loss = 1.6614996194839478 18700
val loss = 2.316645622253418
training loss = 1.6611344814300537 18800
val loss = 2.315326690673828
training loss = 1.6607446670532227 18900
val loss = 2.3127360343933105
training loss = 1.6608999967575073 19000
val loss = 2.30233097076416
training loss = 1.660140037536621 19100
val loss = 2.310669422149658
training loss = 1.6804739236831665 19200
val loss = 2.2497811317443848
training loss = 1.659527063369751 19300
val loss = 2.308468818664551
training loss = 1.6591389179229736 19400
val loss = 2.304921865463257
training loss = 1.659071683883667 19500
val loss = 2.299344062805176
training loss = 1.6585047245025635 19600
val loss = 2.304656744003296
training loss = 1.6585661172866821 19700
val loss = 2.2952466011047363
training loss = 1.6578822135925293 19800
val loss = 2.3034348487854004
training loss = 2.1926372051239014 19900
val loss = 3.1380226612091064
training loss = 1.6573084592819214 20000
val loss = 2.304635763168335
training loss = 1.6568937301635742 20100
val loss = 2.300569772720337
training loss = 1.6809816360473633 20200
val loss = 2.232480049133301
training loss = 1.6562848091125488 20300
val loss = 2.2990541458129883
training loss = 1.655881643295288 20400
val loss = 2.297041893005371
training loss = 1.6556522846221924 20500
val loss = 2.297461986541748
training loss = 1.6552470922470093 20600
val loss = 2.2971127033233643
training loss = 1.6582658290863037 20700
val loss = 2.266780376434326
training loss = 1.6545950174331665 20800
val loss = 2.296333074569702
training loss = 1.6543735265731812 20900
val loss = 2.3013997077941895
training loss = 1.6539618968963623 21000
val loss = 2.295769691467285
training loss = 1.8826054334640503 21100
val loss = 2.750429391860962
training loss = 1.6533386707305908 21200
val loss = 2.297197103500366
training loss = 1.652924656867981 21300
val loss = 2.294239044189453
training loss = 1.652718186378479 21400
val loss = 2.297773838043213
training loss = 1.6523059606552124 21500
val loss = 2.29498028755188
training loss = 1.6527752876281738 21600
val loss = 2.3104729652404785
training loss = 1.6516852378845215 21700
val loss = 2.295686721801758
training loss = 1.6547198295593262 21800
val loss = 2.2630999088287354
training loss = 1.6511216163635254 21900
val loss = 2.2930498123168945
training loss = 1.6506911516189575 22000
val loss = 2.295712947845459
training loss = 1.6668888330459595 22100
val loss = 2.388054132461548
training loss = 1.6500738859176636 22200
val loss = 2.2973201274871826
training loss = 1.7408044338226318 22300
val loss = 2.5468482971191406
training loss = 1.649513840675354 22400
val loss = 2.2988967895507812
training loss = 1.649117350578308 22500
val loss = 2.2982325553894043
training loss = 1.6532771587371826 22600
val loss = 2.342550277709961
training loss = 1.648559808731079 22700
val loss = 2.2991456985473633
training loss = 1.648164987564087 22800
val loss = 2.300065517425537
training loss = 1.64882493019104 22900
val loss = 2.3205697536468506
training loss = 1.6475701332092285 23000
val loss = 2.3021018505096436
training loss = 1.6492063999176025 23100
val loss = 2.280658483505249
training loss = 1.6470047235488892 23200
val loss = 2.304579257965088
training loss = 2.1166181564331055 23300
val loss = 3.0931854248046875
training loss = 1.646478295326233 23400
val loss = 2.308211326599121
training loss = 1.6461024284362793 23500
val loss = 2.3081014156341553
training loss = 1.6465411186218262 23600
val loss = 2.326870918273926
training loss = 1.645564079284668 23700
val loss = 2.3113174438476562
training loss = 1.6542202234268188 23800
val loss = 2.2652535438537598
training loss = 1.6450365781784058 23900
val loss = 2.3148913383483887
training loss = 1.8001961708068848 24000
val loss = 2.197498321533203
training loss = 1.6445218324661255 24100
val loss = 2.3185112476348877
training loss = 1.6442490816116333 24200
val loss = 2.3134920597076416
training loss = 1.6441560983657837 24300
val loss = 2.31549072265625
training loss = 1.6437134742736816 24400
val loss = 2.3232622146606445
training loss = 1.6540658473968506 24500
val loss = 2.270739793777466
training loss = 1.643264651298523 24600
val loss = 2.326812744140625
training loss = 1.6443605422973633 24700
val loss = 2.358999490737915
training loss = 1.6428371667861938 24800
val loss = 2.3336071968078613
training loss = 1.6425622701644897 24900
val loss = 2.32857084274292
training loss = 1.6424849033355713 25000
val loss = 2.3410439491271973
training loss = 1.6421196460723877 25100
val loss = 2.3378124237060547
training loss = 1.6424014568328857 25200
val loss = 2.3535966873168945
training loss = 1.6417248249053955 25300
val loss = 2.3428897857666016
training loss = 1.646671175956726 25400
val loss = 2.3943982124328613
training loss = 1.6414003372192383 25500
val loss = 2.348041534423828
training loss = 1.6411446332931519 25600
val loss = 2.347123146057129
training loss = 1.6412005424499512 25700
val loss = 2.3598265647888184
training loss = 1.64082932472229 25800
val loss = 2.3551747798919678
training loss = 1.6421818733215332 25900
val loss = 2.3826677799224854
training loss = 1.640560507774353 26000
val loss = 2.3602542877197266
training loss = 1.644344687461853 26100
val loss = 2.324760913848877
training loss = 1.6403419971466064 26200
val loss = 2.3620572090148926
training loss = 1.640067219734192 26300
val loss = 2.3686182498931885
training loss = 1.6426230669021606 26400
val loss = 2.3423819541931152
training loss = 1.63991117477417 26500
val loss = 2.3747692108154297
training loss = 1.6397011280059814 26600
val loss = 2.37721586227417
training loss = 1.6406893730163574 26700
val loss = 2.4016215801239014
training loss = 1.639519453048706 26800
val loss = 2.3829259872436523
training loss = 1.6794564723968506 26900
val loss = 2.551039695739746
training loss = 1.639374852180481 27000
val loss = 2.389338254928589
training loss = 1.6395432949066162 27100
val loss = 2.4040780067443848
training loss = 1.639313817024231 27200
val loss = 2.388242483139038
training loss = 1.639055848121643 27300
val loss = 2.3975298404693604
reduced chi^2 level 2 = 1.6390546560287476
Constrained alpha: 1.8845913410186768
Constrained beta: 2.0180423259735107
Constrained gamma: 13.300640106201172
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 866.3932,  864.8334,  977.7844,  957.5731,  983.8975, 1076.2555,
        1157.0012, 1174.9902, 1125.6913, 1188.0294, 1236.2377, 1176.3834,
        1186.9183, 1289.7030, 1325.9729, 1444.5743, 1379.3196, 1488.7963,
        1521.7518, 1501.6742, 1519.4596, 1534.7909, 1589.6334, 1625.3334,
        1647.4658, 1692.2141, 1640.2251, 1718.5680, 1775.2483, 1743.9880,
        1692.6852, 1659.1631, 1706.4745, 1739.6831, 1758.5387, 1787.0542,
        1706.9764, 1670.5699, 1629.0557, 1602.4258, 1677.1333, 1558.0840,
        1502.8193, 1474.2388, 1345.1405, 1356.0369, 1218.6725, 1218.4020,
        1186.2198, 1136.9973, 1141.0941,  989.8472,  962.8004,  950.1582,
         903.1782,  862.4423,  868.7057,  702.7986,  610.2952,  519.7918,
         560.8306,  470.3546,  428.0845,  370.7784,  376.4634,  351.5283,
         289.2918,  246.4678,  170.7696,  131.9377,  148.5679,  140.4454,
         133.1356,   93.1709,  102.0165,   73.4854,   53.5133,   47.3958,
          40.2090,   51.0839,   19.6436,   47.0753,   23.1329])]
2759.829505943609
1.3368130370287283 10.217167567496011 85.0195046207066
val isze = 8
idinces = [31 24 13 41  2 21 57  9 39 72 18 76 61 60 19 20 40 79 54 52 33 10 27 48
 77 81 45 69 32 36  7 65 34  0 23 17 58 47 35 44 50 30 29 78 28 75 43 71
 80  1  8 70 64 55 26  6  5 56  4 82 37 42 22 68  3 46 66 15 11 16 53 25
 14 62 51 63 73 59 67 74 12 49 38]
we are doing training validation split
training loss = 15.079391479492188 100
val loss = 7.933315277099609
training loss = 7.1528639793396 200
val loss = 1.355858325958252
training loss = 7.138524055480957 300
val loss = 1.3256397247314453
training loss = 7.122854232788086 400
val loss = 1.291996955871582
training loss = 7.103649139404297 500
val loss = 1.2458000183105469
training loss = 7.082641124725342 600
val loss = 1.1926178932189941
training loss = 7.06344747543335 700
val loss = 1.141791820526123
training loss = 7.045770168304443 800
val loss = 1.1045327186584473
training loss = 7.028885841369629 900
val loss = 1.073394775390625
training loss = 7.0122151374816895 1000
val loss = 1.0496565103530884
training loss = 6.995347023010254 1100
val loss = 1.0316340923309326
training loss = 6.978926181793213 1200
val loss = 1.0116440057754517
training loss = 6.96014404296875 1300
val loss = 1.0062005519866943
training loss = 6.941638946533203 1400
val loss = 0.9974815845489502
training loss = 6.92275333404541 1500
val loss = 0.98738694190979
training loss = 6.902806758880615 1600
val loss = 0.9846534729003906
training loss = 6.882586479187012 1700
val loss = 0.9791604280471802
training loss = 6.86220645904541 1800
val loss = 0.9709370136260986
training loss = 6.840853214263916 1900
val loss = 0.9698901176452637
training loss = 6.819497585296631 2000
val loss = 0.9662071466445923
training loss = 6.797965049743652 2100
val loss = 0.9638583660125732
training loss = 6.7760009765625 2200
val loss = 0.9575943946838379
training loss = 6.754508018493652 2300
val loss = 0.948384165763855
training loss = 6.731245517730713 2400
val loss = 0.9497016072273254
training loss = 6.707901477813721 2500
val loss = 0.9447201490402222
training loss = 6.683032989501953 2600
val loss = 0.9415246844291687
training loss = 6.6553497314453125 2700
val loss = 0.9368431568145752
training loss = 6.624380111694336 2800
val loss = 0.9477048516273499
training loss = 6.574744701385498 2900
val loss = 0.9241287708282471
training loss = 6.494598865509033 3000
val loss = 0.9089703559875488
training loss = 6.331203937530518 3100
val loss = 0.8954383134841919
training loss = 6.025154113769531 3200
val loss = 0.8629653453826904
training loss = 5.540078639984131 3300
val loss = 0.7482818365097046
training loss = 4.658748626708984 3400
val loss = 0.5266683101654053
training loss = 3.3156497478485107 3500
val loss = 0.27357786893844604
training loss = 2.631937026977539 3600
val loss = 0.3186473250389099
training loss = 2.5744502544403076 3700
val loss = 0.4152962863445282
training loss = 2.5696041584014893 3800
val loss = 0.43418073654174805
training loss = 2.567134380340576 3900
val loss = 0.4358729124069214
training loss = 2.563901901245117 4000
val loss = 0.4462687373161316
training loss = 2.562166929244995 4100
val loss = 0.4569951891899109
training loss = 2.5595221519470215 4200
val loss = 0.4505164325237274
training loss = 2.5575008392333984 4300
val loss = 0.4518909454345703
training loss = 2.555739402770996 4400
val loss = 0.45597904920578003
training loss = 2.553628921508789 4500
val loss = 0.4525115489959717
training loss = 2.5531070232391357 4600
val loss = 0.44371750950813293
training loss = 2.5494747161865234 4700
val loss = 0.45211657881736755
training loss = 2.7045741081237793 4800
val loss = 0.5498311519622803
training loss = 2.544522762298584 4900
val loss = 0.4512586295604706
training loss = 2.541651487350464 5000
val loss = 0.4514462649822235
training loss = 2.5417213439941406 5100
val loss = 0.43937480449676514
training loss = 2.5349643230438232 5200
val loss = 0.45242711901664734
training loss = 2.777768611907959 5300
val loss = 0.6395199298858643
training loss = 2.526909589767456 5400
val loss = 0.4549151659011841
training loss = 2.522329568862915 5500
val loss = 0.45644611120224
training loss = 2.518249750137329 5600
val loss = 0.4692387282848358
training loss = 2.5122365951538086 5700
val loss = 0.46173539757728577
training loss = 2.547405481338501 5800
val loss = 0.4568977355957031
training loss = 2.5006003379821777 5900
val loss = 0.46950069069862366
training loss = 2.494185209274292 6000
val loss = 0.47239401936531067
training loss = 2.4882144927978516 6100
val loss = 0.46896371245384216
training loss = 2.480496883392334 6200
val loss = 0.48127490282058716
training loss = 2.6023030281066895 6300
val loss = 0.5422384738922119
training loss = 2.465512752532959 6400
val loss = 0.49078240990638733
training loss = 2.4575088024139404 6500
val loss = 0.49633657932281494
training loss = 2.4499728679656982 6600
val loss = 0.5106757879257202
training loss = 2.441338539123535 6700
val loss = 0.5076543092727661
training loss = 2.6137900352478027 6800
val loss = 0.6066654324531555
training loss = 2.4244556427001953 6900
val loss = 0.5203798413276672
training loss = 2.415792465209961 7000
val loss = 0.5259966254234314
training loss = 2.408367156982422 7100
val loss = 0.5451933741569519
training loss = 2.3994054794311523 7200
val loss = 0.5385892391204834
training loss = 2.3910324573516846 7300
val loss = 0.5452459454536438
training loss = 2.3856072425842285 7400
val loss = 0.5340331196784973
training loss = 2.374793291091919 7500
val loss = 0.5585634708404541
training loss = 2.452043294906616 7600
val loss = 0.5603375434875488
training loss = 2.3593828678131104 7700
val loss = 0.5728147029876709
training loss = 2.3518640995025635 7800
val loss = 0.5790884494781494
training loss = 2.3446524143218994 7900
val loss = 0.5889302492141724
training loss = 2.3376641273498535 8000
val loss = 0.5929438471794128
training loss = 2.3856449127197266 8100
val loss = 0.5688115358352661
training loss = 2.3242359161376953 8200
val loss = 0.6055054664611816
training loss = 2.3176791667938232 8300
val loss = 0.6131114959716797
training loss = 2.311899423599243 8400
val loss = 0.6297216415405273
training loss = 2.305454730987549 8500
val loss = 0.6266284584999084
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 875.4371,  874.1467,  975.7991,  955.5394, 1022.1592, 1088.6058,
        1051.0759, 1169.8066, 1152.6356, 1170.7870, 1236.2307, 1168.8781,
        1222.6809, 1161.6613, 1402.0559, 1423.8390, 1374.8937, 1447.4293,
        1589.6758, 1504.3278, 1579.9387, 1538.7708, 1609.7866, 1601.3063,
        1624.8156, 1673.3820, 1595.6305, 1721.2670, 1799.9486, 1697.9827,
        1683.0796, 1822.0012, 1692.5355, 1795.0356, 1681.6683, 1704.6047,
        1756.7611, 1707.8920, 1578.2988, 1604.5387, 1617.2035, 1510.0114,
        1542.0020, 1497.6533, 1361.1610, 1324.4926, 1240.1497, 1274.0376,
        1130.9922, 1223.5367, 1130.6770,  952.2972,  955.8640,  924.9395,
         894.2811,  856.5000,  739.6922,  685.4885,  661.8054,  550.5562,
         564.6964,  452.8333,  415.4050,  381.9397,  377.5105,  341.3229,
         305.4930,  264.1316,  197.3102,  162.7469,  152.8011,  168.1148,
         139.0170,  111.0988,   98.1055,   75.0996,   53.0266,   44.6126,
          39.0979,   42.5013,   21.8431,   31.1169,   31.8282])]
2862.952766291898
1.7575980917764595 3.1885726133562264 61.47204305760893
val isze = 8
idinces = [27 11 43 13 74 46 23 33 76 73 70 72 26  0 31 29 52 16 28 20 36 22 75  6
 57 42 51 55 82 47 65 79  5 45 63 12 30 35 67 68 21 71 17 81 77 32  1  8
 24  3 54 49  2 44 69 59 58 80 14 34  9 66 64  4 78 48 19 61 38 15 50 62
 37 56 10 41  7 25 18 53 39 60 40]
we are doing training validation split
training loss = 28.831933975219727 100
val loss = 35.54192352294922
training loss = 22.404945373535156 200
val loss = 28.40003204345703
training loss = 17.3595027923584 300
val loss = 22.732765197753906
training loss = 13.687006950378418 400
val loss = 18.597396850585938
training loss = 11.085663795471191 500
val loss = 15.68010139465332
training loss = 9.262483596801758 600
val loss = 13.64714527130127
training loss = 7.994121074676514 700
val loss = 12.237899780273438
training loss = 7.117886066436768 800
val loss = 11.262823104858398
training loss = 6.51617956161499 900
val loss = 10.587376594543457
training loss = 6.10420036315918 1000
val loss = 10.11705207824707
training loss = 5.820900917053223 1100
val loss = 9.785654067993164
training loss = 5.6221842765808105 1200
val loss = 9.54664134979248
training loss = 5.475886344909668 1300
val loss = 9.366689682006836
training loss = 5.358049392700195 1400
val loss = 9.221319198608398
training loss = 5.250314235687256 1500
val loss = 9.091670989990234
training loss = 5.138369083404541 1600
val loss = 8.962724685668945
training loss = 5.01158332824707 1700
val loss = 8.82260513305664
training loss = 4.863636016845703 1800
val loss = 8.663101196289062
training loss = 4.692492961883545 1900
val loss = 8.47982406616211
training loss = 4.4977898597717285 2000
val loss = 8.269842147827148
training loss = 4.27779483795166 2100
val loss = 8.029250144958496
training loss = 4.029597759246826 2200
val loss = 7.753690719604492
training loss = 3.7515170574188232 2300
val loss = 7.440523147583008
training loss = 3.445586919784546 2400
val loss = 7.090226173400879
training loss = 3.1200363636016846 2500
val loss = 6.708353519439697
training loss = 2.791179895401001 2600
val loss = 6.307957172393799
training loss = 2.4827446937561035 2700
val loss = 5.909731864929199
training loss = 2.2205615043640137 2800
val loss = 5.539130687713623
training loss = 2.0231151580810547 2900
val loss = 5.219616889953613
training loss = 1.8933573961257935 3000
val loss = 4.965421676635742
training loss = 1.8188973665237427 3100
val loss = 4.778031349182129
training loss = 1.7806317806243896 3200
val loss = 4.648467540740967
training loss = 1.761976957321167 3300
val loss = 4.562995433807373
training loss = 1.7525358200073242 3400
val loss = 4.507923126220703
training loss = 1.7470728158950806 3500
val loss = 4.4724955558776855
training loss = 1.7432961463928223 3600
val loss = 4.449056148529053
training loss = 1.7402877807617188 3700
val loss = 4.432878017425537
training loss = 1.7376867532730103 3800
val loss = 4.420912265777588
training loss = 1.7353569269180298 3900
val loss = 4.41141414642334
training loss = 1.733233094215393 4000
val loss = 4.403364181518555
training loss = 1.7312906980514526 4100
val loss = 4.396198272705078
training loss = 1.7295178174972534 4200
val loss = 4.3897271156311035
training loss = 1.7278997898101807 4300
val loss = 4.3838276863098145
training loss = 1.7264368534088135 4400
val loss = 4.378335475921631
training loss = 1.7251198291778564 4500
val loss = 4.372945785522461
training loss = 1.7241058349609375 4600
val loss = 4.397061824798584
training loss = 1.722630262374878 4700
val loss = 4.363616466522217
training loss = 1.731046438217163 4800
val loss = 4.209108352661133
training loss = 1.7203218936920166 4900
val loss = 4.355400562286377
training loss = 1.7193278074264526 5000
val loss = 4.343716144561768
training loss = 1.7182334661483765 5100
val loss = 4.336513996124268
training loss = 1.7172116041183472 5200
val loss = 4.340947151184082
training loss = 1.7162253856658936 5300
val loss = 4.346347332000732
training loss = 1.715213656425476 5400
val loss = 4.3317646980285645
training loss = 1.7354533672332764 5500
val loss = 4.603592872619629
training loss = 1.7133463621139526 5600
val loss = 4.325580596923828
training loss = 1.7125366926193237 5700
val loss = 4.319549083709717
training loss = 1.711667537689209 5800
val loss = 4.333422660827637
training loss = 1.7107025384902954 5900
val loss = 4.311450004577637
training loss = 1.7099908590316772 6000
val loss = 4.309648036956787
training loss = 1.709194540977478 6100
val loss = 4.298191070556641
training loss = 1.708480715751648 6200
val loss = 4.300509929656982
training loss = 1.7416024208068848 6300
val loss = 4.034779071807861
training loss = 1.707107424736023 6400
val loss = 4.294991970062256
training loss = 1.706541657447815 6500
val loss = 4.291667938232422
training loss = 1.7068978548049927 6600
val loss = 4.342028617858887
training loss = 1.705420732498169 6700
val loss = 4.285630226135254
training loss = 1.7436485290527344 6800
val loss = 4.0068359375
training loss = 1.7043945789337158 6900
val loss = 4.282083511352539
training loss = 1.703989028930664 7000
val loss = 4.278617858886719
training loss = 1.7052465677261353 7100
val loss = 4.348163604736328
training loss = 1.7031394243240356 7200
val loss = 4.273998737335205
training loss = 1.7028627395629883 7300
val loss = 4.263765811920166
training loss = 1.7024295330047607 7400
val loss = 4.276978969573975
training loss = 1.7021294832229614 7500
val loss = 4.269521713256836
training loss = 1.7162353992462158 7600
val loss = 4.487326145172119
training loss = 1.7015315294265747 7700
val loss = 4.265659809112549
training loss = 1.701317548751831 7800
val loss = 4.264456748962402
training loss = 1.7012206315994263 7900
val loss = 4.241305351257324
training loss = 1.7008136510849 8000
val loss = 4.263543605804443
training loss = 1.704557180404663 8100
val loss = 4.164051532745361
training loss = 1.7003852128982544 8200
val loss = 4.2613749504089355
training loss = 1.7002407312393188 8300
val loss = 4.260855674743652
training loss = 1.700812816619873 8400
val loss = 4.214805603027344
training loss = 1.6998661756515503 8500
val loss = 4.2597761154174805
training loss = 1.6997981071472168 8600
val loss = 4.27040958404541
training loss = 1.699570894241333 8700
val loss = 4.265954494476318
training loss = 1.6994514465332031 8800
val loss = 4.257991790771484
training loss = 1.7176339626312256 8900
val loss = 4.502676963806152
training loss = 1.6991745233535767 9000
val loss = 4.2563018798828125
training loss = 1.6990981101989746 9100
val loss = 4.256998538970947
training loss = 1.699352741241455 9200
val loss = 4.22509765625
training loss = 1.6988741159439087 9300
val loss = 4.257055282592773
training loss = 1.6988381147384644 9400
val loss = 4.263470649719238
training loss = 1.698686957359314 9500
val loss = 4.252788066864014
training loss = 1.6986353397369385 9600
val loss = 4.255479335784912
training loss = 1.7070626020431519 9700
val loss = 4.418751239776611
training loss = 1.698451280593872 9800
val loss = 4.2542219161987305
training loss = 1.69841730594635 9900
val loss = 4.254838466644287
training loss = 1.6993122100830078 10000
val loss = 4.204555511474609
training loss = 1.698278784751892 10100
val loss = 4.2550201416015625
training loss = 1.7232789993286133 10200
val loss = 4.027364253997803
training loss = 1.698151707649231 10300
val loss = 4.252597332000732
training loss = 1.69814133644104 10400
val loss = 4.250861644744873
training loss = 1.6980791091918945 10500
val loss = 4.243989944458008
training loss = 1.698022484779358 10600
val loss = 4.254161834716797
training loss = 1.7042325735092163 10700
val loss = 4.393711090087891
training loss = 1.6979045867919922 10800
val loss = 4.253872871398926
training loss = 1.6978987455368042 10900
val loss = 4.25428581237793
training loss = 1.6986212730407715 11000
val loss = 4.302140235900879
training loss = 1.697799801826477 11100
val loss = 4.254474639892578
training loss = 1.6978832483291626 11200
val loss = 4.240008354187012
training loss = 1.697742223739624 11300
val loss = 4.264804363250732
training loss = 1.6977049112319946 11400
val loss = 4.254110336303711
training loss = 1.801956057548523 11500
val loss = 3.866631269454956
training loss = 1.6976327896118164 11600
val loss = 4.254894256591797
training loss = 1.697652816772461 11700
val loss = 4.260851860046387
training loss = 1.6975940465927124 11800
val loss = 4.2628679275512695
training loss = 1.6975769996643066 11900
val loss = 4.254234313964844
training loss = 1.698941946029663 12000
val loss = 4.194314002990723
training loss = 1.6975083351135254 12100
val loss = 4.253872394561768
training loss = 1.7181689739227295 12200
val loss = 4.517334938049316
training loss = 1.6974486112594604 12300
val loss = 4.250588417053223
training loss = 1.6974544525146484 12400
val loss = 4.25435733795166
training loss = 1.7009810209274292 12500
val loss = 4.161894798278809
training loss = 1.697394609451294 12600
val loss = 4.25398063659668
training loss = 1.7139500379562378 12700
val loss = 4.067355155944824
training loss = 1.6973477602005005 12800
val loss = 4.25130558013916
training loss = 1.6973557472229004 12900
val loss = 4.254611968994141
training loss = 1.6975069046020508 13000
val loss = 4.279139518737793
training loss = 1.697303056716919 13100
val loss = 4.2542009353637695
training loss = 1.7132717370986938 13200
val loss = 4.480395793914795
training loss = 1.6972465515136719 13300
val loss = 4.254654407501221
training loss = 1.6972620487213135 13400
val loss = 4.254726409912109
training loss = 1.6974146366119385 13500
val loss = 4.279497146606445
training loss = 1.6972111463546753 13600
val loss = 4.254805088043213
training loss = 1.7061885595321655 13700
val loss = 4.421792030334473
training loss = 1.6971592903137207 13800
val loss = 4.2581586837768555
training loss = 1.6971737146377563 13900
val loss = 4.2548041343688965
training loss = 1.7265242338180542 14000
val loss = 4.014017581939697
training loss = 1.6971122026443481 14100
val loss = 4.25387716293335
training loss = 1.6971279382705688 14200
val loss = 4.254787921905518
training loss = 1.703413486480713 14300
val loss = 4.393664360046387
training loss = 1.6970793008804321 14400
val loss = 4.253951072692871
training loss = 1.6971139907836914 14500
val loss = 4.262646198272705
training loss = 1.697078824043274 14600
val loss = 4.24383544921875
training loss = 1.6970475912094116 14700
val loss = 4.254850387573242
training loss = 1.713040828704834 14800
val loss = 4.482515811920166
training loss = 1.6970009803771973 14900
val loss = 4.2538604736328125
training loss = 1.697016954421997 15000
val loss = 4.255426406860352
training loss = 1.6972405910491943 15100
val loss = 4.227034091949463
training loss = 1.6969525814056396 15200
val loss = 4.254775047302246
training loss = 1.6969679594039917 15300
val loss = 4.254755973815918
training loss = 1.697128176689148 15400
val loss = 4.280200004577637
training loss = 1.6969166994094849 15500
val loss = 4.255097389221191
training loss = 1.7973649501800537 15600
val loss = 3.8741989135742188
training loss = 1.6968766450881958 15700
val loss = 4.250726699829102
training loss = 1.6968845129013062 15800
val loss = 4.255391597747803
training loss = 1.69808030128479 15900
val loss = 4.3149943351745605
training loss = 1.69683837890625 16000
val loss = 4.255067825317383
training loss = 1.9341049194335938 16100
val loss = 5.365067005157471
training loss = 1.6967968940734863 16200
val loss = 4.259545803070068
reduced chi^2 level 2 = 1.696794867515564
Constrained alpha: 1.9014657735824585
Constrained beta: 3.9463579654693604
Constrained gamma: 15.051562309265137
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 843.3521,  885.1563,  943.1166,  912.5995, 1058.4692, 1067.2827,
        1073.5186, 1116.2416, 1153.5938, 1155.7583, 1254.0028, 1199.9866,
        1244.1713, 1212.8490, 1326.3390, 1429.3564, 1369.9525, 1401.2712,
        1556.0765, 1541.6304, 1584.4784, 1602.7659, 1564.9116, 1655.2225,
        1706.5802, 1732.1328, 1522.6788, 1734.1864, 1727.4662, 1670.5807,
        1750.5485, 1735.4904, 1741.8015, 1732.1499, 1794.2019, 1743.6597,
        1743.8136, 1584.1512, 1631.9133, 1662.9032, 1659.8407, 1497.1841,
        1493.0050, 1496.7550, 1345.4633, 1329.7367, 1241.3986, 1203.2473,
        1106.5709, 1240.3004, 1066.3054,  902.9796,  976.9482,  821.9952,
         911.1733,  863.1522,  820.9425,  699.7720,  568.6218,  524.5726,
         576.0477,  440.8090,  441.9289,  367.5842,  368.1735,  358.8416,
         265.0297,  238.5267,  210.4608,  172.8086,  153.7791,  135.8327,
         135.0994,  103.1482,   88.4890,   68.7083,   46.4233,   27.8215,
          39.5923,   39.7698,   21.5275,   38.4554,   14.2452])]
3147.058047086198
1.5959843020115305 13.053008281670394 19.26693006831589
val isze = 8
idinces = [35 49 69 53 28 27 33 38 75 29 17 36  6 14 44  4 67 22 80 47 13  3  1 42
 51 57 20 55 50 34 24 77 37  5 15 31 82 30 70 74 11 48 46 45 68 10 76 59
 32 79  2 66 26 78 71 12 40 54  9 81 16  7  8 21 52 73  0 41 18 25 56 65
 19 61 58 23 43 39 72 62 63 60 64]
we are doing training validation split
training loss = 181.2036895751953 100
val loss = 258.9865417480469
training loss = 7.418476104736328 200
val loss = 11.349285125732422
training loss = 7.275041580200195 300
val loss = 10.678447723388672
training loss = 7.191606521606445 400
val loss = 10.362476348876953
training loss = 7.115783214569092 500
val loss = 10.059663772583008
training loss = 7.0497260093688965 600
val loss = 9.77912425994873
training loss = 6.9935808181762695 700
val loss = 9.52676010131836
training loss = 6.946353435516357 800
val loss = 9.304281234741211
training loss = 6.906538486480713 900
val loss = 9.111083030700684
training loss = 6.8724894523620605 1000
val loss = 8.945389747619629
training loss = 6.842682361602783 1100
val loss = 8.804594039916992
training loss = 6.815791606903076 1200
val loss = 8.685667037963867
training loss = 6.790759086608887 1300
val loss = 8.58582878112793
training loss = 6.766778945922852 1400
val loss = 8.502260208129883
training loss = 6.743254661560059 1500
val loss = 8.43227481842041
training loss = 6.7197771072387695 1600
val loss = 8.373579025268555
training loss = 6.696078300476074 1700
val loss = 8.323728561401367
training loss = 6.671993732452393 1800
val loss = 8.28121566772461
training loss = 6.647435665130615 1900
val loss = 8.24415397644043
training loss = 6.622385501861572 2000
val loss = 8.211398124694824
training loss = 6.5968451499938965 2100
val loss = 8.181639671325684
training loss = 6.570872783660889 2200
val loss = 8.154162406921387
training loss = 6.54455041885376 2300
val loss = 8.128281593322754
training loss = 6.517984867095947 2400
val loss = 8.103646278381348
training loss = 6.491321086883545 2500
val loss = 8.079658508300781
training loss = 6.46473503112793 2600
val loss = 8.0567626953125
training loss = 6.438429832458496 2700
val loss = 8.034490585327148
training loss = 6.412639141082764 2800
val loss = 8.013265609741211
training loss = 6.387619495391846 2900
val loss = 7.993317604064941
training loss = 6.363610744476318 3000
val loss = 7.9747819900512695
training loss = 6.340853214263916 3100
val loss = 7.958021640777588
training loss = 6.319521427154541 3200
val loss = 7.9432220458984375
training loss = 6.299716949462891 3300
val loss = 7.930935859680176
training loss = 6.281417369842529 3400
val loss = 7.921125411987305
training loss = 6.264426231384277 3500
val loss = 7.913810729980469
training loss = 6.248273849487305 3600
val loss = 7.908627986907959
training loss = 6.231966495513916 3700
val loss = 7.905069828033447
training loss = 6.213097095489502 3800
val loss = 7.902180194854736
training loss = 6.1844000816345215 3900
val loss = 7.89796257019043
training loss = 6.1179656982421875 4000
val loss = 7.886933326721191
training loss = 5.918224811553955 4100
val loss = 7.851815223693848
training loss = 5.535953998565674 4200
val loss = 7.62477970123291
training loss = 4.938450813293457 4300
val loss = 6.944514274597168
training loss = 3.8795900344848633 4400
val loss = 5.674919605255127
training loss = 2.9271233081817627 4500
val loss = 3.957489490509033
training loss = 2.7683393955230713 4600
val loss = 3.3060758113861084
training loss = 2.7145602703094482 4700
val loss = 3.1484017372131348
training loss = 2.6764883995056152 4800
val loss = 3.055751085281372
training loss = 2.648320436477661 4900
val loss = 2.9863719940185547
training loss = 2.6898372173309326 5000
val loss = 2.607389450073242
training loss = 2.6101880073547363 5100
val loss = 2.895033836364746
training loss = 2.714484930038452 5200
val loss = 2.476949691772461
training loss = 2.5815725326538086 5300
val loss = 2.8281848430633545
training loss = 2.566973924636841 5400
val loss = 2.750823974609375
training loss = 2.546797037124634 5500
val loss = 2.790134906768799
training loss = 2.5238826274871826 5600
val loss = 2.775338888168335
training loss = 2.500499725341797 5700
val loss = 2.738399028778076
training loss = 2.478405714035034 5800
val loss = 2.7516579627990723
training loss = 2.538691997528076 5900
val loss = 2.4292173385620117
training loss = 2.442570686340332 6000
val loss = 2.7249650955200195
training loss = 2.4274377822875977 6100
val loss = 2.7320914268493652
training loss = 2.413867235183716 6200
val loss = 2.712052822113037
training loss = 2.4009850025177 6300
val loss = 2.6958415508270264
training loss = 2.389563798904419 6400
val loss = 2.7198095321655273
training loss = 2.377850294113159 6500
val loss = 2.6843581199645996
training loss = 2.3721115589141846 6600
val loss = 2.779783248901367
training loss = 2.3573617935180664 6700
val loss = 2.678431272506714
training loss = 2.347592830657959 6800
val loss = 2.677004814147949
training loss = 2.3390755653381348 6900
val loss = 2.6922123432159424
training loss = 2.330451011657715 7000
val loss = 2.674154043197632
training loss = 2.496580123901367 7100
val loss = 3.426539897918701
training loss = 2.3146440982818604 7200
val loss = 2.6785056591033936
training loss = 2.3072097301483154 7300
val loss = 2.6754753589630127
training loss = 2.300846576690674 7400
val loss = 2.6999192237854004
training loss = 2.2941434383392334 7500
val loss = 2.678217887878418
training loss = 2.2921557426452637 7600
val loss = 2.605930805206299
training loss = 2.282464027404785 7700
val loss = 2.6871604919433594
training loss = 2.2770650386810303 7800
val loss = 2.6837916374206543
training loss = 2.2731804847717285 7900
val loss = 2.6501972675323486
training loss = 2.2676501274108887 8000
val loss = 2.6875078678131104
training loss = 2.269155740737915 8100
val loss = 2.7889626026153564
training loss = 2.259796142578125 8200
val loss = 2.6928234100341797
training loss = 2.256190299987793 8300
val loss = 2.6948230266571045
training loss = 2.2536463737487793 8400
val loss = 2.727663516998291
training loss = 2.249994993209839 8500
val loss = 2.699531316757202
training loss = 2.273036241531372 8600
val loss = 2.542111873626709
training loss = 2.2448160648345947 8700
val loss = 2.7026937007904053
training loss = 2.242455244064331 8800
val loss = 2.706561803817749
training loss = 2.240616798400879 8900
val loss = 2.6952061653137207
training loss = 2.2385475635528564 9000
val loss = 2.7107672691345215
training loss = 2.4768359661102295 9100
val loss = 2.4690423011779785
training loss = 2.2352519035339355 9200
val loss = 2.711937665939331
training loss = 2.2337188720703125 9300
val loss = 2.7171103954315186
training loss = 2.2334721088409424 9400
val loss = 2.6826658248901367
training loss = 2.231109142303467 9500
val loss = 2.72090482711792
training loss = 2.235452175140381 9600
val loss = 2.8152410984039307
training loss = 2.2289254665374756 9700
val loss = 2.724607467651367
training loss = 2.2279021739959717 9800
val loss = 2.7321667671203613
training loss = 2.227069139480591 9900
val loss = 2.736398458480835
training loss = 2.2260892391204834 10000
val loss = 2.7285983562469482
training loss = 2.2255961894989014 10100
val loss = 2.7493255138397217
training loss = 2.2245430946350098 10200
val loss = 2.731550455093384
training loss = 2.6093497276306152 10300
val loss = 3.9621434211730957
training loss = 2.223170042037964 10400
val loss = 2.732095241546631
training loss = 2.2224884033203125 10500
val loss = 2.7352867126464844
training loss = 2.2238457202911377 10600
val loss = 2.6887004375457764
training loss = 2.221325159072876 10700
val loss = 2.737351894378662
training loss = 2.2416341304779053 10800
val loss = 2.921980381011963
training loss = 2.2202768325805664 10900
val loss = 2.7368574142456055
training loss = 2.219742774963379 11000
val loss = 2.7408695220947266
training loss = 2.2193350791931152 11100
val loss = 2.736229419708252
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 857.3359,  901.7206,  888.2017, 1013.5264, 1001.9438, 1074.1965,
        1088.1310, 1124.7351, 1171.1346, 1155.2491, 1192.1626, 1214.2654,
        1221.9161, 1251.9631, 1327.2799, 1339.6266, 1462.8878, 1443.7198,
        1588.4740, 1499.2844, 1568.6105, 1510.6255, 1627.8047, 1559.5577,
        1678.4592, 1735.5332, 1555.5166, 1758.5679, 1753.6469, 1679.3868,
        1648.7427, 1770.0964, 1782.3038, 1732.1958, 1722.3848, 1752.3903,
        1712.4835, 1573.1896, 1686.0790, 1617.2662, 1629.2075, 1542.1560,
        1461.8085, 1517.9081, 1283.5356, 1296.7532, 1332.1877, 1232.0909,
        1122.0850, 1110.2052, 1060.3660,  975.9357,  970.4782,  930.1252,
         838.9009,  854.2817,  838.8596,  692.0334,  575.1202,  540.8718,
         523.8032,  489.2260,  453.4728,  379.6885,  353.9254,  367.9162,
         306.1625,  219.5046,  214.2739,  186.5438,  166.2447,  169.7782,
         138.9069,  100.6377,   93.6267,   70.3799,   63.9706,   41.9403,
          38.1986,   45.9779,   25.8005,   49.3218,   20.6193])]
2816.4612729341247
2.4601832562548736 19.15041501696297 94.4351242266786
val isze = 8
idinces = [69 66 22 72 63 40 43 76 24 11 18 68  8 25 34 47 17 36 61 57 48 46 23 45
 78 26 77  4 70 60  3 81 39 31 44 15 49 74 12 80 54 58  5 37 10 14  6 67
 71 53 79  0 29 64 20  9 21 35 19 27 30 32 16 59 13 55  2 51 65 62 56 28
  1 50 75 38 82 73 42 52  7 41 33]
we are doing training validation split
training loss = 330.0722351074219 100
val loss = 347.7540283203125
training loss = 53.128662109375 200
val loss = 92.09783935546875
training loss = 11.918416023254395 300
val loss = 29.141338348388672
training loss = 11.54716968536377 400
val loss = 27.652252197265625
training loss = 11.139724731445312 500
val loss = 26.25708770751953
training loss = 10.707797050476074 600
val loss = 24.733701705932617
training loss = 10.265202522277832 700
val loss = 23.116451263427734
training loss = 9.826109886169434 800
val loss = 21.445384979248047
training loss = 9.404617309570312 900
val loss = 19.765430450439453
training loss = 9.013798713684082 1000
val loss = 18.124080657958984
training loss = 8.664416313171387 1100
val loss = 16.56865692138672
training loss = 8.363592147827148 1200
val loss = 15.14079475402832
training loss = 8.113810539245605 1300
val loss = 13.872220993041992
training loss = 7.91329288482666 1400
val loss = 12.780754089355469
training loss = 7.7559003829956055 1500
val loss = 11.868769645690918
training loss = 7.633330345153809 1600
val loss = 11.125338554382324
training loss = 7.536756992340088 1700
val loss = 10.530619621276855
training loss = 7.458296298980713 1800
val loss = 10.059962272644043
training loss = 7.3917694091796875 1900
val loss = 9.689077377319336
training loss = 7.332841396331787 2000
val loss = 9.395477294921875
training loss = 7.278745174407959 2100
val loss = 9.16072940826416
training loss = 7.227789878845215 2200
val loss = 8.970802307128906
training loss = 7.178959369659424 2300
val loss = 8.814286231994629
training loss = 7.131400108337402 2400
val loss = 8.692646026611328
training loss = 7.084765911102295 2500
val loss = 8.590383529663086
training loss = 7.039473056793213 2600
val loss = 8.533862113952637
training loss = 6.9946064949035645 2700
val loss = 8.398049354553223
training loss = 6.951050758361816 2800
val loss = 8.306248664855957
training loss = 6.908487319946289 2900
val loss = 8.209525108337402
training loss = 6.866861343383789 3000
val loss = 8.135381698608398
training loss = 6.830663204193115 3100
val loss = 7.869899749755859
training loss = 6.784379959106445 3200
val loss = 7.95374059677124
training loss = 6.740228176116943 3300
val loss = 7.856375217437744
training loss = 6.692443370819092 3400
val loss = 7.942388534545898
training loss = 6.604514122009277 3500
val loss = 7.584542274475098
training loss = 6.411725044250488 3600
val loss = 7.195023536682129
training loss = 5.716085910797119 3700
val loss = 5.815530776977539
training loss = 4.160463333129883 3800
val loss = 3.6532723903656006
training loss = 2.629145622253418 3900
val loss = 2.0278589725494385
training loss = 2.3816399574279785 4000
val loss = 1.4426920413970947
training loss = 2.3320775032043457 4100
val loss = 1.359483242034912
training loss = 2.298048973083496 4200
val loss = 1.3058143854141235
training loss = 2.285945415496826 4300
val loss = 1.429994821548462
training loss = 2.258100748062134 4400
val loss = 1.2423583269119263
training loss = 2.2463581562042236 4500
val loss = 1.2406549453735352
training loss = 2.238109588623047 4600
val loss = 1.2180242538452148
training loss = 2.232253313064575 4700
val loss = 1.213921308517456
training loss = 2.228182792663574 4800
val loss = 1.2014386653900146
training loss = 2.22532057762146 4900
val loss = 1.1889220476150513
training loss = 2.2252275943756104 5000
val loss = 1.2418153285980225
training loss = 2.2218706607818604 5100
val loss = 1.1800892353057861
training loss = 2.2209229469299316 5200
val loss = 1.1800897121429443
training loss = 2.220189094543457 5300
val loss = 1.1871964931488037
training loss = 2.2195322513580322 5400
val loss = 1.1735050678253174
training loss = 2.2206969261169434 5500
val loss = 1.223933458328247
training loss = 2.218505620956421 5600
val loss = 1.1693321466445923
training loss = 2.2181100845336914 5700
val loss = 1.159372329711914
training loss = 2.217397928237915 5800
val loss = 1.1765072345733643
training loss = 2.2166500091552734 5900
val loss = 1.1681385040283203
training loss = 2.216085195541382 6000
val loss = 1.145824670791626
training loss = 2.214641571044922 6100
val loss = 1.1677451133728027
training loss = 2.2133772373199463 6200
val loss = 1.167831301689148
training loss = 2.212980270385742 6300
val loss = 1.2120579481124878
training loss = 2.2099452018737793 6400
val loss = 1.168890118598938
training loss = 2.20786714553833 6500
val loss = 1.1699864864349365
training loss = 2.2056119441986084 6600
val loss = 1.154592752456665
training loss = 2.202770948410034 6700
val loss = 1.1728962659835815
training loss = 2.1998372077941895 6800
val loss = 1.17641282081604
training loss = 2.196746826171875 6900
val loss = 1.162515640258789
training loss = 2.1932528018951416 7000
val loss = 1.1786495447158813
training loss = 2.3057162761688232 7100
val loss = 1.6770801544189453
training loss = 2.1858668327331543 7200
val loss = 1.1831966638565063
training loss = 2.181943893432617 7300
val loss = 1.1858561038970947
training loss = 2.1866097450256348 7400
val loss = 1.3114745616912842
training loss = 2.1735377311706543 7500
val loss = 1.1911287307739258
training loss = 2.187483310699463 7600
val loss = 1.374596357345581
training loss = 2.164422035217285 7700
val loss = 1.1958640813827515
training loss = 2.159586191177368 7800
val loss = 1.1981362104415894
training loss = 2.1547765731811523 7900
val loss = 1.2205803394317627
training loss = 2.149338722229004 8000
val loss = 1.2050586938858032
training loss = 2.144944906234741 8100
val loss = 1.25181245803833
training loss = 2.138190984725952 8200
val loss = 1.2121269702911377
training loss = 2.2476603984832764 8300
val loss = 0.8572923541069031
training loss = 2.125762701034546 8400
val loss = 1.2211244106292725
training loss = 2.1185922622680664 8500
val loss = 1.2301580905914307
training loss = 2.1109495162963867 8600
val loss = 1.22236967086792
training loss = 2.1010913848876953 8700
val loss = 1.2503610849380493
training loss = 2.1014351844787598 8800
val loss = 1.1282539367675781
training loss = 2.074047565460205 8900
val loss = 1.2880589962005615
training loss = 2.0613253116607666 9000
val loss = 1.395067811012268
training loss = 2.038435459136963 9100
val loss = 1.3347232341766357
training loss = 2.1088130474090576 9200
val loss = 1.8106459379196167
training loss = 2.002999782562256 9300
val loss = 1.3838865756988525
training loss = 1.9871068000793457 9400
val loss = 1.3797401189804077
training loss = 1.975395917892456 9500
val loss = 1.4379216432571411
training loss = 1.9643272161483765 9600
val loss = 1.4557921886444092
training loss = 1.9707235097885132 9700
val loss = 1.29966139793396
training loss = 1.9486992359161377 9800
val loss = 1.4941399097442627
training loss = 1.942699670791626 9900
val loss = 1.5147535800933838
training loss = 1.939427137374878 10000
val loss = 1.5620695352554321
training loss = 1.9353817701339722 10100
val loss = 1.5419013500213623
training loss = 1.9325063228607178 10200
val loss = 1.5499801635742188
training loss = 1.9307399988174438 10300
val loss = 1.574425458908081
training loss = 1.9289904832839966 10400
val loss = 1.5757036209106445
training loss = 1.9281020164489746 10500
val loss = 1.568208932876587
training loss = 1.9270678758621216 10600
val loss = 1.588847279548645
training loss = 1.9262806177139282 10700
val loss = 1.5965709686279297
training loss = 1.9285482168197632 10800
val loss = 1.525026559829712
training loss = 1.9254236221313477 10900
val loss = 1.606136679649353
training loss = 1.9807450771331787 11000
val loss = 2.0029802322387695
training loss = 1.9249379634857178 11100
val loss = 1.6076569557189941
training loss = 1.9246944189071655 11200
val loss = 1.617287516593933
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 882.0263,  913.3051,  930.8249,  935.6996, 1008.1762, 1099.9119,
        1092.8367, 1107.2627, 1121.9000, 1172.7672, 1209.8809, 1269.6663,
        1222.3605, 1229.8455, 1324.9635, 1426.6071, 1395.5424, 1485.5796,
        1453.8065, 1498.9241, 1572.6931, 1552.0658, 1592.7299, 1610.2224,
        1725.8477, 1741.1244, 1542.3433, 1799.7134, 1755.6080, 1713.8052,
        1610.1544, 1780.3877, 1654.1334, 1767.4037, 1644.5571, 1703.2487,
        1727.8302, 1659.1736, 1638.6541, 1585.2727, 1570.1589, 1578.1091,
        1461.8231, 1534.3716, 1397.9941, 1355.4725, 1259.2064, 1226.8859,
        1159.8407, 1118.8395, 1098.9288,  980.9290,  993.4294,  885.9267,
         970.6089,  838.8435,  864.7865,  686.0546,  645.7391,  553.3068,
         577.2278,  443.1923,  458.2526,  410.6718,  363.2523,  335.9201,
         283.2831,  278.3865,  201.3176,  165.6364,  152.6471,  163.5000,
         142.7189,   96.0570,   90.6719,   66.2047,   57.9702,   54.2129,
          32.4659,   35.3268,   17.6573,   39.3529,   38.1442])]
2555.448830557384
3.9231907588751986 15.45525195353078 46.90776033202339
val isze = 8
idinces = [62 13 65 53  7 28 63 82 68 55 79 75 80 22 15 38 35 18 74 60 78  0  4 66
 64 81 14 19 32 48 44 50 25 30 47 11 61 77 34 40 21 57 73 27 39 69  1 56
  5 71 23 52  6 29 67  3 31 10 49 17 54 36  9 51 16 20 24 45 33 37  8 41
 46 12 42 76 43 70 72 58 59 26  2]
we are doing training validation split
training loss = 444.9822082519531 100
val loss = 332.3762512207031
training loss = 7.549378395080566 200
val loss = 8.856544494628906
training loss = 6.510440349578857 300
val loss = 7.569347381591797
training loss = 6.402912616729736 400
val loss = 7.5630106925964355
training loss = 6.3024139404296875 500
val loss = 7.569058895111084
training loss = 6.213473796844482 600
val loss = 7.5882978439331055
training loss = 6.13764762878418 700
val loss = 7.616647720336914
training loss = 6.074216365814209 800
val loss = 7.648763179779053
training loss = 6.021154880523682 900
val loss = 7.679604530334473
training loss = 5.975992679595947 1000
val loss = 7.705361366271973
training loss = 5.93646240234375 1100
val loss = 7.723909854888916
training loss = 5.900810718536377 1200
val loss = 7.73459529876709
training loss = 5.867837905883789 1300
val loss = 7.737942218780518
training loss = 5.836814880371094 1400
val loss = 7.735030651092529
training loss = 5.8073272705078125 1500
val loss = 7.727435111999512
training loss = 5.779138088226318 1600
val loss = 7.7162604331970215
training loss = 5.752107620239258 1700
val loss = 7.702714443206787
training loss = 5.726130485534668 1800
val loss = 7.687613010406494
training loss = 5.7011213302612305 1900
val loss = 7.671507835388184
training loss = 5.676997184753418 2000
val loss = 7.654817581176758
training loss = 5.653675079345703 2100
val loss = 7.637812614440918
training loss = 5.631070137023926 2200
val loss = 7.620629787445068
training loss = 5.609099864959717 2300
val loss = 7.603300094604492
training loss = 5.587686538696289 2400
val loss = 7.58591365814209
training loss = 5.566754341125488 2500
val loss = 7.568380832672119
training loss = 5.546222686767578 2600
val loss = 7.550779342651367
training loss = 5.52602481842041 2700
val loss = 7.533066272735596
training loss = 5.506097316741943 2800
val loss = 7.515174865722656
training loss = 5.486377716064453 2900
val loss = 7.497077465057373
training loss = 5.466813564300537 3000
val loss = 7.47874641418457
training loss = 5.44736909866333 3100
val loss = 7.460142135620117
training loss = 5.428016662597656 3200
val loss = 7.44130802154541
training loss = 5.408769607543945 3300
val loss = 7.422077178955078
training loss = 5.389667987823486 3400
val loss = 7.4026994705200195
training loss = 5.37080717086792 3500
val loss = 7.383110523223877
training loss = 5.352361679077148 3600
val loss = 7.363618850708008
training loss = 5.334580421447754 3700
val loss = 7.344428062438965
training loss = 5.317775726318359 3800
val loss = 7.325479984283447
training loss = 5.3026628494262695 3900
val loss = 7.318528175354004
training loss = 5.321651935577393 4000
val loss = 7.12026309967041
training loss = 5.2784423828125 4100
val loss = 7.2786359786987305
training loss = 5.269474506378174 4200
val loss = 7.284933090209961
training loss = 5.261429309844971 4300
val loss = 7.257608413696289
training loss = 5.256097793579102 4400
val loss = 7.193666934967041
training loss = 5.244184970855713 4500
val loss = 7.233423233032227
training loss = 5.245136260986328 4600
val loss = 7.365859031677246
training loss = 5.186777591705322 4700
val loss = 7.144975662231445
training loss = 4.926409721374512 4800
val loss = 6.79109001159668
training loss = 3.896151542663574 4900
val loss = 5.4538493156433105
training loss = 2.6818594932556152 5000
val loss = 3.3847455978393555
training loss = 2.198814868927002 5100
val loss = 2.341040849685669
training loss = 2.1462833881378174 5200
val loss = 2.1679489612579346
training loss = 2.126185655593872 5300
val loss = 2.064666271209717
training loss = 2.0931615829467773 5400
val loss = 1.948091983795166
training loss = 2.1108155250549316 5500
val loss = 1.908069372177124
training loss = 2.020800828933716 5600
val loss = 1.8506414890289307
training loss = 1.9972000122070312 5700
val loss = 1.8213441371917725
training loss = 1.9776149988174438 5800
val loss = 1.790833592414856
training loss = 1.9592570066452026 5900
val loss = 1.7575297355651855
training loss = 1.9423024654388428 6000
val loss = 1.72400963306427
training loss = 1.926435947418213 6100
val loss = 1.6937414407730103
training loss = 1.9108256101608276 6200
val loss = 1.6607638597488403
training loss = 1.8967534303665161 6300
val loss = 1.6308804750442505
training loss = 1.8828763961791992 6400
val loss = 1.5986312627792358
training loss = 1.87018883228302 6500
val loss = 1.5664507150650024
training loss = 1.8580180406570435 6600
val loss = 1.5381337404251099
training loss = 2.00553560256958 6700
val loss = 1.6516172885894775
training loss = 1.8368078470230103 6800
val loss = 1.481940746307373
training loss = 1.8273165225982666 6900
val loss = 1.4549185037612915
training loss = 1.819451928138733 7000
val loss = 1.4315662384033203
training loss = 1.8119401931762695 7100
val loss = 1.4095282554626465
training loss = 1.8050695657730103 7200
val loss = 1.3874619007110596
training loss = 1.7995659112930298 7300
val loss = 1.3685839176177979
training loss = 1.793987512588501 7400
val loss = 1.3508191108703613
training loss = 1.8112435340881348 7500
val loss = 1.3489824533462524
training loss = 1.7854374647140503 7600
val loss = 1.3211039304733276
training loss = 1.7817851305007935 7700
val loss = 1.3076519966125488
training loss = 1.7793054580688477 7800
val loss = 1.296600341796875
training loss = 1.7760009765625 7900
val loss = 1.2864477634429932
training loss = 1.801870584487915 8000
val loss = 1.2994526624679565
training loss = 1.77156662940979 8100
val loss = 1.2699809074401855
training loss = 1.7696679830551147 8200
val loss = 1.2625646591186523
training loss = 1.7683639526367188 8300
val loss = 1.2575571537017822
training loss = 1.7666577100753784 8400
val loss = 1.2516019344329834
training loss = 1.7723673582077026 8500
val loss = 1.2486618757247925
training loss = 1.7643036842346191 8600
val loss = 1.2432353496551514
training loss = 2.122610330581665 8700
val loss = 1.5306720733642578
training loss = 1.7624475955963135 8800
val loss = 1.2369519472122192
training loss = 1.7616046667099 8900
val loss = 1.2340259552001953
training loss = 1.762062668800354 9000
val loss = 1.2343456745147705
training loss = 1.7602558135986328 9100
val loss = 1.2299357652664185
training loss = 1.843400478363037 9200
val loss = 1.301199197769165
training loss = 1.7591516971588135 9300
val loss = 1.2267508506774902
training loss = 1.7586116790771484 9400
val loss = 1.2249387502670288
training loss = 1.7589588165283203 9500
val loss = 1.2232275009155273
training loss = 1.7577478885650635 9600
val loss = 1.2227414846420288
training loss = 1.770098328590393 9700
val loss = 1.2339611053466797
training loss = 1.7570048570632935 9800
val loss = 1.2206342220306396
training loss = 1.757023572921753 9900
val loss = 1.220799446105957
training loss = 1.7563769817352295 10000
val loss = 1.2187597751617432
training loss = 1.7560139894485474 10100
val loss = 1.218260645866394
training loss = 1.7663519382476807 10200
val loss = 1.22906494140625
training loss = 1.7554752826690674 10300
val loss = 1.2168335914611816
training loss = 1.7552844285964966 10400
val loss = 1.2154994010925293
training loss = 1.75507652759552 10500
val loss = 1.2159736156463623
training loss = 1.7547261714935303 10600
val loss = 1.21511971950531
training loss = 1.846805214881897 10700
val loss = 1.2991008758544922
training loss = 1.7543596029281616 10800
val loss = 1.2139099836349487
training loss = 1.754093050956726 10900
val loss = 1.2136685848236084
training loss = 1.75526762008667 11000
val loss = 1.2118202447891235
training loss = 1.7537864446640015 11100
val loss = 1.2120741605758667
training loss = 1.7535258531570435 11200
val loss = 1.2118959426879883
training loss = 1.7536731958389282 11300
val loss = 1.2108781337738037
training loss = 1.753200888633728 11400
val loss = 1.2110830545425415
training loss = 1.753646731376648 11500
val loss = 1.2081581354141235
training loss = 1.752912998199463 11600
val loss = 1.21051824092865
training loss = 1.7527352571487427 11700
val loss = 1.2098445892333984
training loss = 1.7527062892913818 11800
val loss = 1.2101171016693115
training loss = 1.752479076385498 11900
val loss = 1.2093433141708374
training loss = 1.9949129819869995 12000
val loss = 1.4146995544433594
training loss = 1.752252221107483 12100
val loss = 1.2085806131362915
training loss = 1.7520959377288818 12200
val loss = 1.2082407474517822
training loss = 1.7559515237808228 12300
val loss = 1.2146527767181396
training loss = 1.7518982887268066 12400
val loss = 1.2080082893371582
training loss = 1.751763105392456 12500
val loss = 1.2074949741363525
training loss = 1.7518620491027832 12600
val loss = 1.2081844806671143
training loss = 1.7515771389007568 12700
val loss = 1.2070462703704834
training loss = 1.8516957759857178 12800
val loss = 1.264690637588501
training loss = 1.751417875289917 12900
val loss = 1.2067917585372925
training loss = 1.751295566558838 13000
val loss = 1.2064813375473022
training loss = 1.7514841556549072 13100
val loss = 1.2053718566894531
training loss = 1.751144289970398 13200
val loss = 1.2061703205108643
training loss = 1.757110834121704 13300
val loss = 1.206007957458496
training loss = 1.7510136365890503 13400
val loss = 1.2060214281082153
training loss = 1.7509033679962158 13500
val loss = 1.2058533430099487
training loss = 1.7509188652038574 13600
val loss = 1.2057642936706543
training loss = 1.750771164894104 13700
val loss = 1.2057191133499146
training loss = 1.7852509021759033 13800
val loss = 1.2359094619750977
training loss = 1.750655174255371 13900
val loss = 1.2058576345443726
training loss = 1.750563621520996 14000
val loss = 1.2056574821472168
training loss = 1.7519519329071045 14100
val loss = 1.2049407958984375
training loss = 1.7504537105560303 14200
val loss = 1.2059054374694824
training loss = 2.082869529724121 14300
val loss = 1.4204164743423462
training loss = 1.7503597736358643 14400
val loss = 1.2059617042541504
training loss = 1.7502704858779907 14500
val loss = 1.2061408758163452
training loss = 1.7503433227539062 14600
val loss = 1.205774188041687
training loss = 1.7501674890518188 14700
val loss = 1.2065914869308472
training loss = 1.7622495889663696 14800
val loss = 1.2207205295562744
training loss = 1.7500667572021484 14900
val loss = 1.2072771787643433
training loss = 1.7960734367370605 15000
val loss = 1.253901481628418
training loss = 1.749984622001648 15100
val loss = 1.2077147960662842
training loss = 1.7498897314071655 15200
val loss = 1.2083088159561157
training loss = 1.7514454126358032 15300
val loss = 1.2120695114135742
training loss = 1.7497872114181519 15400
val loss = 1.209537386894226
training loss = 1.8058704137802124 15500
val loss = 1.2650395631790161
training loss = 1.7496944665908813 15600
val loss = 1.210498332977295
training loss = 1.7495907545089722 15700
val loss = 1.211405634880066
training loss = 1.749949336051941 15800
val loss = 1.2134610414505005
training loss = 1.7494560480117798 15900
val loss = 1.2133715152740479
training loss = 1.7494584321975708 16000
val loss = 1.211782693862915
training loss = 1.7493034601211548 16100
val loss = 1.2156518697738647
training loss = 1.7491917610168457 16200
val loss = 1.2166762351989746
training loss = 1.74945068359375 16300
val loss = 1.219184160232544
training loss = 1.7489752769470215 16400
val loss = 1.2198997735977173
training loss = 1.7713438272476196 16500
val loss = 1.2297991514205933
training loss = 1.7487001419067383 16600
val loss = 1.2235932350158691
training loss = 1.7503117322921753 16700
val loss = 1.228898048400879
training loss = 1.748369574546814 16800
val loss = 1.2276363372802734
training loss = 1.748078465461731 16900
val loss = 1.2304455041885376
training loss = 1.755070447921753 17000
val loss = 1.2345731258392334
training loss = 1.7474983930587769 17100
val loss = 1.2358813285827637
training loss = 1.7471448183059692 17200
val loss = 1.2382785081863403
training loss = 1.746746301651001 17300
val loss = 1.2413803339004517
training loss = 1.7462178468704224 17400
val loss = 1.2444701194763184
training loss = 1.7456082105636597 17500
val loss = 1.2464945316314697
training loss = 1.7461845874786377 17600
val loss = 1.2503559589385986
training loss = 1.744377851486206 17700
val loss = 1.2498137950897217
training loss = 1.7435886859893799 17800
val loss = 1.2497260570526123
training loss = 1.7470028400421143 17900
val loss = 1.2497837543487549
training loss = 1.7421338558197021 18000
val loss = 1.2470755577087402
training loss = 1.7413135766983032 18100
val loss = 1.2436461448669434
training loss = 1.7407877445220947 18200
val loss = 1.24009108543396
training loss = 1.7400836944580078 18300
val loss = 1.2360841035842896
training loss = 1.7394344806671143 18400
val loss = 1.2308553457260132
training loss = 1.7392243146896362 18500
val loss = 1.2260735034942627
training loss = 1.7385514974594116 18600
val loss = 1.2220245599746704
training loss = 2.0402333736419678 18700
val loss = 1.4688211679458618
reduced chi^2 level 2 = 1.737932562828064
Constrained alpha: 1.8280930519104004
Constrained beta: 3.5258126258850098
Constrained gamma: 14.494646072387695
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 911.8491,  856.4641, 1002.3281, 1005.8701, 1048.4308, 1064.5557,
        1127.4598, 1183.3556, 1122.7235, 1164.0748, 1194.1332, 1161.8170,
        1247.5214, 1269.2732, 1315.5895, 1402.7416, 1412.9380, 1483.0162,
        1564.7745, 1529.4380, 1613.5995, 1549.0261, 1586.1708, 1567.2300,
        1638.5068, 1652.3293, 1626.1749, 1716.1847, 1694.6414, 1698.7491,
        1607.6698, 1689.2573, 1666.5428, 1713.4867, 1677.4054, 1842.1786,
        1688.6477, 1654.5652, 1679.9904, 1609.6240, 1663.2018, 1570.5992,
        1496.5026, 1443.8728, 1400.1210, 1377.6892, 1237.5282, 1284.9064,
        1205.5972, 1119.7036, 1053.0389, 1002.9617,  920.4647,  887.2866,
         902.5258,  871.0322,  814.8558,  721.5349,  553.4778,  538.3672,
         546.8498,  467.1573,  464.3389,  385.0129,  392.4175,  337.0994,
         278.9357,  226.3672,  237.9863,  172.6568,  160.2005,  141.9382,
         137.5091,  116.3039,  106.6662,   71.2801,   53.3262,   32.4942,
          37.2844,   44.2157,   20.7550,   39.7972,   26.5347])]
2770.208098050339
2.665116225013977 1.0214942059752419 94.64843696283924
val isze = 8
idinces = [70 28 14 12 61  6 37 77 26 58 25 52 79 60 18 53 78 40 30  0 15 71 69 21
 64 51 57  7 56 50 11 66  8 32 39 27 38 19 34 22 59  4 20 47  9 68 73 81
 33  1 35 67 17 54 29 63 49  2 36 82 80 44 23 74 43 31 45 55 76  3 48 62
 10 65 41  5 42 24 72 46 75 16 13]
we are doing training validation split
training loss = 83.59742736816406 100
val loss = 77.27859497070312
training loss = 34.2411003112793 200
val loss = 39.46087646484375
training loss = 11.12470817565918 300
val loss = 13.01863956451416
training loss = 5.727173328399658 400
val loss = 5.115819454193115
training loss = 4.353949546813965 500
val loss = 3.213768482208252
training loss = 3.621617078781128 600
val loss = 2.6304373741149902
training loss = 3.2119967937469482 700
val loss = 2.3454155921936035
training loss = 2.9942777156829834 800
val loss = 2.192816734313965
training loss = 2.8689775466918945 900
val loss = 2.118997097015381
training loss = 2.7817165851593018 1000
val loss = 2.0794882774353027
training loss = 2.708615303039551 1100
val loss = 2.0525290966033936
training loss = 2.6406872272491455 1200
val loss = 2.0290603637695312
training loss = 2.5744528770446777 1300
val loss = 2.0054099559783936
training loss = 2.5076401233673096 1400
val loss = 1.9790664911270142
training loss = 2.4378833770751953 1500
val loss = 1.946427345275879
training loss = 2.364466905593872 1600
val loss = 1.9028949737548828
training loss = 2.2914862632751465 1700
val loss = 1.8490025997161865
training loss = 2.2252044677734375 1800
val loss = 1.7954089641571045
training loss = 2.1678519248962402 1900
val loss = 1.7466843128204346
training loss = 2.1186022758483887 2000
val loss = 1.7204837799072266
training loss = 2.0771424770355225 2100
val loss = 1.7088890075683594
training loss = 2.0415117740631104 2200
val loss = 1.6712605953216553
training loss = 2.012097120285034 2300
val loss = 1.6535894870758057
training loss = 1.9881113767623901 2400
val loss = 1.6423285007476807
training loss = 1.968553066253662 2500
val loss = 1.6236928701400757
training loss = 1.9530888795852661 2600
val loss = 1.6049408912658691
training loss = 1.9405559301376343 2700
val loss = 1.6044012308120728
training loss = 1.930834412574768 2800
val loss = 1.593734860420227
training loss = 1.9230798482894897 2900
val loss = 1.5918890237808228
training loss = 1.9172394275665283 3000
val loss = 1.601850152015686
training loss = 1.912076473236084 3100
val loss = 1.5848933458328247
training loss = 1.908298134803772 3200
val loss = 1.5728644132614136
training loss = 1.9049978256225586 3300
val loss = 1.5816624164581299
training loss = 1.902341365814209 3400
val loss = 1.581673264503479
training loss = 1.90020751953125 3500
val loss = 1.5746827125549316
training loss = 1.8983110189437866 3600
val loss = 1.5808916091918945
training loss = 1.8983337879180908 3700
val loss = 1.616062045097351
training loss = 1.895437240600586 3800
val loss = 1.5813696384429932
training loss = 1.894338607788086 3900
val loss = 1.5764954090118408
training loss = 1.893281102180481 4000
val loss = 1.581102728843689
training loss = 1.8924007415771484 4100
val loss = 1.5828704833984375
training loss = 1.894105315208435 4200
val loss = 1.6283421516418457
training loss = 1.890860676765442 4300
val loss = 1.5839699506759644
training loss = 1.8904954195022583 4400
val loss = 1.5711824893951416
training loss = 1.8895752429962158 4500
val loss = 1.5834860801696777
training loss = 1.888999104499817 4600
val loss = 1.5855005979537964
training loss = 1.8945223093032837 4700
val loss = 1.6586555242538452
training loss = 1.8879060745239258 4800
val loss = 1.5869907140731812
training loss = 1.8918416500091553 4900
val loss = 1.5347615480422974
training loss = 1.8869017362594604 5000
val loss = 1.5875418186187744
training loss = 1.8864402770996094 5100
val loss = 1.5878514051437378
training loss = 1.8908898830413818 5200
val loss = 1.6540473699569702
training loss = 1.8855140209197998 5300
val loss = 1.5885859727859497
training loss = 2.1284501552581787 5400
val loss = 2.25811505317688
training loss = 1.884657382965088 5500
val loss = 1.5874881744384766
training loss = 1.8842709064483643 5600
val loss = 1.588494062423706
training loss = 1.8839856386184692 5700
val loss = 1.602062702178955
training loss = 1.883442997932434 5800
val loss = 1.591240406036377
training loss = 1.8899823427200317 5900
val loss = 1.6718188524246216
training loss = 1.8826497793197632 6000
val loss = 1.5918117761611938
training loss = 1.8823431730270386 6100
val loss = 1.5888853073120117
training loss = 1.8819421529769897 6200
val loss = 1.5913392305374146
training loss = 1.8816055059432983 6300
val loss = 1.5940053462982178
training loss = 1.8812692165374756 6400
val loss = 1.5969473123550415
training loss = 1.8809295892715454 6500
val loss = 1.5950454473495483
training loss = 2.20011043548584 6600
val loss = 1.4118889570236206
training loss = 1.8803009986877441 6700
val loss = 1.5947681665420532
training loss = 1.8800145387649536 6800
val loss = 1.5969340801239014
training loss = 1.8823823928833008 6900
val loss = 1.64630126953125
training loss = 1.879414677619934 7000
val loss = 1.5984231233596802
training loss = 1.87916100025177 7100
val loss = 1.5989429950714111
training loss = 1.8810604810714722 7200
val loss = 1.6434409618377686
training loss = 1.8786308765411377 7300
val loss = 1.6003221273422241
training loss = 1.9456454515457153 7400
val loss = 1.4344172477722168
training loss = 1.8781604766845703 7500
val loss = 1.598373532295227
training loss = 1.8779330253601074 7600
val loss = 1.602408528327942
training loss = 1.8777514696121216 7700
val loss = 1.6100631952285767
training loss = 1.8774715662002563 7800
val loss = 1.6032054424285889
training loss = 1.8777791261672974 7900
val loss = 1.5825505256652832
training loss = 1.877052903175354 8000
val loss = 1.6031975746154785
training loss = 1.8768669366836548 8100
val loss = 1.605420470237732
training loss = 1.8767517805099487 8200
val loss = 1.5967023372650146
training loss = 1.8764398097991943 8300
val loss = 1.606677770614624
training loss = 1.8762634992599487 8400
val loss = 1.607327938079834
training loss = 1.8999725580215454 8500
val loss = 1.7761181592941284
training loss = 1.875900149345398 8600
val loss = 1.6082937717437744
training loss = 1.8757407665252686 8700
val loss = 1.609588623046875
training loss = 1.8756074905395508 8800
val loss = 1.6047918796539307
training loss = 1.875411033630371 8900
val loss = 1.6107079982757568
training loss = 1.9119216203689575 9000
val loss = 1.473120927810669
training loss = 1.8750977516174316 9100
val loss = 1.6124444007873535
training loss = 1.8749576807022095 9200
val loss = 1.611701250076294
training loss = 1.874962568283081 9300
val loss = 1.6250746250152588
training loss = 1.8746607303619385 9400
val loss = 1.6139335632324219
training loss = 1.8745508193969727 9500
val loss = 1.6214334964752197
training loss = 1.8743815422058105 9600
val loss = 1.6151933670043945
training loss = 1.8742626905441284 9700
val loss = 1.6183751821517944
training loss = 1.8741309642791748 9800
val loss = 1.6129825115203857
training loss = 1.8739900588989258 9900
val loss = 1.6169977188110352
training loss = 1.9146411418914795 10000
val loss = 1.4730234146118164
training loss = 1.8737415075302124 10100
val loss = 1.6182037591934204
training loss = 1.87362802028656 10200
val loss = 1.6179873943328857
training loss = 1.8737376928329468 10300
val loss = 1.6338415145874023
training loss = 1.8733911514282227 10400
val loss = 1.620161771774292
training loss = 1.8732963800430298 10500
val loss = 1.6182849407196045
training loss = 1.8731677532196045 10600
val loss = 1.621912956237793
training loss = 1.9304218292236328 10700
val loss = 1.459592580795288
training loss = 1.8729639053344727 10800
val loss = 1.6200804710388184
training loss = 1.872849941253662 10900
val loss = 1.6229541301727295
training loss = 1.8749971389770508 11000
val loss = 1.5821304321289062
training loss = 1.8726481199264526 11100
val loss = 1.6240088939666748
training loss = 2.1685173511505127 11200
val loss = 1.4200687408447266
training loss = 1.8724581003189087 11300
val loss = 1.6239352226257324
training loss = 1.8723564147949219 11400
val loss = 1.62613844871521
training loss = 1.8753520250320435 11500
val loss = 1.5787749290466309
training loss = 1.872173547744751 11600
val loss = 1.627500057220459
training loss = 1.8720768690109253 11700
val loss = 1.6278327703475952
training loss = 1.9852936267852783 11800
val loss = 1.4298226833343506
training loss = 1.8719087839126587 11900
val loss = 1.630918264389038
training loss = 1.8718116283416748 12000
val loss = 1.629448413848877
training loss = 1.8730019330978394 12100
val loss = 1.5989171266555786
training loss = 1.871646523475647 12200
val loss = 1.630993127822876
training loss = 1.8903321027755737 12300
val loss = 1.5224997997283936
training loss = 1.8714892864227295 12400
val loss = 1.630631685256958
training loss = 1.8713982105255127 12500
val loss = 1.6319726705551147
training loss = 1.8714543581008911 12600
val loss = 1.6230237483978271
training loss = 1.8712445497512817 12700
val loss = 1.6330430507659912
training loss = 1.8862781524658203 12800
val loss = 1.7543127536773682
training loss = 1.8710991144180298 12900
val loss = 1.6354230642318726
training loss = 1.8710081577301025 13000
val loss = 1.6346970796585083
training loss = 1.870958924293518 13100
val loss = 1.6372156143188477
reduced chi^2 level 2 = 1.8709269762039185
Constrained alpha: 1.9402351379394531
Constrained beta: 3.571216583251953
Constrained gamma: 18.078907012939453
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 870.2760,  846.8204,  933.1234,  941.6073, 1035.3362, 1059.4150,
        1068.4971, 1125.5388, 1137.1553, 1164.3568, 1212.9186, 1223.3468,
        1259.2129, 1330.4397, 1356.9059, 1380.3254, 1424.2170, 1420.7502,
        1605.0793, 1530.8458, 1647.7234, 1454.4778, 1596.0001, 1640.6279,
        1717.3728, 1748.2902, 1558.5442, 1667.0477, 1657.0469, 1742.9128,
        1594.6436, 1772.3052, 1814.3704, 1634.3711, 1630.9161, 1681.6088,
        1677.8512, 1645.3481, 1659.5179, 1611.6178, 1628.3232, 1587.2643,
        1551.7893, 1467.7325, 1288.9531, 1365.2894, 1268.9006, 1242.4772,
        1192.8352, 1180.8607, 1099.8917,  952.0650,  950.8093,  934.0175,
         886.9061,  862.0204,  853.5060,  663.6249,  611.0151,  508.4572,
         547.0760,  474.3161,  429.4845,  394.0612,  343.0223,  338.6321,
         275.0713,  265.0106,  226.0223,  174.8827,  160.5102,  132.2927,
         147.4102,  117.2873,   83.0232,   70.8203,   44.5098,   36.4261,
          31.1681,   41.2702,   21.8802,   45.1672,   30.6378])]
2727.3930267087876
0.027734970516078983 4.836059992163491 33.43226618031841
val isze = 8
idinces = [52 36 17 78 38 31 41 71 27  9 10 79 28 12 70 77  8 46 62 35 21 15 69 45
 58  1 48 20 29 19 53 60 61 30 63 24  2 22 40 50 32 82 65 39  5 18 73  4
 54 26 47  7  6 42 25 68 33 16 44 66 37 51 74 23  3 14 43 55 59 11 57 64
 67 81 49 56  0 13 80 75 72 34 76]
we are doing training validation split
training loss = 18.269935607910156 100
val loss = 28.275419235229492
training loss = 13.05742359161377 200
val loss = 18.22097396850586
training loss = 11.426557540893555 300
val loss = 15.431665420532227
training loss = 10.057818412780762 400
val loss = 13.07792854309082
training loss = 8.97427749633789 500
val loss = 11.192829132080078
training loss = 8.143651008605957 600
val loss = 9.723379135131836
training loss = 7.519832611083984 700
val loss = 8.594033241271973
training loss = 7.058385372161865 800
val loss = 7.732224464416504
training loss = 6.721381187438965 900
val loss = 7.076986312866211
training loss = 6.478078365325928 1000
val loss = 6.579593658447266
training loss = 6.304276466369629 1100
val loss = 6.202167510986328
training loss = 6.181243419647217 1200
val loss = 5.915439605712891
training loss = 6.0946760177612305 1300
val loss = 5.69691276550293
training loss = 6.033781051635742 1400
val loss = 5.5300612449646
training loss = 5.99049186706543 1500
val loss = 5.401759147644043
training loss = 5.958872318267822 1600
val loss = 5.302472114562988
training loss = 5.934569358825684 1700
val loss = 5.224610328674316
training loss = 5.914418697357178 1800
val loss = 5.1625895500183105
training loss = 5.896097660064697 1900
val loss = 5.112143516540527
training loss = 5.877840518951416 2000
val loss = 5.069507598876953
training loss = 5.8582072257995605 2100
val loss = 5.03190279006958
training loss = 5.8358659744262695 2200
val loss = 4.9966840744018555
training loss = 5.80942440032959 2300
val loss = 4.961392402648926
training loss = 5.777219295501709 2400
val loss = 4.9237518310546875
training loss = 5.737128257751465 2500
val loss = 4.880810737609863
training loss = 5.686304092407227 2600
val loss = 4.829189300537109
training loss = 5.6208815574646 2700
val loss = 4.764455795288086
training loss = 5.5356268882751465 2800
val loss = 4.680974960327148
training loss = 5.423679351806641 2900
val loss = 4.57051944732666
training loss = 5.276472091674805 3000
val loss = 4.422581672668457
training loss = 5.083324909210205 3100
val loss = 4.22226619720459
training loss = 4.829841613769531 3200
val loss = 3.9515581130981445
training loss = 4.496271133422852 3300
val loss = 3.5895655155181885
training loss = 4.061872482299805 3400
val loss = 3.1197750568389893
training loss = 3.542924404144287 3500
val loss = 2.394960880279541
training loss = 2.996839761734009 3600
val loss = 2.0127291679382324
training loss = 2.5970618724823 3700
val loss = 1.7440108060836792
training loss = 2.391063690185547 3800
val loss = 1.402277946472168
training loss = 2.381964683532715 3900
val loss = 1.6106024980545044
training loss = 2.3151228427886963 4000
val loss = 1.3211537599563599
training loss = 2.311904191970825 4100
val loss = 1.3185782432556152
training loss = 2.311051845550537 4200
val loss = 1.3283215761184692
training loss = 2.3103532791137695 4300
val loss = 1.3187569379806519
training loss = 2.398383855819702 4400
val loss = 1.655606985092163
training loss = 2.3092827796936035 4500
val loss = 1.3172314167022705
training loss = 2.3087565898895264 4600
val loss = 1.3262087106704712
training loss = 2.307901620864868 4700
val loss = 1.3107143640518188
training loss = 2.306994915008545 4800
val loss = 1.3149826526641846
training loss = 2.3072729110717773 4900
val loss = 1.3427528142929077
training loss = 2.304921865463257 5000
val loss = 1.3110851049423218
training loss = 2.305168628692627 5100
val loss = 1.3411473035812378
training loss = 2.3024239540100098 5200
val loss = 1.3067960739135742
training loss = 2.4016668796539307 5300
val loss = 1.6680225133895874
training loss = 2.2995388507843018 5400
val loss = 1.2988383769989014
training loss = 2.2979519367218018 5500
val loss = 1.2994463443756104
training loss = 2.296597957611084 5600
val loss = 1.3103283643722534
training loss = 2.2945942878723145 5700
val loss = 1.2941288948059082
training loss = 2.297379970550537 5800
val loss = 1.3485932350158691
training loss = 2.291013479232788 5900
val loss = 1.2883795499801636
training loss = 2.4177768230438232 6000
val loss = 1.7134809494018555
training loss = 2.2873287200927734 6100
val loss = 1.2820491790771484
training loss = 2.285461902618408 6200
val loss = 1.2818875312805176
training loss = 2.283665418624878 6300
val loss = 1.2718915939331055
training loss = 2.2817606925964355 6400
val loss = 1.2770038843154907
training loss = 2.5585169792175293 6500
val loss = 1.9961497783660889
training loss = 2.2781035900115967 6600
val loss = 1.2693381309509277
training loss = 2.2762935161590576 6700
val loss = 1.2692182064056396
training loss = 2.2765378952026367 6800
val loss = 1.3028500080108643
training loss = 2.2727606296539307 6900
val loss = 1.2633073329925537
training loss = 2.276944637298584 7000
val loss = 1.20932936668396
training loss = 2.2693233489990234 7100
val loss = 1.2561200857162476
training loss = 2.267634630203247 7200
val loss = 1.254414677619934
training loss = 2.26668381690979 7300
val loss = 1.271704912185669
training loss = 2.2643814086914062 7400
val loss = 1.248091220855713
training loss = 2.2629072666168213 7500
val loss = 1.235607624053955
training loss = 2.2612411975860596 7600
val loss = 1.2374037504196167
training loss = 2.259669303894043 7700
val loss = 1.2390251159667969
training loss = 2.269468307495117 7800
val loss = 1.3286287784576416
training loss = 2.256669044494629 7900
val loss = 1.2334628105163574
training loss = 2.261573076248169 8000
val loss = 1.296891450881958
training loss = 2.2538163661956787 8100
val loss = 1.2314426898956299
training loss = 2.2523884773254395 8200
val loss = 1.2244306802749634
training loss = 2.2733993530273438 8300
val loss = 1.3571628332138062
training loss = 2.249682903289795 8400
val loss = 1.2187567949295044
training loss = 2.2483463287353516 8500
val loss = 1.2162131071090698
training loss = 2.24749755859375 8600
val loss = 1.198626160621643
training loss = 2.245875597000122 8700
val loss = 1.2109792232513428
training loss = 2.2446415424346924 8800
val loss = 1.2087721824645996
training loss = 2.243553638458252 8900
val loss = 1.1982206106185913
training loss = 2.2422986030578613 9000
val loss = 1.2034512758255005
training loss = 2.242652654647827 9100
val loss = 1.2309484481811523
training loss = 2.240072250366211 9200
val loss = 1.1989212036132812
training loss = 2.3627145290374756 9300
val loss = 1.0771808624267578
training loss = 2.2379770278930664 9400
val loss = 1.1913224458694458
training loss = 2.236952781677246 9500
val loss = 1.1918296813964844
training loss = 2.2378299236297607 9600
val loss = 1.2230339050292969
training loss = 2.235041856765747 9700
val loss = 1.1875431537628174
training loss = 2.258216142654419 9800
val loss = 1.0991358757019043
training loss = 2.2332303524017334 9900
val loss = 1.1841562986373901
training loss = 2.2323617935180664 10000
val loss = 1.1815518140792847
training loss = 2.2315261363983154 10100
val loss = 1.1818594932556152
training loss = 2.2307167053222656 10200
val loss = 1.1776371002197266
training loss = 2.243476152420044 10300
val loss = 1.2709721326828003
training loss = 2.2291738986968994 10400
val loss = 1.1729772090911865
training loss = 2.2284278869628906 10500
val loss = 1.1726353168487549
training loss = 2.2284302711486816 10600
val loss = 1.1906018257141113
training loss = 2.2270307540893555 10700
val loss = 1.1692266464233398
training loss = 2.3610613346099854 10800
val loss = 1.5774922370910645
training loss = 2.225727081298828 10900
val loss = 1.1681638956069946
training loss = 2.225094795227051 11000
val loss = 1.1649024486541748
training loss = 2.2785544395446777 11100
val loss = 1.3876304626464844
training loss = 2.2239041328430176 11200
val loss = 1.1610890626907349
training loss = 2.2233328819274902 11300
val loss = 1.161004900932312
training loss = 2.438640832901001 11400
val loss = 1.7263729572296143
training loss = 2.2222421169281006 11500
val loss = 1.1567808389663696
training loss = 2.2217180728912354 11600
val loss = 1.157443642616272
training loss = 2.3669862747192383 11700
val loss = 1.0421427488327026
training loss = 2.220707654953003 11800
val loss = 1.1551405191421509
training loss = 2.220222234725952 11900
val loss = 1.15409517288208
training loss = 2.2208425998687744 12000
val loss = 1.1305046081542969
training loss = 2.219290018081665 12100
val loss = 1.1522092819213867
training loss = 2.219818115234375 12200
val loss = 1.1743829250335693
training loss = 2.2184464931488037 12300
val loss = 1.1551772356033325
training loss = 2.217970848083496 12400
val loss = 1.1496162414550781
training loss = 2.2176549434661865 12500
val loss = 1.156065821647644
training loss = 2.2171471118927 12600
val loss = 1.147813081741333
training loss = 2.5922858715057373 12700
val loss = 2.0009517669677734
training loss = 2.2163562774658203 12800
val loss = 1.1460870504379272
training loss = 2.215975284576416 12900
val loss = 1.1458756923675537
training loss = 2.2168281078338623 13000
val loss = 1.1712273359298706
training loss = 2.2152373790740967 13100
val loss = 1.1445889472961426
training loss = 2.215195655822754 13200
val loss = 1.1567245721817017
training loss = 2.2145442962646484 13300
val loss = 1.1463466882705688
training loss = 2.214185953140259 13400
val loss = 1.142949104309082
training loss = 2.219550371170044 13500
val loss = 1.0956453084945679
training loss = 2.2135164737701416 13600
val loss = 1.1424006223678589
training loss = 2.213183641433716 13700
val loss = 1.1411372423171997
training loss = 2.2130391597747803 13800
val loss = 1.131772518157959
training loss = 2.212564706802368 13900
val loss = 1.140626072883606
training loss = 2.2122530937194824 14000
val loss = 1.1401252746582031
training loss = 2.2122244834899902 14100
val loss = 1.151412844657898
training loss = 2.211655616760254 14200
val loss = 1.13947594165802
training loss = 2.211406707763672 14300
val loss = 1.1340833902359009
training loss = 2.211087703704834 14400
val loss = 1.1414830684661865
training loss = 2.2107951641082764 14500
val loss = 1.1385228633880615
training loss = 2.236405849456787 14600
val loss = 1.278447151184082
training loss = 2.2102484703063965 14700
val loss = 1.1351311206817627
training loss = 2.2099647521972656 14800
val loss = 1.1377257108688354
training loss = 2.20975923538208 14900
val loss = 1.1306309700012207
training loss = 2.2094366550445557 15000
val loss = 1.1370131969451904
training loss = 2.2091755867004395 15100
val loss = 1.1368597745895386
training loss = 2.2096877098083496 15200
val loss = 1.1178910732269287
training loss = 2.208667278289795 15300
val loss = 1.136717677116394
training loss = 2.315803289413452 15400
val loss = 1.4744939804077148
training loss = 2.2081687450408936 15500
val loss = 1.136669397354126
training loss = 2.207923173904419 15600
val loss = 1.136135220527649
training loss = 2.2085416316986084 15700
val loss = 1.1160300970077515
training loss = 2.207451343536377 15800
val loss = 1.1356947422027588
training loss = 2.207216501235962 15900
val loss = 1.13468337059021
training loss = 2.207176685333252 16000
val loss = 1.1258357763290405
training loss = 2.206756353378296 16100
val loss = 1.1353158950805664
training loss = 2.2589120864868164 16200
val loss = 1.035301685333252
training loss = 2.206310272216797 16300
val loss = 1.1359753608703613
training loss = 2.2060868740081787 16400
val loss = 1.13499116897583
training loss = 2.2078492641448975 16500
val loss = 1.1061534881591797
training loss = 2.2056543827056885 16600
val loss = 1.1349034309387207
training loss = 2.600661039352417 16700
val loss = 1.1526029109954834
training loss = 2.2052292823791504 16800
val loss = 1.1332862377166748
training loss = 2.205019474029541 16900
val loss = 1.1347203254699707
training loss = 2.2089245319366455 17000
val loss = 1.0951461791992188
training loss = 2.2046077251434326 17100
val loss = 1.1337999105453491
training loss = 2.2044084072113037 17200
val loss = 1.1324751377105713
training loss = 2.204291582107544 17300
val loss = 1.1277002096176147
training loss = 2.2040014266967773 17400
val loss = 1.1344337463378906
training loss = 2.203918695449829 17500
val loss = 1.1270201206207275
training loss = 2.203608512878418 17600
val loss = 1.133713722229004
training loss = 2.203411102294922 17700
val loss = 1.134615421295166
training loss = 2.203230857849121 17800
val loss = 1.1364080905914307
training loss = 2.2030327320098877 17900
val loss = 1.1341158151626587
training loss = 2.300544261932373 18000
val loss = 1.0339796543121338
training loss = 2.2026681900024414 18100
val loss = 1.136606216430664
training loss = 2.202460289001465 18200
val loss = 1.1344548463821411
training loss = 2.202596426010132 18300
val loss = 1.145880103111267
training loss = 2.202087163925171 18400
val loss = 1.1338298320770264
training loss = 2.2292275428771973 18500
val loss = 1.273047924041748
training loss = 2.201719284057617 18600
val loss = 1.1331573724746704
training loss = 2.20153546333313 18700
val loss = 1.1339516639709473
training loss = 2.2015380859375 18800
val loss = 1.142256498336792
training loss = 2.201174736022949 18900
val loss = 1.1338539123535156
training loss = 2.200990676879883 19000
val loss = 1.134164810180664
training loss = 2.200849771499634 19100
val loss = 1.137863278388977
training loss = 2.200632333755493 19200
val loss = 1.13380765914917
training loss = 2.41780686378479 19300
val loss = 1.0684102773666382
training loss = 2.2002809047698975 19400
val loss = 1.1312226057052612
training loss = 2.2000882625579834 19500
val loss = 1.1338504552841187
training loss = 2.203387498855591 19600
val loss = 1.0974959135055542
training loss = 2.1997318267822266 19700
val loss = 1.133555293083191
training loss = 2.1995487213134766 19800
val loss = 1.1345229148864746
training loss = 2.1994824409484863 19900
val loss = 1.1405185461044312
training loss = 2.1991889476776123 20000
val loss = 1.1335382461547852
training loss = 2.2455971240997314 20100
val loss = 1.3225226402282715
training loss = 2.198827028274536 20200
val loss = 1.1333540678024292
training loss = 2.1986401081085205 20300
val loss = 1.1335924863815308
training loss = 2.1992428302764893 20400
val loss = 1.1150181293487549
training loss = 2.1982738971710205 20500
val loss = 1.1334431171417236
training loss = 2.2269270420074463 20600
val loss = 1.2719987630844116
training loss = 2.197904586791992 20700
val loss = 1.1323397159576416
training loss = 2.197711706161499 20800
val loss = 1.1335711479187012
training loss = 2.197824239730835 20900
val loss = 1.144168734550476
training loss = 2.197340965270996 21000
val loss = 1.132786512374878
training loss = 2.233501672744751 21100
val loss = 1.2935452461242676
training loss = 2.196965217590332 21200
val loss = 1.1334960460662842
training loss = 2.196768045425415 21300
val loss = 1.1335135698318481
training loss = 2.196753740310669 21400
val loss = 1.1405627727508545
training loss = 2.196385622024536 21500
val loss = 1.1321606636047363
training loss = 2.212222099304199 21600
val loss = 1.0624815225601196
training loss = 2.1960091590881348 21700
val loss = 1.1306782960891724
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 826.7430,  906.5176,  946.4394,  952.4284, 1015.2021, 1060.5870,
        1051.7361, 1130.8153, 1147.2889, 1174.7363, 1182.0771, 1180.6102,
        1273.3584, 1205.8380, 1325.5497, 1380.9569, 1399.7516, 1393.2095,
        1493.3040, 1506.6541, 1563.0376, 1587.5166, 1657.8092, 1632.2897,
        1636.9071, 1722.7325, 1607.9403, 1721.6309, 1705.1821, 1713.0447,
        1715.0651, 1714.9039, 1732.2638, 1650.2031, 1716.3615, 1828.2377,
        1661.0039, 1613.0697, 1638.5681, 1529.9412, 1647.6622, 1569.4268,
        1547.7701, 1524.9832, 1403.9690, 1387.0168, 1287.6580, 1278.0227,
        1145.5695, 1168.2076, 1105.7762,  984.5820,  925.2462,  875.1137,
         892.3630,  843.7368,  846.4377,  681.4066,  596.3007,  525.9051,
         501.2952,  425.1915,  469.3592,  388.3133,  349.5304,  326.8577,
         253.8746,  240.8904,  192.9434,  176.8743,  172.8966,  150.7140,
         154.3951,  111.4030,  107.0881,   55.6186,   56.1915,   40.2473,
          37.3264,   41.1349,   18.3556,   45.3193,   35.6915])]
2451.408390753551
4.487392172854429 2.5120890193880707 23.001663002975214
val isze = 8
idinces = [26 29 37 49 30 74 71 27 14 10 28 21 34 31 75 15 41 48 45 11 65  2 24 50
 73 13 35 59  1  0 42 16 51  5 22 56  8 60 77 53 63 20 23 54 57 62 64  3
 44 70 80 81 55 38 17 12 61  7 39 76 40 66 79 47 52 68 82 58 67  4 78 69
  9 25 72 36 32 33 18 19  6 46 43]
we are doing training validation split
training loss = 233.1143798828125 100
val loss = 255.8431854248047
training loss = 22.140348434448242 200
val loss = 21.831701278686523
training loss = 12.259050369262695 300
val loss = 20.660316467285156
training loss = 6.969351768493652 400
val loss = 6.2559967041015625
training loss = 4.327827453613281 500
val loss = 3.3431098461151123
training loss = 3.4411776065826416 600
val loss = 2.1534035205841064
training loss = 3.051018238067627 700
val loss = 2.005321979522705
training loss = 2.8949100971221924 800
val loss = 1.8077069520950317
training loss = 2.8204216957092285 900
val loss = 1.7514212131500244
training loss = 2.7749087810516357 1000
val loss = 1.6683515310287476
training loss = 2.7436141967773438 1100
val loss = 1.6433799266815186
training loss = 2.7159712314605713 1200
val loss = 1.6317726373672485
training loss = 2.6901824474334717 1300
val loss = 1.6101235151290894
training loss = 2.663107395172119 1400
val loss = 1.6226818561553955
training loss = 2.703580856323242 1500
val loss = 1.8965903520584106
training loss = 2.6067090034484863 1600
val loss = 1.6220040321350098
training loss = 2.5772619247436523 1700
val loss = 1.6263909339904785
training loss = 2.548352003097534 1800
val loss = 1.6446757316589355
training loss = 2.5189170837402344 1900
val loss = 1.6354291439056396
training loss = 2.490816593170166 2000
val loss = 1.6305129528045654
training loss = 2.463253974914551 2100
val loss = 1.6447546482086182
training loss = 2.4447035789489746 2200
val loss = 1.735811471939087
training loss = 2.4106688499450684 2300
val loss = 1.650842547416687
training loss = 2.4021687507629395 2400
val loss = 1.7751193046569824
training loss = 2.360774278640747 2500
val loss = 1.6501102447509766
training loss = 2.4256200790405273 2600
val loss = 1.5084717273712158
training loss = 2.311630964279175 2700
val loss = 1.6379656791687012
training loss = 2.470423698425293 2800
val loss = 1.5042226314544678
training loss = 2.262145757675171 2900
val loss = 1.6195471286773682
training loss = 2.239152669906616 3000
val loss = 1.6522184610366821
training loss = 2.2121663093566895 3100
val loss = 1.5869100093841553
training loss = 2.186789035797119 3200
val loss = 1.5749207735061646
training loss = 2.162869453430176 3300
val loss = 1.5743522644042969
training loss = 2.138594150543213 3400
val loss = 1.5433034896850586
training loss = 2.2119998931884766 3500
val loss = 1.8870396614074707
training loss = 2.0914864540100098 3600
val loss = 1.517657995223999
training loss = 2.0729053020477295 3700
val loss = 1.5627940893173218
training loss = 2.0475921630859375 3800
val loss = 1.5010621547698975
training loss = 2.042717218399048 3900
val loss = 1.5968358516693115
training loss = 2.006821393966675 4000
val loss = 1.4894804954528809
training loss = 2.0107061862945557 4100
val loss = 1.5914921760559082
training loss = 1.9696234464645386 4200
val loss = 1.4854567050933838
training loss = 1.9728035926818848 4300
val loss = 1.622830867767334
training loss = 1.9370687007904053 4400
val loss = 1.484595537185669
training loss = 1.920745849609375 4500
val loss = 1.4794576168060303
training loss = 1.9084501266479492 4600
val loss = 1.4891016483306885
training loss = 1.8945038318634033 4700
val loss = 1.4891180992126465
training loss = 1.9471665620803833 4800
val loss = 1.7308919429779053
training loss = 1.8699935674667358 4900
val loss = 1.4908356666564941
training loss = 1.8589998483657837 5000
val loss = 1.4705653190612793
training loss = 1.8485357761383057 5100
val loss = 1.4866113662719727
training loss = 1.8379594087600708 5200
val loss = 1.494757890701294
training loss = 1.8334530591964722 5300
val loss = 1.4612975120544434
training loss = 1.8194694519042969 5400
val loss = 1.4953969717025757
training loss = 1.817543625831604 5500
val loss = 1.4594237804412842
training loss = 1.8031949996948242 5600
val loss = 1.4939463138580322
training loss = 1.8084214925765991 5700
val loss = 1.4510746002197266
training loss = 1.7888391017913818 5800
val loss = 1.4919553995132446
training loss = 1.7978218793869019 5900
val loss = 1.4497323036193848
training loss = 1.7762441635131836 6000
val loss = 1.4886577129364014
training loss = 1.7708323001861572 6100
val loss = 1.4864897727966309
training loss = 1.7653056383132935 6200
val loss = 1.4841444492340088
training loss = 1.7703214883804321 6300
val loss = 1.452377438545227
training loss = 1.7558114528656006 6400
val loss = 1.4805421829223633
training loss = 1.7519121170043945 6500
val loss = 1.4717819690704346
training loss = 1.7476167678833008 6600
val loss = 1.4755171537399292
training loss = 1.7465583086013794 6700
val loss = 1.4878242015838623
training loss = 1.7405455112457275 6800
val loss = 1.4715546369552612
training loss = 1.7370777130126953 6900
val loss = 1.4696553945541382
training loss = 1.734470248222351 7000
val loss = 1.4644465446472168
training loss = 1.7313283681869507 7100
val loss = 1.4641075134277344
training loss = 1.7344521284103394 7200
val loss = 1.4947497844696045
training loss = 1.7262024879455566 7300
val loss = 1.4601012468338013
training loss = 2.185171365737915 7400
val loss = 1.846358299255371
training loss = 1.7216730117797852 7500
val loss = 1.4562958478927612
training loss = 1.7193444967269897 7600
val loss = 1.453214168548584
training loss = 1.71785569190979 7700
val loss = 1.4587675333023071
training loss = 1.7152272462844849 7800
val loss = 1.449347972869873
training loss = 1.7239701747894287 7900
val loss = 1.4332128763198853
training loss = 1.7113687992095947 8000
val loss = 1.4443860054016113
training loss = 1.7092863321304321 8100
val loss = 1.4413666725158691
training loss = 1.7452716827392578 8200
val loss = 1.452355146408081
training loss = 1.7054181098937988 8300
val loss = 1.4367666244506836
training loss = 1.7032877206802368 8400
val loss = 1.4322707653045654
training loss = 1.7013975381851196 8500
val loss = 1.433582067489624
training loss = 1.6991322040557861 8600
val loss = 1.427605390548706
training loss = 1.7069706916809082 8700
val loss = 1.4304875135421753
training loss = 1.6947648525238037 8800
val loss = 1.4234745502471924
reduced chi^2 level 2 = 1.6927896738052368
Constrained alpha: 1.978620171546936
Constrained beta: -0.23087702691555023
Constrained gamma: 18.18992805480957
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 832.8408,  854.3598,  932.5640,  965.5778, 1002.3110, 1046.8575,
        1110.1779, 1092.7386, 1198.0917, 1206.0133, 1179.9310, 1220.0551,
        1293.9894, 1268.2015, 1286.3313, 1507.7454, 1376.5509, 1435.8867,
        1533.4731, 1537.8495, 1569.6202, 1526.7847, 1623.9913, 1561.6447,
        1700.5043, 1685.6598, 1648.7759, 1731.8444, 1662.7998, 1631.7113,
        1598.5316, 1703.7408, 1732.0659, 1743.1813, 1734.8547, 1802.8121,
        1654.6201, 1588.2668, 1638.6001, 1693.2227, 1579.1411, 1528.5028,
        1475.7714, 1546.2759, 1338.4417, 1254.3333, 1285.4205, 1242.4451,
        1172.9568, 1204.8091, 1088.0682, 1045.8716,  896.4417,  904.1087,
         933.5549,  834.6843,  825.1992,  699.1324,  625.7112,  484.8148,
         536.2886,  480.5170,  455.0713,  412.9366,  361.5545,  376.7899,
         308.3971,  251.3286,  234.3678,  177.3507,  166.2378,  144.1579,
         122.4081,  108.2639,  104.4878,   71.4326,   42.8680,   32.8043,
          35.1298,   50.9315,   30.0122,   38.5053,   35.0877])]
2604.9512088866722
3.5775112315324082 17.96185885038088 71.67290688314654
val isze = 8
idinces = [78 43 38  2 53 19 66  5 34 33 42  6 74 11 18 21 22 48 41 16 61 67 71 54
 58 17 23 25 69 80 35 68 59 50 14 28 31 37 12 15 56 10 75 82 72  3 70 29
  7 73  4  0 36 52 39 24  1 30 65 45 76 26 51 64 40 47  8 60 55 27  9 63
 13 46 32 57 49 81 79 62 20 44 77]
we are doing training validation split
training loss = 451.31219482421875 100
val loss = 536.3229370117188
training loss = 63.41242218017578 200
val loss = 76.23684692382812
training loss = 9.021601676940918 300
val loss = 6.373029708862305
training loss = 8.805968284606934 400
val loss = 6.291688442230225
training loss = 8.587347984313965 500
val loss = 6.162251949310303
training loss = 8.373624801635742 600
val loss = 6.050459384918213
training loss = 8.173751831054688 700
val loss = 5.963315963745117
training loss = 7.994076728820801 800
val loss = 5.903956413269043
training loss = 7.838014602661133 900
val loss = 5.871618270874023
training loss = 7.706085681915283 1000
val loss = 5.862179756164551
training loss = 7.596391201019287 1100
val loss = 5.86916971206665
training loss = 7.505451202392578 1200
val loss = 5.885190010070801
training loss = 7.429148197174072 1300
val loss = 5.9034037590026855
training loss = 7.363548278808594 1400
val loss = 5.918501853942871
training loss = 7.305403232574463 1500
val loss = 5.927201271057129
training loss = 7.2523040771484375 1600
val loss = 5.928089618682861
training loss = 7.202647686004639 1700
val loss = 5.9211955070495605
training loss = 7.155444622039795 1800
val loss = 5.907439231872559
training loss = 7.110121250152588 1900
val loss = 5.888182640075684
training loss = 7.066352844238281 2000
val loss = 5.864836692810059
training loss = 7.023956298828125 2100
val loss = 5.838666915893555
training loss = 6.98281192779541 2200
val loss = 5.810741901397705
training loss = 6.942822456359863 2300
val loss = 5.781810760498047
training loss = 6.903916835784912 2400
val loss = 5.752397537231445
training loss = 6.8660078048706055 2500
val loss = 5.722935676574707
training loss = 6.829016208648682 2600
val loss = 5.6935906410217285
training loss = 6.792879104614258 2700
val loss = 5.664516925811768
training loss = 6.757526874542236 2800
val loss = 5.635796546936035
training loss = 6.722902297973633 2900
val loss = 5.607472896575928
training loss = 6.688960075378418 3000
val loss = 5.579538345336914
training loss = 6.655670642852783 3100
val loss = 5.5520477294921875
training loss = 6.623021602630615 3200
val loss = 5.525028228759766
training loss = 6.591032981872559 3300
val loss = 5.498481750488281
training loss = 6.559737682342529 3400
val loss = 5.472521781921387
training loss = 6.529201507568359 3500
val loss = 5.4472246170043945
training loss = 6.499527454376221 3600
val loss = 5.422702789306641
training loss = 6.470829010009766 3700
val loss = 5.399107933044434
training loss = 6.443201065063477 3800
val loss = 5.37660551071167
training loss = 6.416654586791992 3900
val loss = 5.355310916900635
training loss = 6.390958786010742 4000
val loss = 5.335202217102051
training loss = 6.3652143478393555 4100
val loss = 5.315871715545654
training loss = 6.336602687835693 4200
val loss = 5.2957963943481445
training loss = 6.295822620391846 4300
val loss = 5.269845485687256
training loss = 6.2085723876953125 4400
val loss = 5.219536781311035
training loss = 6.0005950927734375 4500
val loss = 5.097127914428711
training loss = 5.654565334320068 4600
val loss = 4.800353050231934
training loss = 5.012792110443115 4700
val loss = 4.240644454956055
training loss = 3.8254313468933105 4800
val loss = 3.0954549312591553
training loss = 2.8156285285949707 4900
val loss = 2.176081895828247
training loss = 2.6769161224365234 5000
val loss = 1.9821568727493286
training loss = 2.6394550800323486 5100
val loss = 1.8647940158843994
training loss = 2.6137561798095703 5200
val loss = 1.7910596132278442
training loss = 2.5947842597961426 5300
val loss = 1.7276883125305176
training loss = 2.581801414489746 5400
val loss = 1.709641456604004
training loss = 2.5683374404907227 5500
val loss = 1.6431529521942139
training loss = 2.5590033531188965 5600
val loss = 1.5982987880706787
training loss = 2.550077438354492 5700
val loss = 1.5899193286895752
training loss = 2.5677483081817627 5800
val loss = 1.4787567853927612
training loss = 2.5357003211975098 5900
val loss = 1.5543155670166016
training loss = 2.5292577743530273 6000
val loss = 1.5405175685882568
training loss = 2.5335376262664795 6100
val loss = 1.4629402160644531
training loss = 2.51688814163208 6200
val loss = 1.518738031387329
training loss = 2.5106630325317383 6300
val loss = 1.5104126930236816
training loss = 2.5051558017730713 6400
val loss = 1.4807159900665283
training loss = 2.4975619316101074 6500
val loss = 1.4973974227905273
training loss = 2.4904744625091553 6600
val loss = 1.4996569156646729
training loss = 2.4831180572509766 6700
val loss = 1.4859588146209717
training loss = 2.475473403930664 6800
val loss = 1.4838131666183472
training loss = 2.525348663330078 6900
val loss = 1.349279761314392
training loss = 2.4592738151550293 7000
val loss = 1.4762691259384155
training loss = 2.4509007930755615 7100
val loss = 1.474438190460205
training loss = 2.442876100540161 7200
val loss = 1.4646152257919312
training loss = 2.4347100257873535 7300
val loss = 1.4695740938186646
training loss = 2.464285135269165 7400
val loss = 1.3474435806274414
training loss = 2.4189977645874023 7500
val loss = 1.4644954204559326
training loss = 2.411302089691162 7600
val loss = 1.4615061283111572
training loss = 2.40433669090271 7700
val loss = 1.4732059240341187
training loss = 2.3970248699188232 7800
val loss = 1.4562660455703735
training loss = 2.4752302169799805 7900
val loss = 1.2945327758789062
training loss = 2.383788824081421 8000
val loss = 1.451463222503662
training loss = 2.37752628326416 8100
val loss = 1.4491024017333984
training loss = 2.372558355331421 8200
val loss = 1.4212340116500854
training loss = 2.3660635948181152 8300
val loss = 1.4438109397888184
training loss = 2.369307518005371 8400
val loss = 1.3676570653915405
training loss = 2.3558290004730225 8500
val loss = 1.4383686780929565
training loss = 2.3510570526123047 8600
val loss = 1.435664415359497
training loss = 2.346886157989502 8700
val loss = 1.4172263145446777
training loss = 2.342378854751587 8800
val loss = 1.4290828704833984
training loss = 2.4061973094940186 8900
val loss = 1.723545789718628
training loss = 2.334610939025879 9000
val loss = 1.4232631921768188
training loss = 2.3310062885284424 9100
val loss = 1.4204840660095215
training loss = 2.32820463180542 9200
val loss = 1.4377386569976807
training loss = 2.324524402618408 9300
val loss = 1.414115309715271
training loss = 2.4117848873138428 9400
val loss = 1.226567029953003
training loss = 2.3187835216522217 9500
val loss = 1.4083726406097412
training loss = 2.3161187171936035 9600
val loss = 1.4052623510360718
training loss = 2.3158538341522217 9700
val loss = 1.447649359703064
training loss = 2.3112614154815674 9800
val loss = 1.400048851966858
training loss = 2.3089725971221924 9900
val loss = 1.3986639976501465
training loss = 2.3068981170654297 10000
val loss = 1.3969695568084717
training loss = 2.304871082305908 10100
val loss = 1.3917667865753174
training loss = 2.306849718093872 10200
val loss = 1.4501311779022217
training loss = 2.3011927604675293 10300
val loss = 1.386331558227539
training loss = 2.299450635910034 10400
val loss = 1.384479284286499
training loss = 2.2978851795196533 10500
val loss = 1.3883308172225952
training loss = 2.2962825298309326 10600
val loss = 1.3795815706253052
training loss = 2.567168951034546 10700
val loss = 1.155598521232605
training loss = 2.293381690979004 10800
val loss = 1.375736951828003
training loss = 2.291998863220215 10900
val loss = 1.3729290962219238
training loss = 2.293548345565796 11000
val loss = 1.4237083196640015
training loss = 2.2894062995910645 11100
val loss = 1.3681004047393799
training loss = 2.7035021781921387 11200
val loss = 2.3832013607025146
training loss = 2.2869930267333984 11300
val loss = 1.3648370504379272
training loss = 2.2858216762542725 11400
val loss = 1.3618974685668945
training loss = 2.2852511405944824 11500
val loss = 1.3838684558868408
training loss = 2.283498525619507 11600
val loss = 1.3581888675689697
training loss = 2.282334089279175 11700
val loss = 1.35466730594635
training loss = 2.299964666366577 11800
val loss = 1.238463282585144
training loss = 2.2799665927886963 11900
val loss = 1.3505197763442993
training loss = 2.278707265853882 12000
val loss = 1.3468776941299438
training loss = 2.2777466773986816 12100
val loss = 1.3606317043304443
training loss = 2.2761056423187256 12200
val loss = 1.3409497737884521
training loss = 2.2905373573303223 12300
val loss = 1.2323130369186401
training loss = 2.2731733322143555 12400
val loss = 1.3345555067062378
training loss = 2.2715673446655273 12500
val loss = 1.3328676223754883
training loss = 2.2967069149017334 12600
val loss = 1.1978321075439453
training loss = 2.2681655883789062 12700
val loss = 1.327494740486145
training loss = 2.266357183456421 12800
val loss = 1.3268184661865234
training loss = 2.4835426807403564 12900
val loss = 1.0827549695968628
training loss = 2.262570858001709 13000
val loss = 1.3234736919403076
training loss = 2.260653018951416 13100
val loss = 1.3235459327697754
training loss = 2.258772850036621 13200
val loss = 1.3185635805130005
training loss = 2.2569003105163574 13300
val loss = 1.3203319311141968
training loss = 2.259352684020996 13400
val loss = 1.3881316184997559
training loss = 2.253190755844116 13500
val loss = 1.3196380138397217
training loss = 2.251340389251709 13600
val loss = 1.3189196586608887
training loss = 2.2497124671936035 13700
val loss = 1.3048124313354492
training loss = 2.247760772705078 13800
val loss = 1.3192224502563477
training loss = 2.312319278717041 13900
val loss = 1.6422687768936157
training loss = 2.2443552017211914 14000
val loss = 1.3224239349365234
training loss = 2.242748975753784 14100
val loss = 1.3236429691314697
training loss = 2.245671033859253 14200
val loss = 1.2617006301879883
training loss = 2.2397384643554688 14300
val loss = 1.3277575969696045
training loss = 2.2591769695281982 14400
val loss = 1.494781732559204
training loss = 2.237008810043335 14500
val loss = 1.3305622339248657
training loss = 2.235736608505249 14600
val loss = 1.3296222686767578
training loss = 2.234609603881836 14700
val loss = 1.3434401750564575
training loss = 2.2333741188049316 14800
val loss = 1.3357094526290894
training loss = 2.232492208480835 14900
val loss = 1.3210712671279907
training loss = 2.2312047481536865 15000
val loss = 1.337780237197876
training loss = 2.2534749507904053 15100
val loss = 1.204352617263794
training loss = 2.229234218597412 15200
val loss = 1.3448750972747803
training loss = 2.2282981872558594 15300
val loss = 1.3429410457611084
training loss = 2.229192018508911 15400
val loss = 1.3022820949554443
training loss = 2.2266082763671875 15500
val loss = 1.3455705642700195
training loss = 2.225811719894409 15600
val loss = 1.3468762636184692
training loss = 2.2252416610717773 15700
val loss = 1.3625013828277588
training loss = 2.2243738174438477 15800
val loss = 1.3504722118377686
training loss = 2.2278077602386475 15900
val loss = 1.4219906330108643
training loss = 2.2231106758117676 16000
val loss = 1.3548985719680786
training loss = 2.2225253582000732 16100
val loss = 1.3548409938812256
training loss = 2.2242274284362793 16200
val loss = 1.3084172010421753
training loss = 2.221491575241089 16300
val loss = 1.3572258949279785
training loss = 2.224825859069824 16400
val loss = 1.297118902206421
training loss = 2.2205991744995117 16500
val loss = 1.3594396114349365
training loss = 2.2201974391937256 16600
val loss = 1.3618502616882324
training loss = 2.226358652114868 16700
val loss = 1.4558295011520386
training loss = 2.219498872756958 16800
val loss = 1.3653202056884766
training loss = 2.395630121231079 16900
val loss = 1.0898486375808716
training loss = 2.2189202308654785 17000
val loss = 1.3650527000427246
training loss = 2.2186553478240967 17100
val loss = 1.3687587976455688
training loss = 2.218451976776123 17200
val loss = 1.373512625694275
training loss = 2.218231201171875 17300
val loss = 1.3713206052780151
training loss = 2.2710249423980713 17400
val loss = 1.6703970432281494
training loss = 2.217888832092285 17500
val loss = 1.375704050064087
training loss = 2.2177326679229736 17600
val loss = 1.3745437860488892
training loss = 2.2179672718048096 17700
val loss = 1.3562912940979004
training loss = 2.2175004482269287 17800
val loss = 1.377164363861084
training loss = 2.373431921005249 17900
val loss = 1.9550830125808716
training loss = 2.2173373699188232 18000
val loss = 1.375917673110962
training loss = 2.2172505855560303 18100
val loss = 1.380418300628662
training loss = 2.2172224521636963 18200
val loss = 1.378002405166626
training loss = 2.2171552181243896 18300
val loss = 1.382870078086853
training loss = 2.380324363708496 18400
val loss = 1.1082026958465576
training loss = 2.217104196548462 18500
val loss = 1.3861072063446045
training loss = 2.217078447341919 18600
val loss = 1.3860443830490112
training loss = 2.2245993614196777 18700
val loss = 1.4879955053329468
training loss = 2.2170767784118652 18800
val loss = 1.3874603509902954
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]