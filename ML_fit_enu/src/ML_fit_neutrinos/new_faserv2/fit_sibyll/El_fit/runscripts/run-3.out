84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 832.3665,  907.4854,  953.5618,  894.1406, 1020.2365, 1121.9342,
        1142.8475, 1162.0204, 1080.6786, 1158.1699, 1226.1458, 1193.5000,
        1186.6414, 1266.9763, 1349.2600, 1445.5585, 1404.7891, 1362.6758,
        1635.1466, 1508.7322, 1545.0016, 1498.1672, 1563.3086, 1606.7131,
        1593.4611, 1802.2640, 1547.8446, 1787.7006, 1716.4980, 1686.0638,
        1709.7239, 1734.4406, 1649.9645, 1778.5743, 1744.7107, 1740.0358,
        1687.2021, 1643.3564, 1633.3247, 1560.4335, 1634.5037, 1524.8684,
        1520.2367, 1439.9180, 1412.3713, 1384.0380, 1282.2292, 1189.6569,
        1148.7622, 1189.0366, 1085.9724,  998.5466,  932.4121,  944.6458,
         901.4665,  920.2711,  768.0074,  682.6956,  649.8025,  508.6382,
         565.8211,  456.3096,  451.2099,  396.7949,  340.4574,  338.7736,
         260.0916,  237.9960,  233.5450,  174.2041,  161.2788,  150.6328,
         134.3585,  115.0816,  107.8115,   71.9198,   55.8786,   39.5165,
          35.1762,   43.6238,   13.1299,   39.8916,   49.3352])]
3039.959789647739
2.2361299236626135 3.0708492700540546 94.00972496108369
val isze = 8
idinces = [10 60 35 44 59 32 41 78 38 46 51 24 42 70 66 71 79  8 76  7 27 55 56  0
 40 28 43 61 74 64 30 65 34 62  4 58  2 17 75 69  9 77 68 33 37 53 25 49
 20  3 16 11 50 39 82  1 36  5 13 15 54 21 26 22 45 47 29 12 73 63 52 18
 72 81 31  6 57 14 48 80 67 23 19]
we are doing training validation split
training loss = 30.013334274291992 100
val loss = 32.254676818847656
training loss = 23.058753967285156 200
val loss = 25.600610733032227
training loss = 17.74536895751953 300
val loss = 19.997886657714844
training loss = 14.03609561920166 400
val loss = 16.089746475219727
training loss = 11.512852668762207 500
val loss = 13.379892349243164
training loss = 9.808176040649414 600
val loss = 11.476385116577148
training loss = 8.659635543823242 700
val loss = 10.117088317871094
training loss = 7.887789726257324 800
val loss = 9.131550788879395
training loss = 7.370490074157715 900
val loss = 8.407270431518555
training loss = 7.024299621582031 1000
val loss = 7.868353366851807
training loss = 6.7919697761535645 1100
val loss = 7.462410926818848
training loss = 6.634032726287842 1200
val loss = 7.152469158172607
training loss = 6.523022651672363 1300
val loss = 6.911852836608887
training loss = 6.439538478851318 1400
val loss = 6.720763683319092
training loss = 6.369377136230469 1500
val loss = 6.563971519470215
training loss = 6.301510334014893 1600
val loss = 6.42921257019043
training loss = 6.226625919342041 1700
val loss = 6.306004047393799
training loss = 6.1363205909729 1800
val loss = 6.184819221496582
training loss = 6.023118495941162 1900
val loss = 6.056516170501709
training loss = 5.881030559539795 2000
val loss = 5.911150932312012
training loss = 5.7053542137146 2100
val loss = 5.73655891418457
training loss = 5.490387439727783 2200
val loss = 5.517489433288574
training loss = 5.227484226226807 2300
val loss = 5.237744331359863
training loss = 4.906936168670654 2400
val loss = 4.884373664855957
training loss = 4.523347854614258 2500
val loss = 4.451413631439209
training loss = 4.082602500915527 2600
val loss = 3.9465627670288086
training loss = 3.6112353801727295 2700
val loss = 3.405024528503418
training loss = 3.165684461593628 2800
val loss = 2.902754783630371
training loss = 2.819946050643921 2900
val loss = 2.5370569229125977
training loss = 2.613811492919922 3000
val loss = 2.353278398513794
training loss = 2.5208866596221924 3100
val loss = 2.3038206100463867
training loss = 2.487682819366455 3200
val loss = 2.3102355003356934
training loss = 2.4772956371307373 3300
val loss = 2.325990676879883
training loss = 2.473984956741333 3400
val loss = 2.3372251987457275
training loss = 2.472670793533325 3500
val loss = 2.3434431552886963
training loss = 2.4719197750091553 3600
val loss = 2.3466646671295166
training loss = 2.4713265895843506 3700
val loss = 2.3484416007995605
training loss = 2.4707658290863037 3800
val loss = 2.3495519161224365
training loss = 2.4701943397521973 3900
val loss = 2.350405693054199
training loss = 2.4695937633514404 4000
val loss = 2.3511264324188232
training loss = 2.4689548015594482 4100
val loss = 2.351803779602051
training loss = 2.468270778656006 4200
val loss = 2.3524317741394043
training loss = 2.467538595199585 4300
val loss = 2.3530311584472656
training loss = 2.466752052307129 4400
val loss = 2.3536276817321777
training loss = 2.4659054279327393 4500
val loss = 2.35418438911438
training loss = 2.464994192123413 4600
val loss = 2.354703426361084
training loss = 2.4640109539031982 4700
val loss = 2.3552260398864746
training loss = 2.479170083999634 4800
val loss = 2.4227914810180664
training loss = 2.461709499359131 4900
val loss = 2.355719804763794
training loss = 2.470452308654785 5000
val loss = 2.3286898136138916
training loss = 2.4589717388153076 5100
val loss = 2.3569250106811523
training loss = 2.457491397857666 5200
val loss = 2.355830669403076
training loss = 2.455933094024658 5300
val loss = 2.360403299331665
training loss = 2.4541497230529785 5400
val loss = 2.356288194656372
training loss = 2.4545915126800537 5500
val loss = 2.3767542839050293
training loss = 2.4504928588867188 5600
val loss = 2.3560476303100586
training loss = 2.6888668537139893 5700
val loss = 2.4142520427703857
training loss = 2.446627378463745 5800
val loss = 2.354738712310791
training loss = 2.4446427822113037 5900
val loss = 2.355745315551758
training loss = 2.4430201053619385 6000
val loss = 2.3647866249084473
training loss = 2.440537691116333 6100
val loss = 2.3554768562316895
training loss = 2.4785079956054688 6200
val loss = 2.3156490325927734
training loss = 2.4363744258880615 6300
val loss = 2.3539373874664307
training loss = 2.43426513671875 6400
val loss = 2.3553030490875244
training loss = 2.4322664737701416 6500
val loss = 2.350823402404785
training loss = 2.4300355911254883 6600
val loss = 2.355426788330078
training loss = 2.5600438117980957 6700
val loss = 2.6495742797851562
training loss = 2.4258017539978027 6800
val loss = 2.3547122478485107
training loss = 2.423694372177124 6900
val loss = 2.3551998138427734
training loss = 2.42258358001709 7000
val loss = 2.342700958251953
training loss = 2.4194915294647217 7100
val loss = 2.3552980422973633
training loss = 2.427797317504883 7200
val loss = 2.413036584854126
training loss = 2.4153172969818115 7300
val loss = 2.35587477684021
training loss = 2.525763511657715 7400
val loss = 2.321967363357544
training loss = 2.4111762046813965 7500
val loss = 2.354254722595215
training loss = 2.4091129302978516 7600
val loss = 2.3532819747924805
training loss = 2.40712308883667 7700
val loss = 2.3513429164886475
training loss = 2.4049901962280273 7800
val loss = 2.354912519454956
training loss = 2.4068033695220947 7900
val loss = 2.3886094093322754
training loss = 2.4008986949920654 8000
val loss = 2.3546228408813477
training loss = 2.39957594871521 8100
val loss = 2.3689513206481934
training loss = 2.3967671394348145 8200
val loss = 2.3571739196777344
training loss = 2.3946197032928467 8300
val loss = 2.3544888496398926
training loss = 2.3952903747558594 8400
val loss = 2.3319523334503174
training loss = 2.3903419971466064 8500
val loss = 2.3541526794433594
training loss = 2.388066053390503 8600
val loss = 2.354137420654297
training loss = 2.3859407901763916 8700
val loss = 2.348203659057617
training loss = 2.3834359645843506 8800
val loss = 2.3540937900543213
training loss = 2.3808867931365967 8900
val loss = 2.3538625240325928
training loss = 2.378159761428833 9000
val loss = 2.3557047843933105
training loss = 2.375286340713501 9100
val loss = 2.3537845611572266
training loss = 2.3723676204681396 9200
val loss = 2.3458383083343506
training loss = 2.3687593936920166 9300
val loss = 2.3587088584899902
training loss = 2.3649518489837646 9400
val loss = 2.355988025665283
training loss = 2.401038885116577 9500
val loss = 2.296400785446167
training loss = 2.3564200401306152 9600
val loss = 2.3586792945861816
training loss = 2.3515944480895996 9700
val loss = 2.3614630699157715
training loss = 2.3468170166015625 9800
val loss = 2.3679027557373047
training loss = 2.3417928218841553 9900
val loss = 2.3672726154327393
training loss = 2.3370015621185303 10000
val loss = 2.3826816082000732
training loss = 2.33152174949646 10100
val loss = 2.3694751262664795
training loss = 2.3261771202087402 10200
val loss = 2.3758866786956787
training loss = 2.3703184127807617 10300
val loss = 2.5528323650360107
training loss = 2.3153507709503174 10400
val loss = 2.3808276653289795
training loss = 2.3096628189086914 10500
val loss = 2.3848464488983154
training loss = 2.304358720779419 10600
val loss = 2.3994104862213135
training loss = 2.2984871864318848 10700
val loss = 2.392120361328125
training loss = 2.2926836013793945 10800
val loss = 2.396454334259033
training loss = 2.2904367446899414 10900
val loss = 2.37026309967041
training loss = 2.2812559604644775 11000
val loss = 2.4055752754211426
training loss = 2.5733771324157715 11100
val loss = 2.4073963165283203
training loss = 2.2700459957122803 11200
val loss = 2.4135828018188477
training loss = 2.2644429206848145 11300
val loss = 2.4208109378814697
training loss = 2.2596957683563232 11400
val loss = 2.41298508644104
training loss = 2.253889799118042 11500
val loss = 2.4319264888763428
training loss = 2.259387969970703 11600
val loss = 2.385080575942993
training loss = 2.2438998222351074 11700
val loss = 2.444334030151367
training loss = 2.248316764831543 11800
val loss = 2.5215628147125244
training loss = 2.234595537185669 11900
val loss = 2.4534363746643066
training loss = 2.2300939559936523 12000
val loss = 2.4624221324920654
training loss = 2.226804733276367 12100
val loss = 2.4875385761260986
training loss = 2.222177028656006 12200
val loss = 2.475066661834717
training loss = 2.2183151245117188 12300
val loss = 2.481156826019287
training loss = 2.2201297283172607 12400
val loss = 2.5420327186584473
training loss = 2.211422920227051 12500
val loss = 2.4936509132385254
training loss = 2.20835280418396 12600
val loss = 2.5121822357177734
training loss = 2.2052059173583984 12700
val loss = 2.5098624229431152
training loss = 2.202225685119629 12800
val loss = 2.5122828483581543
training loss = 2.2017695903778076 12900
val loss = 2.5528151988983154
training loss = 2.196981191635132 13000
val loss = 2.523679733276367
training loss = 2.3037545680999756 13100
val loss = 2.885247230529785
training loss = 2.192323684692383 13200
val loss = 2.5324320793151855
training loss = 2.190042018890381 13300
val loss = 2.539980173110962
training loss = 2.205711603164673 13400
val loss = 2.469029426574707
training loss = 2.1859288215637207 13500
val loss = 2.5510025024414062
training loss = 2.1841447353363037 13600
val loss = 2.5686397552490234
training loss = 2.182170867919922 13700
val loss = 2.55686616897583
training loss = 2.1802542209625244 13800
val loss = 2.5648012161254883
training loss = 2.19136381149292 13900
val loss = 2.667962074279785
training loss = 2.176917314529419 14000
val loss = 2.574111223220825
training loss = 2.1751418113708496 14100
val loss = 2.57843279838562
training loss = 2.177713394165039 14200
val loss = 2.6352739334106445
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 897.2650,  904.7018,  870.3822,  985.6336, 1018.1398, 1062.6711,
        1118.7124, 1156.8972, 1123.1528, 1144.0658, 1185.6180, 1186.6012,
        1213.6995, 1220.8008, 1312.0428, 1437.8993, 1343.6235, 1398.1069,
        1563.0472, 1485.3073, 1684.0059, 1522.7611, 1621.1882, 1651.1000,
        1669.6832, 1686.2238, 1603.5172, 1646.0011, 1765.5211, 1642.8629,
        1590.4874, 1755.1331, 1770.9552, 1715.6587, 1689.9435, 1770.9603,
        1645.2947, 1699.2002, 1670.6565, 1515.5416, 1616.8705, 1549.3296,
        1539.4583, 1476.4525, 1373.7097, 1263.9923, 1254.7180, 1285.7864,
        1159.7816, 1212.9319, 1090.1639,  987.3608,  894.0431,  924.0457,
         889.5501,  860.7281,  803.9338,  672.2606,  622.5281,  537.0542,
         541.3223,  477.8991,  416.5576,  411.7449,  375.1123,  306.5123,
         303.6592,  234.2276,  209.0514,  153.1813,  164.6898,  143.7511,
         149.9254,   92.3192,  110.4517,   59.6701,   57.4638,   38.0530,
          32.5580,   35.5239,   18.5032,   46.3177,   39.1608])]
3039.175671962624
3.7910642258243614 13.061911364216233 50.857492050409434
val isze = 8
idinces = [61 30 14 75 73 11 18 55 78 33 68  8 27 70  0 74 51 25  5 21 76 37 32 29
 16  4 77 65 38 59 57 45 80 67 64  1  2 20 47 10 24 39 15  6 26 53 31 34
 40  7 12  3 23 48 54 58 19 71 49 79 60 36 46 66 42 22 52 44 41 50 43 62
 63 81 35 82 28 72 13  9 69 56 17]
we are doing training validation split
training loss = 330.94970703125 100
val loss = 189.63540649414062
training loss = 7.254651069641113 200
val loss = 4.038041114807129
training loss = 7.123479843139648 300
val loss = 4.026884078979492
training loss = 7.072928428649902 400
val loss = 4.048774242401123
training loss = 7.0288543701171875 500
val loss = 4.068065166473389
training loss = 6.990812301635742 600
val loss = 4.0825395584106445
training loss = 6.957544326782227 700
val loss = 4.090819358825684
training loss = 6.927709102630615 800
val loss = 4.092450141906738
training loss = 6.900213241577148 900
val loss = 4.087696552276611
training loss = 6.874295234680176 1000
val loss = 4.077376842498779
training loss = 6.849459648132324 1100
val loss = 4.062546730041504
training loss = 6.8254218101501465 1200
val loss = 4.0443549156188965
training loss = 6.802004814147949 1300
val loss = 4.02386474609375
training loss = 6.779099941253662 1400
val loss = 4.001954078674316
training loss = 6.756617069244385 1500
val loss = 3.9793341159820557
training loss = 6.734476089477539 1600
val loss = 3.9564473628997803
training loss = 6.7125959396362305 1700
val loss = 3.9336609840393066
training loss = 6.690882682800293 1800
val loss = 3.911078929901123
training loss = 6.669250011444092 1900
val loss = 3.8887875080108643
training loss = 6.647603511810303 2000
val loss = 3.8666892051696777
training loss = 6.625849723815918 2100
val loss = 3.8448100090026855
training loss = 6.603899002075195 2200
val loss = 3.823012113571167
training loss = 6.58167028427124 2300
val loss = 3.8011717796325684
training loss = 6.559100151062012 2400
val loss = 3.7792558670043945
training loss = 6.5361409187316895 2500
val loss = 3.7571523189544678
training loss = 6.512786388397217 2600
val loss = 3.7347989082336426
training loss = 6.48903751373291 2700
val loss = 3.7121710777282715
training loss = 6.464840888977051 2800
val loss = 3.689030885696411
training loss = 6.439794063568115 2900
val loss = 3.6648879051208496
training loss = 6.411951065063477 3000
val loss = 3.6375198364257812
training loss = 6.371088981628418 3100
val loss = 3.5966796875
training loss = 6.235388278961182 3200
val loss = 3.4666712284088135
training loss = 5.449400424957275 3300
val loss = 2.7215301990509033
training loss = 3.884648084640503 3400
val loss = 2.3103866577148438
training loss = 2.3896961212158203 3500
val loss = 2.984952211380005
training loss = 2.2031311988830566 3600
val loss = 3.8508992195129395
training loss = 2.1735527515411377 3700
val loss = 3.8896913528442383
training loss = 2.162477970123291 3800
val loss = 3.9255616664886475
training loss = 2.154015302658081 3900
val loss = 3.939970016479492
training loss = 2.147862195968628 4000
val loss = 3.945068359375
training loss = 2.142336845397949 4100
val loss = 3.964230537414551
training loss = 2.138633966445923 4200
val loss = 3.9573090076446533
training loss = 2.1342997550964355 4300
val loss = 3.977152109146118
training loss = 2.1361663341522217 4400
val loss = 4.020853042602539
training loss = 2.1281609535217285 4500
val loss = 3.9834229946136475
training loss = 2.125587224960327 4600
val loss = 3.9844248294830322
training loss = 2.1269943714141846 4700
val loss = 3.953676223754883
training loss = 2.1209352016448975 4800
val loss = 3.9857094287872314
training loss = 2.1261303424835205 4900
val loss = 3.9450626373291016
training loss = 2.116868495941162 5000
val loss = 3.98807430267334
training loss = 2.115027666091919 5100
val loss = 3.985018730163574
training loss = 2.1137921810150146 5200
val loss = 3.9701809883117676
training loss = 2.111541509628296 5300
val loss = 3.983901023864746
training loss = 2.452242851257324 5400
val loss = 3.960604667663574
training loss = 2.1083481311798096 5500
val loss = 3.9835045337677
training loss = 2.106895685195923 5600
val loss = 3.980449914932251
training loss = 2.109419345855713 5700
val loss = 4.017823696136475
training loss = 2.1040759086608887 5800
val loss = 3.97827410697937
training loss = 2.103266716003418 5900
val loss = 3.964181900024414
training loss = 2.1014461517333984 6000
val loss = 3.9784858226776123
training loss = 2.1002237796783447 6100
val loss = 3.9742236137390137
training loss = 2.09908127784729 6200
val loss = 3.974048137664795
training loss = 2.0979084968566895 6300
val loss = 3.9750452041625977
training loss = 2.0967705249786377 6400
val loss = 3.9691591262817383
training loss = 2.1061742305755615 6500
val loss = 3.917692184448242
training loss = 2.094588041305542 6600
val loss = 3.966092348098755
training loss = 2.0935885906219482 6700
val loss = 3.9635231494903564
training loss = 2.092531442642212 6800
val loss = 3.9647562503814697
training loss = 2.091562509536743 6900
val loss = 3.961207389831543
training loss = 2.108870029449463 7000
val loss = 4.051632881164551
training loss = 2.0896172523498535 7100
val loss = 3.957679271697998
training loss = 2.088895320892334 7200
val loss = 3.963857412338257
training loss = 2.087733030319214 7300
val loss = 3.9561800956726074
training loss = 2.0868406295776367 7400
val loss = 3.95241641998291
training loss = 2.0880775451660156 7500
val loss = 3.9795172214508057
training loss = 2.085007667541504 7600
val loss = 3.9484987258911133
training loss = 2.117168664932251 7700
val loss = 3.8670291900634766
training loss = 2.083177089691162 7800
val loss = 3.9447033405303955
training loss = 2.0822970867156982 7900
val loss = 3.9430925846099854
training loss = 2.0815792083740234 8000
val loss = 3.949679136276245
training loss = 2.0804381370544434 8100
val loss = 3.9386956691741943
training loss = 2.09930419921875 8200
val loss = 3.872744083404541
training loss = 2.0785114765167236 8300
val loss = 3.934570789337158
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 907.8271,  852.5016, 1002.8623,  891.1956, 1062.9238, 1034.4293,
        1095.6055, 1115.6982, 1179.7549, 1188.2021, 1189.9969, 1169.9388,
        1251.6603, 1291.6191, 1343.6848, 1434.7848, 1380.3744, 1380.6533,
        1530.6713, 1514.8083, 1690.0009, 1587.4420, 1571.6726, 1577.7872,
        1677.1674, 1658.4591, 1547.0317, 1725.0327, 1780.6924, 1734.9059,
        1606.6560, 1754.2711, 1750.8846, 1695.8239, 1619.1761, 1784.2404,
        1656.4888, 1630.5737, 1629.7407, 1622.5931, 1626.7509, 1544.1425,
        1486.0704, 1475.7748, 1425.7147, 1277.9668, 1233.0693, 1280.1238,
        1192.8300, 1151.4119, 1042.1136, 1045.7981,  966.4165,  992.8173,
         869.4915,  891.9100,  842.2399,  665.6323,  626.7324,  543.4138,
         537.1296,  484.3960,  430.4601,  392.1486,  390.1982,  322.1714,
         284.2289,  248.3339,  195.1059,  166.7256,  149.6810,  153.1001,
         127.6266,   96.7329,   87.9879,   68.3378,   57.9978,   47.4952,
          39.2096,   49.8809,   20.1569,   37.2741,   35.0662])]
2674.763814013124
2.3218189901224875 0.119016782541852 39.00295987934054
val isze = 8
idinces = [69 58 14 10 65 79 36 80 33 41 77 55 76  3 12 43 24 60 26 35 13 59 16 46
 39 19 75 42 74 63 68  1 21 49 30  9 70 50 32 34 17  5 44 66 67 27  6 28
 56 48 38 22 64 45  8 31 73 25  0 71 81 57 62  2 40 52 53 23 37 18  4 61
 78 72 51 47 20 29 11 15 54 82  7]
we are doing training validation split
training loss = 162.9999237060547 100
val loss = 98.66596984863281
training loss = 16.38874053955078 200
val loss = 17.1544189453125
training loss = 12.073970794677734 300
val loss = 10.289271354675293
training loss = 8.693546295166016 400
val loss = 7.717682361602783
training loss = 6.45121955871582 500
val loss = 5.786884307861328
training loss = 4.479296684265137 600
val loss = 4.315118789672852
training loss = 3.589838981628418 700
val loss = 3.798780679702759
training loss = 3.2932095527648926 800
val loss = 3.987030029296875
training loss = 3.154742479324341 900
val loss = 4.216942310333252
training loss = 3.094533920288086 1000
val loss = 4.3320417404174805
training loss = 3.052813768386841 1100
val loss = 4.3901472091674805
training loss = 3.018336772918701 1200
val loss = 4.369694232940674
training loss = 2.9863314628601074 1300
val loss = 4.343581199645996
training loss = 2.9547951221466064 1400
val loss = 4.278012275695801
training loss = 2.9237594604492188 1500
val loss = 4.19254207611084
training loss = 2.8911118507385254 1600
val loss = 4.104494094848633
training loss = 2.879537343978882 1700
val loss = 4.016262531280518
training loss = 2.828291177749634 1800
val loss = 3.910783290863037
training loss = 2.798267126083374 1900
val loss = 3.815375328063965
training loss = 2.7711658477783203 2000
val loss = 3.7317087650299072
training loss = 2.7425310611724854 2100
val loss = 3.639963150024414
training loss = 2.7171828746795654 2200
val loss = 3.564391613006592
training loss = 2.6934196949005127 2300
val loss = 3.4910476207733154
training loss = 2.713449001312256 2400
val loss = 3.455007553100586
training loss = 2.649906635284424 2500
val loss = 3.3665578365325928
training loss = 2.629488229751587 2600
val loss = 3.3104591369628906
training loss = 2.609647274017334 2700
val loss = 3.2603282928466797
training loss = 2.590853452682495 2800
val loss = 3.220165252685547
training loss = 2.538008213043213 2900
val loss = 3.0221452713012695
training loss = 2.577826738357544 3000
val loss = 3.1080520153045654
training loss = 2.4812965393066406 3100
val loss = 2.8194971084594727
training loss = 2.4605793952941895 3200
val loss = 2.7135214805603027
training loss = 2.465609073638916 3300
val loss = 2.626459836959839
training loss = 2.553454637527466 3400
val loss = 2.898132801055908
training loss = 2.399683952331543 3500
val loss = 2.5603504180908203
training loss = 2.3896467685699463 3600
val loss = 2.4589099884033203
training loss = 2.3651652336120605 3700
val loss = 2.4162726402282715
training loss = 2.363048553466797 3800
val loss = 2.5208988189697266
training loss = 2.3151769638061523 3900
val loss = 2.3348629474639893
training loss = 2.291093349456787 4000
val loss = 2.3065295219421387
training loss = 2.2735462188720703 4100
val loss = 2.426832914352417
training loss = 2.2007105350494385 4200
val loss = 2.263518810272217
training loss = 2.172900438308716 4300
val loss = 2.235291004180908
training loss = 2.1391947269439697 4400
val loss = 2.217559814453125
training loss = 2.030884027481079 4500
val loss = 2.200026035308838
training loss = 2.0424537658691406 4600
val loss = 2.4256932735443115
training loss = 1.9391568899154663 4700
val loss = 2.168893814086914
training loss = 1.9424129724502563 4800
val loss = 2.0941848754882812
training loss = 1.9509265422821045 4900
val loss = 2.3645431995391846
training loss = 1.9906429052352905 5000
val loss = 2.506720542907715
training loss = 1.8604183197021484 5100
val loss = 2.063870429992676
training loss = 1.858130693435669 5200
val loss = 2.0590837001800537
training loss = 1.8701200485229492 5300
val loss = 2.256920099258423
training loss = 1.9172998666763306 5400
val loss = 2.3705668449401855
training loss = 1.8697079420089722 5500
val loss = 2.2651429176330566
training loss = 1.8772770166397095 5600
val loss = 2.281820297241211
training loss = 1.8074394464492798 5700
val loss = 2.040264844894409
training loss = 1.803992509841919 5800
val loss = 2.080575942993164
training loss = 1.8009496927261353 5900
val loss = 2.041273593902588
training loss = 1.7978121042251587 6000
val loss = 2.046823740005493
training loss = 1.810356855392456 6100
val loss = 2.015291213989258
training loss = 1.8003406524658203 6200
val loss = 2.07423996925354
training loss = 1.885383129119873 6300
val loss = 2.2784903049468994
training loss = 1.839038372039795 6400
val loss = 2.0252058506011963
training loss = 1.7973062992095947 6500
val loss = 2.0197207927703857
training loss = 1.8729840517044067 6600
val loss = 2.04079008102417
training loss = 1.7931632995605469 6700
val loss = 2.015258312225342
training loss = 1.9664925336837769 6800
val loss = 2.399413824081421
training loss = 1.832035779953003 6900
val loss = 2.1571857929229736
training loss = 1.8187742233276367 7000
val loss = 2.0089731216430664
training loss = 1.7900334596633911 7100
val loss = 2.0257720947265625
training loss = 1.8397619724273682 7200
val loss = 2.0147955417633057
training loss = 1.8056306838989258 7300
val loss = 2.001368761062622
training loss = 1.8638851642608643 7400
val loss = 2.2041728496551514
training loss = 1.801026463508606 7500
val loss = 2.069323778152466
training loss = 1.7901355028152466 7600
val loss = 2.0191872119903564
training loss = 1.829614281654358 7700
val loss = 2.0102767944335938
training loss = 1.831917405128479 7800
val loss = 2.0058135986328125
training loss = 1.7899805307388306 7900
val loss = 2.0243911743164062
training loss = 1.8534971475601196 8000
val loss = 2.024742364883423
training loss = 1.791227102279663 8100
val loss = 2.032886266708374
training loss = 1.8275638818740845 8200
val loss = 2.003199577331543
training loss = 1.808807373046875 8300
val loss = 2.0766167640686035
training loss = 1.791512131690979 8400
val loss = 1.9982402324676514
training loss = 1.788663387298584 8500
val loss = 2.0144879817962646
training loss = 1.8597627878189087 8600
val loss = 2.177978992462158
training loss = 1.796829104423523 8700
val loss = 1.9930429458618164
training loss = 1.8204765319824219 8800
val loss = 2.0097687244415283
training loss = 2.0020623207092285 8900
val loss = 2.1543781757354736
training loss = 1.8113998174667358 9000
val loss = 1.9896225929260254
training loss = 1.7881120443344116 9100
val loss = 2.0087389945983887
training loss = 1.8843880891799927 9200
val loss = 2.215639591217041
training loss = 1.8007609844207764 9300
val loss = 2.0558812618255615
training loss = 1.8053302764892578 9400
val loss = 1.9978723526000977
training loss = 1.831540822982788 9500
val loss = 2.118798017501831
training loss = 1.7879226207733154 9600
val loss = 2.011709451675415
training loss = 1.788194179534912 9700
val loss = 2.013929843902588
training loss = 1.8901591300964355 9800
val loss = 2.2227866649627686
training loss = 1.7871720790863037 9900
val loss = 2.0074102878570557
training loss = 1.790571689605713 10000
val loss = 1.9936351776123047
training loss = 1.8392326831817627 10100
val loss = 2.0068817138671875
training loss = 1.8335055112838745 10200
val loss = 2.1231112480163574
training loss = 1.798561692237854 10300
val loss = 2.0496199131011963
training loss = 1.8828058242797852 10400
val loss = 2.210183620452881
training loss = 1.8563053607940674 10500
val loss = 2.0284016132354736
training loss = 1.8882218599319458 10600
val loss = 2.219356060028076
training loss = 1.7908016443252563 10700
val loss = 2.0319607257843018
training loss = 1.814422845840454 10800
val loss = 2.0074572563171387
training loss = 1.8070735931396484 10900
val loss = 2.00386905670166
training loss = 1.8006720542907715 11000
val loss = 1.997328281402588
training loss = 1.8250961303710938 11100
val loss = 2.1097729206085205
training loss = 1.808262586593628 11200
val loss = 2.0759894847869873
training loss = 1.7959587574005127 11300
val loss = 2.0521090030670166
training loss = 1.808099389076233 11400
val loss = 2.0784847736358643
training loss = 1.7949564456939697 11500
val loss = 2.0517263412475586
training loss = 1.8029704093933105 11600
val loss = 2.0646932125091553
training loss = 1.7899229526519775 11700
val loss = 2.037980556488037
training loss = 1.8358643054962158 11800
val loss = 2.1332995891571045
training loss = 1.7893339395523071 11900
val loss = 2.041057586669922
training loss = 1.811872959136963 12000
val loss = 2.0160956382751465
training loss = 1.7860990762710571 12100
val loss = 2.012758493423462
training loss = 1.7957631349563599 12200
val loss = 2.0594263076782227
training loss = 2.086557388305664 12300
val loss = 2.516652822494507
training loss = 2.0090818405151367 12400
val loss = 2.2029457092285156
training loss = 1.8223553895950317 12500
val loss = 2.1118526458740234
training loss = 1.8569763898849487 12600
val loss = 2.0570602416992188
training loss = 1.7911195755004883 12700
val loss = 2.025078773498535
training loss = 1.803314208984375 12800
val loss = 2.027743339538574
training loss = 2.2280242443084717 12900
val loss = 2.1523938179016113
training loss = 1.8021732568740845 13000
val loss = 2.061007261276245
training loss = 1.7977958917617798 13100
val loss = 2.048989772796631
training loss = 1.794945478439331 13200
val loss = 2.0442779064178467
training loss = 1.792752742767334 13300
val loss = 2.0416617393493652
training loss = 1.7909164428710938 13400
val loss = 2.0404982566833496
training loss = 1.7893949747085571 13500
val loss = 2.0402519702911377
training loss = 1.7879657745361328 13600
val loss = 2.0396201610565186
training loss = 1.7879517078399658 13700
val loss = 2.0220208168029785
training loss = 1.7856444120407104 13800
val loss = 2.040369987487793
training loss = 1.7846442461013794 13900
val loss = 2.0412850379943848
training loss = 1.783733606338501 14000
val loss = 2.0424413681030273
training loss = 1.8209844827651978 14100
val loss = 2.1058149337768555
training loss = 1.8223295211791992 14200
val loss = 2.07104754447937
training loss = 1.7838139533996582 14300
val loss = 2.0542972087860107
training loss = 1.783115267753601 14400
val loss = 2.0551834106445312
training loss = 1.7825927734375 14500
val loss = 2.0531513690948486
training loss = 1.7827755212783813 14600
val loss = 2.056929349899292
training loss = 1.7998325824737549 14700
val loss = 2.1029956340789795
training loss = 1.7825288772583008 14800
val loss = 2.0639750957489014
training loss = 1.7862085103988647 14900
val loss = 2.0571036338806152
training loss = 1.7823889255523682 15000
val loss = 2.064282178878784
training loss = 1.8023079633712769 15100
val loss = 2.064772129058838
training loss = 1.794936180114746 15200
val loss = 2.0637013912200928
training loss = 1.8153305053710938 15300
val loss = 2.0855255126953125
training loss = 1.7845711708068848 15400
val loss = 2.0860610008239746
training loss = 1.7822538614273071 15500
val loss = 2.0748095512390137
training loss = 1.7859517335891724 15600
val loss = 2.090967893600464
training loss = 1.7826601266860962 15700
val loss = 2.0781936645507812
training loss = 1.7955200672149658 15800
val loss = 2.067902088165283
training loss = 1.784885048866272 15900
val loss = 2.088904619216919
training loss = 1.8239315748214722 16000
val loss = 2.1661734580993652
training loss = 1.7851924896240234 16100
val loss = 2.069676637649536
training loss = 1.7993212938308716 16200
val loss = 2.0733070373535156
training loss = 1.8139030933380127 16300
val loss = 2.082041025161743
training loss = 1.7830023765563965 16400
val loss = 2.08130145072937
training loss = 1.7848869562149048 16500
val loss = 2.073941230773926
training loss = 1.868411898612976 16600
val loss = 2.1284494400024414
training loss = 1.783065676689148 16700
val loss = 2.086521625518799
training loss = 1.7829785346984863 16800
val loss = 2.0984482765197754
training loss = 1.9940112829208374 16900
val loss = 2.416379451751709
training loss = 2.0022668838500977 17000
val loss = 2.433187484741211
training loss = 1.8077294826507568 17100
val loss = 2.077362060546875
training loss = 1.8014352321624756 17200
val loss = 2.077528238296509
training loss = 1.7860356569290161 17300
val loss = 2.0727810859680176
training loss = 1.8437672853469849 17400
val loss = 2.1099753379821777
training loss = 1.9240059852600098 17500
val loss = 2.3196847438812256
training loss = 1.7843408584594727 17600
val loss = 2.090712070465088
training loss = 1.7984552383422852 17700
val loss = 2.125579595565796
training loss = 1.7829854488372803 17800
val loss = 2.086784839630127
training loss = 1.799212098121643 17900
val loss = 2.0843095779418945
reduced chi^2 level 2 = 1.7849794626235962
Constrained alpha: 2.101053476333618
Constrained beta: -0.027882181107997894
Constrained gamma: 15.920271873474121
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 761.2648,  868.8654,  946.6719,  962.8176,  953.0887, 1083.5824,
        1065.0516, 1064.0157, 1196.0569, 1147.3423, 1161.5208, 1174.2935,
        1228.7274, 1272.3456, 1326.1337, 1447.7219, 1403.6283, 1448.5001,
        1539.5851, 1528.5063, 1557.4059, 1537.7992, 1600.7012, 1671.0809,
        1606.5938, 1700.5834, 1628.2894, 1771.7473, 1673.3419, 1752.8608,
        1615.2261, 1749.5021, 1717.1357, 1741.8682, 1656.7590, 1801.2667,
        1671.1128, 1609.2008, 1629.1528, 1628.5176, 1657.6914, 1613.5986,
        1510.5494, 1498.9449, 1356.0544, 1347.4495, 1278.4435, 1300.5435,
        1149.3394, 1167.4243, 1114.2821,  951.4841,  925.9504,  881.4932,
         903.3090,  838.4943,  786.7775,  717.7728,  616.0312,  496.8215,
         576.1326,  449.0494,  441.6603,  397.2460,  317.7437,  307.2379,
         301.9631,  257.1257,  217.5820,  185.1103,  158.5450,  149.0867,
         155.0575,   93.7950,  103.2223,   75.1152,   64.3706,   40.5486,
          41.2027,   48.4734,   19.4849,   34.0543,   26.4153])]
2793.957313142594
4.4317704620699745 19.061540742484016 95.18260215928817
val isze = 8
idinces = [69 20  6 61 26 59 71 39 66  2 27 10  5 43 81 16  7 53 57 62 55  9 30 56
 78  3 11 12  8 63 48 40 21 45 24 37 64 70  4 25  0 65 44  1 33 50 67 22
 60 77 38 76 13 73 41 14 46 74 49 47 42 17 34 75 68 31 58 79 19 51 23 15
 54 32 72 35 28 18 52 29 80 36 82]
we are doing training validation split
training loss = 369.6723937988281 100
val loss = 284.16241455078125
training loss = 17.34770965576172 200
val loss = 21.21148681640625
training loss = 16.150257110595703 300
val loss = 18.975933074951172
training loss = 14.83813190460205 400
val loss = 17.07829475402832
training loss = 13.424715042114258 500
val loss = 15.078742027282715
training loss = 12.000020027160645 600
val loss = 13.126693725585938
training loss = 10.662484169006348 700
val loss = 11.389019966125488
training loss = 9.49857234954834 800
val loss = 10.00666618347168
training loss = 8.594646453857422 900
val loss = 9.08510684967041
training loss = 8.013094902038574 1000
val loss = 8.643633842468262
training loss = 7.657925605773926 1100
val loss = 8.49282455444336
training loss = 7.420734882354736 1200
val loss = 8.459126472473145
training loss = 7.239609718322754 1300
val loss = 8.452483177185059
training loss = 7.080880641937256 1400
val loss = 8.436267852783203
training loss = 6.9241132736206055 1500
val loss = 8.396367073059082
training loss = 6.752999782562256 1600
val loss = 8.323932647705078
training loss = 6.550851821899414 1700
val loss = 8.208447456359863
training loss = 6.298849582672119 1800
val loss = 8.034873008728027
training loss = 5.97725772857666 1900
val loss = 7.782646656036377
training loss = 5.569719314575195 2000
val loss = 7.4259114265441895
training loss = 5.0658979415893555 2100
val loss = 6.929771423339844
training loss = 4.445796012878418 2200
val loss = 6.222139358520508
training loss = 3.685415029525757 2300
val loss = 5.173864364624023
training loss = 2.961574077606201 2400
val loss = 4.017566680908203
training loss = 2.5367727279663086 2500
val loss = 2.7706527709960938
training loss = 2.4001951217651367 2600
val loss = 2.300170660018921
training loss = 2.3325183391571045 2700
val loss = 2.083158493041992
training loss = 2.291142225265503 2800
val loss = 1.9769586324691772
training loss = 2.2621192932128906 2900
val loss = 1.893405556678772
training loss = 2.2406437397003174 3000
val loss = 1.8693609237670898
training loss = 2.2220330238342285 3100
val loss = 1.8182110786437988
training loss = 2.2070834636688232 3200
val loss = 1.82106614112854
training loss = 2.192469596862793 3300
val loss = 1.784869909286499
training loss = 2.179985761642456 3400
val loss = 1.7598965167999268
training loss = 2.1678006649017334 3500
val loss = 1.768372893333435
training loss = 2.15926194190979 3600
val loss = 1.721084713935852
training loss = 2.1461267471313477 3700
val loss = 1.7587511539459229
training loss = 2.183533191680908 3800
val loss = 1.9829411506652832
training loss = 2.1267123222351074 3900
val loss = 1.7509360313415527
training loss = 2.117640733718872 4000
val loss = 1.750598669052124
training loss = 2.1090610027313232 4100
val loss = 1.7491825819015503
training loss = 2.1010701656341553 4200
val loss = 1.7454736232757568
training loss = 2.0931830406188965 4300
val loss = 1.737734317779541
training loss = 2.085648775100708 4400
val loss = 1.7485870122909546
training loss = 2.0783443450927734 4500
val loss = 1.7426286935806274
training loss = 2.072244882583618 4600
val loss = 1.7684226036071777
training loss = 2.064709424972534 4700
val loss = 1.740126371383667
training loss = 2.0718302726745605 4800
val loss = 1.8581796884536743
training loss = 2.0518267154693604 4900
val loss = 1.7406001091003418
training loss = 2.0456488132476807 5000
val loss = 1.7431449890136719
training loss = 2.0397534370422363 5100
val loss = 1.7440083026885986
training loss = 2.0339999198913574 5200
val loss = 1.7410624027252197
training loss = 2.0399701595306396 5300
val loss = 1.849919319152832
training loss = 2.022993803024292 5400
val loss = 1.7417941093444824
training loss = 2.017658233642578 5500
val loss = 1.7442301511764526
training loss = 2.0132250785827637 5600
val loss = 1.7724192142486572
training loss = 2.007418155670166 5700
val loss = 1.7472875118255615
training loss = 2.0119543075561523 5800
val loss = 1.6715826988220215
training loss = 1.997597336769104 5900
val loss = 1.7504768371582031
training loss = 1.992842435836792 6000
val loss = 1.7544736862182617
training loss = 1.9882957935333252 6100
val loss = 1.767369270324707
training loss = 1.983808994293213 6200
val loss = 1.7598741054534912
training loss = 1.9793999195098877 6300
val loss = 1.7631380558013916
training loss = 1.9774425029754639 6400
val loss = 1.7247066497802734
training loss = 1.971091389656067 6500
val loss = 1.7691915035247803
training loss = 1.9670994281768799 6600
val loss = 1.773036241531372
training loss = 1.9668679237365723 6700
val loss = 1.8362207412719727
training loss = 1.9594967365264893 6800
val loss = 1.7803062200546265
training loss = 1.9638302326202393 6900
val loss = 1.7100918292999268
training loss = 1.9525378942489624 7000
val loss = 1.7839365005493164
training loss = 1.9493333101272583 7100
val loss = 1.7908978462219238
training loss = 1.9986162185668945 7200
val loss = 2.0522451400756836
training loss = 1.9434237480163574 7300
val loss = 1.797589898109436
training loss = 1.9407808780670166 7400
val loss = 1.8004308938980103
training loss = 1.9384855031967163 7500
val loss = 1.8167552947998047
training loss = 1.936089277267456 7600
val loss = 1.8058795928955078
training loss = 1.9837642908096313 7700
val loss = 1.652537226676941
training loss = 1.9321823120117188 7800
val loss = 1.8082066774368286
training loss = 1.9305223226547241 7900
val loss = 1.811895728111267
training loss = 1.9297298192977905 8000
val loss = 1.791086196899414
training loss = 1.9277230501174927 8100
val loss = 1.8150324821472168
training loss = 1.9278370141983032 8200
val loss = 1.8498743772506714
training loss = 1.925574541091919 8300
val loss = 1.8129963874816895
training loss = 1.924690842628479 8400
val loss = 1.8172094821929932
training loss = 1.9240820407867432 8500
val loss = 1.8080432415008545
training loss = 1.9232898950576782 8600
val loss = 1.8191930055618286
training loss = 1.9227266311645508 8700
val loss = 1.8206604719161987
training loss = 1.9222815036773682 8800
val loss = 1.8202393054962158
training loss = 1.9313442707061768 8900
val loss = 1.7411108016967773
training loss = 1.9215905666351318 9000
val loss = 1.8182637691497803
training loss = 1.9213478565216064 9100
val loss = 1.8209474086761475
training loss = 1.923716425895691 9200
val loss = 1.871504783630371
training loss = 1.920974612236023 9300
val loss = 1.821603775024414
training loss = 1.926432728767395 9400
val loss = 1.7587926387786865
training loss = 1.9207748174667358 9500
val loss = 1.8174879550933838
training loss = 1.9207035303115845 9600
val loss = 1.8208132982254028
training loss = 1.9206639528274536 9700
val loss = 1.8178400993347168
training loss = 1.9205862283706665 9800
val loss = 1.8213523626327515
training loss = 1.920587420463562 9900
val loss = 1.8207099437713623
training loss = 1.9212006330490112 10000
val loss = 1.7997846603393555
training loss = 1.9205660820007324 10100
val loss = 1.8215491771697998
training loss = 1.9205971956253052 10200
val loss = 1.8188616037368774
reduced chi^2 level 2 = 1.9230012893676758
Constrained alpha: 1.7682383060455322
Constrained beta: 4.374568462371826
Constrained gamma: 19.1025390625
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 830.2808,  835.8577,  963.1277,  920.6130,  970.7136, 1052.5361,
        1093.5007, 1078.3408, 1080.4971, 1175.5161, 1273.6010, 1232.0594,
        1217.8744, 1231.4540, 1353.1266, 1488.8429, 1423.5398, 1459.0472,
        1543.0663, 1547.7502, 1586.4976, 1516.7794, 1570.2354, 1633.6090,
        1630.0753, 1752.8818, 1634.7896, 1714.5773, 1736.4725, 1672.3131,
        1654.5388, 1743.2931, 1756.2047, 1782.4220, 1658.2900, 1790.9900,
        1691.4515, 1565.2736, 1564.0135, 1663.3357, 1600.4126, 1562.4805,
        1433.2996, 1559.1063, 1374.5123, 1339.6475, 1261.9742, 1205.8004,
        1209.3604, 1153.2031, 1095.8126, 1012.8295,  978.4014,  994.4442,
         837.7720,  865.3039,  854.5301,  727.9220,  637.1658,  547.3204,
         562.0164,  490.0945,  424.2383,  357.9823,  357.6449,  346.7524,
         275.5395,  260.9294,  220.0319,  165.5649,  181.4058,  143.7724,
         143.4047,  112.7374,   95.1274,   73.1279,   43.5241,   30.6389,
          30.4332,   53.1080,   21.6326,   38.7880,   33.6404])]
2794.7389591991773
2.0389833512418027 4.888978304673137 0.6549772552468802
val isze = 8
idinces = [ 8 27 31 58 28  2 70 79 41 56 65 40 71 13  9 55 66 59 72 42 14 68 33  3
 62 49 29 18 17 54 50 76  1 74 32 78 80 46 26 10 61 15 67  7 69 45 34 73
 35 43 30 25 23 52 44 21 81 57 53 39 60  4 77  6 64 51 16 12 47 63 48 11
 82 37 38 20  0 24 75  5 22 36 19]
we are doing training validation split
training loss = 53.78557586669922 100
val loss = 56.75048065185547
training loss = 33.30322265625 200
val loss = 39.2286376953125
training loss = 22.37225341796875 300
val loss = 28.822202682495117
training loss = 16.223417282104492 400
val loss = 23.081636428833008
training loss = 12.576848983764648 500
val loss = 19.743196487426758
training loss = 10.309595108032227 600
val loss = 17.703102111816406
training loss = 8.848531723022461 700
val loss = 16.407947540283203
training loss = 7.882073879241943 800
val loss = 15.562236785888672
training loss = 7.230669021606445 900
val loss = 14.998523712158203
training loss = 6.7856645584106445 1000
val loss = 14.616965293884277
training loss = 6.478689193725586 1100
val loss = 14.355506896972656
training loss = 6.265326499938965 1200
val loss = 14.174222946166992
training loss = 6.116022109985352 1300
val loss = 14.046875
training loss = 6.010726451873779 1400
val loss = 13.955793380737305
training loss = 5.935654640197754 1500
val loss = 13.88891315460205
training loss = 5.8812408447265625 1600
val loss = 13.837944030761719
training loss = 5.8408284187316895 1700
val loss = 13.797199249267578
training loss = 5.809772968292236 1800
val loss = 13.762670516967773
training loss = 5.784854412078857 1900
val loss = 13.7317476272583
training loss = 5.763855934143066 2000
val loss = 13.702550888061523
training loss = 5.745266914367676 2100
val loss = 13.673943519592285
training loss = 5.728074550628662 2200
val loss = 13.64505386352539
training loss = 5.711624622344971 2300
val loss = 13.615569114685059
training loss = 5.695497512817383 2400
val loss = 13.585136413574219
training loss = 5.679437637329102 2500
val loss = 13.553718566894531
training loss = 5.663320541381836 2600
val loss = 13.521312713623047
training loss = 5.647086143493652 2700
val loss = 13.488028526306152
training loss = 5.6307373046875 2800
val loss = 13.45400333404541
training loss = 5.614325046539307 2900
val loss = 13.419422149658203
training loss = 5.597933292388916 3000
val loss = 13.384581565856934
training loss = 5.581667900085449 3100
val loss = 13.349717140197754
training loss = 5.565666675567627 3200
val loss = 13.315252304077148
training loss = 5.5500898361206055 3300
val loss = 13.281417846679688
training loss = 5.535094261169434 3400
val loss = 13.248702049255371
training loss = 5.5208611488342285 3500
val loss = 13.217425346374512
training loss = 5.507551193237305 3600
val loss = 13.187975883483887
training loss = 5.495304584503174 3700
val loss = 13.16069221496582
training loss = 5.4842329025268555 3800
val loss = 13.135783195495605
training loss = 5.474390506744385 3900
val loss = 13.11337661743164
training loss = 5.465780735015869 4000
val loss = 13.093555450439453
training loss = 5.458337306976318 4100
val loss = 13.076104164123535
training loss = 5.451923370361328 4200
val loss = 13.060860633850098
training loss = 5.44632625579834 4300
val loss = 13.047260284423828
training loss = 5.441191673278809 4400
val loss = 13.034586906433105
training loss = 5.435931205749512 4500
val loss = 13.021599769592285
training loss = 5.42934513092041 4600
val loss = 13.005424499511719
training loss = 5.418337345123291 4700
val loss = 12.978985786437988
training loss = 5.39242696762085 4800
val loss = 12.917914390563965
training loss = 5.301665782928467 4900
val loss = 12.703657150268555
training loss = 4.973674297332764 5000
val loss = 11.905400276184082
training loss = 4.538601398468018 5100
val loss = 10.897115707397461
training loss = 4.0091962814331055 5200
val loss = 9.658154487609863
training loss = 3.3047494888305664 5300
val loss = 7.7842254638671875
training loss = 2.6593735218048096 5400
val loss = 5.555635452270508
training loss = 2.449514150619507 5500
val loss = 4.274867534637451
training loss = 2.4128520488739014 5600
val loss = 4.020015716552734
training loss = 2.3903918266296387 5700
val loss = 3.9762043952941895
training loss = 2.374326229095459 5800
val loss = 3.9122824668884277
training loss = 2.3587028980255127 5900
val loss = 3.9289913177490234
training loss = 2.3874237537384033 6000
val loss = 4.166131019592285
training loss = 2.3357014656066895 6100
val loss = 3.892486095428467
training loss = 2.3260390758514404 6200
val loss = 3.87583065032959
training loss = 2.3193604946136475 6300
val loss = 3.912059783935547
training loss = 2.3090009689331055 6400
val loss = 3.847259521484375
training loss = 2.3188998699188232 6500
val loss = 3.713387966156006
training loss = 2.294128894805908 6600
val loss = 3.8192532062530518
training loss = 2.2872369289398193 6700
val loss = 3.8096044063568115
training loss = 2.2806413173675537 6800
val loss = 3.7956430912017822
training loss = 2.2744503021240234 6900
val loss = 3.7880547046661377
training loss = 2.4902117252349854 7000
val loss = 4.475022315979004
training loss = 2.2626562118530273 7100
val loss = 3.7671093940734863
training loss = 2.257075309753418 7200
val loss = 3.758284330368042
training loss = 2.2677149772644043 7300
val loss = 3.6393094062805176
training loss = 2.2466447353363037 7400
val loss = 3.7402374744415283
training loss = 2.2416651248931885 7500
val loss = 3.732067346572876
training loss = 2.2758283615112305 7600
val loss = 3.5685644149780273
training loss = 2.2321977615356445 7700
val loss = 3.7153284549713135
training loss = 2.2276358604431152 7800
val loss = 3.7074122428894043
training loss = 2.2234604358673096 7900
val loss = 3.69047212600708
training loss = 2.2192630767822266 8000
val loss = 3.6933441162109375
training loss = 2.225975751876831 8100
val loss = 3.7977700233459473
training loss = 2.2115018367767334 8200
val loss = 3.6819376945495605
training loss = 2.207836866378784 8300
val loss = 3.6734063625335693
training loss = 2.204500436782837 8400
val loss = 3.6514391899108887
training loss = 2.2009925842285156 8500
val loss = 3.662022590637207
training loss = 2.1982650756835938 8600
val loss = 3.6792490482330322
training loss = 2.194758176803589 8700
val loss = 3.655416250228882
training loss = 2.1918389797210693 8800
val loss = 3.645799160003662
training loss = 2.1964340209960938 8900
val loss = 3.564380168914795
training loss = 2.1864023208618164 9000
val loss = 3.637031078338623
training loss = 2.183830738067627 9100
val loss = 3.631809949874878
training loss = 2.1815738677978516 9200
val loss = 3.61665940284729
training loss = 2.1791670322418213 9300
val loss = 3.6245572566986084
training loss = 2.242550849914551 9400
val loss = 3.4390933513641357
training loss = 2.1748948097229004 9500
val loss = 3.615778923034668
training loss = 2.1728830337524414 9600
val loss = 3.6144981384277344
training loss = 2.171356678009033 9700
val loss = 3.6290175914764404
training loss = 2.16924786567688 9800
val loss = 3.608308792114258
training loss = 2.344059467315674 9900
val loss = 4.184135437011719
training loss = 2.1659140586853027 10000
val loss = 3.6063694953918457
training loss = 2.1643424034118652 10100
val loss = 3.6004419326782227
training loss = 2.1646058559417725 10200
val loss = 3.6392202377319336
training loss = 2.1614813804626465 10300
val loss = 3.595777988433838
training loss = 2.160693407058716 10400
val loss = 3.61714506149292
training loss = 2.1588618755340576 10500
val loss = 3.5886406898498535
training loss = 2.157644748687744 10600
val loss = 3.590305805206299
training loss = 2.22672176361084 10700
val loss = 3.9118332862854004
training loss = 2.1554012298583984 10800
val loss = 3.585453987121582
training loss = 2.1543774604797363 10900
val loss = 3.5853075981140137
training loss = 2.1534650325775146 11000
val loss = 3.593812942504883
training loss = 2.152401924133301 11100
val loss = 3.5857584476470947
training loss = 2.1514792442321777 11200
val loss = 3.5811290740966797
training loss = 2.171995162963867 11300
val loss = 3.7383480072021484
training loss = 2.149747371673584 11400
val loss = 3.578746795654297
training loss = 2.1489367485046387 11500
val loss = 3.575146436691284
training loss = 2.1482291221618652 11600
val loss = 3.5845537185668945
training loss = 2.1474251747131348 11700
val loss = 3.5754823684692383
training loss = 2.170788288116455 11800
val loss = 3.449690341949463
training loss = 2.1460301876068115 11900
val loss = 3.5726165771484375
training loss = 2.1453781127929688 12000
val loss = 3.57297682762146
training loss = 2.1447765827178955 12100
val loss = 3.577683925628662
training loss = 2.1441562175750732 12200
val loss = 3.5710227489471436
training loss = 2.4836015701293945 12300
val loss = 3.355721950531006
training loss = 2.1430156230926514 12400
val loss = 3.5714070796966553
training loss = 2.1424882411956787 12500
val loss = 3.568870782852173
training loss = 2.1445765495300293 12600
val loss = 3.5208191871643066
training loss = 2.1414740085601807 12700
val loss = 3.5679914951324463
training loss = 2.141040325164795 12800
val loss = 3.5730481147766113
training loss = 2.1405553817749023 12900
val loss = 3.560789108276367
training loss = 2.1401004791259766 13000
val loss = 3.565916061401367
training loss = 2.2367873191833496 13100
val loss = 3.9562363624572754
training loss = 2.1392557621002197 13200
val loss = 3.567295551300049
training loss = 2.1388697624206543 13300
val loss = 3.5644400119781494
training loss = 2.326317548751831 13400
val loss = 4.159961700439453
training loss = 2.138105630874634 13500
val loss = 3.5652194023132324
training loss = 2.137751579284668 13600
val loss = 3.563164472579956
training loss = 2.14202880859375 13700
val loss = 3.5011043548583984
training loss = 2.1370623111724854 13800
val loss = 3.56268048286438
training loss = 2.136744499206543 13900
val loss = 3.5618462562561035
training loss = 2.137660026550293 14000
val loss = 3.5956125259399414
training loss = 2.136118173599243 14100
val loss = 3.561340808868408
training loss = 2.3608548641204834 14200
val loss = 3.3362584114074707
training loss = 2.1355204582214355 14300
val loss = 3.55999755859375
training loss = 2.135256290435791 14400
val loss = 3.5603528022766113
training loss = 2.135406732559204 14500
val loss = 3.578531503677368
training loss = 2.134713649749756 14600
val loss = 3.558725357055664
training loss = 2.1344850063323975 14700
val loss = 3.555149793624878
training loss = 2.134225845336914 14800
val loss = 3.5636672973632812
training loss = 2.1339731216430664 14900
val loss = 3.5589728355407715
training loss = 2.1360111236572266 15000
val loss = 3.514437198638916
training loss = 2.133502244949341 15100
val loss = 3.5586469173431396
training loss = 2.1338720321655273 15200
val loss = 3.5357372760772705
training loss = 2.1330718994140625 15300
val loss = 3.5535032749176025
training loss = 2.132859706878662 15400
val loss = 3.557891845703125
training loss = 2.2713515758514404 15500
val loss = 3.3470842838287354
training loss = 2.132446765899658 15600
val loss = 3.556560754776001
training loss = 2.132262945175171 15700
val loss = 3.5573208332061768
training loss = 2.1324665546417236 15800
val loss = 3.576172351837158
training loss = 2.1318814754486084 15900
val loss = 3.556741714477539
training loss = 2.1317121982574463 16000
val loss = 3.5567126274108887
training loss = 2.132798910140991 16100
val loss = 3.5910425186157227
training loss = 2.1313579082489014 16200
val loss = 3.5566155910491943
training loss = 2.420933485031128 16300
val loss = 4.351062297821045
training loss = 2.1310195922851562 16400
val loss = 3.556962251663208
training loss = 2.130878210067749 16500
val loss = 3.558213233947754
training loss = 2.1308391094207764 16600
val loss = 3.5444326400756836
training loss = 2.1305577754974365 16700
val loss = 3.5560572147369385
training loss = 2.1304969787597656 16800
val loss = 3.545289993286133
training loss = 2.130258798599243 16900
val loss = 3.555377960205078
training loss = 2.133354425430298 17000
val loss = 3.610518455505371
training loss = 2.1299707889556885 17100
val loss = 3.5546278953552246
training loss = 2.1299984455108643 17200
val loss = 3.5432746410369873
training loss = 2.1297144889831543 17300
val loss = 3.559175491333008
training loss = 2.1295764446258545 17400
val loss = 3.555025577545166
training loss = 2.1322920322418213 17500
val loss = 3.5054800510406494
training loss = 2.129317283630371 17600
val loss = 3.5544371604919434
training loss = 2.2421770095825195 17700
val loss = 3.349015712738037
training loss = 2.129075527191162 17800
val loss = 3.5567712783813477
training loss = 2.1289656162261963 17900
val loss = 3.5546534061431885
training loss = 2.128840684890747 18000
val loss = 3.5577430725097656
training loss = 2.1287310123443604 18100
val loss = 3.554189682006836
training loss = 2.272581100463867 18200
val loss = 4.057547569274902
training loss = 2.128511905670166 18300
val loss = 3.5559823513031006
training loss = 2.1284124851226807 18400
val loss = 3.5537309646606445
training loss = 2.128532886505127 18500
val loss = 3.5391414165496826
training loss = 2.1282031536102295 18600
val loss = 3.554053544998169
training loss = 2.187246561050415 18700
val loss = 3.3817617893218994
training loss = 2.1279983520507812 18800
val loss = 3.5553948879241943
training loss = 2.1279141902923584 18900
val loss = 3.553694725036621
training loss = 2.128624677658081 19000
val loss = 3.525369167327881
training loss = 2.1277177333831787 19100
val loss = 3.553823709487915
training loss = 2.1276423931121826 19200
val loss = 3.551910400390625
training loss = 2.127546787261963 19300
val loss = 3.557330846786499
training loss = 2.1274573802948 19400
val loss = 3.5535144805908203
training loss = 2.176912546157837 19500
val loss = 3.3921921253204346
training loss = 2.12728214263916 19600
val loss = 3.5547685623168945
training loss = 2.1272099018096924 19700
val loss = 3.5532350540161133
training loss = 2.1272871494293213 19800
val loss = 3.5652709007263184
training loss = 2.127037763595581 19900
val loss = 3.553154945373535
training loss = 2.1269731521606445 20000
val loss = 3.553115129470825
training loss = 2.1287057399749756 20100
val loss = 3.5913171768188477
training loss = 2.12679123878479 20200
val loss = 3.553206443786621
training loss = 2.126739978790283 20300
val loss = 3.552814245223999
training loss = 2.12668514251709 20400
val loss = 3.552833080291748
training loss = 2.3643267154693604 20500
val loss = 3.3270561695098877
training loss = 2.126530885696411 20600
val loss = 3.5502562522888184
training loss = 2.126471757888794 20700
val loss = 3.5526537895202637
training loss = 2.134173631668091 20800
val loss = 3.6443023681640625
training loss = 2.126328229904175 20900
val loss = 3.5515122413635254
training loss = 2.1262869834899902 21000
val loss = 3.5555896759033203
training loss = 2.1263036727905273 21100
val loss = 3.542259693145752
training loss = 2.1261398792266846 21200
val loss = 3.5525174140930176
training loss = 2.1270980834960938 21300
val loss = 3.5819764137268066
training loss = 2.126009464263916 21400
val loss = 3.553241729736328
training loss = 2.1259689331054688 21500
val loss = 3.549387216567993
training loss = 2.1259546279907227 21600
val loss = 3.5605342388153076
training loss = 2.125835657119751 21700
val loss = 3.552391529083252
training loss = 2.1340131759643555 21800
val loss = 3.6449778079986572
training loss = 2.1257143020629883 21900
val loss = 3.552382230758667
training loss = 2.1257431507110596 22000
val loss = 3.5601229667663574
training loss = 2.125619888305664 22100
val loss = 3.5472428798675537
training loss = 2.1255578994750977 22200
val loss = 3.552414655685425
training loss = 2.1287992000579834 22300
val loss = 3.4991414546966553
training loss = 2.12544322013855 22400
val loss = 3.5520687103271484
training loss = 2.1254048347473145 22500
val loss = 3.551279067993164
training loss = 2.1255266666412354 22600
val loss = 3.56544828414917
training loss = 2.125295400619507 22700
val loss = 3.5521676540374756
training loss = 2.4702274799346924 22800
val loss = 4.447716236114502
training loss = 2.125196695327759 22900
val loss = 3.5544700622558594
training loss = 2.1251561641693115 23000
val loss = 3.5522944927215576
training loss = 2.1253015995025635 23100
val loss = 3.565524101257324
training loss = 2.1250548362731934 23200
val loss = 3.552262783050537
training loss = 2.133296251296997 23300
val loss = 3.6458818912506104
training loss = 2.1249732971191406 23400
val loss = 3.546954393386841
training loss = 2.124922752380371 23500
val loss = 3.552020311355591
training loss = 2.4329493045806885 23600
val loss = 4.380640029907227
training loss = 2.124825954437256 23700
val loss = 3.5500223636627197
training loss = 2.1247971057891846 23800
val loss = 3.551940441131592
training loss = 2.1302590370178223 23900
val loss = 3.485872268676758
training loss = 2.1247079372406006 24000
val loss = 3.5522735118865967
training loss = 2.1246964931488037 24100
val loss = 3.555713176727295
training loss = 2.1246306896209717 24200
val loss = 3.5556130409240723
training loss = 2.1245925426483154 24300
val loss = 3.551903486251831
training loss = 2.1529667377471924 24400
val loss = 3.4212448596954346
training loss = 2.124507188796997 24500
val loss = 3.551239490509033
training loss = 2.124485731124878 24600
val loss = 3.553062915802002
training loss = 2.124422311782837 24700
val loss = 3.5503807067871094
training loss = 2.124401092529297 24800
val loss = 3.551827907562256
training loss = 2.1342105865478516 24900
val loss = 3.6554794311523438
training loss = 2.1243200302124023 25000
val loss = 3.551927328109741
training loss = 2.1242988109588623 25100
val loss = 3.5523929595947266
training loss = 2.124281167984009 25200
val loss = 3.557603359222412
training loss = 2.1242244243621826 25300
val loss = 3.5519583225250244
training loss = 2.2659809589385986 25400
val loss = 4.048240661621094
training loss = 2.1241419315338135 25500
val loss = 3.5537400245666504
training loss = 2.1241190433502197 25600
val loss = 3.551781177520752
training loss = 2.1242284774780273 25700
val loss = 3.562370777130127
training loss = 2.1240909099578857 25800
val loss = 3.54459547996521
training loss = 2.124023199081421 25900
val loss = 3.5517609119415283
training loss = 2.1241753101348877 26000
val loss = 3.5392584800720215
training loss = 2.1239538192749023 26100
val loss = 3.549863576889038
training loss = 2.123932361602783 26200
val loss = 3.551983118057251
training loss = 2.124375581741333 26300
val loss = 3.5308570861816406
training loss = 2.123859405517578 26400
val loss = 3.5516510009765625
training loss = 2.1244683265686035 26500
val loss = 3.5758161544799805
training loss = 2.123810291290283 26600
val loss = 3.5464582443237305
training loss = 2.123767852783203 26700
val loss = 3.5516796112060547
training loss = 2.197298288345337 26800
val loss = 3.8810958862304688
training loss = 2.123694658279419 26900
val loss = 3.5509819984436035
training loss = 2.123673915863037 27000
val loss = 3.5515291690826416
training loss = 2.1238622665405273 27100
val loss = 3.5665676593780518
training loss = 2.1236016750335693 27200
val loss = 3.551529884338379
training loss = 2.1894612312316895 27300
val loss = 3.372976303100586
training loss = 2.123527765274048 27400
val loss = 3.550084114074707
training loss = 2.123504400253296 27500
val loss = 3.5515074729919434
training loss = 2.12406587600708 27600
val loss = 3.5286073684692383
training loss = 2.123424530029297 27700
val loss = 3.5517988204956055
training loss = 2.1715898513793945 27800
val loss = 3.8074727058410645
training loss = 2.123340606689453 27900
val loss = 3.5505194664001465
training loss = 2.125079870223999 28000
val loss = 3.5133464336395264
training loss = 2.123265027999878 28100
val loss = 3.54724383354187
training loss = 2.1232118606567383 28200
val loss = 3.551671266555786
training loss = 2.123133897781372 28300
val loss = 3.5446653366088867
training loss = 2.123086929321289 28400
val loss = 3.551487445831299
training loss = 2.1230437755584717 28500
val loss = 3.55148983001709
training loss = 2.1230099201202393 28600
val loss = 3.5556318759918213
training loss = 2.1229586601257324 28700
val loss = 3.5587494373321533
training loss = 2.122826099395752 28800
val loss = 3.5513815879821777
training loss = 2.1251134872436523 28900
val loss = 3.505587577819824
training loss = 2.122621536254883 29000
val loss = 3.551307201385498
training loss = 2.473905324935913 29100
val loss = 4.457472324371338
training loss = 2.1223480701446533 29200
val loss = 3.5505387783050537
training loss = 2.1221916675567627 29300
val loss = 3.5508809089660645
training loss = 2.1226742267608643 29400
val loss = 3.5249526500701904
training loss = 2.1217358112335205 29500
val loss = 3.550045967102051
training loss = 2.2388486862182617 29600
val loss = 3.342597723007202
training loss = 2.1210520267486572 29700
val loss = 3.5467369556427
training loss = 2.120573043823242 29800
val loss = 3.5494251251220703
training loss = 2.1262106895446777 29900
val loss = 3.478623628616333
training loss = 2.1191389560699463 30000
val loss = 3.5477471351623535
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 898.7404,  843.5523,  972.1614,  925.4166, 1002.6481, 1101.8658,
        1096.1804, 1134.5087, 1177.4701, 1178.1512, 1227.5015, 1171.7313,
        1300.5754, 1239.2211, 1362.9836, 1436.5479, 1402.3602, 1435.0289,
        1571.1440, 1442.5425, 1581.2659, 1564.8505, 1608.2319, 1681.7795,
        1654.7345, 1685.7938, 1551.2151, 1726.7534, 1715.3844, 1700.9073,
        1696.7744, 1734.7501, 1740.0554, 1757.5856, 1757.6255, 1811.6528,
        1621.4373, 1663.6473, 1599.5991, 1629.7812, 1577.6622, 1546.1685,
        1516.1362, 1512.5811, 1350.5936, 1371.9353, 1254.1345, 1245.0094,
        1112.7346, 1189.3672, 1091.8721,  990.8562,  993.9247,  937.7679,
         858.1775,  842.3325,  843.0055,  665.7902,  590.2663,  543.3574,
         538.6462,  460.3350,  476.1014,  407.0623,  310.4941,  349.4986,
         300.4735,  254.1713,  216.8000,  144.3866,  186.4107,  141.2406,
         154.4133,  105.6015,   85.5588,   57.0893,   56.9005,   48.0808,
          37.1135,   37.8322,   27.0803,   44.4679,   24.9417])]
2433.5261539058038
0.5012337164095054 6.54567063124323 33.86448763132396
val isze = 8
idinces = [76 37 34 74 28 77 50 70 81 72  3 33 19 42 40 31 46 27 10 80 60 64 61 14
 67 23 79 25  8  5 12 63 71 36 58  2 32 26 56 22 69 54  6 21  0 45 39  4
 65 13 30 51 11 29 68 15 78 41  9 62 53 73 48 47 66 43 52 82 35 44 20 55
 16  7 17 59 38  1 24 57 18 75 49]
we are doing training validation split
training loss = 8.368154525756836 100
val loss = 6.379246234893799
training loss = 7.342716693878174 200
val loss = 7.009253978729248
training loss = 7.092426300048828 300
val loss = 7.033724308013916
training loss = 6.845667362213135 400
val loss = 7.11025857925415
training loss = 6.618890285491943 500
val loss = 7.233551979064941
training loss = 6.4199700355529785 600
val loss = 7.397089004516602
training loss = 6.251331329345703 700
val loss = 7.590775012969971
training loss = 6.112071990966797 800
val loss = 7.803956031799316
training loss = 5.999480247497559 900
val loss = 8.026368141174316
training loss = 5.910025596618652 1000
val loss = 8.249192237854004
training loss = 5.839965343475342 1100
val loss = 8.465167999267578
training loss = 5.785713195800781 1200
val loss = 8.668951988220215
training loss = 5.744032382965088 1300
val loss = 8.85633659362793
training loss = 5.712118625640869 1400
val loss = 9.024930000305176
training loss = 5.687618732452393 1500
val loss = 9.173208236694336
training loss = 5.668604373931885 1600
val loss = 9.300704956054688
training loss = 5.65353536605835 1700
val loss = 9.407541275024414
training loss = 5.641201019287109 1800
val loss = 9.494657516479492
training loss = 5.630665302276611 1900
val loss = 9.563152313232422
training loss = 5.621220111846924 2000
val loss = 9.614313125610352
training loss = 5.61231803894043 2100
val loss = 9.649818420410156
training loss = 5.603541851043701 2200
val loss = 9.671102523803711
training loss = 5.594550132751465 2300
val loss = 9.679519653320312
training loss = 5.585046291351318 2400
val loss = 9.676515579223633
training loss = 5.574732303619385 2500
val loss = 9.662742614746094
training loss = 5.563276290893555 2600
val loss = 9.638891220092773
training loss = 5.550259590148926 2700
val loss = 9.604621887207031
training loss = 5.535104274749756 2800
val loss = 9.559028625488281
training loss = 5.516972541809082 2900
val loss = 9.4998197555542
training loss = 5.494631290435791 3000
val loss = 9.422830581665039
training loss = 5.466212749481201 3100
val loss = 9.321338653564453
training loss = 5.4289631843566895 3200
val loss = 9.185101509094238
training loss = 5.379030227661133 3300
val loss = 8.999605178833008
training loss = 5.311592102050781 3400
val loss = 8.748111724853516
training loss = 5.2214813232421875 3500
val loss = 8.418113708496094
training loss = 5.103216171264648 3600
val loss = 8.005753517150879
training loss = 4.948336601257324 3700
val loss = 7.5056657791137695
training loss = 4.7406086921691895 3800
val loss = 6.8849968910217285
training loss = 4.452371120452881 3900
val loss = 6.06915807723999
training loss = 4.04591178894043 4000
val loss = 4.965641975402832
training loss = 3.4950790405273438 4100
val loss = 3.5653021335601807
training loss = 2.8605129718780518 4200
val loss = 2.141787528991699
training loss = 2.3677873611450195 4300
val loss = 1.2415480613708496
training loss = 2.174994468688965 4400
val loss = 0.9834018349647522
training loss = 2.1367835998535156 4500
val loss = 0.951447606086731
training loss = 2.127558708190918 4600
val loss = 0.9389101266860962
training loss = 2.122039794921875 4700
val loss = 0.9241281151771545
training loss = 2.117530584335327 4800
val loss = 0.9094343185424805
training loss = 2.1136624813079834 4900
val loss = 0.895898699760437
training loss = 2.118933916091919 5000
val loss = 0.8153144121170044
training loss = 2.1074106693267822 5100
val loss = 0.8736493587493896
training loss = 2.1049399375915527 5200
val loss = 0.8686186075210571
training loss = 2.102710485458374 5300
val loss = 0.8612526655197144
training loss = 2.100705623626709 5400
val loss = 0.8484112620353699
training loss = 2.0989296436309814 5500
val loss = 0.8381367921829224
training loss = 2.097337484359741 5600
val loss = 0.8363931179046631
training loss = 2.193880558013916 5700
val loss = 1.14192795753479
training loss = 2.0945208072662354 5800
val loss = 0.8246668577194214
training loss = 2.093287944793701 5900
val loss = 0.8224281072616577
training loss = 2.093662738800049 6000
val loss = 0.8490875959396362
training loss = 2.0909461975097656 6100
val loss = 0.8143357634544373
training loss = 2.224987745285034 6200
val loss = 1.1888779401779175
training loss = 2.088735818862915 6300
val loss = 0.8095279932022095
training loss = 2.087660074234009 6400
val loss = 0.8049825429916382
training loss = 2.101320743560791 6500
val loss = 0.7201843857765198
training loss = 2.0853893756866455 6600
val loss = 0.7983967065811157
training loss = 2.0842084884643555 6700
val loss = 0.7965008020401001
training loss = 2.0855584144592285 6800
val loss = 0.8338194489479065
training loss = 2.081545352935791 6900
val loss = 0.7910981178283691
training loss = 2.0813076496124268 7000
val loss = 0.8156298995018005
training loss = 2.0783913135528564 7100
val loss = 0.7846190929412842
training loss = 2.076611280441284 7200
val loss = 0.783052384853363
training loss = 2.1171364784240723 7300
val loss = 0.6573813557624817
training loss = 2.072467803955078 7400
val loss = 0.777238130569458
training loss = 2.0701181888580322 7500
val loss = 0.7766857147216797
training loss = 2.067650556564331 7600
val loss = 0.7703076601028442
training loss = 2.0650506019592285 7700
val loss = 0.7729878425598145
training loss = 2.133464813232422 7800
val loss = 1.0227142572402954
training loss = 2.059631824493408 7900
val loss = 0.7703306674957275
training loss = 2.0568695068359375 8000
val loss = 0.7688558101654053
training loss = 2.054222345352173 8100
val loss = 0.7590550780296326
training loss = 2.0514042377471924 8200
val loss = 0.7659316062927246
training loss = 2.1028003692626953 8300
val loss = 0.9770349264144897
training loss = 2.0459611415863037 8400
val loss = 0.7613784670829773
training loss = 2.043220281600952 8500
val loss = 0.7583276033401489
training loss = 2.0405640602111816 8600
val loss = 0.7518028020858765
training loss = 2.037824869155884 8700
val loss = 0.7541718482971191
training loss = 2.08243989944458 8800
val loss = 0.624652624130249
training loss = 2.032407760620117 8900
val loss = 0.7472923994064331
training loss = 2.0296630859375 9000
val loss = 0.745794951915741
training loss = 2.0345308780670166 9100
val loss = 0.6833847165107727
training loss = 2.024203300476074 9200
val loss = 0.7405354380607605
training loss = 2.0214293003082275 9300
val loss = 0.7375468611717224
training loss = 2.018786668777466 9400
val loss = 0.7445336580276489
training loss = 2.015869379043579 9500
val loss = 0.7320165634155273
training loss = 2.4302008152008057 9600
val loss = 0.5705587863922119
training loss = 2.010388135910034 9700
val loss = 0.7280633449554443
training loss = 2.0076687335968018 9800
val loss = 0.7257431745529175
training loss = 2.025226593017578 9900
val loss = 0.8421894311904907
training loss = 2.0022778511047363 10000
val loss = 0.721939206123352
training loss = 1.9995672702789307 10100
val loss = 0.7211542129516602
training loss = 1.9972426891326904 10200
val loss = 0.7279919385910034
training loss = 1.9945518970489502 10300
val loss = 0.7168845534324646
training loss = 1.9973584413528442 10400
val loss = 0.66563880443573
training loss = 1.9896774291992188 10500
val loss = 0.7099341154098511
training loss = 1.9872764348983765 10600
val loss = 0.7132532596588135
training loss = 1.9895917177200317 10700
val loss = 0.6654254198074341
training loss = 1.9827381372451782 10800
val loss = 0.711618185043335
training loss = 1.998048186302185 10900
val loss = 0.8197323083877563
training loss = 1.9784985780715942 11000
val loss = 0.7113863229751587
training loss = 1.9764280319213867 11100
val loss = 0.7098984718322754
training loss = 1.9754753112792969 11200
val loss = 0.7316339612007141
training loss = 1.9727370738983154 11300
val loss = 0.7090391516685486
training loss = 1.9709022045135498 11400
val loss = 0.7094160318374634
training loss = 1.9696295261383057 11500
val loss = 0.6935470104217529
training loss = 1.9675729274749756 11600
val loss = 0.7093860507011414
training loss = 2.049365520477295 11700
val loss = 0.559796154499054
training loss = 1.9644958972930908 11800
val loss = 0.7066717147827148
training loss = 1.9630106687545776 11900
val loss = 0.7103133201599121
training loss = 1.9659987688064575 12000
val loss = 0.7613949775695801
training loss = 1.9603697061538696 12100
val loss = 0.7109328508377075
training loss = 1.9590775966644287 12200
val loss = 0.7120577096939087
training loss = 1.958016037940979 12300
val loss = 0.7040310502052307
training loss = 1.9567749500274658 12400
val loss = 0.7125086188316345
training loss = 2.0662434101104736 12500
val loss = 1.0410447120666504
training loss = 1.9547160863876343 12600
val loss = 0.7118043899536133
training loss = 1.953726887702942 12700
val loss = 0.7144274115562439
training loss = 2.0614888668060303 12800
val loss = 0.5512616634368896
training loss = 1.9519517421722412 12900
val loss = 0.7138761878013611
training loss = 1.9510952234268188 13000
val loss = 0.7164726853370667
training loss = 1.950345516204834 13100
val loss = 0.7119307518005371
training loss = 1.9495587348937988 13200
val loss = 0.7173771858215332
training loss = 1.9493505954742432 13300
val loss = 0.7364127039909363
training loss = 1.948215365409851 13400
val loss = 0.713358998298645
training loss = 1.947500467300415 13500
val loss = 0.7199344635009766
training loss = 1.9484747648239136 13600
val loss = 0.7507501840591431
training loss = 1.946290135383606 13700
val loss = 0.7208579778671265
training loss = 1.9857163429260254 13800
val loss = 0.6009310483932495
training loss = 1.9451955556869507 13900
val loss = 0.724338710308075
training loss = 1.9446390867233276 14000
val loss = 0.7246111631393433
training loss = 1.944369912147522 14100
val loss = 0.7128568291664124
training loss = 1.9436860084533691 14200
val loss = 0.7249588966369629
training loss = 1.9441354274749756 14300
val loss = 0.7012913227081299
training loss = 1.9427964687347412 14400
val loss = 0.7243635654449463
training loss = 1.942335605621338 14500
val loss = 0.7266496419906616
training loss = 1.9420337677001953 14600
val loss = 0.7206639647483826
training loss = 1.9415556192398071 14700
val loss = 0.728021502494812
training loss = 2.075486183166504 14800
val loss = 1.1091196537017822
training loss = 1.9408341646194458 14900
val loss = 0.7318001985549927
training loss = 1.9404191970825195 15000
val loss = 0.7301973104476929
training loss = 1.942935585975647 15100
val loss = 0.6905902624130249
training loss = 1.9397611618041992 15200
val loss = 0.7307420969009399
training loss = 1.939399003982544 15300
val loss = 0.7338865399360657
training loss = 1.9391778707504272 15400
val loss = 0.7261223196983337
training loss = 1.9387813806533813 15500
val loss = 0.7330752611160278
training loss = 1.9564311504364014 15600
val loss = 0.6442818641662598
training loss = 1.9382023811340332 15700
val loss = 0.7335287928581238
training loss = 1.9382821321487427 15800
val loss = 0.750413179397583
training loss = 1.9376672506332397 15900
val loss = 0.7300907373428345
training loss = 1.937318205833435 16000
val loss = 0.7358617782592773
training loss = 1.9387004375457764 16100
val loss = 0.7052750587463379
training loss = 1.9368236064910889 16200
val loss = 0.7365723848342896
training loss = 1.9365241527557373 16300
val loss = 0.7371580004692078
training loss = 1.9363652467727661 16400
val loss = 0.7410597801208496
training loss = 1.9360339641571045 16500
val loss = 0.7382798194885254
training loss = 1.939437747001648 16600
val loss = 0.6938732266426086
training loss = 1.935563564300537 16700
val loss = 0.7384306192398071
training loss = 1.9352893829345703 16800
val loss = 0.7391133308410645
training loss = 1.9351587295532227 16900
val loss = 0.7333835363388062
reduced chi^2 level 2 = 1.934960961341858
Constrained alpha: 1.8485972881317139
Constrained beta: 3.6069350242614746
Constrained gamma: 15.085846900939941
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 910.0875,  885.4969,  892.1918,  977.0786,  998.2794, 1065.6171,
        1110.2899, 1067.9636, 1248.2314, 1179.3684, 1171.5358, 1173.4146,
        1250.6083, 1319.2535, 1279.9058, 1405.4539, 1449.9341, 1469.6376,
        1504.6888, 1500.9576, 1643.3225, 1609.1605, 1640.6339, 1573.3088,
        1667.0881, 1632.0807, 1512.7990, 1734.3434, 1783.9476, 1688.4712,
        1732.6979, 1692.3977, 1705.3590, 1731.5640, 1665.5116, 1778.5104,
        1728.5413, 1638.2643, 1617.8270, 1675.4487, 1621.7405, 1601.3920,
        1516.9167, 1483.7817, 1306.3785, 1337.8517, 1234.0953, 1211.2646,
        1141.8688, 1174.5409, 1020.1005,  982.2919,  974.2847,  931.0804,
         902.3855,  858.6434,  815.2213,  694.1158,  642.3479,  533.7323,
         557.2623,  453.1358,  454.9788,  392.4297,  341.9372,  351.3550,
         297.7591,  244.3721,  203.3815,  166.5329,  139.9485,  137.4421,
         131.8312,  111.2661,   79.4652,   71.9179,   63.6541,   40.6220,
          31.1594,   38.3953,   17.8220,   33.6680,   29.3268])]
2730.6126040960016
3.843205062167399 18.631331765836098 32.79938391520848
val isze = 8
idinces = [78 47 42 69 53 50 11 28 72 57 19 75 51 68 35 77 15 43 32 58 27  3 55 71
 21 45 33 62 23 16 61  9 66 82 34 79  6  4 49 14 70 76 20  8 54 24 73 40
 36 48 80 10 59  7 52 41 63 12 29  2 44 74  1 56 65 13 26 46 31 37 60 25
  0 67 22 39 17  5 81 30 18 64 38]
we are doing training validation split
training loss = 477.1221923828125 100
val loss = 602.0997924804688
training loss = 55.46693420410156 200
val loss = 53.56336975097656
training loss = 8.959807395935059 300
val loss = 10.85234546661377
training loss = 8.47689437866211 400
val loss = 10.297027587890625
training loss = 7.998326778411865 500
val loss = 9.870577812194824
training loss = 7.551865577697754 600
val loss = 9.450933456420898
training loss = 7.159610271453857 700
val loss = 9.057422637939453
training loss = 6.833943843841553 800
val loss = 8.705160140991211
training loss = 6.576925754547119 900
val loss = 8.402749061584473
training loss = 6.38175630569458 1000
val loss = 8.15198040008545
training loss = 6.236261367797852 1100
val loss = 7.948834419250488
training loss = 6.126974582672119 1200
val loss = 7.786088943481445
training loss = 6.042262077331543 1300
val loss = 7.655482769012451
training loss = 5.973646640777588 1400
val loss = 7.5495805740356445
training loss = 5.915676593780518 1500
val loss = 7.462299346923828
training loss = 5.865118503570557 1600
val loss = 7.389104843139648
training loss = 5.820094585418701 1700
val loss = 7.326567649841309
training loss = 5.779477596282959 1800
val loss = 7.272399425506592
training loss = 5.742523670196533 1900
val loss = 7.224748611450195
training loss = 5.708681106567383 2000
val loss = 7.182326316833496
training loss = 5.677509784698486 2100
val loss = 7.144162654876709
training loss = 5.648647785186768 2200
val loss = 7.109443664550781
training loss = 5.621771812438965 2300
val loss = 7.077690601348877
training loss = 5.596612453460693 2400
val loss = 7.048463344573975
training loss = 5.5729289054870605 2500
val loss = 7.0211896896362305
training loss = 5.550507068634033 2600
val loss = 6.995736122131348
training loss = 5.529166221618652 2700
val loss = 6.971799850463867
training loss = 5.5087385177612305 2800
val loss = 6.9491095542907715
training loss = 5.489078044891357 2900
val loss = 6.927422523498535
training loss = 5.470053195953369 3000
val loss = 6.906675815582275
training loss = 5.451542377471924 3100
val loss = 6.886635780334473
training loss = 5.433441162109375 3200
val loss = 6.867227554321289
training loss = 5.415651798248291 3300
val loss = 6.848292350769043
training loss = 5.39809513092041 3400
val loss = 6.829815864562988
training loss = 5.380706787109375 3500
val loss = 6.8115668296813965
training loss = 5.363442420959473 3600
val loss = 6.793645858764648
training loss = 5.346304893493652 3700
val loss = 6.775908470153809
training loss = 5.368020057678223 3800
val loss = 7.025590896606445
training loss = 5.31303071975708 3900
val loss = 6.743204593658447
training loss = 5.297471523284912 4000
val loss = 6.724721908569336
training loss = 5.2831711769104 4100
val loss = 6.717327117919922
training loss = 5.270385265350342 4200
val loss = 6.697859764099121
training loss = 5.259608745574951 4300
val loss = 6.685673236846924
training loss = 5.250600814819336 4400
val loss = 6.683588981628418
training loss = 5.24639368057251 4500
val loss = 6.6182098388671875
training loss = 5.237725734710693 4600
val loss = 6.672301292419434
training loss = 5.2389373779296875 4700
val loss = 6.765118598937988
training loss = 5.229135036468506 4800
val loss = 6.666601181030273
training loss = 5.225194454193115 4900
val loss = 6.674513816833496
training loss = 5.220053195953369 5000
val loss = 6.659187316894531
training loss = 5.21168327331543 5100
val loss = 6.647633075714111
training loss = 5.193424224853516 5200
val loss = 6.650214195251465
training loss = 5.134066104888916 5300
val loss = 6.570376396179199
training loss = 4.953773021697998 5400
val loss = 6.534515380859375
training loss = 4.551936149597168 5500
val loss = 5.969770908355713
training loss = 3.4564104080200195 5600
val loss = 4.899263381958008
training loss = 2.2079110145568848 5700
val loss = 3.914257526397705
training loss = 2.008927345275879 5800
val loss = 3.7596423625946045
training loss = 2.000873327255249 5900
val loss = 3.7630550861358643
training loss = 1.9959882497787476 6000
val loss = 3.7682034969329834
training loss = 1.9910922050476074 6100
val loss = 3.792794704437256
training loss = 1.9839799404144287 6200
val loss = 3.7549190521240234
training loss = 2.0877528190612793 6300
val loss = 3.468752384185791
training loss = 1.9661434888839722 6400
val loss = 3.7221627235412598
training loss = 1.9546501636505127 6500
val loss = 3.696288585662842
training loss = 1.9431384801864624 6600
val loss = 3.667583703994751
training loss = 1.9319452047348022 6700
val loss = 3.641806125640869
training loss = 1.9240710735321045 6800
val loss = 3.668410539627075
training loss = 1.9135264158248901 6900
val loss = 3.589055299758911
training loss = 1.9062764644622803 7000
val loss = 3.5651602745056152
training loss = 1.9006669521331787 7100
val loss = 3.5280370712280273
training loss = 1.8956854343414307 7200
val loss = 3.5284500122070312
training loss = 1.8915356397628784 7300
val loss = 3.513092517852783
training loss = 1.888044834136963 7400
val loss = 3.4971232414245605
training loss = 1.8851271867752075 7500
val loss = 3.4892756938934326
training loss = 1.9754090309143066 7600
val loss = 3.23602294921875
training loss = 1.8802587985992432 7700
val loss = 3.4676430225372314
training loss = 1.8782711029052734 7800
val loss = 3.4630684852600098
training loss = 1.876621961593628 7900
val loss = 3.4716238975524902
training loss = 1.8747448921203613 8000
val loss = 3.448953628540039
training loss = 1.8731876611709595 8100
val loss = 3.441450595855713
training loss = 1.871712327003479 8200
val loss = 3.4268133640289307
training loss = 1.8700741529464722 8300
val loss = 3.4327757358551025
training loss = 1.8728902339935303 8400
val loss = 3.5038723945617676
training loss = 1.866540789604187 8500
val loss = 3.4196596145629883
training loss = 1.8644698858261108 8600
val loss = 3.4143240451812744
training loss = 1.8621777296066284 8700
val loss = 3.40922474861145
training loss = 1.859982967376709 8800
val loss = 3.4027721881866455
training loss = 1.9080371856689453 8900
val loss = 3.6812801361083984
training loss = 1.8556629419326782 9000
val loss = 3.3912107944488525
training loss = 1.8567023277282715 9100
val loss = 3.4456255435943604
training loss = 1.8510549068450928 9200
val loss = 3.373997926712036
training loss = 1.8485455513000488 9300
val loss = 3.365039348602295
training loss = 1.8462940454483032 9400
val loss = 3.3811450004577637
training loss = 1.8427414894104004 9500
val loss = 3.3449878692626953
training loss = 1.8394039869308472 9600
val loss = 3.3246021270751953
training loss = 1.8357198238372803 9700
val loss = 3.3109893798828125
training loss = 1.8315279483795166 9800
val loss = 3.3049352169036865
training loss = 1.8736211061477661 9900
val loss = 3.549135684967041
training loss = 1.8219879865646362 10000
val loss = 3.269399404525757
training loss = 1.816489338874817 10100
val loss = 3.2515902519226074
training loss = 1.8930550813674927 10200
val loss = 3.028625011444092
training loss = 1.803826093673706 10300
val loss = 3.2079644203186035
training loss = 1.7966439723968506 10400
val loss = 3.174377202987671
training loss = 1.7894821166992188 10500
val loss = 3.1490578651428223
training loss = 1.7817684412002563 10600
val loss = 3.125844955444336
training loss = 1.787916898727417 10700
val loss = 3.2142674922943115
training loss = 1.7659595012664795 10800
val loss = 3.066133737564087
training loss = 1.7576981782913208 10900
val loss = 3.0254335403442383
training loss = 1.7499245405197144 11000
val loss = 2.9980063438415527
training loss = 1.7419286966323853 11100
val loss = 2.9718074798583984
training loss = 1.7392218112945557 11200
val loss = 3.001570224761963
training loss = 1.727044939994812 11300
val loss = 2.909142017364502
training loss = 1.7206120491027832 11400
val loss = 2.8542001247406006
training loss = 1.7134287357330322 11500
val loss = 2.8440418243408203
training loss = 1.7070740461349487 11600
val loss = 2.8194942474365234
training loss = 1.783915638923645 11700
val loss = 2.6583170890808105
training loss = 1.6958194971084595 11800
val loss = 2.762357473373413
training loss = 1.6906073093414307 11900
val loss = 2.736762046813965
training loss = 1.6995731592178345 12000
val loss = 2.6448473930358887
training loss = 1.6818066835403442 12100
val loss = 2.68723726272583
training loss = 1.6777167320251465 12200
val loss = 2.6635396480560303
training loss = 1.6760175228118896 12300
val loss = 2.6705379486083984
training loss = 1.67097008228302 12400
val loss = 2.620924472808838
training loss = 1.729957103729248 12500
val loss = 2.5047760009765625
training loss = 1.6653374433517456 12600
val loss = 2.5849740505218506
training loss = 1.6627610921859741 12700
val loss = 2.564631938934326
training loss = 1.6610127687454224 12800
val loss = 2.5382895469665527
training loss = 1.6586134433746338 12900
val loss = 2.532528877258301
training loss = 1.6581445932388306 13000
val loss = 2.540560722351074
training loss = 1.6552252769470215 13100
val loss = 2.504488229751587
training loss = 1.692389726638794 13200
val loss = 2.6371424198150635
training loss = 1.6524211168289185 13300
val loss = 2.4816734790802
training loss = 1.651099681854248 13400
val loss = 2.4692256450653076
training loss = 1.6527726650238037 13500
val loss = 2.490391731262207
training loss = 1.6489754915237427 13600
val loss = 2.449991226196289
training loss = 1.6479594707489014 13700
val loss = 2.438246726989746
training loss = 1.647292971611023 13800
val loss = 2.43894100189209
training loss = 1.6463133096694946 13900
val loss = 2.425676107406616
training loss = 1.648777723312378 14000
val loss = 2.391296863555908
training loss = 1.6448777914047241 14100
val loss = 2.4112415313720703
training loss = 1.644156575202942 14200
val loss = 2.4054691791534424
training loss = 1.6436569690704346 14300
val loss = 2.3984313011169434
training loss = 1.6430131196975708 14400
val loss = 2.395172595977783
training loss = 1.6424272060394287 14500
val loss = 2.3899989128112793
training loss = 1.642073392868042 14600
val loss = 2.382195472717285
training loss = 1.641484260559082 14700
val loss = 2.381434440612793
training loss = 1.6611050367355347 14800
val loss = 2.467872142791748
training loss = 1.6406590938568115 14900
val loss = 2.3735568523406982
training loss = 1.6402812004089355 15000
val loss = 2.366670608520508
training loss = 1.6399701833724976 15100
val loss = 2.364877223968506
training loss = 1.639527678489685 15200
val loss = 2.3646035194396973
training loss = 1.63978910446167 15300
val loss = 2.3520147800445557
training loss = 1.638919711112976 15400
val loss = 2.359931230545044
training loss = 1.6606252193450928 15500
val loss = 2.3017232418060303
training loss = 1.638401746749878 15600
val loss = 2.356276750564575
training loss = 1.6380910873413086 15700
val loss = 2.353452205657959
training loss = 1.639670968055725 15800
val loss = 2.3357343673706055
training loss = 1.6376367807388306 15900
val loss = 2.3504457473754883
training loss = 1.6373718976974487 16000
val loss = 2.347252368927002
training loss = 1.6374303102493286 16100
val loss = 2.354210615158081
training loss = 1.6369805335998535 16200
val loss = 2.345339298248291
training loss = 1.6454179286956787 16300
val loss = 2.307079315185547
training loss = 1.636643886566162 16400
val loss = 2.3423314094543457
training loss = 1.6364045143127441 16500
val loss = 2.3411824703216553
training loss = 1.6416138410568237 16600
val loss = 2.3813693523406982
training loss = 1.6360912322998047 16700
val loss = 2.33880615234375
training loss = 1.6358792781829834 16800
val loss = 2.337416172027588
training loss = 1.6359002590179443 16900
val loss = 2.3404979705810547
training loss = 1.6355969905853271 17000
val loss = 2.335651397705078
training loss = 1.6354053020477295 17100
val loss = 2.3342905044555664
training loss = 1.635380744934082 17200
val loss = 2.3310790061950684
training loss = 1.6351394653320312 17300
val loss = 2.3324496746063232
training loss = 1.6960341930389404 17400
val loss = 2.2695581912994385
training loss = 1.6348824501037598 17500
val loss = 2.3299942016601562
training loss = 1.6811898946762085 17600
val loss = 2.2721362113952637
training loss = 1.6346453428268433 17700
val loss = 2.3274502754211426
training loss = 1.6344637870788574 17800
val loss = 2.3281497955322266
training loss = 1.6346650123596191 17900
val loss = 2.3203487396240234
training loss = 1.6342353820800781 18000
val loss = 2.3263587951660156
training loss = 1.640895962715149 18100
val loss = 2.293832302093506
training loss = 1.6340222358703613 18200
val loss = 2.3248491287231445
training loss = 1.6611500978469849 18300
val loss = 2.2727861404418945
training loss = 1.633829951286316 18400
val loss = 2.324521064758301
training loss = 1.6336778402328491 18500
val loss = 2.322636127471924
training loss = 1.636799931526184 18600
val loss = 2.298983097076416
training loss = 1.6334881782531738 18700
val loss = 2.3215785026550293
training loss = 1.63350248336792 18800
val loss = 2.323525905609131
training loss = 1.6333222389221191 18900
val loss = 2.3218092918395996
training loss = 1.6331785917282104 19000
val loss = 2.319279670715332
training loss = 1.6334401369094849 19100
val loss = 2.328097343444824
training loss = 1.633002758026123 19200
val loss = 2.318328857421875
training loss = 1.6415796279907227 19300
val loss = 2.2839062213897705
training loss = 1.6328370571136475 19400
val loss = 2.317408561706543
training loss = 1.6351289749145508 19500
val loss = 2.2963790893554688
training loss = 1.6327258348464966 19600
val loss = 2.3146815299987793
training loss = 1.6325669288635254 19700
val loss = 2.3160390853881836
training loss = 1.6621959209442139 19800
val loss = 2.263373851776123
training loss = 1.6324381828308105 19900
val loss = 2.314836025238037
training loss = 1.632312297821045 20000
val loss = 2.3145999908447266
training loss = 1.6461431980133057 20100
val loss = 2.2757606506347656
training loss = 1.6321696043014526 20200
val loss = 2.314509630203247
training loss = 1.6320639848709106 20300
val loss = 2.312112331390381
training loss = 1.6320589780807495 20400
val loss = 2.312425136566162
training loss = 1.6319177150726318 20500
val loss = 2.3131215572357178
training loss = 1.642872929573059 20600
val loss = 2.275428295135498
training loss = 1.631782054901123 20700
val loss = 2.3131208419799805
training loss = 1.6316661834716797 20800
val loss = 2.312157154083252
training loss = 1.6316750049591064 20900
val loss = 2.3152501583099365
training loss = 1.6315170526504517 21000
val loss = 2.311645030975342
training loss = 1.6422960758209229 21100
val loss = 2.3698649406433105
training loss = 1.6313714981079102 21200
val loss = 2.311739921569824
training loss = 1.6351540088653564 21300
val loss = 2.286787509918213
training loss = 1.631255865097046 21400
val loss = 2.31398344039917
training loss = 1.6311137676239014 21500
val loss = 2.3111696243286133
training loss = 1.6314691305160522 21600
val loss = 2.3040788173675537
training loss = 1.6309633255004883 21700
val loss = 2.311152935028076
training loss = 1.6315770149230957 21800
val loss = 2.3228042125701904
training loss = 1.6308085918426514 21900
val loss = 2.3117308616638184
training loss = 1.630697250366211 22000
val loss = 2.3102238178253174
training loss = 1.6306742429733276 22100
val loss = 2.3134074211120605
training loss = 1.6305350065231323 22200
val loss = 2.311601161956787
training loss = 1.6311147212982178 22300
val loss = 2.323507785797119
training loss = 1.6303935050964355 22400
val loss = 2.3132214546203613
training loss = 1.6302597522735596 22500
val loss = 2.3118486404418945
training loss = 1.6312514543533325 22600
val loss = 2.30008864402771
training loss = 1.6300880908966064 22700
val loss = 2.311807870864868
training loss = 1.6299608945846558 22800
val loss = 2.310753345489502
training loss = 1.6299749612808228 22900
val loss = 2.309736967086792
training loss = 1.6297909021377563 23000
val loss = 2.3119261264801025
training loss = 1.6298267841339111 23100
val loss = 2.317098379135132
training loss = 1.6296228170394897 23200
val loss = 2.31231951713562
training loss = 1.6294857263565063 23300
val loss = 2.3111400604248047
training loss = 1.6294797658920288 23400
val loss = 2.314241409301758
training loss = 1.6293138265609741 23500
val loss = 2.3105061054229736
training loss = 1.629292607307434 23600
val loss = 2.3108937740325928
training loss = 1.6291711330413818 23700
val loss = 2.309523105621338
training loss = 1.6290490627288818 23800
val loss = 2.309511423110962
training loss = 1.6297446489334106 23900
val loss = 2.3223700523376465
training loss = 1.6289184093475342 24000
val loss = 2.309471607208252
training loss = 1.9677661657333374 24100
val loss = 2.3731861114501953
training loss = 1.628807783126831 24200
val loss = 2.309704542160034
training loss = 1.6287035942077637 24300
val loss = 2.308654308319092
training loss = 1.6292245388031006 24400
val loss = 2.3196632862091064
training loss = 1.6285979747772217 24500
val loss = 2.3088650703430176
training loss = 1.695192575454712 24600
val loss = 2.2584264278411865
training loss = 1.6285247802734375 24700
val loss = 2.3074262142181396
training loss = 1.6284089088439941 24800
val loss = 2.308661460876465
training loss = 1.634139060974121 24900
val loss = 2.3466293811798096
training loss = 1.628307580947876 25000
val loss = 2.308983325958252
training loss = 1.934181809425354 25100
val loss = 2.3541808128356934
training loss = 1.6282299757003784 25200
val loss = 2.3104724884033203
training loss = 1.6281259059906006 25300
val loss = 2.308830738067627
training loss = 1.6281354427337646 25400
val loss = 2.3104166984558105
training loss = 1.6280280351638794 25500
val loss = 2.309051275253296
training loss = 1.6943109035491943 25600
val loss = 2.2588860988616943
training loss = 1.6279724836349487 25700
val loss = 2.3109703063964844
training loss = 1.627866268157959 25800
val loss = 2.309516668319702
reduced chi^2 level 2 = 1.6278257369995117
Constrained alpha: 1.8320579528808594
Constrained beta: 3.7172532081604004
Constrained gamma: 12.813437461853027
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 800.1029,  880.9995, 1041.0048,  960.7462,  980.0944, 1074.7896,
        1123.0183, 1135.6594, 1133.4725, 1168.4175, 1113.0247, 1177.4272,
        1240.4703, 1238.0437, 1338.0184, 1358.1714, 1421.1193, 1469.7380,
        1552.7446, 1453.9125, 1561.4094, 1611.8334, 1617.0988, 1557.6173,
        1627.3865, 1647.9531, 1573.1838, 1767.3215, 1652.9778, 1673.2976,
        1706.2295, 1772.1718, 1725.8711, 1716.9622, 1691.9171, 1809.2686,
        1633.3103, 1644.1111, 1644.6218, 1608.3016, 1695.7937, 1624.9408,
        1500.0380, 1522.4850, 1439.3307, 1305.5623, 1309.7012, 1216.6906,
        1216.3268, 1219.6302, 1087.5836,  991.3439,  897.3124,  935.5466,
         875.9673,  872.0712,  806.2222,  703.4175,  584.1538,  558.2619,
         493.1504,  497.8415,  412.7528,  365.3759,  352.0238,  333.8610,
         300.3325,  239.8794,  208.9401,  149.1589,  186.6048,  120.2499,
         149.2081,  103.1935,  103.6597,   74.6406,   47.0819,   49.2402,
          41.3887,   38.1105,   22.9410,   38.7486,   33.3269])]
2879.4551990738287
2.639693974562473 7.218255469899557 86.63576217647659
val isze = 8
idinces = [55 22 47 78  5 45 82 42 36 17  1 81 65 30 66 67 51 80  0 54 37 20 53 21
 11 48  8 50  9 28 18 77 64 38 57 75 79  4 61  7 68  3 15 72 58 29  2  6
 19 27 33 60 70 56 24 16 52 35 25 41 76 13 26 59 23 39 31 46 32 34 69 44
 73 71 49 63 62 10 43 14 12 74 40]
we are doing training validation split
training loss = 32.876502990722656 100
val loss = 25.241443634033203
training loss = 21.809160232543945 200
val loss = 11.117140769958496
training loss = 15.654752731323242 300
val loss = 7.126818656921387
training loss = 11.965911865234375 400
val loss = 5.622869968414307
training loss = 9.775130271911621 500
val loss = 5.205684661865234
training loss = 8.461742401123047 600
val loss = 5.242659568786621
training loss = 7.667273044586182 700
val loss = 5.455860137939453
training loss = 7.183619976043701 800
val loss = 5.720092296600342
training loss = 6.887399196624756 900
val loss = 5.978446006774902
training loss = 6.7041449546813965 1000
val loss = 6.206071853637695
training loss = 6.5883965492248535 1100
val loss = 6.393796920776367
training loss = 6.51228141784668 1200
val loss = 6.540169715881348
training loss = 6.45864725112915 1300
val loss = 6.64741849899292
training loss = 6.416828632354736 1400
val loss = 6.719143867492676
training loss = 6.379753112792969 1500
val loss = 6.758745193481445
training loss = 6.341615676879883 1600
val loss = 6.767513275146484
training loss = 6.294747829437256 1700
val loss = 6.740734100341797
training loss = 6.221920013427734 1800
val loss = 6.655378341674805
training loss = 6.070222854614258 1900
val loss = 6.425060272216797
training loss = 5.7234320640563965 2000
val loss = 5.855781078338623
training loss = 5.199497699737549 2100
val loss = 5.169750213623047
training loss = 4.481980800628662 2200
val loss = 4.479761123657227
training loss = 3.563400983810425 2300
val loss = 3.561282157897949
training loss = 2.660271406173706 2400
val loss = 2.6506519317626953
training loss = 2.1229944229125977 2500
val loss = 2.2416563034057617
training loss = 1.9858964681625366 2600
val loss = 2.3203892707824707
training loss = 1.9731734991073608 2700
val loss = 2.390397548675537
training loss = 1.9710818529129028 2800
val loss = 2.3968520164489746
training loss = 1.9696232080459595 2900
val loss = 2.3901166915893555
training loss = 1.9682438373565674 3000
val loss = 2.382046699523926
training loss = 1.966895341873169 3100
val loss = 2.3743345737457275
training loss = 1.9655766487121582 3200
val loss = 2.3670809268951416
training loss = 1.9642914533615112 3300
val loss = 2.3603568077087402
training loss = 1.9630416631698608 3400
val loss = 2.354034423828125
training loss = 1.9618321657180786 3500
val loss = 2.3481550216674805
training loss = 1.9606627225875854 3600
val loss = 2.342667579650879
training loss = 1.9597221612930298 3700
val loss = 2.346358060836792
training loss = 1.9584660530090332 3800
val loss = 2.33363938331604
training loss = 1.9575508832931519 3900
val loss = 2.3242745399475098
training loss = 1.9565048217773438 4000
val loss = 2.326171398162842
training loss = 1.9557275772094727 4100
val loss = 2.3291234970092773
training loss = 1.9547996520996094 4200
val loss = 2.320101022720337
training loss = 1.9542819261550903 4300
val loss = 2.3257699012756348
training loss = 1.9533566236495972 4400
val loss = 2.315312385559082
training loss = 1.9527677297592163 4500
val loss = 2.316117763519287
training loss = 1.9521698951721191 4600
val loss = 2.311089515686035
training loss = 2.0509583950042725 4700
val loss = 2.2452504634857178
training loss = 1.951193928718567 4800
val loss = 2.3073651790618896
training loss = 1.9507992267608643 4900
val loss = 2.3073854446411133
training loss = 1.9507051706314087 5000
val loss = 2.2976770401000977
training loss = 1.9500222206115723 5100
val loss = 2.3061108589172363
training loss = 1.950009822845459 5200
val loss = 2.316072940826416
training loss = 1.949233889579773 5300
val loss = 2.306244373321533
training loss = 1.9499194622039795 5400
val loss = 2.323739767074585
training loss = 1.9483121633529663 5500
val loss = 2.3081839084625244
training loss = 2.1071996688842773 5600
val loss = 2.652038335800171
training loss = 1.9471739530563354 5700
val loss = 2.310776948928833
training loss = 1.9465080499649048 5800
val loss = 2.312281370162964
training loss = 1.945539951324463 5900
val loss = 2.3180267810821533
training loss = 1.9444177150726318 6000
val loss = 2.3192362785339355
training loss = 1.943550705909729 6100
val loss = 2.3341150283813477
training loss = 1.9410454034805298 6200
val loss = 2.3240020275115967
training loss = 2.1226234436035156 6300
val loss = 2.305553913116455
training loss = 1.9362800121307373 6400
val loss = 2.3230645656585693
training loss = 1.9337419271469116 6500
val loss = 2.3176846504211426
training loss = 1.9315820932388306 6600
val loss = 2.3203868865966797
training loss = 1.9291818141937256 6700
val loss = 2.3093719482421875
training loss = 2.282825469970703 6800
val loss = 2.900183916091919
training loss = 1.9254488945007324 6900
val loss = 2.3055882453918457
training loss = 1.9237903356552124 7000
val loss = 2.302959442138672
training loss = 1.9222192764282227 7100
val loss = 2.3044545650482178
training loss = 1.9206771850585938 7200
val loss = 2.305115222930908
training loss = 1.9191652536392212 7300
val loss = 2.3034324645996094
training loss = 1.9185822010040283 7400
val loss = 2.2929880619049072
training loss = 1.9163060188293457 7500
val loss = 2.305678367614746
reduced chi^2 level 2 = 1.9160840511322021
Constrained alpha: 1.884244680404663
Constrained beta: 3.7347640991210938
Constrained gamma: 26.5561580657959
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 899.2630,  822.7499,  942.4839,  939.3964,  983.6627, 1128.4447,
        1094.9802, 1116.6697, 1145.3182, 1138.8143, 1201.6764, 1189.0134,
        1202.0365, 1251.9185, 1406.4343, 1407.7551, 1448.4764, 1443.1559,
        1535.1168, 1466.5168, 1535.6947, 1591.0729, 1602.8624, 1601.3170,
        1631.3085, 1678.9229, 1596.2340, 1681.2861, 1732.8789, 1664.7921,
        1639.2113, 1776.7778, 1709.2534, 1771.8455, 1711.8667, 1773.1313,
        1721.6714, 1640.9064, 1701.4210, 1626.4662, 1654.9900, 1543.2705,
        1549.7578, 1529.9983, 1317.2156, 1339.5199, 1331.0538, 1234.4504,
        1222.9454, 1206.2239, 1063.2736,  984.6823,  926.1414,  899.5820,
         838.8981,  855.0101,  822.3594,  681.2491,  648.1949,  518.7107,
         565.2090,  489.5702,  433.0899,  407.2625,  369.3367,  373.9006,
         269.3047,  259.2586,  205.6230,  155.6914,  152.3229,  156.7815,
         145.9664,  107.4498,  118.8964,   65.0591,   44.2056,   46.2369,
          41.0637,   44.7595,   21.9972,   27.3341,   35.8232])]
2644.1338999617747
3.242225441406586 3.4792346501172533 19.490233989501238
val isze = 8
idinces = [67 38 63  8 34  7 74 48 25 55 77 40 81 42 80  6 73  0 61 11 47 53 12 59
 72  4 35 52 49 69 50 43  5 45 16 24 33  2  9 79 21 51 56  3 39 30 54 18
 27 78 29 13 68 14 57 17 71 65 26 76 82 58 62 75 20 31 44 32 70 41  1 37
 60 19 10 64 22 66 15 36 23 28 46]
we are doing training validation split
training loss = 72.9321517944336 100
val loss = 77.44454193115234
training loss = 37.505672454833984 200
val loss = 42.44375991821289
training loss = 22.436914443969727 300
val loss = 26.060609817504883
training loss = 15.426227569580078 400
val loss = 17.837284088134766
training loss = 11.73812198638916 500
val loss = 13.40791130065918
training loss = 9.62723159790039 600
val loss = 10.911886215209961
training loss = 8.350553512573242 700
val loss = 9.473560333251953
training loss = 7.549549102783203 800
val loss = 8.64229965209961
training loss = 7.034270286560059 900
val loss = 8.17004680633545
training loss = 6.696918964385986 1000
val loss = 7.913087844848633
training loss = 6.473148345947266 1100
val loss = 7.785161018371582
training loss = 6.32304573059082 1200
val loss = 7.733188629150391
training loss = 6.221189975738525 1300
val loss = 7.724107265472412
training loss = 6.151080131530762 1400
val loss = 7.737283706665039
training loss = 6.10186767578125 1500
val loss = 7.759836673736572
training loss = 6.066366672515869 1600
val loss = 7.784089088439941
training loss = 6.039787292480469 1700
val loss = 7.805578231811523
training loss = 6.018889427185059 1800
val loss = 7.821932315826416
training loss = 6.001406192779541 1900
val loss = 7.831989765167236
training loss = 5.985556602478027 2000
val loss = 7.835186004638672
training loss = 5.969554424285889 2100
val loss = 7.830737590789795
training loss = 5.950688362121582 2200
val loss = 7.816445350646973
training loss = 5.922792911529541 2300
val loss = 7.784937858581543
training loss = 5.865489959716797 2400
val loss = 7.706461429595947
training loss = 5.688694000244141 2500
val loss = 7.423206806182861
training loss = 5.22108793258667 2600
val loss = 6.536536693572998
training loss = 4.740616798400879 2700
val loss = 5.763420104980469
training loss = 4.181612968444824 2800
val loss = 4.968547344207764
training loss = 3.525981903076172 2900
val loss = 3.98309588432312
training loss = 2.884031295776367 3000
val loss = 2.9531502723693848
training loss = 2.4435160160064697 3100
val loss = 2.1701912879943848
training loss = 2.2564821243286133 3200
val loss = 1.7716646194458008
training loss = 2.201023578643799 3300
val loss = 1.6192817687988281
training loss = 2.1810476779937744 3400
val loss = 1.5612738132476807
training loss = 2.168361186981201 3500
val loss = 1.5342391729354858
training loss = 2.1577250957489014 3600
val loss = 1.5182216167449951
training loss = 2.148124933242798 3700
val loss = 1.5068912506103516
training loss = 2.13924241065979 3800
val loss = 1.4981664419174194
training loss = 2.130922555923462 3900
val loss = 1.491358995437622
training loss = 2.1230552196502686 4000
val loss = 1.4861688613891602
training loss = 2.1155481338500977 4100
val loss = 1.482363224029541
training loss = 2.108327627182007 4200
val loss = 1.4798349142074585
training loss = 2.10132098197937 4300
val loss = 1.478428602218628
training loss = 2.094468593597412 4400
val loss = 1.4781019687652588
training loss = 2.0877163410186768 4500
val loss = 1.478683590888977
training loss = 2.081014633178711 4600
val loss = 1.480149745941162
training loss = 2.074323892593384 4700
val loss = 1.4823660850524902
training loss = 2.0676071643829346 4800
val loss = 1.485290288925171
training loss = 2.060720205307007 4900
val loss = 1.4800399541854858
training loss = 2.0537967681884766 5000
val loss = 1.4877960681915283
training loss = 2.0553336143493652 5100
val loss = 1.4178667068481445
training loss = 2.040208578109741 5200
val loss = 1.4912961721420288
training loss = 2.047875165939331 5300
val loss = 1.6154193878173828
training loss = 2.0271506309509277 5400
val loss = 1.4968522787094116
training loss = 2.113445997238159 5500
val loss = 1.860564947128296
training loss = 2.014936923980713 5600
val loss = 1.504638910293579
training loss = 2.0100603103637695 5700
val loss = 1.4793646335601807
training loss = 2.0036978721618652 5800
val loss = 1.5078765153884888
training loss = 1.9983958005905151 5900
val loss = 1.5142011642456055
training loss = 1.9933964014053345 6000
val loss = 1.5186879634857178
training loss = 1.9885773658752441 6100
val loss = 1.5210909843444824
training loss = 1.984676718711853 6200
val loss = 1.5508129596710205
training loss = 1.9795663356781006 6300
val loss = 1.5294629335403442
training loss = 2.340026617050171 6400
val loss = 2.429643154144287
training loss = 1.9713383913040161 6500
val loss = 1.5407062768936157
training loss = 1.9674779176712036 6600
val loss = 1.5426950454711914
training loss = 1.9693498611450195 6700
val loss = 1.6235203742980957
training loss = 1.9600948095321655 6800
val loss = 1.5478942394256592
training loss = 1.956559658050537 6900
val loss = 1.5570968389511108
training loss = 1.9545581340789795 7000
val loss = 1.5277643203735352
training loss = 1.9500069618225098 7100
val loss = 1.5667929649353027
training loss = 1.9471049308776855 7200
val loss = 1.5556784868240356
training loss = 1.943942904472351 7300
val loss = 1.5701318979263306
training loss = 1.941046953201294 7400
val loss = 1.5807546377182007
training loss = 2.044910430908203 7500
val loss = 1.376943588256836
training loss = 1.9356024265289307 7600
val loss = 1.5883179903030396
training loss = 1.9329948425292969 7700
val loss = 1.5943872928619385
training loss = 1.9308905601501465 7800
val loss = 1.6164606809616089
training loss = 1.928202509880066 7900
val loss = 1.6030874252319336
training loss = 1.9297363758087158 8000
val loss = 1.5508432388305664
training loss = 1.9237773418426514 8100
val loss = 1.6076383590698242
training loss = 1.9217039346694946 8200
val loss = 1.6149221658706665
training loss = 1.9196385145187378 8300
val loss = 1.6183480024337769
training loss = 1.9178181886672974 8400
val loss = 1.631126880645752
training loss = 1.9158897399902344 8500
val loss = 1.6262645721435547
training loss = 1.966094970703125 8600
val loss = 1.454848051071167
training loss = 1.9123951196670532 8700
val loss = 1.6369681358337402
training loss = 1.910713791847229 8800
val loss = 1.6367919445037842
training loss = 1.963240623474121 8900
val loss = 1.9218348264694214
training loss = 1.907606601715088 9000
val loss = 1.6418260335922241
training loss = 1.9061145782470703 9100
val loss = 1.6463661193847656
training loss = 1.914147138595581 9200
val loss = 1.5615859031677246
training loss = 1.9032727479934692 9300
val loss = 1.652838945388794
training loss = 1.924981951713562 9400
val loss = 1.8281875848770142
training loss = 1.9005967378616333 9500
val loss = 1.6543543338775635
training loss = 1.8992688655853271 9600
val loss = 1.661834716796875
training loss = 1.8992196321487427 9700
val loss = 1.6301143169403076
training loss = 1.8967595100402832 9800
val loss = 1.6669777631759644
training loss = 1.9317623376846313 9900
val loss = 1.5134589672088623
training loss = 1.894280195236206 10000
val loss = 1.673812747001648
training loss = 1.8930245637893677 10100
val loss = 1.6762895584106445
training loss = 1.8955981731414795 10200
val loss = 1.6203781366348267
training loss = 1.8904850482940674 10300
val loss = 1.6830930709838867
training loss = 1.8891594409942627 10400
val loss = 1.6916762590408325
training loss = 1.8878133296966553 10500
val loss = 1.6841984987258911
training loss = 1.8863674402236938 10600
val loss = 1.6928071975708008
training loss = 1.893522024154663 10700
val loss = 1.7975103855133057
reduced chi^2 level 2 = 1.8868781328201294
Constrained alpha: 1.8452352285385132
Constrained beta: 2.820160388946533
Constrained gamma: 15.687081336975098
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 849.1891,  855.5901,  905.3193,  956.2806,  972.3939, 1097.0602,
        1129.7786, 1164.7152, 1155.4373, 1169.3204, 1202.3416, 1202.8523,
        1231.2896, 1239.0898, 1394.4469, 1403.8635, 1390.9442, 1445.3962,
        1535.9127, 1539.3320, 1681.7397, 1515.0328, 1634.2959, 1676.1692,
        1666.0465, 1685.2681, 1649.9574, 1743.2511, 1716.1816, 1728.4047,
        1675.3668, 1745.5807, 1732.5278, 1704.8226, 1704.8365, 1763.7611,
        1684.4921, 1568.3290, 1697.8507, 1618.9392, 1620.2581, 1550.9879,
        1498.3983, 1453.5988, 1380.4375, 1382.2872, 1281.8822, 1228.3536,
        1233.4293, 1218.3724, 1051.4918,  963.6021,  973.5506,  874.1279,
         906.4233,  904.2338,  831.3926,  723.5653,  601.6563,  568.1924,
         549.6931,  450.2932,  470.3747,  406.4553,  313.3128,  311.9930,
         278.5257,  234.9586,  193.2729,  170.2114,  159.9593,  161.9978,
         129.1507,  110.5371,   94.3714,   72.3159,   55.4182,   44.1698,
          38.6947,   34.2384,   16.2189,   40.2491,   38.7226])]
2405.3356959921034
3.090653139423105 14.735434022100778 53.893288733187916
val isze = 8
idinces = [65  9 46 78 20 12 36 24  3 35 31 75 48 11  6 33 30 57 55 82 53 40  5 59
 42 43 64 58  4 73 52 39 74 70 38 37 32 49 76 21 63 62 67 26 68 69 56 19
 27 71 16 25 18 17 22 81  2 80 54 23 77 66 50 15 10  8 34 51  1 44 28 79
 29 72  7 41 61 14 45 47 60  0 13]
we are doing training validation split
training loss = 359.761474609375 100
val loss = 260.88092041015625
training loss = 6.881507396697998 200
val loss = 7.131620407104492
training loss = 6.710421562194824 300
val loss = 6.863544940948486
training loss = 6.664281368255615 400
val loss = 6.829543113708496
training loss = 6.616301536560059 500
val loss = 6.795146465301514
training loss = 6.567918300628662 600
val loss = 6.76036262512207
training loss = 6.520061016082764 700
val loss = 6.725778102874756
training loss = 6.473330974578857 800
val loss = 6.691723823547363
training loss = 6.428102970123291 900
val loss = 6.658466815948486
training loss = 6.38459587097168 1000
val loss = 6.626090049743652
training loss = 6.3429036140441895 1100
val loss = 6.594710350036621
training loss = 6.303024768829346 1200
val loss = 6.564334869384766
training loss = 6.2649006843566895 1300
val loss = 6.534937858581543
training loss = 6.228429317474365 1400
val loss = 6.506560325622559
training loss = 6.19348669052124 1500
val loss = 6.4791107177734375
training loss = 6.159939289093018 1600
val loss = 6.452520847320557
training loss = 6.127650737762451 1700
val loss = 6.426799774169922
training loss = 6.096484661102295 1800
val loss = 6.401797771453857
training loss = 6.066311359405518 1900
val loss = 6.377448081970215
training loss = 6.036989688873291 2000
val loss = 6.353765964508057
training loss = 6.008356094360352 2100
val loss = 6.330511093139648
training loss = 5.980123519897461 2200
val loss = 6.307476997375488
training loss = 5.951536655426025 2300
val loss = 6.283910751342773
training loss = 5.921705722808838 2400
val loss = 6.258999347686768
training loss = 5.8901896476745605 2500
val loss = 6.232568264007568
training loss = 5.855027198791504 2600
val loss = 6.203178405761719
training loss = 5.806668281555176 2700
val loss = 6.161518096923828
training loss = 5.704523086547852 2800
val loss = 6.198469161987305
training loss = 5.294055461883545 2900
val loss = 5.637345314025879
training loss = 3.985295534133911 3000
val loss = 4.658249855041504
training loss = 2.4390718936920166 3100
val loss = 3.279951810836792
training loss = 2.0739073753356934 3200
val loss = 2.9658327102661133
training loss = 2.045734405517578 3300
val loss = 3.0196871757507324
training loss = 2.038520097732544 3400
val loss = 3.058387517929077
training loss = 2.0353682041168213 3500
val loss = 3.09610652923584
training loss = 2.033954620361328 3600
val loss = 3.125898838043213
training loss = 2.035212516784668 3700
val loss = 3.1602511405944824
training loss = 2.0336227416992188 3800
val loss = 3.1590490341186523
training loss = 2.0650668144226074 3900
val loss = 3.128082752227783
training loss = 2.033531427383423 4000
val loss = 3.175283432006836
training loss = 2.033280372619629 4100
val loss = 3.1800131797790527
training loss = 2.033608913421631 4200
val loss = 3.1903464794158936
training loss = 2.032771348953247 4300
val loss = 3.1837282180786133
training loss = 2.1000452041625977 4400
val loss = 3.3732447624206543
training loss = 2.0320825576782227 4500
val loss = 3.1816787719726562
training loss = 2.0315964221954346 4600
val loss = 3.184873104095459
training loss = 2.0566153526306152 4700
val loss = 3.1364800930023193
training loss = 2.030724287033081 4800
val loss = 3.184046745300293
training loss = 2.0350441932678223 4900
val loss = 3.220287322998047
training loss = 2.0298516750335693 5000
val loss = 3.184683322906494
training loss = 2.029285192489624 5100
val loss = 3.1818385124206543
training loss = 2.029536247253418 5200
val loss = 3.1918587684631348
training loss = 2.0283193588256836 5300
val loss = 3.1794309616088867
training loss = 2.1034274101257324 5400
val loss = 3.13275146484375
training loss = 2.0273234844207764 5500
val loss = 3.1769120693206787
training loss = 2.0267717838287354 5600
val loss = 3.1764252185821533
training loss = 2.0263376235961914 5700
val loss = 3.1750030517578125
training loss = 2.0257372856140137 5800
val loss = 3.1733837127685547
training loss = 2.033781051635742 5900
val loss = 3.2249579429626465
training loss = 2.0246670246124268 6000
val loss = 3.1722283363342285
training loss = 2.0240495204925537 6100
val loss = 3.170355796813965
training loss = 2.025289535522461 6200
val loss = 3.149742603302002
training loss = 2.02290940284729 6300
val loss = 3.1671009063720703
training loss = 2.272456169128418 6400
val loss = 3.670290946960449
training loss = 2.0217294692993164 6500
val loss = 3.1652426719665527
training loss = 2.0210652351379395 6600
val loss = 3.1635916233062744
training loss = 2.0215766429901123 6700
val loss = 3.14705753326416
training loss = 2.019803762435913 6800
val loss = 3.160538673400879
training loss = 2.0691654682159424 6900
val loss = 3.3157336711883545
training loss = 2.0184884071350098 7000
val loss = 3.1573073863983154
training loss = 2.0177688598632812 7100
val loss = 3.157555103302002
training loss = 2.017155408859253 7200
val loss = 3.154924154281616
training loss = 2.016395330429077 7300
val loss = 3.153069257736206
training loss = 2.0239315032958984 7400
val loss = 3.214242696762085
training loss = 2.0149338245391846 7500
val loss = 3.1496188640594482
training loss = 2.014132022857666 7600
val loss = 3.1491894721984863
training loss = 2.013414144515991 7700
val loss = 3.144719123840332
training loss = 2.0125644207000732 7800
val loss = 3.1453166007995605
training loss = 2.0243513584136963 7900
val loss = 3.1068496704101562
training loss = 2.010913848876953 8000
val loss = 3.1413440704345703
training loss = 2.324897050857544 8100
val loss = 3.231400966644287
training loss = 2.009187936782837 8200
val loss = 3.1398541927337646
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 898.9811,  871.6937,  994.6065, 1008.6111, 1003.2119, 1086.2979,
        1095.0387, 1119.8116, 1113.2605, 1145.9099, 1201.4492, 1233.3102,
        1245.1357, 1337.6376, 1345.7671, 1466.9281, 1286.3223, 1462.1023,
        1570.4904, 1504.7357, 1575.7126, 1492.7629, 1565.4193, 1607.5624,
        1689.1896, 1655.2942, 1546.8558, 1811.8273, 1676.3414, 1702.7013,
        1683.7811, 1740.8047, 1737.4291, 1791.2314, 1708.9585, 1648.5293,
        1606.4047, 1639.5067, 1647.0894, 1606.4918, 1613.5498, 1557.8663,
        1516.4817, 1482.8584, 1403.9139, 1293.7046, 1217.4502, 1214.3329,
        1153.7191, 1211.8475, 1075.8390,  931.8686,  990.2827,  936.6302,
         879.5459,  887.0197,  808.9897,  673.3553,  630.1998,  534.5800,
         570.8087,  473.3925,  440.2233,  394.7373,  368.5882,  332.2995,
         288.3896,  252.4265,  183.2352,  183.9034,  129.1593,  181.3241,
         148.8519,  107.7873,   93.2903,   76.4694,   52.3957,   40.9999,
          41.3694,   34.5362,   17.5634,   35.8174,   34.9683])]
2689.4323641019373
3.391460932132369 15.280079098917314 32.14569425064697
val isze = 8
idinces = [48 69  7 31  2  4 66 23 27  0 24 22 65  5 43 42 16 79 56 28 73 59 38 49
 30 58 29 64  6 51 17 18 71 72 10 44 20 81 36 14 25  9 11 40 54 45 78 57
 12 82 60 32 61 68 46  8 19 50 67 33 13 75 70 52  3 21 62 26 41  1 63 15
 74 77 34 55 76 47 35 39 37 80 53]
we are doing training validation split
training loss = 363.8031921386719 100
val loss = 349.1155700683594
training loss = 8.398086547851562 200
val loss = 5.625651836395264
training loss = 7.21900749206543 300
val loss = 3.2368955612182617
training loss = 7.154707431793213 400
val loss = 3.1687393188476562
training loss = 7.089716911315918 500
val loss = 3.113236427307129
training loss = 7.026315212249756 600
val loss = 3.0659303665161133
training loss = 6.965843200683594 700
val loss = 3.0275120735168457
training loss = 6.908848762512207 800
val loss = 2.99735951423645
training loss = 6.8553385734558105 900
val loss = 2.973961353302002
training loss = 6.8050103187561035 1000
val loss = 2.9554834365844727
training loss = 6.7574262619018555 1100
val loss = 2.94010853767395
training loss = 6.712122440338135 1200
val loss = 2.926323890686035
training loss = 6.668702125549316 1300
val loss = 2.91294527053833
training loss = 6.626838684082031 1400
val loss = 2.899278163909912
training loss = 6.586280345916748 1500
val loss = 2.8849072456359863
training loss = 6.546846866607666 1600
val loss = 2.86975359916687
training loss = 6.508395195007324 1700
val loss = 2.8538479804992676
training loss = 6.470814228057861 1800
val loss = 2.8373684883117676
training loss = 6.433993339538574 1900
val loss = 2.820526123046875
training loss = 6.3978424072265625 2000
val loss = 2.803506851196289
training loss = 6.36226749420166 2100
val loss = 2.7864718437194824
training loss = 6.327163219451904 2200
val loss = 2.7696211338043213
training loss = 6.292441368103027 2300
val loss = 2.7529194355010986
training loss = 6.258007049560547 2400
val loss = 2.7365386486053467
training loss = 6.223761081695557 2500
val loss = 2.720477342605591
training loss = 6.189622402191162 2600
val loss = 2.7047677040100098
training loss = 6.155511856079102 2700
val loss = 2.6894314289093018
training loss = 6.121365547180176 2800
val loss = 2.674438953399658
training loss = 6.087152004241943 2900
val loss = 2.659886360168457
training loss = 6.052870750427246 3000
val loss = 2.6457605361938477
training loss = 6.018558979034424 3100
val loss = 2.6322617530822754
training loss = 5.984277725219727 3200
val loss = 2.619610548019409
training loss = 5.949900150299072 3300
val loss = 2.6083197593688965
training loss = 5.9138031005859375 3400
val loss = 2.59993314743042
training loss = 5.8387956619262695 3500
val loss = 2.599764823913574
training loss = 5.488486289978027 3600
val loss = 2.4489030838012695
training loss = 3.8376269340515137 3700
val loss = 1.8087745904922485
training loss = 2.4502294063568115 3800
val loss = 1.2896993160247803
training loss = 2.4306271076202393 3900
val loss = 1.3841309547424316
training loss = 2.3673369884490967 4000
val loss = 1.2310128211975098
training loss = 2.3662495613098145 4100
val loss = 1.198941946029663
training loss = 2.3524484634399414 4200
val loss = 1.2046613693237305
training loss = 2.363713264465332 4300
val loss = 1.2552483081817627
training loss = 2.337695360183716 4400
val loss = 1.1792460680007935
training loss = 2.3298544883728027 4500
val loss = 1.1657637357711792
training loss = 2.3222737312316895 4600
val loss = 1.1596012115478516
training loss = 2.312757730484009 4700
val loss = 1.1350648403167725
training loss = 2.3030178546905518 4800
val loss = 1.114572525024414
training loss = 2.2926812171936035 4900
val loss = 1.0950894355773926
training loss = 2.2811622619628906 5000
val loss = 1.0733108520507812
training loss = 2.2704410552978516 5100
val loss = 1.037677526473999
training loss = 2.2550008296966553 5200
val loss = 1.0163838863372803
training loss = 2.2419042587280273 5300
val loss = 0.9929984211921692
training loss = 2.2242562770843506 5400
val loss = 0.9429717659950256
training loss = 2.2075626850128174 5500
val loss = 0.9023836255073547
training loss = 2.1899313926696777 5600
val loss = 0.8564842939376831
training loss = 2.178403377532959 5700
val loss = 0.8310707807540894
training loss = 2.155378580093384 5800
val loss = 0.7735706567764282
training loss = 2.1393983364105225 5900
val loss = 0.7382160425186157
training loss = 2.125157117843628 6000
val loss = 0.7177900075912476
training loss = 2.112642765045166 6100
val loss = 0.7022806406021118
training loss = 2.12019944190979 6200
val loss = 0.7134396433830261
training loss = 2.0914478302001953 6300
val loss = 0.6869020462036133
training loss = 2.0821785926818848 6400
val loss = 0.6828786730766296
training loss = 2.075103282928467 6500
val loss = 0.6842201948165894
training loss = 2.0660417079925537 6600
val loss = 0.6795932650566101
training loss = 2.1033358573913574 6700
val loss = 0.7454609870910645
training loss = 2.052551031112671 6800
val loss = 0.6803420186042786
training loss = 2.0466198921203613 6900
val loss = 0.6822302937507629
training loss = 2.0413472652435303 7000
val loss = 0.6835184693336487
training loss = 2.0365302562713623 7100
val loss = 0.6865073442459106
training loss = 2.103210926055908 7200
val loss = 0.7340198159217834
training loss = 2.0283448696136475 7300
val loss = 0.6913536787033081
training loss = 2.0248260498046875 7400
val loss = 0.6935100555419922
training loss = 2.0226657390594482 7500
val loss = 0.6995639801025391
training loss = 2.019106864929199 7600
val loss = 0.6967564821243286
training loss = 2.0171525478363037 7700
val loss = 0.6953659057617188
training loss = 2.014616012573242 7800
val loss = 0.6992083787918091
training loss = 2.012640953063965 7900
val loss = 0.6993421316146851
training loss = 2.0110719203948975 8000
val loss = 0.7007451057434082
training loss = 2.0094153881073 8100
val loss = 0.699648380279541
training loss = 2.0118579864501953 8200
val loss = 0.6941446661949158
training loss = 2.0068564414978027 8300
val loss = 0.6999033093452454
training loss = 2.005620241165161 8400
val loss = 0.6988785862922668
training loss = 2.1759095191955566 8500
val loss = 0.9473370313644409
training loss = 2.003633499145508 8600
val loss = 0.6973293423652649
training loss = 2.0026843547821045 8700
val loss = 0.69732666015625
training loss = 2.006742000579834 8800
val loss = 0.7126667499542236
training loss = 2.0011870861053467 8900
val loss = 0.696170449256897
training loss = 2.000443935394287 9000
val loss = 0.6956196427345276
training loss = 2.0007071495056152 9100
val loss = 0.6906874179840088
training loss = 1.9992363452911377 9200
val loss = 0.6943597793579102
training loss = 2.0309181213378906 9300
val loss = 0.7029611468315125
training loss = 1.9982471466064453 9400
val loss = 0.6928824186325073
training loss = 1.9977333545684814 9500
val loss = 0.6926789283752441
training loss = 2.056727647781372 9600
val loss = 0.7132267355918884
training loss = 1.9969135522842407 9700
val loss = 0.691109299659729
training loss = 1.996484637260437 9800
val loss = 0.6909799575805664
training loss = 1.9966660737991333 9900
val loss = 0.6867372393608093
training loss = 1.995805263519287 10000
val loss = 0.6898576021194458
training loss = 1.9987846612930298 10100
val loss = 0.6832695603370667
training loss = 1.9952118396759033 10200
val loss = 0.688861608505249
training loss = 2.2332637310028076 10300
val loss = 0.852853000164032
training loss = 1.9946941137313843 10400
val loss = 0.6874575018882751
training loss = 1.9944010972976685 10500
val loss = 0.6873393058776855
training loss = 1.994676113128662 10600
val loss = 0.6836862564086914
training loss = 1.9939522743225098 10700
val loss = 0.6865955591201782
training loss = 1.9998605251312256 10800
val loss = 0.7054305076599121
training loss = 1.993546962738037 10900
val loss = 0.6858843564987183
training loss = 1.9938396215438843 11000
val loss = 0.6858119368553162
training loss = 1.993170976638794 11100
val loss = 0.6850271224975586
training loss = 1.9930050373077393 11200
val loss = 0.6859077215194702
training loss = 1.9928570985794067 11300
val loss = 0.6849883794784546
training loss = 1.9926270246505737 11400
val loss = 0.6840901374816895
training loss = 1.9925159215927124 11500
val loss = 0.6821537613868713
training loss = 1.9923222064971924 11600
val loss = 0.6834695935249329
training loss = 1.9921437501907349 11700
val loss = 0.6836754083633423
training loss = 1.9921804666519165 11800
val loss = 0.6850568056106567
training loss = 1.9918420314788818 11900
val loss = 0.6826502680778503
training loss = 1.9978619813919067 12000
val loss = 0.7019615769386292
training loss = 1.9915719032287598 12100
val loss = 0.6821100115776062
training loss = 1.9924272298812866 12200
val loss = 0.6775009632110596
training loss = 1.991323471069336 12300
val loss = 0.6808338761329651
training loss = 1.991133451461792 12400
val loss = 0.6814916133880615
training loss = 1.9918408393859863 12500
val loss = 0.6861408352851868
training loss = 1.9908899068832397 12600
val loss = 0.6811155080795288
training loss = 1.9907649755477905 12700
val loss = 0.6798460483551025
training loss = 1.9906789064407349 12800
val loss = 0.6816432476043701
training loss = 1.9904758930206299 12900
val loss = 0.6805871725082397
training loss = 1.9929466247558594 13000
val loss = 0.6918321847915649
training loss = 1.9902377128601074 13100
val loss = 0.6802725791931152
training loss = 1.9904061555862427 13200
val loss = 0.6773070096969604
training loss = 1.9900492429733276 13300
val loss = 0.6786932349205017
training loss = 1.9898381233215332 13400
val loss = 0.6798147559165955
training loss = 1.9946268796920776 13500
val loss = 0.6964983344078064
training loss = 1.9896037578582764 13600
val loss = 0.6794865727424622
training loss = 1.9902645349502563 13700
val loss = 0.675336480140686
training loss = 1.9893746376037598 13800
val loss = 0.6793731451034546
training loss = 1.989211916923523 13900
val loss = 0.6791344881057739
training loss = 1.9989879131317139 14000
val loss = 0.706031322479248
training loss = 1.9889779090881348 14100
val loss = 0.6788166165351868
training loss = 1.988816738128662 14200
val loss = 0.6786718368530273
training loss = 1.9888696670532227 14300
val loss = 0.6804322004318237
training loss = 1.9886093139648438 14400
val loss = 0.6786788105964661
training loss = 1.9884498119354248 14500
val loss = 0.6784508228302002
training loss = 1.9886894226074219 14600
val loss = 0.6753542423248291
training loss = 1.9882100820541382 14700
val loss = 0.6782478094100952
training loss = 2.032238721847534 14800
val loss = 0.7583693861961365
training loss = 1.987967610359192 14900
val loss = 0.6778252720832825
training loss = 1.9891186952590942 15000
val loss = 0.6731104850769043
training loss = 1.987762451171875 15100
val loss = 0.6766897439956665
training loss = 1.9875513315200806 15200
val loss = 0.6777651309967041
training loss = 1.9893536567687988 15300
val loss = 0.6871306300163269
training loss = 1.987321376800537 15400
val loss = 0.6776456236839294
training loss = 1.9871488809585571 15500
val loss = 0.6775953769683838
training loss = 1.9871063232421875 15600
val loss = 0.6762205362319946
training loss = 1.9868874549865723 15700
val loss = 0.6772450804710388
training loss = 1.9882092475891113 15800
val loss = 0.6852118372917175
training loss = 1.98664391040802 15900
val loss = 0.6770365834236145
training loss = 1.9864628314971924 16000
val loss = 0.6769905090332031
training loss = 1.9870789051055908 16100
val loss = 0.6817692518234253
training loss = 1.9861950874328613 16200
val loss = 0.6767386198043823
training loss = 2.0184030532836914 16300
val loss = 0.7385581731796265
training loss = 1.9859212636947632 16400
val loss = 0.6763651371002197
training loss = 1.985752820968628 16500
val loss = 0.6754910349845886
training loss = 1.9856903553009033 16600
val loss = 0.677402913570404
training loss = 1.98544442653656 16700
val loss = 0.6762353181838989
training loss = 1.9868508577346802 16800
val loss = 0.6847878694534302
training loss = 1.9851490259170532 16900
val loss = 0.6758948564529419
training loss = 2.0103561878204346 17000
val loss = 0.7280580997467041
training loss = 1.9848881959915161 17100
val loss = 0.6767221093177795
training loss = 1.984641194343567 17200
val loss = 0.6756625175476074
training loss = 1.9887585639953613 17300
val loss = 0.6916453838348389
training loss = 1.9843412637710571 17400
val loss = 0.6755706071853638
training loss = 1.9841132164001465 17500
val loss = 0.6752094626426697
training loss = 1.9845021963119507 17600
val loss = 0.6792352199554443
training loss = 1.9837819337844849 17700
val loss = 0.6750690340995789
training loss = 2.036722183227539 17800
val loss = 0.7664966583251953
training loss = 1.983432650566101 17900
val loss = 0.6748268008232117
training loss = 1.9831922054290771 18000
val loss = 0.6740814447402954
training loss = 1.983232855796814 18100
val loss = 0.6727112531661987
training loss = 1.982830286026001 18200
val loss = 0.6744977235794067
training loss = 2.070925235748291 18300
val loss = 0.8135161399841309
training loss = 1.9824475049972534 18400
val loss = 0.6745234727859497
training loss = 1.9821710586547852 18500
val loss = 0.6743648648262024
training loss = 1.9821118116378784 18600
val loss = 0.6752256751060486
training loss = 1.9817558526992798 18700
val loss = 0.6738325953483582
training loss = 2.0056967735290527 18800
val loss = 0.7227168083190918
training loss = 1.9813605546951294 18900
val loss = 0.6737596392631531
training loss = 1.9810510873794556 19000
val loss = 0.6734417080879211
training loss = 1.984979271888733 19100
val loss = 0.6875856518745422
training loss = 1.9805997610092163 19200
val loss = 0.6731832027435303
training loss = 1.9804539680480957 19300
val loss = 0.6709608435630798
training loss = 1.9801546335220337 19400
val loss = 0.6735723614692688
training loss = 1.979798436164856 19500
val loss = 0.6728255748748779
training loss = 1.986343264579773 19600
val loss = 0.6658723950386047
training loss = 1.979261875152588 19700
val loss = 0.6725679636001587
training loss = 1.982589602470398 19800
val loss = 0.6856811046600342
training loss = 1.9787423610687256 19900
val loss = 0.6714320778846741
training loss = 1.9783262014389038 20000
val loss = 0.6722111701965332
training loss = 1.9800442457199097 20100
val loss = 0.6805217266082764
training loss = 1.9777480363845825 20200
val loss = 0.6719220876693726
training loss = 1.9872404336929321 20300
val loss = 0.6973127722740173
training loss = 1.9771909713745117 20400
val loss = 0.6728042960166931
training loss = 1.9767147302627563 20500
val loss = 0.6716403365135193
training loss = 1.9832478761672974 20600
val loss = 0.6654461622238159
training loss = 1.9760586023330688 20700
val loss = 0.6715966463088989
training loss = 1.9771167039871216 20800
val loss = 0.6667964458465576
training loss = 1.9753996133804321 20900
val loss = 0.6721261739730835
training loss = 1.9748868942260742 21000
val loss = 0.671181321144104
training loss = 1.9749577045440674 21100
val loss = 0.6677947044372559
training loss = 1.9741675853729248 21200
val loss = 0.6711214780807495
training loss = 2.0157439708709717 21300
val loss = 0.7446963787078857
training loss = 1.9733984470367432 21400
val loss = 0.6702573299407959
training loss = 1.9728559255599976 21500
val loss = 0.670991063117981
training loss = 1.9727643728256226 21600
val loss = 0.6729140281677246
training loss = 1.9720659255981445 21700
val loss = 0.6707776784896851
training loss = 2.100308656692505 21800
val loss = 0.74427729845047
training loss = 1.9712883234024048 21900
val loss = 0.6705633401870728
training loss = 1.9707309007644653 22000
val loss = 0.6707257032394409
training loss = 1.9704171419143677 22100
val loss = 0.670634388923645
training loss = 1.96985924243927 22200
val loss = 0.6707063913345337
training loss = 2.020781993865967 22300
val loss = 0.6872222423553467
training loss = 1.9690300226211548 22400
val loss = 0.6709446310997009
training loss = 1.9684453010559082 22500
val loss = 0.6705855131149292
training loss = 1.968170166015625 22600
val loss = 0.669688880443573
training loss = 1.9675533771514893 22700
val loss = 0.670712411403656
training loss = 1.9753060340881348 22800
val loss = 0.6914840936660767
training loss = 1.9666997194290161 22900
val loss = 0.6705896854400635
training loss = 2.330251932144165 23000
val loss = 1.1254844665527344
training loss = 1.9658424854278564 23100
val loss = 0.6710739135742188
training loss = 1.9652661085128784 23200
val loss = 0.6704611778259277
training loss = 1.9650146961212158 23300
val loss = 0.6698192954063416
training loss = 1.964420199394226 23400
val loss = 0.6707510948181152
training loss = 1.9660958051681519 23500
val loss = 0.6659282445907593
training loss = 1.9635941982269287 23600
val loss = 0.6707258820533752
training loss = 1.9663220643997192 23700
val loss = 0.6675533056259155
training loss = 1.962878704071045 23800
val loss = 0.6709005236625671
training loss = 1.9623539447784424 23900
val loss = 0.6710124015808105
training loss = 1.9626671075820923 24000
val loss = 0.6750622391700745
training loss = 1.9616119861602783 24100
val loss = 0.6708086729049683
training loss = 1.9689284563064575 24200
val loss = 0.6917647123336792
training loss = 1.960931658744812 24300
val loss = 0.6708510518074036
training loss = 1.9613779783248901 24400
val loss = 0.6819969415664673
training loss = 1.960270881652832 24500
val loss = 0.670746386051178
training loss = 1.95984947681427 24600
val loss = 0.671908438205719
training loss = 1.9597440958023071 24700
val loss = 0.6723120212554932
training loss = 1.959187626838684 24800
val loss = 0.6708062291145325
training loss = 1.9590706825256348 24900
val loss = 0.6708865165710449
training loss = 1.9586368799209595 25000
val loss = 0.6706085801124573
training loss = 2.1519172191619873 25100
val loss = 0.9387743473052979
training loss = 1.9581047296524048 25200
val loss = 0.6704431176185608
training loss = 1.9577080011367798 25300
val loss = 0.6702293753623962
training loss = 1.9576473236083984 25400
val loss = 0.6688964366912842
training loss = 1.9571949243545532 25500
val loss = 0.6704468727111816
training loss = 1.9585034847259521 25600
val loss = 0.6651548147201538
training loss = 1.9567395448684692 25700
val loss = 0.670264720916748
training loss = 1.9635169506072998 25800
val loss = 0.6982881426811218
training loss = 1.9563077688217163 25900
val loss = 0.6699166893959045
training loss = 1.9559611082077026 26000
val loss = 0.6704343557357788
training loss = 1.9561524391174316 26100
val loss = 0.6679164171218872
training loss = 1.9556066989898682 26200
val loss = 0.6705270409584045
training loss = 2.0752408504486084 26300
val loss = 0.8454490900039673
training loss = 1.955245018005371 26400
val loss = 0.6701725125312805
training loss = 1.9624361991882324 26500
val loss = 0.6634100675582886
training loss = 1.954980492591858 26600
val loss = 0.6716667413711548
training loss = 1.9546456336975098 26700
val loss = 0.6709750890731812
training loss = 1.9547717571258545 26800
val loss = 0.6684499382972717
training loss = 1.954359531402588 26900
val loss = 0.6709160804748535
training loss = 1.9661953449249268 27000
val loss = 0.700455904006958
training loss = 1.9541181325912476 27100
val loss = 0.6707447171211243
training loss = 1.9672367572784424 27200
val loss = 0.6641204953193665
training loss = 1.9539400339126587 27300
val loss = 0.6718281507492065
training loss = 1.9536666870117188 27400
val loss = 0.6711013317108154
training loss = 1.9553487300872803 27500
val loss = 0.6792746186256409
training loss = 1.9534839391708374 27600
val loss = 0.6710829734802246
training loss = 2.0386877059936523 27700
val loss = 0.7023257613182068
training loss = 1.9533175230026245 27800
val loss = 0.6707626581192017
training loss = 1.9531258344650269 27900
val loss = 0.670590877532959
training loss = 1.9531958103179932 28000
val loss = 0.6704968214035034
training loss = 1.9529911279678345 28100
val loss = 0.6712488532066345
training loss = 1.9624723196029663 28200
val loss = 0.699845552444458
training loss = 1.9528770446777344 28300
val loss = 0.6713939309120178
training loss = 1.952783226966858 28400
val loss = 0.6697272658348083
training loss = 1.9529064893722534 28500
val loss = 0.6732519865036011
training loss = 1.9525765180587769 28600
val loss = 0.6713458299636841
training loss = 1.9586992263793945 28700
val loss = 0.6609491109848022
training loss = 1.952589511871338 28800
val loss = 0.6709036231040955
training loss = 1.952416181564331 28900
val loss = 0.6713999509811401
training loss = 2.2196333408355713 29000
val loss = 1.031548023223877
training loss = 1.9523767232894897 29100
val loss = 0.6705293655395508
training loss = 1.952203392982483 29200
val loss = 0.6714066863059998
training loss = 1.9531278610229492 29300
val loss = 0.6662014126777649
training loss = 1.9521304368972778 29400
val loss = 0.6714184880256653
training loss = 2.013070821762085 29500
val loss = 0.7773680686950684
training loss = 1.9520790576934814 29600
val loss = 0.6714590191841125
training loss = 1.9519411325454712 29700
val loss = 0.6721717715263367
training loss = 1.9521684646606445 29800
val loss = 0.6690990328788757
training loss = 1.9518959522247314 29900
val loss = 0.6714293956756592
training loss = 2.0425846576690674 30000
val loss = 0.8162821531295776
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 856.3554,  859.4932,  916.4385,  951.8823,  991.3350, 1043.2302,
        1008.1952, 1155.6569, 1202.3289, 1148.2451, 1194.6710, 1219.5422,
        1302.6661, 1233.9209, 1339.6389, 1464.8713, 1388.6055, 1448.9630,
        1654.5536, 1514.0087, 1556.5607, 1529.6984, 1623.3081, 1556.6799,
        1620.5524, 1758.0040, 1623.6672, 1816.4269, 1717.4115, 1602.2897,
        1647.6254, 1746.0743, 1701.6638, 1826.0436, 1728.1674, 1821.8097,
        1724.6431, 1651.1423, 1653.8101, 1619.7307, 1623.2570, 1611.4177,
        1453.1371, 1485.1671, 1339.4908, 1320.7990, 1330.0332, 1250.8894,
        1168.9073, 1176.9969, 1055.7655, 1031.1754,  965.2087,  915.1186,
         854.4101,  856.7552,  806.4283,  673.7339,  602.3028,  531.5820,
         538.7098,  434.0577,  425.8579,  395.3903,  400.8786,  323.7077,
         273.9348,  279.8499,  240.2208,  160.9882,  189.9553,  155.9856,
         140.0578,   94.5743,   89.0423,   77.4964,   60.6697,   49.7064,
          39.0447,   48.3607,   26.0635,   28.7028,   39.0804])]
2806.892564940098
3.6659480369265527 4.913526557774124 50.33482655965861
val isze = 8
idinces = [27 69 65 64 23 24 42 78 47 48 59 20  8 79  0 67 19 41  7 39 82 13  9 15
 37 74  1 44 16 51  3 68 38 76 18 32 12 35 45 58 10 75 34 28 54 62  4 26
 30 11 81 43 56 46 50 25 60 17 70 31 33 52 72 29 49 21 66 53 55  6 71 73
 61 63  2 80 77 40 57 14 36  5 22]
we are doing training validation split
training loss = 64.61023712158203 100
val loss = 63.68120193481445
training loss = 29.123985290527344 200
val loss = 29.10492706298828
training loss = 17.849407196044922 300
val loss = 18.113204956054688
training loss = 12.986851692199707 400
val loss = 12.394392013549805
training loss = 10.578068733215332 500
val loss = 9.25722885131836
training loss = 9.287229537963867 600
val loss = 7.493009567260742
training loss = 8.562370300292969 700
val loss = 6.492587089538574
training loss = 8.14377498626709 800
val loss = 5.928991794586182
training loss = 7.897793292999268 900
val loss = 5.617947578430176
training loss = 7.75130558013916 1000
val loss = 5.452356815338135
training loss = 7.662671089172363 1100
val loss = 5.368999481201172
training loss = 7.607633590698242 1200
val loss = 5.330348014831543
training loss = 7.571949005126953 1300
val loss = 5.314544677734375
training loss = 7.547275543212891 1400
val loss = 5.30924129486084
training loss = 7.528815269470215 1500
val loss = 5.307689666748047
training loss = 7.513876914978027 1600
val loss = 5.306547164916992
training loss = 7.5010271072387695 1700
val loss = 5.304591178894043
training loss = 7.489541053771973 1800
val loss = 5.301422119140625
training loss = 7.479086875915527 1900
val loss = 5.297231197357178
training loss = 7.469551086425781 2000
val loss = 5.292298316955566
training loss = 7.460909366607666 2100
val loss = 5.286949634552002
training loss = 7.453162670135498 2200
val loss = 5.281652927398682
training loss = 7.446315288543701 2300
val loss = 5.276313781738281
training loss = 7.440340518951416 2400
val loss = 5.271342754364014
training loss = 7.435184001922607 2500
val loss = 5.266724586486816
training loss = 7.430760860443115 2600
val loss = 5.262503623962402
training loss = 7.426959991455078 2700
val loss = 5.258690357208252
training loss = 7.423636436462402 2800
val loss = 5.25520133972168
training loss = 7.4206461906433105 2900
val loss = 5.252011299133301
training loss = 7.417814254760742 3000
val loss = 5.249002456665039
training loss = 7.414928436279297 3100
val loss = 5.246077537536621
training loss = 7.41171407699585 3200
val loss = 5.242984771728516
training loss = 7.407683372497559 3300
val loss = 5.239472389221191
training loss = 7.40186882019043 3400
val loss = 5.235037326812744
training loss = 7.39198112487793 3500
val loss = 5.228024482727051
training loss = 7.372128486633301 3600
val loss = 5.214588165283203
training loss = 7.327529430389404 3700
val loss = 5.185553550720215
training loss = 7.2308807373046875 3800
val loss = 5.060180187225342
training loss = 7.0246663093566895 3900
val loss = 5.007734775543213
training loss = 6.489895343780518 4000
val loss = 4.73333740234375
training loss = 4.724960803985596 4100
val loss = 3.95034122467041
training loss = 2.17158842086792 4200
val loss = 4.061827659606934
training loss = 2.04421067237854 4300
val loss = 4.274613380432129
training loss = 2.0566136837005615 4400
val loss = 4.194981098175049
training loss = 2.0031673908233643 4500
val loss = 4.098663330078125
training loss = 2.0166356563568115 4600
val loss = 4.068272590637207
training loss = 1.9863358736038208 4700
val loss = 3.9973669052124023
training loss = 2.0684595108032227 4800
val loss = 4.05648136138916
training loss = 1.9748663902282715 4900
val loss = 3.917588233947754
training loss = 1.9944852590560913 5000
val loss = 3.9180474281311035
training loss = 1.9652527570724487 5100
val loss = 3.8470704555511475
training loss = 1.9643622636795044 5200
val loss = 3.807603120803833
training loss = 1.9566634893417358 5300
val loss = 3.781937599182129
training loss = 1.9526416063308716 5400
val loss = 3.7512118816375732
training loss = 1.9489017724990845 5500
val loss = 3.723815679550171
training loss = 1.9451990127563477 5600
val loss = 3.696728229522705
training loss = 1.9417355060577393 5700
val loss = 3.673851728439331
training loss = 1.9383488893508911 5800
val loss = 3.650737762451172
training loss = 1.936015009880066 5900
val loss = 3.6341965198516846
training loss = 1.9318444728851318 6000
val loss = 3.6115403175354004
training loss = 1.9286518096923828 6100
val loss = 3.5927252769470215
training loss = 1.9277706146240234 6200
val loss = 3.5880608558654785
training loss = 1.9225033521652222 6300
val loss = 3.5631444454193115
training loss = 1.9542794227600098 6400
val loss = 3.6107001304626465
training loss = 1.9164620637893677 6500
val loss = 3.5382323265075684
training loss = 1.9133962392807007 6600
val loss = 3.5269365310668945
training loss = 1.922489047050476 6700
val loss = 3.5096054077148438
training loss = 1.9073139429092407 6800
val loss = 3.508335590362549
training loss = 1.904192328453064 6900
val loss = 3.4992971420288086
training loss = 1.9012573957443237 7000
val loss = 3.495983600616455
training loss = 1.89803147315979 7100
val loss = 3.4860141277313232
training loss = 1.8948259353637695 7200
val loss = 3.4786431789398193
training loss = 1.8916958570480347 7300
val loss = 3.475006580352783
training loss = 1.8884907960891724 7400
val loss = 3.470067024230957
training loss = 1.9091907739639282 7500
val loss = 3.5183794498443604
training loss = 1.8819847106933594 7600
val loss = 3.4635140895843506
training loss = 1.879325032234192 7700
val loss = 3.4559590816497803
training loss = 1.8754147291183472 7800
val loss = 3.459507942199707
training loss = 1.8721106052398682 7900
val loss = 3.457894802093506
training loss = 1.8698930740356445 8000
val loss = 3.465601921081543
training loss = 1.8655794858932495 8100
val loss = 3.458021879196167
training loss = 1.8713372945785522 8200
val loss = 3.449467182159424
training loss = 1.8591350317001343 8300
val loss = 3.460452079772949
training loss = 2.0781712532043457 8400
val loss = 3.5676112174987793
training loss = 1.8528141975402832 8500
val loss = 3.4654550552368164
training loss = 1.8496659994125366 8600
val loss = 3.468590259552002
training loss = 1.8467706441879272 8700
val loss = 3.471162796020508
training loss = 1.843812108039856 8800
val loss = 3.476292133331299
training loss = 1.848527431488037 8900
val loss = 3.4710233211517334
training loss = 1.8381879329681396 9000
val loss = 3.485189437866211
training loss = 1.8353756666183472 9100
val loss = 3.490997076034546
training loss = 1.8332035541534424 9200
val loss = 3.4927287101745605
training loss = 1.8302654027938843 9300
val loss = 3.502483367919922
training loss = 1.8283365964889526 9400
val loss = 3.5146942138671875
training loss = 1.8254598379135132 9500
val loss = 3.5148844718933105
training loss = 1.8230780363082886 9600
val loss = 3.5223991870880127
training loss = 1.8210138082504272 9700
val loss = 3.5301120281219482
training loss = 1.818776249885559 9800
val loss = 3.5366621017456055
training loss = 1.8409889936447144 9900
val loss = 3.5985095500946045
training loss = 1.81479012966156 10000
val loss = 3.552371025085449
training loss = 1.812758445739746 10100
val loss = 3.5611002445220947
training loss = 1.8526924848556519 10200
val loss = 3.6491684913635254
training loss = 1.808817744255066 10300
val loss = 3.579585552215576
training loss = 1.8066785335540771 10400
val loss = 3.5903701782226562
training loss = 1.8047665357589722 10500
val loss = 3.600635528564453
training loss = 1.80243980884552 10600
val loss = 3.6083292961120605
training loss = 1.800399661064148 10700
val loss = 3.615699052810669
training loss = 1.7982393503189087 10800
val loss = 3.6199028491973877
training loss = 1.8355908393859863 10900
val loss = 3.614591598510742
training loss = 1.7948132753372192 11000
val loss = 3.622880220413208
training loss = 1.7933919429779053 11100
val loss = 3.6213064193725586
training loss = 1.792681097984314 11200
val loss = 3.622816801071167
training loss = 1.7914929389953613 11300
val loss = 3.6184964179992676
training loss = 1.8467110395431519 11400
val loss = 3.720609664916992
training loss = 1.7901118993759155 11500
val loss = 3.616025447845459
training loss = 1.789448857307434 11600
val loss = 3.614182472229004
training loss = 1.7891732454299927 11700
val loss = 3.616522789001465
training loss = 1.7884318828582764 11800
val loss = 3.613205909729004
training loss = 1.7880098819732666 11900
val loss = 3.6106338500976562
training loss = 1.787482500076294 12000
val loss = 3.612694501876831
training loss = 1.7869268655776978 12100
val loss = 3.611642360687256
training loss = 1.787794589996338 12200
val loss = 3.603064775466919
training loss = 1.7859513759613037 12300
val loss = 3.6109988689422607
training loss = 1.907470941543579 12400
val loss = 3.6344127655029297
training loss = 1.7850220203399658 12500
val loss = 3.6112990379333496
training loss = 1.784448266029358 12600
val loss = 3.610541343688965
training loss = 1.7912280559539795 12700
val loss = 3.5955543518066406
training loss = 1.7834135293960571 12800
val loss = 3.6084017753601074
training loss = 1.7827637195587158 12900
val loss = 3.609221935272217
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 874.6338,  888.8468,  972.6832,  928.2502,  988.6973, 1097.5371,
        1123.4354, 1069.8209, 1127.1094, 1220.3739, 1196.7096, 1253.3202,
        1269.1451, 1268.0629, 1341.7971, 1386.9167, 1449.7036, 1428.9674,
        1523.7607, 1583.0366, 1543.3422, 1517.0183, 1531.3086, 1612.5404,
        1619.2256, 1778.6854, 1539.8878, 1684.9113, 1751.3446, 1656.2313,
        1667.6715, 1784.3069, 1709.4717, 1755.4230, 1700.7252, 1809.7703,
        1693.1261, 1625.8477, 1628.0938, 1579.3873, 1585.2833, 1552.4020,
        1426.9078, 1467.5178, 1354.1108, 1334.0172, 1314.9563, 1160.9156,
        1156.8694, 1179.9081, 1066.9906,  988.2301,  874.3706,  973.6083,
         907.9393,  837.7388,  856.5143,  666.5107,  603.0493,  563.3922,
         531.1617,  487.7019,  418.8431,  362.0247,  344.9035,  362.8134,
         270.5530,  262.2437,  199.4368,  163.6063,  149.5988,  136.5171,
         133.4062,  105.8489,  105.4610,   69.9367,   48.0225,   41.9694,
          35.6420,   34.8028,   25.2749,   40.8971,   35.5891])]
2765.707871100933
0.1363233642214523 9.94394409515207 69.8459075814151
val isze = 8
idinces = [65 80 46  7 57 82 72 30 21 59 78 54 29 34 16 66  9 51 79 56 62 12 14  3
 24 74  5 44  0 58  2 63 68 17 36 61 13 25  8 64  6 70 42 53 45 50 11 71
 81 19 41 31 32 28 55 77 48 23  1  4 27 18 52 69 20 76 47 22 73 60 67 37
 35 38 10 75 49 26 15 43 39 40 33]
we are doing training validation split
training loss = 7.390272617340088 100
val loss = 14.376021385192871
training loss = 6.079704761505127 200
val loss = 13.38778305053711
training loss = 6.024207592010498 300
val loss = 13.194449424743652
training loss = 5.963467121124268 400
val loss = 12.971628189086914
training loss = 5.901226997375488 500
val loss = 12.730457305908203
training loss = 5.8404388427734375 600
val loss = 12.479244232177734
training loss = 5.783374309539795 700
val loss = 12.225263595581055
training loss = 5.73163366317749 800
val loss = 11.974912643432617
training loss = 5.686149597167969 900
val loss = 11.733573913574219
training loss = 5.647239685058594 1000
val loss = 11.505672454833984
training loss = 5.614679336547852 1100
val loss = 11.294532775878906
training loss = 5.587839603424072 1200
val loss = 11.102415084838867
training loss = 5.5658135414123535 1300
val loss = 10.930471420288086
training loss = 5.547584056854248 1400
val loss = 10.778870582580566
training loss = 5.532126426696777 1500
val loss = 10.646971702575684
training loss = 5.518524646759033 1600
val loss = 10.533349990844727
training loss = 5.506004810333252 1700
val loss = 10.436149597167969
training loss = 5.493951320648193 1800
val loss = 10.353185653686523
training loss = 5.481895446777344 1900
val loss = 10.282060623168945
training loss = 5.469465255737305 2000
val loss = 10.22037410736084
training loss = 5.456367492675781 2100
val loss = 10.165685653686523
training loss = 5.442300319671631 2200
val loss = 10.115718841552734
training loss = 5.426959991455078 2300
val loss = 10.068233489990234
training loss = 5.409933567047119 2400
val loss = 10.020952224731445
training loss = 5.390682697296143 2500
val loss = 9.971452713012695
training loss = 5.368441104888916 2600
val loss = 9.917028427124023
training loss = 5.342109680175781 2700
val loss = 9.854283332824707
training loss = 5.310108184814453 2800
val loss = 9.778853416442871
training loss = 5.270180702209473 2900
val loss = 9.684985160827637
training loss = 5.2191691398620605 3000
val loss = 9.565200805664062
training loss = 5.152690887451172 3100
val loss = 9.410199165344238
training loss = 5.064601421356201 3200
val loss = 9.208611488342285
training loss = 4.945679664611816 3300
val loss = 8.945003509521484
training loss = 4.780966281890869 3400
val loss = 8.593255996704102
training loss = 4.545998573303223 3500
val loss = 8.104476928710938
training loss = 4.2048258781433105 3600
val loss = 7.393496990203857
training loss = 3.7202272415161133 3700
val loss = 6.3489203453063965
training loss = 3.1028895378112793 3800
val loss = 4.934382438659668
training loss = 2.5055928230285645 3900
val loss = 3.4334330558776855
training loss = 2.1639552116394043 4000
val loss = 2.44743013381958
training loss = 2.0693302154541016 4100
val loss = 2.106616497039795
training loss = 2.055361270904541 4200
val loss = 2.033100128173828
training loss = 2.0537161827087402 4300
val loss = 2.0189011096954346
training loss = 2.053537130355835 4400
val loss = 2.0163564682006836
training loss = 2.0535640716552734 4500
val loss = 2.0162508487701416
training loss = 2.053614616394043 4600
val loss = 2.016704559326172
training loss = 2.0536513328552246 4700
val loss = 2.017357349395752
training loss = 2.0536651611328125 4800
val loss = 2.018059253692627
training loss = 2.054720878601074 4900
val loss = 2.0118002891540527
training loss = 2.053476095199585 5000
val loss = 2.0190982818603516
training loss = 2.055821180343628 5100
val loss = 2.0320279598236084
training loss = 2.0530757904052734 5200
val loss = 2.0196151733398438
training loss = 2.0614967346191406 5300
val loss = 2.0008938312530518
training loss = 2.0524706840515137 5400
val loss = 2.0196290016174316
training loss = 2.0522541999816895 5500
val loss = 2.0176167488098145
training loss = 2.051820993423462 5600
val loss = 2.0170440673828125
training loss = 2.0513904094696045 5700
val loss = 2.0187647342681885
training loss = 2.0514910221099854 5800
val loss = 2.024139404296875
training loss = 2.0505311489105225 5900
val loss = 2.017592191696167
training loss = 2.0502450466156006 6000
val loss = 2.0142874717712402
training loss = 2.0496208667755127 6100
val loss = 2.0164694786071777
training loss = 2.2710840702056885 6200
val loss = 2.0379798412323
training loss = 2.048715591430664 6300
val loss = 2.0150210857391357
training loss = 2.048321485519409 6400
val loss = 2.0148444175720215
training loss = 2.047877550125122 6500
val loss = 2.0118024349212646
training loss = 2.0474114418029785 6600
val loss = 2.0129904747009277
training loss = 2.053149938583374 6700
val loss = 2.0341644287109375
training loss = 2.0465335845947266 6800
val loss = 2.0115668773651123
training loss = 2.1139028072357178 6900
val loss = 1.9868870973587036
training loss = 2.0456881523132324 7000
val loss = 2.010650634765625
training loss = 2.045330762863159 7100
val loss = 2.008369207382202
training loss = 2.0449016094207764 7200
val loss = 2.010617256164551
training loss = 2.0444486141204834 7300
val loss = 2.007812976837158
training loss = 2.069774866104126 7400
val loss = 2.0616321563720703
training loss = 2.0435848236083984 7500
val loss = 2.0064492225646973
training loss = 2.0431911945343018 7600
val loss = 2.005357503890991
training loss = 2.042893409729004 7700
val loss = 2.001936435699463
training loss = 2.0422873497009277 7800
val loss = 2.0038540363311768
training loss = 2.05362868309021 7900
val loss = 2.0334372520446777
training loss = 2.0413119792938232 8000
val loss = 2.0017948150634766
training loss = 2.04087233543396 8100
val loss = 1.999436616897583
training loss = 2.0402681827545166 8200
val loss = 2.0014736652374268
training loss = 2.0396361351013184 8300
val loss = 1.998802661895752
training loss = 2.0530171394348145 8400
val loss = 1.9785892963409424
training loss = 2.038224220275879 8500
val loss = 1.9962835311889648
training loss = 2.0374128818511963 8600
val loss = 1.994952917098999
training loss = 2.0369651317596436 8700
val loss = 1.9992029666900635
training loss = 2.0354270935058594 8800
val loss = 1.9918328523635864
training loss = 2.034210681915283 8900
val loss = 1.990173578262329
training loss = 2.0327882766723633 9000
val loss = 1.9885470867156982
training loss = 2.0312376022338867 9100
val loss = 1.9866588115692139
training loss = 2.0293946266174316 9200
val loss = 1.9850167036056519
training loss = 2.0275583267211914 9300
val loss = 1.9872164726257324
training loss = 2.0251071453094482 9400
val loss = 1.9821226596832275
training loss = 2.0225932598114014 9500
val loss = 1.9812281131744385
training loss = 2.0199830532073975 9600
val loss = 1.9822473526000977
training loss = 2.0171244144439697 9700
val loss = 1.9795153141021729
training loss = 2.0384726524353027 9800
val loss = 2.0289480686187744
training loss = 2.0110504627227783 9900
val loss = 1.977959156036377
training loss = 2.012436866760254 10000
val loss = 1.9657045602798462
training loss = 2.004289150238037 10100
val loss = 1.9744447469711304
training loss = 2.000487804412842 10200
val loss = 1.9737787246704102
training loss = 1.9965466260910034 10300
val loss = 1.9701464176177979
training loss = 1.9920743703842163 10400
val loss = 1.9693806171417236
training loss = 1.9903935194015503 10500
val loss = 1.9573323726654053
training loss = 1.982094168663025 10600
val loss = 1.9645514488220215
training loss = 2.0051631927490234 10700
val loss = 1.9484213590621948
training loss = 1.9702377319335938 10800
val loss = 1.960263729095459
training loss = 1.9633921384811401 10900
val loss = 1.9593676328659058
training loss = 1.9562445878982544 11000
val loss = 1.9550180435180664
training loss = 1.9482547044754028 11100
val loss = 1.9550836086273193
training loss = 1.9414278268814087 11200
val loss = 1.9627021551132202
training loss = 1.9311306476593018 11300
val loss = 1.9547029733657837
training loss = 1.9212876558303833 11400
val loss = 1.9552431106567383
training loss = 1.9118887186050415 11500
val loss = 1.9591931104660034
training loss = 1.9016380310058594 11600
val loss = 1.9626777172088623
training loss = 1.891547441482544 11700
val loss = 1.9764243364334106
training loss = 1.8797065019607544 11800
val loss = 1.9751530885696411
training loss = 1.8680347204208374 11900
val loss = 1.987618088722229
training loss = 1.8588887453079224 12000
val loss = 2.0090012550354004
training loss = 1.8451225757598877 12100
val loss = 2.016144037246704
training loss = 1.8700370788574219 12200
val loss = 2.0234389305114746
training loss = 1.822445273399353 12300
val loss = 2.0559093952178955
training loss = 1.8110437393188477 12400
val loss = 2.083867073059082
training loss = 1.8011494874954224 12500
val loss = 2.1060943603515625
training loss = 1.7911255359649658 12600
val loss = 2.139350175857544
training loss = 1.8364858627319336 12700
val loss = 2.150118112564087
training loss = 1.773451805114746 12800
val loss = 2.2028636932373047
training loss = 1.7657475471496582 12900
val loss = 2.2459933757781982
training loss = 1.7588139772415161 13000
val loss = 2.268991708755493
training loss = 1.7521926164627075 13100
val loss = 2.3061139583587646
training loss = 1.747864842414856 13200
val loss = 2.338301658630371
training loss = 1.741749882698059 13300
val loss = 2.3682589530944824
training loss = 1.767649531364441 13400
val loss = 2.4568099975585938
training loss = 1.7335779666900635 13500
val loss = 2.4233522415161133
training loss = 1.7298989295959473 13600
val loss = 2.4546422958374023
training loss = 1.7555500268936157 13700
val loss = 2.524034261703491
training loss = 1.7241097688674927 13800
val loss = 2.49900221824646
training loss = 1.7214901447296143 13900
val loss = 2.523477554321289
training loss = 1.7195528745651245 14000
val loss = 2.537248134613037
training loss = 1.7172856330871582 14100
val loss = 2.558051824569702
training loss = 1.7178928852081299 14200
val loss = 2.558387517929077
training loss = 1.713853359222412 14300
val loss = 2.584841251373291
training loss = 1.7124654054641724 14400
val loss = 2.5903069972991943
training loss = 1.7110109329223633 14500
val loss = 2.6050267219543457
training loss = 1.716964602470398 14600
val loss = 2.609623908996582
training loss = 1.7086304426193237 14700
val loss = 2.619555711746216
training loss = 1.7074406147003174 14800
val loss = 2.6332786083221436
training loss = 1.7076103687286377 14900
val loss = 2.6254959106445312
training loss = 1.705561637878418 15000
val loss = 2.643016815185547
training loss = 1.9127824306488037 15100
val loss = 2.862349510192871
training loss = 1.703921914100647 15200
val loss = 2.6485304832458496
training loss = 1.7030682563781738 15300
val loss = 2.6583504676818848
training loss = 1.704410195350647 15400
val loss = 2.647493362426758
training loss = 1.7016454935073853 15500
val loss = 2.663571834564209
training loss = 1.7747961282730103 15600
val loss = 2.758571147918701
training loss = 1.7003326416015625 15700
val loss = 2.666961908340454
training loss = 1.6996275186538696 15800
val loss = 2.673736095428467
training loss = 1.6991965770721436 15900
val loss = 2.6699419021606445
training loss = 1.6983866691589355 16000
val loss = 2.6762008666992188
training loss = 1.7312109470367432 16100
val loss = 2.6611602306365967
reduced chi^2 level 2 = 1.6973806619644165
Constrained alpha: 1.8265095949172974
Constrained beta: 2.89980149269104
Constrained gamma: 16.946990966796875
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 851.3986,  832.8561,  931.6360,  946.0223,  977.1325, 1075.8583,
        1084.6890, 1160.2212, 1134.0853, 1132.7483, 1250.3594, 1168.4176,
        1236.2941, 1211.9332, 1302.4437, 1477.4490, 1316.9513, 1464.5725,
        1516.6542, 1524.0883, 1561.7651, 1565.4930, 1613.6089, 1551.0502,
        1637.9819, 1639.2034, 1535.0548, 1676.5574, 1671.5397, 1706.8301,
        1642.2759, 1714.9058, 1711.5070, 1689.8284, 1654.1178, 1709.6898,
        1599.4254, 1602.9912, 1609.4202, 1650.8423, 1589.2274, 1621.0093,
        1471.1216, 1530.2972, 1312.4186, 1339.3607, 1309.2527, 1284.2480,
        1199.9833, 1194.3127, 1132.6974,  943.8704, 1003.1551,  994.6164,
         925.4138,  864.5291,  795.1755,  697.4748,  644.1228,  546.7801,
         536.3775,  460.2961,  448.8588,  383.5367,  394.7519,  297.1804,
         273.8192,  266.4311,  189.2678,  184.0833,  173.0856,  163.4360,
         138.5842,   94.4931,   84.7420,   72.8524,   64.6887,   35.8001,
          34.8378,   39.9185,   21.7697,   37.2337,   35.5257])]
2825.643245511045
0.8852163781185102 16.77858672644224 66.89109237623127
val isze = 8
idinces = [19 12  7 55 66  9 43 79 54 47 73 50 21 17 13 44 72 30 20 64  1 45 27  8
 48 25 71 37  5 77 53 63 57  4 52 67 11 34 75 24 74 51 10 41 18 81 32 14
 33  2 70 76  3 59 39 68 31 38 62 36 22 35 40 65 26 78 58 61 29  6 42 28
 82  0 56 80 60 46 16 15 49 23 69]
we are doing training validation split
training loss = 136.1502685546875 100
val loss = 142.6579132080078
training loss = 13.543153762817383 200
val loss = 13.030237197875977
training loss = 10.672407150268555 300
val loss = 9.919769287109375
training loss = 10.164219856262207 400
val loss = 9.459842681884766
training loss = 9.607254028320312 500
val loss = 8.96452522277832
training loss = 9.02322769165039 600
val loss = 8.454983711242676
training loss = 8.434837341308594 700
val loss = 7.953271865844727
training loss = 7.8655829429626465 800
val loss = 7.480943202972412
training loss = 7.338317394256592 900
val loss = 7.057435512542725
training loss = 6.8727827072143555 1000
val loss = 6.697843551635742
training loss = 6.482779502868652 1100
val loss = 6.4105424880981445
training loss = 6.1737141609191895 1200
val loss = 6.196016311645508
training loss = 5.9418182373046875 1300
val loss = 6.046784400939941
training loss = 5.775667667388916 1400
val loss = 5.949702262878418
training loss = 5.659620761871338 1500
val loss = 5.889372825622559
training loss = 5.57777214050293 1600
val loss = 5.851513862609863
training loss = 5.516918182373047 1700
val loss = 5.825145721435547
training loss = 5.467748165130615 1800
val loss = 5.803223609924316
training loss = 5.424599647521973 1900
val loss = 5.781939506530762
training loss = 5.384466648101807 2000
val loss = 5.7597551345825195
training loss = 5.3459882736206055 2100
val loss = 5.736365795135498
training loss = 5.308657646179199 2200
val loss = 5.711997032165527
training loss = 5.272355079650879 2300
val loss = 5.687027454376221
training loss = 5.237097263336182 2400
val loss = 5.661837577819824
training loss = 5.20289945602417 2500
val loss = 5.636654853820801
training loss = 5.1697282791137695 2600
val loss = 5.611652374267578
training loss = 5.137457847595215 2700
val loss = 5.586846351623535
training loss = 5.10579776763916 2800
val loss = 5.562070369720459
training loss = 5.074240684509277 2900
val loss = 5.536957740783691
training loss = 5.041866302490234 3000
val loss = 5.510778427124023
training loss = 5.006940841674805 3100
val loss = 5.482064723968506
training loss = 4.96574592590332 3200
val loss = 5.44767951965332
training loss = 4.908924102783203 3300
val loss = 5.39970064163208
training loss = 4.808333396911621 3400
val loss = 5.314974308013916
training loss = 4.580804347991943 3500
val loss = 5.129475116729736
training loss = 4.140462398529053 3600
val loss = 4.7893571853637695
training loss = 3.5145561695098877 3700
val loss = 4.265833854675293
training loss = 2.6818525791168213 3800
val loss = 3.440565824508667
training loss = 1.9614518880844116 3900
val loss = 2.5333633422851562
training loss = 1.7517061233520508 4000
val loss = 2.129277229309082
training loss = 1.733623743057251 4100
val loss = 2.0636942386627197
training loss = 1.7301669120788574 4200
val loss = 2.063683032989502
training loss = 1.7278878688812256 4300
val loss = 2.0710694789886475
training loss = 1.7261178493499756 4400
val loss = 2.0780105590820312
training loss = 1.7247282266616821 4500
val loss = 2.0837364196777344
training loss = 1.7236405611038208 4600
val loss = 2.0883216857910156
training loss = 1.722802996635437 4700
val loss = 2.0919029712677
training loss = 1.730581283569336 4800
val loss = 1.9679596424102783
training loss = 1.7213761806488037 4900
val loss = 2.102661371231079
training loss = 1.7208250761032104 5000
val loss = 2.0997936725616455
training loss = 1.7217285633087158 5100
val loss = 2.1586992740631104
training loss = 1.7197091579437256 5200
val loss = 2.104498863220215
training loss = 1.7212517261505127 5300
val loss = 2.1701011657714844
training loss = 1.7187048196792603 5400
val loss = 2.114086627960205
training loss = 1.7182607650756836 5500
val loss = 2.1083550453186035
training loss = 1.7191789150238037 5600
val loss = 2.1654932498931885
training loss = 1.7173429727554321 5700
val loss = 2.1103262901306152
training loss = 1.7252038717269897 5800
val loss = 2.2514326572418213
training loss = 1.716518759727478 5900
val loss = 2.1108670234680176
training loss = 1.7162898778915405 6000
val loss = 2.101161479949951
training loss = 1.715776801109314 6100
val loss = 2.1153337955474854
training loss = 1.7154964208602905 6200
val loss = 2.115847587585449
training loss = 1.7183476686477661 6300
val loss = 2.202333450317383
training loss = 1.714837670326233 6400
val loss = 2.11710262298584
training loss = 1.7146248817443848 6500
val loss = 2.114833116531372
training loss = 1.7143194675445557 6600
val loss = 2.126162528991699
training loss = 1.7140930891036987 6700
val loss = 2.119053602218628
training loss = 1.7168546915054321 6800
val loss = 2.2024238109588623
training loss = 1.7135952711105347 6900
val loss = 2.1191697120666504
training loss = 1.7134424448013306 7000
val loss = 2.1203131675720215
training loss = 1.7139848470687866 7100
val loss = 2.1627445220947266
training loss = 1.7130237817764282 7200
val loss = 2.120699882507324
training loss = 1.71298348903656 7300
val loss = 2.1333632469177246
training loss = 1.71267831325531 7400
val loss = 2.118335723876953
training loss = 1.7125651836395264 7500
val loss = 2.121633529663086
training loss = 1.7131160497665405 7600
val loss = 2.162693738937378
training loss = 1.7122327089309692 7700
val loss = 2.1225972175598145
training loss = 1.7121458053588867 7800
val loss = 2.1213808059692383
training loss = 1.7122807502746582 7900
val loss = 2.1480507850646973
training loss = 1.7118722200393677 8000
val loss = 2.1219310760498047
training loss = 1.715721845626831 8100
val loss = 2.032785177230835
training loss = 1.7116280794143677 8200
val loss = 2.125170946121216
training loss = 1.7115609645843506 8300
val loss = 2.1218626499176025
training loss = 1.7131867408752441 8400
val loss = 2.062432289123535
training loss = 1.7113261222839355 8500
val loss = 2.1217050552368164
training loss = 1.7112793922424316 8600
val loss = 2.121612310409546
training loss = 1.711125373840332 8700
val loss = 2.127918243408203
training loss = 1.7110657691955566 8800
val loss = 2.122288703918457
training loss = 1.711030125617981 8900
val loss = 2.123243808746338
training loss = 1.7110850811004639 9000
val loss = 2.1422996520996094
training loss = 1.7108519077301025 9100
val loss = 2.1217517852783203
training loss = 1.7107906341552734 9200
val loss = 2.126633405685425
training loss = 1.710693359375 9300
val loss = 2.1210556030273438
training loss = 1.7106716632843018 9400
val loss = 2.1223926544189453
training loss = 1.710585117340088 9500
val loss = 2.12962007522583
training loss = 1.7105343341827393 9600
val loss = 2.1218128204345703
training loss = 1.723059892654419 9700
val loss = 1.9708951711654663
training loss = 1.7104122638702393 9800
val loss = 2.1204419136047363
training loss = 1.8937654495239258 9900
val loss = 2.919426679611206
training loss = 1.7103077173233032 10000
val loss = 2.117784023284912
training loss = 1.710382103919983 10100
val loss = 2.107296943664551
training loss = 1.710218906402588 10200
val loss = 2.114962577819824
training loss = 1.710198998451233 10300
val loss = 2.1211185455322266
training loss = 1.7136589288711548 10400
val loss = 2.210806369781494
training loss = 1.7100800275802612 10500
val loss = 2.120633125305176
training loss = 1.710084319114685 10600
val loss = 2.120863914489746
training loss = 1.7117118835449219 10700
val loss = 2.062403678894043
training loss = 1.709989070892334 10800
val loss = 2.121351957321167
training loss = 1.7106823921203613 10900
val loss = 2.158571243286133
training loss = 1.709948182106018 11000
val loss = 2.112370491027832
training loss = 1.7099288702011108 11100
val loss = 2.120476722717285
training loss = 1.7102409601211548 11200
val loss = 2.091683864593506
training loss = 1.709854006767273 11300
val loss = 2.120457172393799
training loss = 1.856082797050476 11400
val loss = 2.816040515899658
training loss = 1.7097904682159424 11500
val loss = 2.1197619438171387
training loss = 1.709809422492981 11600
val loss = 2.119985580444336
training loss = 1.710649013519287 11700
val loss = 2.164515256881714
training loss = 1.709755539894104 11800
val loss = 2.1201858520507812
training loss = 1.755090355873108 11900
val loss = 2.47182035446167
training loss = 1.709704875946045 12000
val loss = 2.1203243732452393
training loss = 1.7097285985946655 12100
val loss = 2.118842124938965
training loss = 1.7099159955978394 12200
val loss = 2.1430554389953613
training loss = 1.7096861600875854 12300
val loss = 2.1202220916748047
training loss = 1.7102344036102295 12400
val loss = 2.0838704109191895
training loss = 1.7096309661865234 12500
val loss = 2.1210646629333496
training loss = 1.7096598148345947 12600
val loss = 2.119823455810547
training loss = 1.7102510929107666 12700
val loss = 2.1571433544158936
training loss = 1.7096285820007324 12800
val loss = 2.1198348999023438
training loss = 1.7304362058639526 12900
val loss = 2.347698926925659
training loss = 1.7096070051193237 13000
val loss = 2.114609956741333
training loss = 1.7096301317214966 13100
val loss = 2.1193695068359375
training loss = 1.7144962549209595 13200
val loss = 2.2255852222442627
training loss = 1.7095972299575806 13300
val loss = 2.119688034057617
training loss = 1.7096309661865234 13400
val loss = 2.1207845211029053
training loss = 1.7097111940383911 13500
val loss = 2.136885643005371
training loss = 1.709602952003479 13600
val loss = 2.1193795204162598
training loss = 1.7126855850219727 13700
val loss = 2.20235538482666
training loss = 1.7095903158187866 13800
val loss = 2.121422052383423
training loss = 1.7096309661865234 13900
val loss = 2.1227455139160156
training loss = 1.7096842527389526 14000
val loss = 2.134791374206543
training loss = 1.7096078395843506 14100
val loss = 2.119474172592163
training loss = 1.7103700637817383 14200
val loss = 2.1607799530029297
training loss = 1.709588885307312 14300
val loss = 2.120054244995117
training loss = 1.7096260786056519 14400
val loss = 2.1189827919006348
training loss = 1.7100239992141724 14500
val loss = 2.1505777835845947
training loss = 1.7096081972122192 14600
val loss = 2.1195926666259766
training loss = 1.7599589824676514 14700
val loss = 1.8444770574569702
training loss = 1.7096010446548462 14800
val loss = 2.1259078979492188
training loss = 1.7096185684204102 14900
val loss = 2.119659662246704
training loss = 1.7717633247375488 15000
val loss = 1.8193529844284058
training loss = 1.7096012830734253 15100
val loss = 2.118269681930542
training loss = 1.7096381187438965 15200
val loss = 2.119675636291504
training loss = 1.7095943689346313 15300
val loss = 2.124539613723755
reduced chi^2 level 2 = 1.7096071243286133
Constrained alpha: 1.9574103355407715
Constrained beta: 3.7946882247924805
Constrained gamma: 15.310428619384766
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 869.9984,  851.6281,  928.8936,  926.2708,  994.9674, 1053.3198,
        1089.7729, 1159.5356, 1135.8916, 1134.5465, 1173.4708, 1166.2369,
        1291.8347, 1250.8756, 1327.6075, 1390.6924, 1435.0588, 1481.0405,
        1515.6893, 1543.3357, 1621.0060, 1564.2844, 1630.1613, 1576.6910,
        1569.2124, 1664.1843, 1553.4922, 1671.3715, 1776.9974, 1704.5256,
        1703.0486, 1751.4124, 1731.6207, 1794.7339, 1761.1990, 1818.5968,
        1673.0469, 1632.8013, 1607.0688, 1652.8630, 1617.0383, 1551.6968,
        1553.6853, 1426.4192, 1371.2109, 1350.2052, 1300.7146, 1291.2135,
        1161.7521, 1158.7946, 1111.5776, 1036.2946,  939.7629,  894.9926,
         837.2852,  857.7692,  853.3920,  731.1407,  623.1681,  527.4240,
         561.2228,  455.7867,  431.6991,  320.5338,  351.7509,  329.5001,
         283.0033,  285.3308,  203.7764,  154.5130,  167.7019,  141.6893,
         127.0973,  112.7882,  110.0553,   68.6215,   48.1457,   47.9468,
          31.5027,   48.7480,   15.0621,   37.0211,   40.0716])]
2521.9230639825446
1.0308820196682689 15.647467824921343 62.9046062308433
val isze = 8
idinces = [76 19 27 21 35 47  0  6 22 52 12 31 54 80 82 28 77 55 11 59 63 51 60 36
 29 13 41 43 15 30 18 42 79  8 25 44 38 14  9 81 23  4 26 71  3 65 16 67
 73 72 24  5 64 49 69  7 32 78 56 20 66 40 53 50 17  1 39 68 61 58 57 74
 75 48 37  2 62 46 10 45 34 33 70]
we are doing training validation split
training loss = 121.27059173583984 100
val loss = 80.04034423828125
training loss = 10.813105583190918 200
val loss = 4.700345993041992
training loss = 10.140839576721191 300
val loss = 5.0898518562316895
training loss = 9.792438507080078 400
val loss = 5.021171569824219
training loss = 9.421574592590332 500
val loss = 4.9653520584106445
training loss = 9.045356750488281 600
val loss = 4.928492546081543
training loss = 8.67975902557373 700
val loss = 4.917919158935547
training loss = 8.339030265808105 800
val loss = 4.938895225524902
training loss = 8.03465461730957 900
val loss = 4.993586540222168
training loss = 7.774248123168945 1000
val loss = 5.080458641052246
training loss = 7.5608344078063965 1100
val loss = 5.193785190582275
training loss = 7.392773628234863 1200
val loss = 5.324197769165039
training loss = 7.264601230621338 1300
val loss = 5.46040153503418
training loss = 7.168520450592041 1400
val loss = 5.590851783752441
training loss = 7.096108436584473 1500
val loss = 5.706150054931641
training loss = 7.039760112762451 1600
val loss = 5.800220966339111
training loss = 6.993503570556641 1700
val loss = 5.870654106140137
training loss = 6.953204154968262 1800
val loss = 5.918034553527832
training loss = 6.916292667388916 1900
val loss = 5.945352554321289
training loss = 6.881354331970215 2000
val loss = 5.956335544586182
training loss = 6.847672462463379 2100
val loss = 5.954930305480957
training loss = 6.814913749694824 2200
val loss = 5.944714546203613
training loss = 6.782896518707275 2300
val loss = 5.928561210632324
training loss = 6.751453876495361 2400
val loss = 5.9086174964904785
training loss = 6.720351219177246 2500
val loss = 5.886331558227539
training loss = 6.689218521118164 2600
val loss = 5.862619876861572
training loss = 6.6574811935424805 2700
val loss = 5.837646961212158
training loss = 6.624228477478027 2800
val loss = 5.811469078063965
training loss = 6.587948799133301 2900
val loss = 5.783329010009766
training loss = 6.545873165130615 3000
val loss = 5.751739978790283
training loss = 6.492217540740967 3100
val loss = 5.7135748863220215
training loss = 6.4127373695373535 3200
val loss = 5.661712169647217
training loss = 6.2670063972473145 3300
val loss = 5.578388214111328
training loss = 5.959072589874268 3400
val loss = 5.423826217651367
training loss = 5.41951322555542 3500
val loss = 5.096749782562256
training loss = 4.598859786987305 3600
val loss = 4.41267204284668
training loss = 3.413693904876709 3700
val loss = 3.4569995403289795
training loss = 2.351834297180176 3800
val loss = 2.6986608505249023
training loss = 2.1095590591430664 3900
val loss = 2.4928557872772217
training loss = 2.0900204181671143 4000
val loss = 2.444213628768921
training loss = 2.0805084705352783 4100
val loss = 2.4118354320526123
training loss = 2.073204755783081 4200
val loss = 2.385369300842285
training loss = 2.0673937797546387 4300
val loss = 2.3634684085845947
training loss = 2.0626866817474365 4400
val loss = 2.3451898097991943
training loss = 2.0587985515594482 4500
val loss = 2.3299508094787598
training loss = 2.055532693862915 4600
val loss = 2.31720232963562
training loss = 2.052739381790161 4700
val loss = 2.3063502311706543
training loss = 2.050314426422119 4800
val loss = 2.2973549365997314
training loss = 2.0490012168884277 4900
val loss = 2.334585666656494
training loss = 2.045969009399414 5000
val loss = 2.280747890472412
training loss = 2.061631202697754 5100
val loss = 2.0988266468048096
training loss = 2.0421977043151855 5200
val loss = 2.2670376300811768
training loss = 2.0925028324127197 5300
val loss = 1.9879719018936157
training loss = 2.0387790203094482 5400
val loss = 2.252549409866333
training loss = 2.0372514724731445 5500
val loss = 2.251417398452759
training loss = 2.035872220993042 5600
val loss = 2.2254691123962402
training loss = 2.034346103668213 5700
val loss = 2.2385149002075195
training loss = 2.0335958003997803 5800
val loss = 2.2697641849517822
training loss = 2.0317625999450684 5900
val loss = 2.2298948764801025
training loss = 2.2352819442749023 6000
val loss = 3.1484642028808594
training loss = 2.0294604301452637 6100
val loss = 2.2176427841186523
training loss = 2.028425455093384 6200
val loss = 2.21777606010437
training loss = 2.0911600589752197 6300
val loss = 1.9194824695587158
training loss = 2.0265040397644043 6400
val loss = 2.208461046218872
training loss = 2.0256361961364746 6500
val loss = 2.206894636154175
training loss = 2.0248312950134277 6600
val loss = 2.193903684616089
training loss = 2.024000406265259 6700
val loss = 2.2029027938842773
training loss = 2.051254987716675 6800
val loss = 2.4805305004119873
training loss = 2.0225234031677246 6900
val loss = 2.1962203979492188
training loss = 2.021857738494873 7000
val loss = 2.195880174636841
training loss = 2.021450996398926 7100
val loss = 2.1685409545898438
training loss = 2.020575761795044 7200
val loss = 2.191706895828247
training loss = 2.0358080863952637 7300
val loss = 2.0239431858062744
training loss = 2.0194194316864014 7400
val loss = 2.1939921379089355
training loss = 2.0188639163970947 7500
val loss = 2.185638666152954
training loss = 2.018810749053955 7600
val loss = 2.1510133743286133
training loss = 2.0178093910217285 7700
val loss = 2.182858943939209
training loss = 2.078543186187744 7800
val loss = 2.622603178024292
training loss = 2.0168137550354004 7900
val loss = 2.1776063442230225
training loss = 2.0163674354553223 8000
val loss = 2.170931816101074
training loss = 2.015902042388916 8100
val loss = 2.1859312057495117
training loss = 2.0154078006744385 8200
val loss = 2.1745502948760986
training loss = 2.0589730739593506 8300
val loss = 2.535578489303589
training loss = 2.014470338821411 8400
val loss = 2.1698200702667236
training loss = 2.014025926589966 8500
val loss = 2.170084238052368
training loss = 2.0137035846710205 8600
val loss = 2.147357940673828
training loss = 2.0130598545074463 8700
val loss = 2.1681196689605713
training loss = 2.0125715732574463 8800
val loss = 2.165067195892334
training loss = 2.0122454166412354 8900
val loss = 2.1412572860717773
training loss = 2.0114896297454834 9000
val loss = 2.1623311042785645
training loss = 2.0304172039031982 9100
val loss = 2.3897337913513184
training loss = 2.010254383087158 9200
val loss = 2.159271717071533
training loss = 2.009575605392456 9300
val loss = 2.1517529487609863
training loss = 2.0087850093841553 9400
val loss = 2.155332565307617
training loss = 2.00795841217041 9500
val loss = 2.152514696121216
training loss = 2.0189156532287598 9600
val loss = 2.004237413406372
training loss = 2.005953550338745 9700
val loss = 2.1470298767089844
training loss = 2.004767656326294 9800
val loss = 2.1468505859375
training loss = 2.003641366958618 9900
val loss = 2.120588779449463
training loss = 2.0019543170928955 10000
val loss = 2.140443801879883
training loss = 2.0120809078216553 10100
val loss = 1.9930082559585571
training loss = 1.9985154867172241 10200
val loss = 2.1340248584747314
training loss = 1.9966917037963867 10300
val loss = 2.1489670276641846
training loss = 1.9946305751800537 10400
val loss = 2.1415748596191406
training loss = 1.9925222396850586 10500
val loss = 2.1274707317352295
training loss = 1.9903488159179688 10600
val loss = 2.1207244396209717
training loss = 1.9882556200027466 10700
val loss = 2.1350722312927246
training loss = 1.9860059022903442 10800
val loss = 2.118191719055176
training loss = 1.983745813369751 10900
val loss = 2.1179087162017822
training loss = 1.981618881225586 11000
val loss = 2.1276652812957764
training loss = 1.979236125946045 11100
val loss = 2.1072330474853516
training loss = 2.1175825595855713 11200
val loss = 2.833075761795044
training loss = 1.9745451211929321 11300
val loss = 2.101001024246216
training loss = 1.9721373319625854 11400
val loss = 2.0943856239318848
training loss = 1.9701621532440186 11500
val loss = 2.12237286567688
training loss = 1.9672213792800903 11600
val loss = 2.0854618549346924
training loss = 2.019558906555176 11700
val loss = 2.493299722671509
training loss = 1.9621124267578125 11800
val loss = 2.0806615352630615
training loss = 1.959456205368042 11900
val loss = 2.0717997550964355
training loss = 1.957167148590088 12000
val loss = 2.0396554470062256
training loss = 1.954092264175415 12100
val loss = 2.064281702041626
training loss = 1.9513355493545532 12200
val loss = 2.065772533416748
training loss = 1.9486480951309204 12300
val loss = 2.0565261840820312
training loss = 1.9458520412445068 12400
val loss = 2.0443038940429688
training loss = 1.9431427717208862 12500
val loss = 2.0372159481048584
training loss = 1.9403239488601685 12600
val loss = 2.0429813861846924
training loss = 1.9393490552902222 12700
val loss = 1.9784388542175293
training loss = 1.934883713722229 12800
val loss = 2.0359649658203125
training loss = 2.3024885654449463 12900
val loss = 3.3987879753112793
training loss = 1.929600715637207 13000
val loss = 2.029359817504883
training loss = 1.9270191192626953 13100
val loss = 2.0248894691467285
training loss = 1.929579257965088 13200
val loss = 1.9220616817474365
training loss = 1.9220064878463745 13300
val loss = 2.01770281791687
training loss = 1.9215763807296753 13400
val loss = 2.084214687347412
training loss = 1.9172770977020264 13500
val loss = 2.0045509338378906
training loss = 1.9149664640426636 13600
val loss = 2.0096116065979004
training loss = 1.9128739833831787 13700
val loss = 2.0234968662261963
training loss = 1.9106568098068237 13800
val loss = 2.005345106124878
training loss = 1.9606801271438599 13900
val loss = 2.4039134979248047
training loss = 1.9066400527954102 14000
val loss = 2.0049710273742676
training loss = 1.904701828956604 14100
val loss = 1.9984889030456543
training loss = 1.9040693044662476 14200
val loss = 2.049670934677124
training loss = 1.9010792970657349 14300
val loss = 1.994979977607727
training loss = 2.373352527618408 14400
val loss = 1.573317050933838
training loss = 1.8977683782577515 14500
val loss = 1.9910207986831665
training loss = 1.8962033987045288 14600
val loss = 1.991407036781311
training loss = 1.9713670015335083 14700
val loss = 1.6788873672485352
training loss = 1.8931964635849 14800
val loss = 1.993548035621643
training loss = 1.891730546951294 14900
val loss = 1.9875335693359375
training loss = 1.8906919956207275 15000
val loss = 2.0132923126220703
training loss = 1.8890676498413086 15100
val loss = 1.987467885017395
training loss = 1.9117411375045776 15200
val loss = 1.789082646369934
training loss = 1.8865890502929688 15300
val loss = 1.9867451190948486
training loss = 1.8853774070739746 15400
val loss = 1.9880770444869995
training loss = 1.8843512535095215 15500
val loss = 1.972989559173584
training loss = 1.8831974267959595 15600
val loss = 1.9851746559143066
training loss = 1.8916656970977783 15700
val loss = 2.1350929737091064
training loss = 1.8811466693878174 15800
val loss = 1.9844268560409546
training loss = 1.8801573514938354 15900
val loss = 1.9847781658172607
training loss = 1.8815189599990845 16000
val loss = 2.055964469909668
training loss = 1.878337025642395 16100
val loss = 1.98495352268219
training loss = 1.9804444313049316 16200
val loss = 2.5783774852752686
training loss = 1.8766429424285889 16300
val loss = 1.987630009651184
training loss = 1.8758193254470825 16400
val loss = 1.9846031665802002
training loss = 1.8769084215164185 16500
val loss = 1.9233037233352661
training loss = 1.8743280172348022 16600
val loss = 1.9845045804977417
training loss = 1.8735976219177246 16700
val loss = 1.9803599119186401
training loss = 1.8729848861694336 16800
val loss = 1.9960026741027832
training loss = 1.8722530603408813 16900
val loss = 1.9849539995193481
training loss = 1.8726550340652466 17000
val loss = 1.9398128986358643
training loss = 1.8710020780563354 17100
val loss = 1.9848816394805908
training loss = 1.8756998777389526 17200
val loss = 2.0970489978790283
training loss = 1.8698346614837646 17300
val loss = 1.98854660987854
training loss = 1.8692625761032104 17400
val loss = 1.9852659702301025
training loss = 1.8689221143722534 17500
val loss = 2.005011558532715
training loss = 1.868226408958435 17600
val loss = 1.9857947826385498
training loss = 1.9492586851119995 17700
val loss = 2.492579936981201
training loss = 1.8672534227371216 17800
val loss = 1.9833093881607056
training loss = 1.8667755126953125 17900
val loss = 1.9863641262054443
training loss = 1.8671923875808716 18000
val loss = 2.028949737548828
training loss = 1.865895390510559 18100
val loss = 1.9863706827163696
training loss = 1.8655660152435303 18200
val loss = 2.0018792152404785
training loss = 1.865113615989685 18300
val loss = 1.9972398281097412
training loss = 1.8646578788757324 18400
val loss = 1.9876093864440918
training loss = 1.8649346828460693 18500
val loss = 1.9499294757843018
training loss = 1.863892912864685 18600
val loss = 1.9872994422912598
training loss = 1.8635239601135254 18700
val loss = 1.9921073913574219
training loss = 1.8631738424301147 18800
val loss = 1.9841140508651733
training loss = 1.8628219366073608 18900
val loss = 1.988290786743164
training loss = 1.9221086502075195 19000
val loss = 2.410311222076416
training loss = 1.8621578216552734 19100
val loss = 1.98558509349823
training loss = 1.8618278503417969 19200
val loss = 1.9887646436691284
training loss = 1.8626046180725098 19300
val loss = 1.9416722059249878
training loss = 1.8612102270126343 19400
val loss = 1.988797903060913
training loss = 2.1241605281829834 19500
val loss = 1.5719082355499268
training loss = 1.8606258630752563 19600
val loss = 1.992944359779358
training loss = 1.860329270362854 19700
val loss = 1.990084171295166
training loss = 1.8633341789245605 19800
val loss = 1.9114296436309814
training loss = 1.8597780466079712 19900
val loss = 1.9893558025360107
training loss = 1.8595027923583984 20000
val loss = 1.9901247024536133
training loss = 1.859762191772461 20100
val loss = 2.0235509872436523
training loss = 1.858986496925354 20200
val loss = 1.990950584411621
training loss = 2.1026203632354736 20300
val loss = 1.5759716033935547
training loss = 1.8584896326065063 20400
val loss = 1.9868266582489014
training loss = 1.8582311868667603 20500
val loss = 1.992173671722412
training loss = 1.8582512140274048 20600
val loss = 2.014692783355713
reduced chi^2 level 2 = 1.8577641248703003
Constrained alpha: 1.8167320489883423
Constrained beta: 2.8685495853424072
Constrained gamma: 14.179438591003418
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 823.7523,  854.7106,  926.9271,  882.6389, 1063.6912, 1079.7362,
        1086.6212, 1197.2664, 1122.6854, 1080.8030, 1173.5674, 1196.1910,
        1220.4840, 1289.3809, 1354.0679, 1398.8987, 1433.4541, 1429.4956,
        1555.4459, 1568.0956, 1630.0698, 1499.6776, 1604.4246, 1576.0115,
        1605.8042, 1718.5745, 1601.7104, 1693.2048, 1708.4622, 1738.7802,
        1692.0946, 1753.2522, 1738.7225, 1661.5925, 1634.1426, 1747.5632,
        1686.0524, 1651.7764, 1669.9772, 1635.1094, 1644.8860, 1695.6407,
        1549.5667, 1560.9204, 1410.2271, 1337.3783, 1285.3777, 1242.8198,
        1184.9478, 1160.2750, 1028.5934,  964.9016,  961.2132,  908.4660,
         890.0090,  823.2538,  837.5507,  680.6653,  590.9058,  493.7856,
         506.3533,  472.4947,  408.6619,  415.4779,  371.3306,  318.7977,
         252.7668,  270.3442,  214.1291,  187.1434,  177.9171,  147.2113,
         167.3597,   89.6592,  106.5365,   73.1848,   41.7883,   23.2981,
          41.2189,   44.7148,   19.0738,   50.6035,   49.8682])]
2959.6322695281697
4.702201878024902 11.979715140566721 7.851413360791382
val isze = 8
idinces = [25 68  8 52 75  4 60 33 29 10 51 35 59 30 32 21 40 55 15 72 13 46 74 19
 16 80 43 73 79 36 61 45 37 26 38 77 31 17 41 53 57  1 23 24 54 28  3 48
 76 78 42 39 34 49 22 82  5 44 58 63 56 64 65 12 69  7 66 71  0 67 20 18
 27  9 81  2 47 14 50  6 62 70 11]
we are doing training validation split
training loss = 366.6294250488281 100
val loss = 311.02044677734375
training loss = 11.21512222290039 200
val loss = 8.276128768920898
training loss = 10.40563678741455 300
val loss = 7.463284492492676
training loss = 9.809725761413574 400
val loss = 6.924167156219482
training loss = 9.397404670715332 500
val loss = 6.5753889083862305
training loss = 9.122103691101074 600
val loss = 6.365605354309082
training loss = 8.942090034484863 700
val loss = 6.248019695281982
training loss = 8.825716018676758 800
val loss = 6.187853813171387
training loss = 8.750629425048828 900
val loss = 6.161238193511963
training loss = 8.701654434204102 1000
val loss = 6.152552604675293
training loss = 8.668781280517578 1100
val loss = 6.152073860168457
training loss = 8.645550727844238 1200
val loss = 6.154031276702881
training loss = 8.627912521362305 1300
val loss = 6.155350685119629
training loss = 8.613385200500488 1400
val loss = 6.154520511627197
training loss = 8.60050106048584 1500
val loss = 6.151100158691406
training loss = 8.588404655456543 1600
val loss = 6.145145416259766
training loss = 8.576613426208496 1700
val loss = 6.136999130249023
training loss = 8.564848899841309 1800
val loss = 6.12704610824585
training loss = 8.552924156188965 1900
val loss = 6.1156229972839355
training loss = 8.540670394897461 2000
val loss = 6.102982997894287
training loss = 8.527861595153809 2100
val loss = 6.0892109870910645
training loss = 8.514108657836914 2200
val loss = 6.074099540710449
training loss = 8.498563766479492 2300
val loss = 6.056923866271973
training loss = 8.479179382324219 2400
val loss = 6.035519599914551
training loss = 8.449999809265137 2500
val loss = 6.0034260749816895
training loss = 8.389881134033203 2600
val loss = 5.9368109703063965
training loss = 8.209237098693848 2700
val loss = 5.734218120574951
training loss = 7.58723258972168 2800
val loss = 5.049453258514404
training loss = 6.0712056159973145 2900
val loss = 3.4865317344665527
training loss = 4.072618007659912 3000
val loss = 1.794783353805542
training loss = 3.160754919052124 3100
val loss = 1.452512502670288
training loss = 3.0628223419189453 3200
val loss = 1.5281888246536255
training loss = 3.02532958984375 3300
val loss = 1.5246496200561523
training loss = 3.0090291500091553 3400
val loss = 1.5112011432647705
training loss = 2.995744228363037 3500
val loss = 1.5051312446594238
training loss = 2.979536294937134 3600
val loss = 1.5017808675765991
training loss = 2.9677884578704834 3700
val loss = 1.5035555362701416
training loss = 2.955690622329712 3800
val loss = 1.4996172189712524
training loss = 2.9442672729492188 3900
val loss = 1.4997179508209229
training loss = 2.9347565174102783 4000
val loss = 1.4975944757461548
training loss = 2.9229578971862793 4100
val loss = 1.4991528987884521
training loss = 2.920482635498047 4200
val loss = 1.514268398284912
training loss = 2.903414726257324 4300
val loss = 1.4988071918487549
training loss = 2.8996901512145996 4400
val loss = 1.4966740608215332
training loss = 2.885237216949463 4500
val loss = 1.498387098312378
training loss = 2.8764045238494873 4600
val loss = 1.4978883266448975
training loss = 2.867872476577759 4700
val loss = 1.497870922088623
training loss = 2.8594398498535156 4800
val loss = 1.4969902038574219
training loss = 2.886141538619995 4900
val loss = 1.5133419036865234
training loss = 2.842878818511963 5000
val loss = 1.4962408542633057
training loss = 2.8347206115722656 5100
val loss = 1.4954817295074463
training loss = 2.832017183303833 5200
val loss = 1.5072951316833496
training loss = 2.818678140640259 5300
val loss = 1.4945831298828125
training loss = 2.8107097148895264 5400
val loss = 1.494072675704956
training loss = 2.802995204925537 5500
val loss = 1.4936755895614624
training loss = 2.7954342365264893 5600
val loss = 1.4938552379608154
training loss = 2.82624888420105 5700
val loss = 1.5151575803756714
training loss = 2.780670166015625 5800
val loss = 1.4938808679580688
training loss = 2.7734880447387695 5900
val loss = 1.4941062927246094
training loss = 2.7667765617370605 6000
val loss = 1.4952480792999268
training loss = 2.760056257247925 6100
val loss = 1.4949830770492554
training loss = 2.7650771141052246 6200
val loss = 1.4975500106811523
training loss = 2.747731924057007 6300
val loss = 1.496620535850525
training loss = 2.7418460845947266 6400
val loss = 1.4975168704986572
training loss = 2.737344980239868 6500
val loss = 1.496718168258667
training loss = 2.731232166290283 6600
val loss = 1.4998974800109863
training loss = 2.812775135040283 6700
val loss = 1.5549769401550293
training loss = 2.721687078475952 6800
val loss = 1.502734661102295
training loss = 2.7172415256500244 6900
val loss = 1.5037612915039062
training loss = 2.7133119106292725 7000
val loss = 1.5052707195281982
training loss = 2.7094528675079346 7100
val loss = 1.5071369409561157
training loss = 2.7076592445373535 7200
val loss = 1.5138798952102661
training loss = 2.702860116958618 7300
val loss = 1.5103063583374023
training loss = 2.7003726959228516 7400
val loss = 1.510136365890503
training loss = 2.697183609008789 7500
val loss = 1.512751579284668
training loss = 2.6945226192474365 7600
val loss = 1.5148699283599854
training loss = 2.6923396587371826 7700
val loss = 1.516278862953186
training loss = 2.6901261806488037 7800
val loss = 1.5176843404769897
training loss = 2.70613169670105 7900
val loss = 1.5542352199554443
training loss = 2.6864333152770996 8000
val loss = 1.5202001333236694
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 865.3755,  854.3765,  997.8286,  941.3677,  943.4448, 1065.1248,
        1076.9307, 1123.3624, 1076.0375, 1221.4468, 1200.9210, 1150.7910,
        1338.5808, 1246.2292, 1351.4050, 1490.6692, 1426.0347, 1445.8666,
        1631.5004, 1532.8781, 1658.5426, 1518.2213, 1627.7465, 1616.4119,
        1777.2402, 1679.8303, 1605.3622, 1699.3934, 1674.8564, 1807.7216,
        1666.4092, 1685.1963, 1701.2046, 1757.7297, 1733.6547, 1796.3707,
        1709.9594, 1608.4761, 1588.0814, 1606.7152, 1619.8152, 1529.3092,
        1459.4294, 1509.8098, 1372.9709, 1374.3287, 1285.4147, 1210.3876,
        1137.3297, 1207.3450, 1101.1289,  947.4769,  948.4211,  898.5685,
         900.3217,  850.0311,  804.8487,  704.6200,  632.3697,  549.9827,
         582.2203,  443.6211,  457.6222,  381.2776,  352.0009,  345.9007,
         309.2175,  262.5238,  212.1664,  198.1766,  174.3073,  143.6029,
         142.3663,  119.0974,   99.7729,   81.7149,   60.6217,   35.3833,
          38.8499,   34.1859,   20.1189,   33.1361,   46.7819])]
2862.2017847418465
0.46351229045313913 0.4041298399788795 46.81274779972328
val isze = 8
idinces = [12 45 38 43 49 60 23 47 20 18 76 41 82 56 51 64 26 10 37 73 71 67 42 39
 25 63 72 40 61  0 34 59  5 55 48 77 32 13  6 31 70 69 62 44 30  3 17 33
 80 27 28 57 74 52 11  1 65 50 19 24  2  7 29 36  8 14 53 78 75 81 66 35
  4 46 22 21 16  9 58 15 54 68 79]
we are doing training validation split
training loss = 24.161455154418945 100
val loss = 14.151225090026855
training loss = 9.870565414428711 200
val loss = 4.702364444732666
training loss = 6.901968002319336 300
val loss = 1.8015540838241577
training loss = 5.586113452911377 400
val loss = 1.3239606618881226
training loss = 4.561453342437744 500
val loss = 1.2207409143447876
training loss = 4.058681964874268 600
val loss = 1.3279297351837158
training loss = 3.8535540103912354 700
val loss = 1.477986216545105
training loss = 3.7140088081359863 800
val loss = 1.558946132659912
training loss = 3.573087692260742 900
val loss = 1.5879982709884644
training loss = 3.408514976501465 1000
val loss = 1.5918011665344238
training loss = 3.1890804767608643 1100
val loss = 1.5836966037750244
training loss = 2.8964216709136963 1200
val loss = 1.6076065301895142
training loss = 2.609358787536621 1300
val loss = 1.7190282344818115
training loss = 2.5408859252929688 1400
val loss = 1.9455056190490723
training loss = 2.2904603481292725 1500
val loss = 1.939069151878357
training loss = 2.402876377105713 1600
val loss = 2.0822606086730957
training loss = 2.1870791912078857 1700
val loss = 1.9830323457717896
training loss = 2.1588399410247803 1800
val loss = 1.9808576107025146
training loss = 2.1348326206207275 1900
val loss = 1.985603928565979
training loss = 2.1126632690429688 2000
val loss = 1.9851783514022827
training loss = 2.0938305854797363 2100
val loss = 1.991607904434204
training loss = 2.076509952545166 2200
val loss = 1.9826003313064575
training loss = 2.0632359981536865 2300
val loss = 1.9583983421325684
training loss = 2.04836106300354 2400
val loss = 1.9882636070251465
training loss = 2.0389485359191895 2500
val loss = 1.9616652727127075
training loss = 2.0264971256256104 2600
val loss = 1.9998466968536377
training loss = 2.017040967941284 2700
val loss = 2.003154993057251
training loss = 2.013331413269043 2800
val loss = 2.0707054138183594
training loss = 2.0006003379821777 2900
val loss = 2.0169663429260254
training loss = 1.992985486984253 3000
val loss = 2.022423267364502
training loss = 1.9976656436920166 3100
val loss = 2.1293296813964844
training loss = 1.9790621995925903 3200
val loss = 2.0350570678710938
training loss = 1.9722387790679932 3300
val loss = 2.03859806060791
training loss = 1.965922474861145 3400
val loss = 2.0408473014831543
training loss = 1.9593197107315063 3500
val loss = 2.0546817779541016
training loss = 1.9551457166671753 3600
val loss = 2.104862689971924
training loss = 1.946479082107544 3700
val loss = 2.0668411254882812
training loss = 2.099745273590088 3800
val loss = 2.6445870399475098
training loss = 1.9335864782333374 3900
val loss = 2.0804169178009033
training loss = 1.9269371032714844 4000
val loss = 2.0866761207580566
training loss = 1.921000599861145 4100
val loss = 2.0728249549865723
training loss = 1.9141206741333008 4200
val loss = 2.0988717079162598
training loss = 1.9225122928619385 4300
val loss = 2.00701642036438
training loss = 1.901799201965332 4400
val loss = 2.1130502223968506
training loss = 1.89565110206604 4500
val loss = 2.1221923828125
training loss = 1.8899595737457275 4600
val loss = 2.1296398639678955
training loss = 1.8841748237609863 4700
val loss = 2.142218589782715
training loss = 1.8794950246810913 4800
val loss = 2.119774341583252
training loss = 1.8731807470321655 4900
val loss = 2.162407875061035
training loss = 1.8764809370040894 5000
val loss = 2.0817432403564453
training loss = 1.8625799417495728 5100
val loss = 2.191831111907959
training loss = 1.857181429862976 5200
val loss = 2.1980252265930176
training loss = 1.8552807569503784 5300
val loss = 2.2798566818237305
training loss = 1.847036600112915 5400
val loss = 2.2222096920013428
reduced chi^2 level 2 = 1.8419597148895264
Constrained alpha: 1.7959442138671875
Constrained beta: 2.095118761062622
Constrained gamma: 28.141307830810547
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 870.7143,  872.7974,  910.6393, 1015.8974,  979.3603, 1043.8054,
        1039.7227, 1112.3040, 1193.9939, 1258.0602, 1206.6150, 1225.2523,
        1189.1980, 1260.6821, 1334.5149, 1460.2410, 1449.6244, 1440.3400,
        1576.2744, 1547.3628, 1544.1541, 1505.9531, 1629.0287, 1596.7423,
        1735.7802, 1659.5964, 1590.4434, 1703.1143, 1743.8241, 1732.0254,
        1627.4941, 1762.7985, 1757.0094, 1787.0917, 1719.6608, 1737.4630,
        1708.1323, 1635.0797, 1630.4478, 1614.6844, 1638.4668, 1595.9028,
        1443.2352, 1413.3331, 1320.2899, 1313.6188, 1269.2777, 1263.8481,
        1180.2893, 1096.8983, 1073.0396,  974.2432,  941.8187,  896.0679,
         879.9763,  881.9845,  806.7076,  724.8314,  590.3232,  573.6166,
         546.0578,  496.5269,  448.9185,  407.5287,  360.7390,  356.2646,
         313.8695,  251.7241,  211.4605,  174.9190,  185.2188,  156.5776,
         148.5211,  104.3512,  101.5911,   65.4665,   45.3594,   45.5482,
          39.8957,   41.1076,   26.8024,   40.7456,   44.6441])]
2557.4668015678644
2.443986611469783 14.838676166483786 33.47192490158866
val isze = 8
idinces = [56  0 24  7 17 62 60 63 54 51 73 61 16 25  8 53 49 33 41 79 80 72 44 81
 43 36 48 15 52 32  1 37 47 29 76 78 14  9 23  2 66 11 35 27 39 40 26 55
 22 50 10 13 46 57 21 71  3 70 28 45 31 34 75 12 67 58  5 38  4 64 19 59
 77 42 65 18 68  6 20 82 74 69 30]
we are doing training validation split
training loss = 237.95469665527344 100
val loss = 292.6082458496094
training loss = 9.61708927154541 200
val loss = 4.149639129638672
training loss = 8.439247131347656 300
val loss = 2.884781837463379
training loss = 8.306194305419922 400
val loss = 3.071732997894287
training loss = 8.171764373779297 500
val loss = 3.2900428771972656
training loss = 8.042618751525879 600
val loss = 3.533486843109131
training loss = 7.923355579376221 700
val loss = 3.7927467823028564
training loss = 7.816507339477539 800
val loss = 4.056768417358398
training loss = 7.722674369812012 900
val loss = 4.31379508972168
training loss = 7.640885829925537 1000
val loss = 4.552921772003174
training loss = 7.569136142730713 1100
val loss = 4.765510559082031
training loss = 7.504988670349121 1200
val loss = 4.945749282836914
training loss = 7.446067810058594 1300
val loss = 5.09127140045166
training loss = 7.39039421081543 1400
val loss = 5.202688694000244
training loss = 7.336503982543945 1500
val loss = 5.282840728759766
training loss = 7.283442497253418 1600
val loss = 5.335995674133301
training loss = 7.230649471282959 1700
val loss = 5.366910934448242
training loss = 7.177781105041504 1800
val loss = 5.379647731781006
training loss = 7.124999523162842 1900
val loss = 5.379426956176758
training loss = 7.072251796722412 2000
val loss = 5.369939804077148
training loss = 7.019651889801025 2100
val loss = 5.353522300720215
training loss = 6.967357635498047 2200
val loss = 5.332267761230469
training loss = 6.915560722351074 2300
val loss = 5.3070878982543945
training loss = 6.8643717765808105 2400
val loss = 5.275529861450195
training loss = 6.813912868499756 2500
val loss = 5.237981796264648
training loss = 6.764588356018066 2600
val loss = 5.200096130371094
training loss = 6.716653347015381 2700
val loss = 5.163661956787109
training loss = 6.670255661010742 2800
val loss = 5.130489826202393
training loss = 6.62531042098999 2900
val loss = 5.100978374481201
training loss = 6.581476211547852 3000
val loss = 5.088326454162598
training loss = 6.5372843742370605 3100
val loss = 5.055627822875977
training loss = 6.490031719207764 3200
val loss = 5.032422065734863
training loss = 6.432439804077148 3300
val loss = 4.976640701293945
training loss = 6.348621368408203 3400
val loss = 4.853143692016602
training loss = 6.145904064178467 3500
val loss = 4.888178825378418
training loss = 5.7615227699279785 3600
val loss = 4.703493595123291
training loss = 5.008869171142578 3700
val loss = 4.082833290100098
training loss = 3.5065860748291016 3800
val loss = 2.849734306335449
training loss = 2.29927134513855 3900
val loss = 1.9977532625198364
training loss = 2.060502052307129 4000
val loss = 2.087057113647461
training loss = 2.0029983520507812 4100
val loss = 2.126427173614502
training loss = 1.9695295095443726 4200
val loss = 2.043726682662964
training loss = 1.9429364204406738 4300
val loss = 2.093543767929077
training loss = 1.9300392866134644 4400
val loss = 2.180652618408203
training loss = 1.9116860628128052 4500
val loss = 2.0658040046691895
training loss = 1.9379761219024658 4600
val loss = 2.3505842685699463
training loss = 1.8922666311264038 4700
val loss = 2.0414373874664307
training loss = 1.9048198461532593 4800
val loss = 1.8541444540023804
training loss = 1.878225564956665 4900
val loss = 2.0267724990844727
training loss = 1.8846501111984253 5000
val loss = 1.8715765476226807
training loss = 1.8667927980422974 5100
val loss = 2.007521629333496
training loss = 1.865376591682434 5200
val loss = 2.0890138149261475
training loss = 1.8567363023757935 5300
val loss = 1.9950580596923828
training loss = 1.945418357849121 5400
val loss = 1.6397240161895752
training loss = 1.847307562828064 5500
val loss = 1.9773952960968018
training loss = 1.8425545692443848 5600
val loss = 1.9700403213500977
training loss = 1.8381266593933105 5700
val loss = 1.9732003211975098
training loss = 1.8334808349609375 5800
val loss = 1.9587273597717285
training loss = 1.8289610147476196 5900
val loss = 1.9408034086227417
training loss = 1.8243542909622192 6000
val loss = 1.9451426267623901
training loss = 2.0937321186065674 6100
val loss = 2.9196372032165527
training loss = 1.815055012702942 6200
val loss = 1.9284125566482544
training loss = 1.8101725578308105 6300
val loss = 1.9209587574005127
training loss = 1.8057235479354858 6400
val loss = 1.9365606307983398
training loss = 1.8005183935165405 6500
val loss = 1.9056495428085327
training loss = 2.2408041954040527 6600
val loss = 3.2389230728149414
training loss = 1.7901391983032227 6700
val loss = 1.8834376335144043
training loss = 1.7844260931015015 6800
val loss = 1.8789304494857788
training loss = 1.8718341588974 6900
val loss = 2.3806190490722656
training loss = 1.7716553211212158 7000
val loss = 1.8590223789215088
training loss = 1.7640368938446045 7100
val loss = 1.842913269996643
training loss = 1.7563496828079224 7200
val loss = 1.8629001379013062
training loss = 1.7465004920959473 7300
val loss = 1.8082151412963867
training loss = 1.7537187337875366 7400
val loss = 1.6188712120056152
training loss = 1.7258368730545044 7500
val loss = 1.7594763040542603
training loss = 1.715149164199829 7600
val loss = 1.7371490001678467
training loss = 1.712924599647522 7700
val loss = 1.8414653539657593
training loss = 1.6947345733642578 7800
val loss = 1.690670132637024
training loss = 1.6873788833618164 7900
val loss = 1.6019595861434937
training loss = 1.6752723455429077 8000
val loss = 1.6532230377197266
training loss = 1.6656479835510254 8100
val loss = 1.631901741027832
training loss = 1.6730091571807861 8200
val loss = 1.7981007099151611
training loss = 1.6466847658157349 8300
val loss = 1.5926378965377808
training loss = 1.6371445655822754 8400
val loss = 1.5765433311462402
training loss = 1.6353845596313477 8500
val loss = 1.6759811639785767
training loss = 1.6186529397964478 8600
val loss = 1.5389800071716309
training loss = 1.825212836265564 8700
val loss = 1.1398744583129883
training loss = 1.6008764505386353 8800
val loss = 1.4996602535247803
training loss = 1.5922088623046875 8900
val loss = 1.4851393699645996
training loss = 1.5844604969024658 9000
val loss = 1.4535117149353027
training loss = 1.5767006874084473 9100
val loss = 1.4515501260757446
training loss = 1.626951813697815 9200
val loss = 1.7710866928100586
training loss = 1.5625425577163696 9300
val loss = 1.4150865077972412
training loss = 1.555938959121704 9400
val loss = 1.4058598279953003
training loss = 1.5509740114212036 9500
val loss = 1.3630751371383667
training loss = 1.5448544025421143 9600
val loss = 1.3809826374053955
training loss = 1.5398175716400146 9700
val loss = 1.3859270811080933
training loss = 1.5353150367736816 9800
val loss = 1.3524059057235718
training loss = 1.5310277938842773 9900
val loss = 1.3508456945419312
training loss = 1.5858856439590454 10000
val loss = 1.136704921722412
training loss = 1.5241509675979614 10100
val loss = 1.3350096940994263
training loss = 1.5211458206176758 10200
val loss = 1.329856038093567
training loss = 1.563969612121582 10300
val loss = 1.5907979011535645
training loss = 1.516503095626831 10400
val loss = 1.3195137977600098
training loss = 1.5146151781082153 10500
val loss = 1.3169732093811035
training loss = 1.513433575630188 10600
val loss = 1.3285815715789795
training loss = 1.512059211730957 10700
val loss = 1.3134350776672363
training loss = 1.5212266445159912 10800
val loss = 1.2216728925704956
training loss = 1.5105308294296265 10900
val loss = 1.3117616176605225
training loss = 1.5101401805877686 11000
val loss = 1.3238741159439087
training loss = 1.5096635818481445 11100
val loss = 1.3133115768432617
training loss = 1.5093072652816772 11200
val loss = 1.309299349784851
training loss = 1.5220023393630981 11300
val loss = 1.2101826667785645
training loss = 1.5089210271835327 11400
val loss = 1.3076863288879395
training loss = 1.5298988819122314 11500
val loss = 1.468004584312439
training loss = 1.50870943069458 11600
val loss = 1.3034648895263672
training loss = 1.508557915687561 11700
val loss = 1.3062609434127808
training loss = 1.516741394996643 11800
val loss = 1.4023451805114746
training loss = 1.5084140300750732 11900
val loss = 1.306054711341858
training loss = 1.5083004236221313 12000
val loss = 1.3049017190933228
training loss = 1.5083998441696167 12100
val loss = 1.2936512231826782
training loss = 1.508186936378479 12200
val loss = 1.3023560047149658
training loss = 1.7245312929153442 12300
val loss = 1.0564378499984741
training loss = 1.5080668926239014 12400
val loss = 1.2998409271240234
training loss = 1.5079578161239624 12500
val loss = 1.300356388092041
training loss = 1.5084428787231445 12600
val loss = 1.3205692768096924
training loss = 1.5078225135803223 12700
val loss = 1.2975986003875732
training loss = 1.5222774744033813 12800
val loss = 1.4247384071350098
training loss = 1.5077091455459595 12900
val loss = 1.2953834533691406
training loss = 1.5075948238372803 13000
val loss = 1.2951405048370361
training loss = 1.5075874328613281 13100
val loss = 1.295411467552185
training loss = 1.507463812828064 13200
val loss = 1.292455792427063
training loss = 1.5504014492034912 13300
val loss = 1.529658555984497
training loss = 1.50733482837677 13400
val loss = 1.2913028001785278
training loss = 1.5072351694107056 13500
val loss = 1.2934783697128296
training loss = 1.5073497295379639 13600
val loss = 1.2773115634918213
training loss = 1.5070852041244507 13700
val loss = 1.287391185760498
training loss = 1.5207538604736328 13800
val loss = 1.4094257354736328
training loss = 1.5069530010223389 13900
val loss = 1.2858651876449585
training loss = 1.5088177919387817 14000
val loss = 1.24422025680542
training loss = 1.5068919658660889 14100
val loss = 1.2890753746032715
training loss = 1.5067282915115356 14200
val loss = 1.2823383808135986
training loss = 1.5846471786499023 14300
val loss = 1.0927152633666992
training loss = 1.5066202878952026 14400
val loss = 1.2815361022949219
training loss = 1.5064940452575684 14500
val loss = 1.2791051864624023
training loss = 1.5171529054641724 14600
val loss = 1.1929835081100464
training loss = 1.5063931941986084 14700
val loss = 1.2781215906143188
training loss = 1.5062689781188965 14800
val loss = 1.275985836982727
training loss = 1.50665283203125 14900
val loss = 1.2938042879104614
training loss = 1.506155014038086 15000
val loss = 1.2742677927017212
training loss = 1.901052713394165 15100
val loss = 1.0580261945724487
training loss = 1.5060480833053589 15200
val loss = 1.2684575319290161
reduced chi^2 level 2 = 1.5059301853179932
Constrained alpha: 1.8643434047698975
Constrained beta: 0.9860212802886963
Constrained gamma: 15.275425910949707
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 850.9942,  819.5640,  960.9832,  916.2527,  974.6379, 1020.4876,
        1131.6854, 1156.6492, 1144.8333, 1155.1440, 1184.4813, 1186.7120,
        1260.4348, 1242.2849, 1369.7460, 1348.1433, 1424.8618, 1426.8583,
        1572.3291, 1481.7869, 1545.8170, 1613.6953, 1609.3661, 1638.7087,
        1688.9570, 1655.6681, 1649.9912, 1744.4716, 1674.0798, 1656.6638,
        1681.4105, 1681.4177, 1637.9241, 1695.6017, 1645.2716, 1791.1519,
        1662.1658, 1659.2056, 1573.5272, 1633.3020, 1628.6152, 1600.3690,
        1454.2870, 1455.7079, 1420.9115, 1393.9719, 1309.3562, 1201.5782,
        1123.7618, 1177.6251, 1089.3882,  960.4585,  953.1760,  869.2749,
         849.8445,  898.9265,  848.3481,  740.6664,  643.0794,  547.5562,
         527.7162,  516.2266,  423.5214,  419.3071,  348.3264,  348.8841,
         328.4699,  254.6066,  205.0048,  180.2623,  177.8506,  133.2522,
         148.0895,   95.1039,   97.3696,   62.8523,   57.7429,   37.4477,
          29.3829,   49.6675,   24.7452,   39.6564,   38.5360])]
2666.1009325450204
2.7033142760759072 7.972666708136913 99.85591746085505
val isze = 8
idinces = [40 68 74 27 17 13 80  1 47 16  9 66 32  0 77 23 39 22 75  5 33 44 15 64
 38 51 78 53 52 76 65 25 61 14 50 11 18 69  8 57 26 72 59 28 81 58 21 20
 60 56 67 45 37 71 19  7 54 34 41 24 73  4  3 35 70 63 55 82 12 46 43 62
  6 49 29  2 10 79 48 36 31 30 42]
we are doing training validation split
training loss = 12.723748207092285 100
val loss = 25.669452667236328
training loss = 9.786656379699707 200
val loss = 18.560916900634766
training loss = 8.7105073928833 300
val loss = 16.672420501708984
training loss = 7.8565754890441895 400
val loss = 15.125530242919922
training loss = 7.223442554473877 500
val loss = 13.927778244018555
training loss = 6.77088737487793 600
val loss = 13.024812698364258
training loss = 6.454307556152344 700
val loss = 12.35148811340332
training loss = 6.23572301864624 800
val loss = 11.850549697875977
training loss = 6.085703372955322 900
val loss = 11.476806640625
training loss = 5.982512950897217 1000
val loss = 11.195977210998535
training loss = 5.9105610847473145 1100
val loss = 10.982664108276367
training loss = 5.85892915725708 1200
val loss = 10.818387985229492
training loss = 5.820153713226318 1300
val loss = 10.68971061706543
training loss = 5.789244174957275 1400
val loss = 10.586907386779785
training loss = 5.762950897216797 1500
val loss = 10.50285530090332
training loss = 5.739212512969971 1600
val loss = 10.432435035705566
training loss = 5.716768264770508 1700
val loss = 10.371883392333984
training loss = 5.694852352142334 1800
val loss = 10.318294525146484
training loss = 5.673006534576416 1900
val loss = 10.269615173339844
training loss = 5.650928974151611 2000
val loss = 10.224211692810059
training loss = 5.628391265869141 2100
val loss = 10.180717468261719
training loss = 5.6051130294799805 2200
val loss = 10.138050079345703
training loss = 5.580677032470703 2300
val loss = 10.095027923583984
training loss = 5.554337024688721 2400
val loss = 10.050262451171875
training loss = 5.524676322937012 2500
val loss = 10.001543045043945
training loss = 5.488803863525391 2600
val loss = 9.944724082946777
training loss = 5.440659523010254 2700
val loss = 9.871479034423828
training loss = 5.367645263671875 2800
val loss = 9.764659881591797
training loss = 5.248990058898926 2900
val loss = 9.59587287902832
training loss = 5.0682172775268555 3000
val loss = 9.33651065826416
training loss = 4.816745758056641 3100
val loss = 8.948265075683594
training loss = 4.45880126953125 3200
val loss = 8.354496002197266
training loss = 3.939286470413208 3300
val loss = 7.464000701904297
training loss = 3.2520837783813477 3400
val loss = 6.221302032470703
training loss = 2.5870537757873535 3500
val loss = 4.819633483886719
training loss = 2.2589030265808105 3600
val loss = 3.8053948879241943
training loss = 2.191368579864502 3700
val loss = 3.3870997428894043
training loss = 2.177483081817627 3800
val loss = 3.2636003494262695
training loss = 2.1692252159118652 3900
val loss = 3.2212579250335693
training loss = 2.162250280380249 4000
val loss = 3.197275161743164
training loss = 2.156061887741089 4100
val loss = 3.1783547401428223
training loss = 2.150486946105957 4200
val loss = 3.161958932876587
training loss = 2.1454131603240967 4300
val loss = 3.1474950313568115
training loss = 2.140753746032715 4400
val loss = 3.1346747875213623
training loss = 2.1364307403564453 4500
val loss = 3.1232848167419434
training loss = 2.1323862075805664 4600
val loss = 3.1133673191070557
training loss = 2.1287920475006104 4700
val loss = 3.1005241870880127
training loss = 2.125401258468628 4800
val loss = 3.0946266651153564
training loss = 2.12298321723938 4900
val loss = 3.0973572731018066
training loss = 2.1193768978118896 5000
val loss = 3.080777168273926
training loss = 2.121791124343872 5100
val loss = 3.1053121089935303
training loss = 2.1138904094696045 5200
val loss = 3.070286273956299
training loss = 2.123255968093872 5300
val loss = 3.116234302520752
training loss = 2.1086435317993164 5400
val loss = 3.0604264736175537
training loss = 2.1060447692871094 5500
val loss = 3.057525634765625
training loss = 2.103379249572754 5600
val loss = 3.0549509525299072
training loss = 2.1007485389709473 5700
val loss = 3.050410747528076
training loss = 2.100184440612793 5800
val loss = 3.0324792861938477
training loss = 2.0952823162078857 5900
val loss = 3.043179512023926
training loss = 2.2006475925445557 6000
val loss = 3.0119593143463135
training loss = 2.0896151065826416 6100
val loss = 3.034963846206665
training loss = 2.086698293685913 6200
val loss = 3.0331881046295166
training loss = 2.0908098220825195 6300
val loss = 3.071113348007202
training loss = 2.080691337585449 6400
val loss = 3.026327133178711
training loss = 2.0775697231292725 6500
val loss = 3.023907423019409
training loss = 2.0745270252227783 6600
val loss = 3.014862060546875
training loss = 2.0711514949798584 6700
val loss = 3.016071319580078
training loss = 2.068265914916992 6800
val loss = 3.0215046405792236
training loss = 2.064516544342041 6900
val loss = 3.0089211463928223
training loss = 2.070314884185791 7000
val loss = 3.052363395690918
training loss = 2.0576093196868896 7100
val loss = 3.0012216567993164
training loss = 2.0540542602539062 7200
val loss = 2.9980311393737793
training loss = 2.0521278381347656 7300
val loss = 3.0120246410369873
training loss = 2.046858310699463 7400
val loss = 2.9906320571899414
training loss = 2.0430901050567627 7500
val loss = 2.988581418991089
training loss = 2.0394339561462402 7600
val loss = 2.9835925102233887
training loss = 2.0356621742248535 7700
val loss = 2.9793128967285156
training loss = 2.0376269817352295 7800
val loss = 2.9476635456085205
training loss = 2.028080701828003 7900
val loss = 2.971073865890503
training loss = 2.0241541862487793 8000
val loss = 2.9682865142822266
training loss = 2.0203866958618164 8100
val loss = 2.9613583087921143
training loss = 2.0164685249328613 8200
val loss = 2.9599709510803223
training loss = 2.019862413406372 8300
val loss = 2.9290051460266113
training loss = 2.0087738037109375 8400
val loss = 2.9521074295043945
training loss = 2.06296443939209 8500
val loss = 2.9086430072784424
training loss = 2.0010859966278076 8600
val loss = 2.944385051727295
training loss = 1.9971938133239746 8700
val loss = 2.940291404724121
training loss = 1.9935123920440674 8800
val loss = 2.9326252937316895
training loss = 1.9897054433822632 8900
val loss = 2.932117462158203
training loss = 2.0008544921875 9000
val loss = 2.8957133293151855
training loss = 1.982209324836731 9100
val loss = 2.9241809844970703
training loss = 1.978427767753601 9200
val loss = 2.9202003479003906
training loss = 1.9758585691452026 9300
val loss = 2.9040610790252686
training loss = 1.971100091934204 9400
val loss = 2.9119343757629395
training loss = 1.9672750234603882 9500
val loss = 2.9087491035461426
training loss = 1.9637455940246582 9600
val loss = 2.900474786758423
training loss = 1.9598971605300903 9700
val loss = 2.899759292602539
training loss = 1.9774194955825806 9800
val loss = 2.8565568923950195
training loss = 1.9521708488464355 9900
val loss = 2.89174747467041
training loss = 1.9480701684951782 10000
val loss = 2.883759021759033
training loss = 1.9439655542373657 10100
val loss = 2.879540205001831
training loss = 1.9393343925476074 10200
val loss = 2.8781590461730957
training loss = 1.9392859935760498 10300
val loss = 2.9012436866760254
training loss = 1.9291988611221313 10400
val loss = 2.867664098739624
training loss = 1.923020839691162 10500
val loss = 2.862487554550171
training loss = 1.9163668155670166 10600
val loss = 2.8565986156463623
training loss = 1.9087566137313843 10700
val loss = 2.84810471534729
training loss = 2.357102632522583 10800
val loss = 3.0035576820373535
training loss = 1.8915189504623413 10900
val loss = 2.8267929553985596
training loss = 1.882591962814331 11000
val loss = 2.8156580924987793
training loss = 1.8767883777618408 11100
val loss = 2.8211967945098877
training loss = 1.8664441108703613 11200
val loss = 2.787202835083008
training loss = 2.078282594680786 11300
val loss = 3.1283583641052246
training loss = 1.8527040481567383 11400
val loss = 2.7590408325195312
training loss = 1.8463505506515503 11500
val loss = 2.743786096572876
training loss = 1.8415861129760742 11600
val loss = 2.720257043838501
training loss = 1.8349320888519287 11700
val loss = 2.718139886856079
training loss = 1.8294697999954224 11800
val loss = 2.709383487701416
training loss = 1.8244366645812988 11900
val loss = 2.6960954666137695
training loss = 1.8195552825927734 12000
val loss = 2.6859917640686035
training loss = 1.8211568593978882 12100
val loss = 2.656315565109253
training loss = 1.8108781576156616 12200
val loss = 2.6686222553253174
training loss = 1.8068490028381348 12300
val loss = 2.660938262939453
training loss = 1.8037129640579224 12400
val loss = 2.660433530807495
training loss = 1.8000855445861816 12500
val loss = 2.647719144821167
training loss = 1.8186532258987427 12600
val loss = 2.709069013595581
training loss = 1.7945467233657837 12700
val loss = 2.6352005004882812
training loss = 1.7921499013900757 12800
val loss = 2.632580518722534
training loss = 1.8261700868606567 12900
val loss = 2.596599578857422
training loss = 1.7882827520370483 13000
val loss = 2.6256582736968994
training loss = 1.7868939638137817 13100
val loss = 2.6176981925964355
training loss = 1.785385012626648 13200
val loss = 2.6206817626953125
training loss = 1.78420090675354 13300
val loss = 2.6178083419799805
training loss = 1.808462381362915 13400
val loss = 2.587144136428833
training loss = 1.7823824882507324 13500
val loss = 2.6144888401031494
training loss = 1.7817041873931885 13600
val loss = 2.6110267639160156
training loss = 1.781097650527954 13700
val loss = 2.6143012046813965
training loss = 1.7805368900299072 13800
val loss = 2.6117305755615234
training loss = 1.7828630208969116 13900
val loss = 2.6302084922790527
training loss = 1.7797632217407227 14000
val loss = 2.610684394836426
training loss = 1.7875370979309082 14100
val loss = 2.6459195613861084
training loss = 1.7792139053344727 14200
val loss = 2.6105456352233887
training loss = 1.7789838314056396 14300
val loss = 2.609930992126465
training loss = 1.7793307304382324 14400
val loss = 2.603022575378418
training loss = 1.7786662578582764 14500
val loss = 2.609610080718994
training loss = 1.9523608684539795 14600
val loss = 2.8900129795074463
training loss = 1.7784552574157715 14700
val loss = 2.608593463897705
training loss = 1.7783467769622803 14800
val loss = 2.6092545986175537
training loss = 1.7812473773956299 14900
val loss = 2.5950098037719727
training loss = 1.778222680091858 15000
val loss = 2.6091697216033936
training loss = 1.7781600952148438 15100
val loss = 2.608830451965332
training loss = 1.7781788110733032 15200
val loss = 2.6071088314056396
training loss = 1.7780976295471191 15300
val loss = 2.608677387237549
training loss = 1.7780532836914062 15400
val loss = 2.608386516571045
training loss = 1.7791016101837158 15500
val loss = 2.6192359924316406
training loss = 1.7780144214630127 15600
val loss = 2.6081185340881348
training loss = 1.7782350778579712 15700
val loss = 2.611830711364746
training loss = 1.7779923677444458 15800
val loss = 2.6072049140930176
training loss = 1.7807072401046753 15900
val loss = 2.6259539127349854
training loss = 1.7780179977416992 16000
val loss = 2.605182409286499
training loss = 1.7779500484466553 16100
val loss = 2.6065969467163086
training loss = 1.781077265739441 16200
val loss = 2.627035140991211
training loss = 1.777947187423706 16300
val loss = 2.6056716442108154
training loss = 1.7780550718307495 16400
val loss = 2.609097719192505
training loss = 1.7779494524002075 16500
val loss = 2.604637384414673
training loss = 1.7779223918914795 16600
val loss = 2.6045498847961426
training loss = 1.7812004089355469 16700
val loss = 2.6247687339782715
training loss = 1.7779206037521362 16800
val loss = 2.603616714477539
training loss = 1.7779574394226074 16900
val loss = 2.5976176261901855
training loss = 1.7779276371002197 17000
val loss = 2.6030917167663574
training loss = 1.7779061794281006 17100
val loss = 2.601919174194336
training loss = 1.7782379388809204 17200
val loss = 2.607081890106201
training loss = 1.7779102325439453 17300
val loss = 2.601057529449463
training loss = 1.8141294717788696 17400
val loss = 2.57084321975708
training loss = 1.7779185771942139 17500
val loss = 2.6000142097473145
training loss = 1.7778964042663574 17600
val loss = 2.599246025085449
training loss = 1.7788692712783813 17700
val loss = 2.6086535453796387
training loss = 1.777921199798584 17800
val loss = 2.598201274871826
training loss = 1.777896761894226 17900
val loss = 2.5974748134613037
training loss = 1.7887606620788574 18000
val loss = 2.5757570266723633
training loss = 1.7779085636138916 18100
val loss = 2.5963447093963623
training loss = 1.7815173864364624 18200
val loss = 2.6171822547912598
training loss = 1.7779791355133057 18300
val loss = 2.5973548889160156
training loss = 1.7779138088226318 18400
val loss = 2.5945165157318115
training loss = 1.7779048681259155 18500
val loss = 2.5948596000671387
training loss = 1.7779401540756226 18600
val loss = 2.5941460132598877
training loss = 1.7779088020324707 18700
val loss = 2.592560291290283
training loss = 1.7790535688400269 18800
val loss = 2.6034982204437256
training loss = 1.777924656867981 18900
val loss = 2.5914382934570312
training loss = 2.0973126888275146 19000
val loss = 3.041539192199707
training loss = 1.777948260307312 19100
val loss = 2.590898275375366
training loss = 1.7779206037521362 19200
val loss = 2.5891518592834473
training loss = 1.7783008813858032 19300
val loss = 2.594600200653076
training loss = 1.777938723564148 19400
val loss = 2.5881595611572266
training loss = 1.7904736995697021 19500
val loss = 2.5597028732299805
training loss = 1.7779663801193237 19600
val loss = 2.5867161750793457
training loss = 1.7779419422149658 19700
val loss = 2.5860791206359863
training loss = 1.7787408828735352 19800
val loss = 2.5780324935913086
training loss = 1.7779641151428223 19900
val loss = 2.585020065307617
training loss = 1.85201096534729 20000
val loss = 2.5619795322418213
training loss = 1.7779977321624756 20100
val loss = 2.584433078765869
training loss = 1.7779721021652222 20200
val loss = 2.5830464363098145
training loss = 1.7804512977600098 20300
val loss = 2.5700526237487793
training loss = 1.7780064344406128 20400
val loss = 2.5819382667541504
training loss = 1.7780053615570068 20500
val loss = 2.5825841426849365
training loss = 1.7781327962875366 20600
val loss = 2.5781373977661133
training loss = 1.7780133485794067 20700
val loss = 2.5800187587738037
training loss = 1.7857768535614014 20800
val loss = 2.5607657432556152
training loss = 1.7780412435531616 20900
val loss = 2.5787601470947266
training loss = 1.9504097700119019 21000
val loss = 2.6073241233825684
training loss = 1.7780781984329224 21100
val loss = 2.577807903289795
training loss = 1.778052806854248 21200
val loss = 2.5770459175109863
training loss = 1.7815113067626953 21300
val loss = 2.5979628562927246
training loss = 1.778093934059143 21400
val loss = 2.5761256217956543
training loss = 1.7780689001083374 21500
val loss = 2.5756824016571045
training loss = 1.7781552076339722 21600
val loss = 2.5733866691589355
training loss = 1.778099775314331 21700
val loss = 2.574246406555176
training loss = 1.7799931764602661 21800
val loss = 2.5637528896331787
training loss = 1.7781357765197754 21900
val loss = 2.573369264602661
training loss = 1.7781639099121094 22000
val loss = 2.574700355529785
training loss = 1.7781823873519897 22100
val loss = 2.5729639530181885
training loss = 1.7781506776809692 22200
val loss = 2.5715363025665283
training loss = 1.8683505058288574 22300
val loss = 2.728466272354126
training loss = 1.7782056331634521 22400
val loss = 2.5707783699035645
training loss = 1.7781740427017212 22500
val loss = 2.569962978363037
training loss = 1.781227707862854 22600
val loss = 2.544654607772827
training loss = 1.7782306671142578 22700
val loss = 2.5687026977539062
training loss = 1.7781953811645508 22800
val loss = 2.5683889389038086
training loss = 1.791322112083435 22900
val loss = 2.5470974445343018
training loss = 1.778232216835022 23000
val loss = 2.5677647590637207
training loss = 1.778235673904419 23100
val loss = 2.5685129165649414
training loss = 1.7783931493759155 23200
val loss = 2.564077615737915
training loss = 1.778247356414795 23300
val loss = 2.5659966468811035
training loss = 2.1566410064697266 23400
val loss = 3.0768463611602783
training loss = 1.7782890796661377 23500
val loss = 2.5658814907073975
training loss = 1.7782477140426636 23600
val loss = 2.5644161701202393
training loss = 1.780998945236206 23700
val loss = 2.5820112228393555
training loss = 1.7782917022705078 23800
val loss = 2.563836097717285
training loss = 1.778253197669983 23900
val loss = 2.563251256942749
training loss = 1.7783799171447754 24000
val loss = 2.5651280879974365
training loss = 1.7782868146896362 24100
val loss = 2.562230110168457
training loss = 1.850250244140625 24200
val loss = 2.539402723312378
training loss = 1.7783187627792358 24300
val loss = 2.56101131439209
training loss = 1.7782684564590454 24400
val loss = 2.560746669769287
training loss = 1.7792725563049316 24500
val loss = 2.5525925159454346
training loss = 1.7783132791519165 24600
val loss = 2.5602195262908936
training loss = 1.778263807296753 24700
val loss = 2.559361457824707
training loss = 1.7785005569458008 24800
val loss = 2.5555508136749268
training loss = 1.7782849073410034 24900
val loss = 2.558764934539795
training loss = 1.999728798866272 25000
val loss = 2.6174063682556152
training loss = 1.778323769569397 25100
val loss = 2.5593514442443848
training loss = 1.7782468795776367 25200
val loss = 2.5573110580444336
training loss = 1.7899874448776245 25300
val loss = 2.6006054878234863
training loss = 1.7782613039016724 25400
val loss = 2.5565993785858154
training loss = 1.7781926393508911 25500
val loss = 2.55586576461792
training loss = 1.7783029079437256 25600
val loss = 2.5573883056640625
training loss = 1.7781968116760254 25700
val loss = 2.555377721786499
training loss = 1.7781482934951782 25800
val loss = 2.556035041809082
training loss = 1.778215765953064 25900
val loss = 2.5541810989379883
training loss = 1.7781227827072144 26000
val loss = 2.5539798736572266
training loss = 1.7784184217453003 26100
val loss = 2.558931589126587
training loss = 1.7781331539154053 26200
val loss = 2.55488920211792
training loss = 1.7780190706253052 26300
val loss = 2.5523955821990967
training loss = 1.8830609321594238 26400
val loss = 2.541855812072754
training loss = 1.7779995203018188 26500
val loss = 2.5524139404296875
training loss = 1.7778878211975098 26600
val loss = 2.5508158206939697
training loss = 1.8014925718307495 26700
val loss = 2.6150097846984863
training loss = 1.7778452634811401 26800
val loss = 2.549741744995117
training loss = 1.777724027633667 26900
val loss = 2.548921585083008
training loss = 1.778483510017395 27000
val loss = 2.5564708709716797
training loss = 1.7776566743850708 27100
val loss = 2.5481953620910645
training loss = 1.7820440530776978 27200
val loss = 2.5695838928222656
training loss = 1.777616024017334 27300
val loss = 2.5469703674316406
training loss = 1.7774741649627686 27400
val loss = 2.5463216304779053
training loss = 1.8372611999511719 27500
val loss = 2.650303602218628
training loss = 1.7773702144622803 27600
val loss = 2.5455150604248047
training loss = 1.7772196531295776 27700
val loss = 2.5434653759002686
training loss = 1.7772929668426514 27800
val loss = 2.544243097305298
training loss = 1.7771306037902832 27900
val loss = 2.54304575920105
training loss = 1.8187048435211182 28000
val loss = 2.6155266761779785
training loss = 1.7770413160324097 28100
val loss = 2.5430238246917725
training loss = 1.77686607837677 28200
val loss = 2.5406627655029297
training loss = 1.7772278785705566 28300
val loss = 2.545233964920044
training loss = 1.776727318763733 28400
val loss = 2.5395824909210205
training loss = 2.21408748626709 28500
val loss = 3.0951666831970215
training loss = 1.776605248451233 28600
val loss = 2.538407325744629
training loss = 1.7764122486114502 28700
val loss = 2.5368597507476807
training loss = 1.8539742231369019 28800
val loss = 2.519282102584839
training loss = 1.7762911319732666 28900
val loss = 2.5357003211975098
training loss = 1.776076316833496 29000
val loss = 2.534343719482422
training loss = 1.8086166381835938 29100
val loss = 2.6178059577941895
training loss = 1.775862693786621 29200
val loss = 2.533151865005493
training loss = 1.7779415845870972 29300
val loss = 2.5205576419830322
training loss = 1.7756600379943848 29400
val loss = 2.533207893371582
training loss = 1.7753885984420776 29500
val loss = 2.529585599899292
training loss = 1.7770264148712158 29600
val loss = 2.5197806358337402
training loss = 1.7751179933547974 29700
val loss = 2.5282087326049805
training loss = 1.7852535247802734 29800
val loss = 2.4997105598449707
training loss = 1.7748490571975708 29900
val loss = 2.5275402069091797
training loss = 1.7745888233184814 30000
val loss = 2.5250296592712402
reduced chi^2 level 2 = 1.7745862007141113
Constrained alpha: 1.966680884361267
Constrained beta: 0.6577739715576172
Constrained gamma: 12.413630485534668
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 818.9817,  864.8495,  955.4348,  907.0615,  993.9169, 1042.9652,
        1121.4189, 1117.5404, 1150.7760, 1131.3322, 1213.8724, 1184.0875,
        1196.4846, 1302.4263, 1339.5825, 1402.3761, 1397.7521, 1534.3789,
        1508.3989, 1533.1501, 1565.9336, 1555.4305, 1618.6975, 1530.3685,
        1640.5393, 1737.3844, 1621.5416, 1743.7408, 1733.1885, 1659.5978,
        1615.5565, 1694.6161, 1692.9894, 1754.8030, 1643.1234, 1759.5439,
        1643.9297, 1572.9177, 1699.7384, 1654.1533, 1629.8890, 1605.6699,
        1447.5763, 1446.8870, 1346.6393, 1348.8940, 1264.0824, 1206.6449,
        1152.3405, 1220.2820, 1068.3716,  962.6821,  955.8249,  899.0197,
         864.2859,  823.2436,  834.6614,  693.5810,  645.4091,  558.3801,
         541.8173,  515.3983,  479.8942,  374.4624,  351.3505,  371.8902,
         289.3282,  240.1941,  200.2789,  175.9167,  190.3188,  136.4679,
         139.7181,  114.2595,   97.9557,   70.3626,   45.3425,   47.0989,
          39.4722,   40.6715,   26.1480,   35.5575,   40.1985])]
2702.899240741514
3.964530760831625 12.587115775214265 2.3649362235230953
val isze = 8
idinces = [80 78 53  0 69 44 34 82 33 36 58  9 77 15 43 40 23 55 37 21 20 19 72  3
 70 45 27 28 14 47 63  4 60 17 56  5 52 73 22  1 13 25 41 54 30 61 42 74
 59 51 10 64 66  8 24 65 35 76 71 75 79 68 48 29 50  2 12  7 67 46 38 18
 11 31 32 26 62 57 81 39 49 16  6]
we are doing training validation split
training loss = 501.3972473144531 100
val loss = 506.0439758300781
training loss = 16.766088485717773 200
val loss = 19.807226181030273
training loss = 13.024188041687012 300
val loss = 17.142879486083984
training loss = 10.72199535369873 400
val loss = 15.124147415161133
training loss = 9.173678398132324 500
val loss = 13.922440528869629
training loss = 8.140887260437012 600
val loss = 13.22181510925293
training loss = 7.452597618103027 700
val loss = 12.823286056518555
training loss = 6.993442535400391 800
val loss = 12.606188774108887
training loss = 6.6867547035217285 900
val loss = 12.49697208404541
training loss = 6.481547832489014 1000
val loss = 12.450517654418945
training loss = 6.343780040740967 1100
val loss = 12.438835144042969
training loss = 6.250604629516602 1200
val loss = 12.444616317749023
training loss = 6.186660289764404 1300
val loss = 12.457098007202148
training loss = 6.141642093658447 1400
val loss = 12.470020294189453
training loss = 6.108667850494385 1500
val loss = 12.479728698730469
training loss = 6.083187580108643 1600
val loss = 12.48442268371582
training loss = 6.06223726272583 1700
val loss = 12.483294486999512
training loss = 6.043915271759033 1800
val loss = 12.476491928100586
training loss = 6.027036666870117 1900
val loss = 12.464305877685547
training loss = 6.0108723640441895 2000
val loss = 12.447332382202148
training loss = 5.994992733001709 2100
val loss = 12.426387786865234
training loss = 5.979149341583252 2200
val loss = 12.402109146118164
training loss = 5.963203430175781 2300
val loss = 12.374991416931152
training loss = 5.947082996368408 2400
val loss = 12.345684051513672
training loss = 5.930753707885742 2500
val loss = 12.314529418945312
training loss = 5.914196968078613 2600
val loss = 12.282024383544922
training loss = 5.8974103927612305 2700
val loss = 12.248374938964844
training loss = 5.880391597747803 2800
val loss = 12.213644981384277
training loss = 5.863152980804443 2900
val loss = 12.177995681762695
training loss = 5.845695972442627 3000
val loss = 12.141740798950195
training loss = 5.828032493591309 3100
val loss = 12.104621887207031
training loss = 5.810173988342285 3200
val loss = 12.066927909851074
training loss = 5.792134761810303 3300
val loss = 12.028605461120605
training loss = 5.773928165435791 3400
val loss = 11.989617347717285
training loss = 5.755564212799072 3500
val loss = 11.950075149536133
training loss = 5.737083435058594 3600
val loss = 11.909979820251465
training loss = 5.718499183654785 3700
val loss = 11.869325637817383
training loss = 5.699828624725342 3800
val loss = 11.82838249206543
training loss = 5.681132793426514 3900
val loss = 11.786928176879883
training loss = 5.662452220916748 4000
val loss = 11.745024681091309
training loss = 5.643847942352295 4100
val loss = 11.702848434448242
training loss = 5.625404357910156 4200
val loss = 11.660629272460938
training loss = 5.607227802276611 4300
val loss = 11.618481636047363
training loss = 5.589439392089844 4400
val loss = 11.576909065246582
training loss = 5.572212219238281 4500
val loss = 11.536205291748047
training loss = 5.555723667144775 4600
val loss = 11.496676445007324
training loss = 5.540203094482422 4700
val loss = 11.458915710449219
training loss = 5.543203830718994 4800
val loss = 11.160755157470703
training loss = 5.513549327850342 4900
val loss = 11.38825798034668
training loss = 5.528642177581787 5000
val loss = 11.044713973999023
training loss = 5.494104385375977 5100
val loss = 11.336642265319824
training loss = 5.4870100021362305 5200
val loss = 11.323841094970703
training loss = 5.481767654418945 5300
val loss = 11.293317794799805
training loss = 5.477748394012451 5400
val loss = 11.290750503540039
training loss = 5.475809097290039 5500
val loss = 11.345956802368164
training loss = 5.472833156585693 5600
val loss = 11.27306079864502
training loss = 5.475627422332764 5700
val loss = 11.405403137207031
training loss = 5.4703874588012695 5800
val loss = 11.257030487060547
training loss = 5.469644546508789 5900
val loss = 11.2597017288208
training loss = 5.469112873077393 6000
val loss = 11.25747013092041
training loss = 5.468722343444824 6100
val loss = 11.255522727966309
training loss = 5.46842098236084 6200
val loss = 11.266082763671875
training loss = 5.46816349029541 6300
val loss = 11.25384521484375
training loss = 5.468043804168701 6400
val loss = 11.27258586883545
training loss = 5.46776008605957 6500
val loss = 11.25330638885498
training loss = 5.467599868774414 6600
val loss = 11.25
training loss = 5.467850685119629 6700
val loss = 11.293686866760254
training loss = 5.467297077178955 6800
val loss = 11.24936294555664
training loss = 5.483772277832031 6900
val loss = 10.994938850402832
training loss = 5.4670186042785645 7000
val loss = 11.248819351196289
training loss = 5.487737655639648 7100
val loss = 11.557215690612793
training loss = 5.466753005981445 7200
val loss = 11.252113342285156
training loss = 5.466623783111572 7300
val loss = 11.243731498718262
training loss = 5.466492652893066 7400
val loss = 11.240194320678711
training loss = 5.466367244720459 7500
val loss = 11.24614429473877
training loss = 5.505069255828857 7600
val loss = 10.865059852600098
training loss = 5.466105937957764 7700
val loss = 11.243735313415527
training loss = 5.465974807739258 7800
val loss = 11.244582176208496
training loss = 5.466505527496338 7900
val loss = 11.192571640014648
training loss = 5.465719699859619 8000
val loss = 11.244373321533203
training loss = 5.465583801269531 8100
val loss = 11.243856430053711
training loss = 5.465879917144775 8200
val loss = 11.201427459716797
training loss = 5.4653191566467285 8300
val loss = 11.24268627166748
training loss = 5.557899475097656 8400
val loss = 11.922739028930664
training loss = 5.4650373458862305 8500
val loss = 11.239631652832031
training loss = 5.464904308319092 8600
val loss = 11.241386413574219
training loss = 5.465134620666504 8700
val loss = 11.283977508544922
training loss = 5.464619159698486 8800
val loss = 11.240262031555176
training loss = 5.464568138122559 8900
val loss = 11.26533317565918
training loss = 5.464325428009033 9000
val loss = 11.242588996887207
training loss = 5.465423583984375 9100
val loss = 11.313124656677246
training loss = 5.46402645111084 9200
val loss = 11.247476577758789
training loss = 5.463869094848633 9300
val loss = 11.23861312866211
training loss = 5.464252471923828 9400
val loss = 11.289464950561523
training loss = 5.463550090789795 9500
val loss = 11.236686706542969
training loss = 5.490058898925781 9600
val loss = 11.58895492553711
training loss = 5.463215351104736 9700
val loss = 11.237651824951172
training loss = 5.485359191894531 9800
val loss = 11.55571460723877
training loss = 5.462876796722412 9900
val loss = 11.243964195251465
training loss = 5.4626874923706055 10000
val loss = 11.232486724853516
training loss = 5.462480545043945 10100
val loss = 11.238607406616211
training loss = 5.462306499481201 10200
val loss = 11.233866691589355
training loss = 5.467958450317383 10300
val loss = 11.395998001098633
training loss = 5.46188497543335 10400
val loss = 11.230997085571289
training loss = 5.6343994140625 10500
val loss = 12.186119079589844
training loss = 5.461401462554932 10600
val loss = 11.238706588745117
training loss = 5.461127758026123 10700
val loss = 11.229116439819336
training loss = 5.481861114501953 10800
val loss = 11.541170120239258
training loss = 5.460424900054932 10900
val loss = 11.227516174316406
training loss = 5.461287498474121 11000
val loss = 11.15205192565918
training loss = 5.459394454956055 11100
val loss = 11.227731704711914
training loss = 5.458658218383789 11200
val loss = 11.221646308898926
training loss = 5.4581685066223145 11300
val loss = 11.266242027282715
training loss = 5.4563469886779785 11400
val loss = 11.213114738464355
training loss = 5.455187797546387 11500
val loss = 11.148225784301758
training loss = 5.451482772827148 11600
val loss = 11.200334548950195
training loss = 5.464224815368652 11700
val loss = 10.921295166015625
training loss = 5.438628196716309 11800
val loss = 11.161531448364258
training loss = 5.421469688415527 11900
val loss = 11.101033210754395
training loss = 5.374961853027344 12000
val loss = 10.9656400680542
training loss = 5.149499416351318 12100
val loss = 10.283784866333008
training loss = 4.310120105743408 12200
val loss = 8.116722106933594
training loss = 3.2992188930511475 12300
val loss = 6.014450550079346
training loss = 2.390113592147827 12400
val loss = 3.4310455322265625
training loss = 2.1668741703033447 12500
val loss = 2.602597951889038
training loss = 2.1094248294830322 12600
val loss = 2.505833148956299
training loss = 2.072024345397949 12700
val loss = 2.520578384399414
training loss = 2.045001983642578 12800
val loss = 2.556173801422119
training loss = 2.0245823860168457 12900
val loss = 2.571389675140381
training loss = 2.00848650932312 13000
val loss = 2.5879967212677
training loss = 1.9963452816009521 13100
val loss = 2.6078076362609863
training loss = 1.9861186742782593 13200
val loss = 2.6225826740264893
training loss = 1.9787321090698242 13300
val loss = 2.578819751739502
training loss = 1.9708958864212036 13400
val loss = 2.6405720710754395
training loss = 2.0326144695281982 13500
val loss = 3.1648154258728027
training loss = 1.9600282907485962 13600
val loss = 2.6470351219177246
training loss = 1.955671787261963 13700
val loss = 2.650592565536499
training loss = 1.951877474784851 13800
val loss = 2.6667439937591553
training loss = 1.9485994577407837 13900
val loss = 2.6505305767059326
training loss = 1.945595383644104 14000
val loss = 2.647822856903076
training loss = 1.943820834159851 14100
val loss = 2.703829765319824
training loss = 1.9404809474945068 14200
val loss = 2.64210844039917
training loss = 2.1316871643066406 14300
val loss = 3.5459096431732178
training loss = 1.936188817024231 14400
val loss = 2.6322875022888184
training loss = 1.9342981576919556 14500
val loss = 2.6277403831481934
training loss = 1.9399646520614624 14600
val loss = 2.785122871398926
training loss = 1.9308234453201294 14700
val loss = 2.618347644805908
training loss = 1.9292187690734863 14800
val loss = 2.6077449321746826
training loss = 1.927908182144165 14900
val loss = 2.6323113441467285
training loss = 1.9262205362319946 15000
val loss = 2.595855951309204
training loss = 1.9307869672775269 15100
val loss = 2.733901023864746
training loss = 1.9234163761138916 15200
val loss = 2.5828189849853516
training loss = 1.922052264213562 15300
val loss = 2.5740575790405273
training loss = 1.9207040071487427 15400
val loss = 2.5685019493103027
training loss = 1.919421672821045 15500
val loss = 2.557159423828125
training loss = 2.2141757011413574 15600
val loss = 1.7627041339874268
training loss = 1.916768193244934 15700
val loss = 2.540414571762085
training loss = 1.9154386520385742 15800
val loss = 2.530841112136841
training loss = 1.914147138595581 15900
val loss = 2.507573127746582
training loss = 1.9127482175827026 16000
val loss = 2.5100080966949463
training loss = 1.9212536811828613 16100
val loss = 2.3306989669799805
training loss = 1.9099370241165161 16200
val loss = 2.487257957458496
training loss = 2.0840327739715576 16300
val loss = 1.839820384979248
training loss = 1.9069840908050537 16400
val loss = 2.4647903442382812
training loss = 1.9054570198059082 16500
val loss = 2.4518163204193115
training loss = 1.903992772102356 16600
val loss = 2.419832229614258
training loss = 1.902273416519165 16700
val loss = 2.4258391857147217
training loss = 1.9405267238616943 16800
val loss = 2.0805037021636963
training loss = 1.898871660232544 16900
val loss = 2.400580406188965
training loss = 1.8971314430236816 17000
val loss = 2.3823740482330322
training loss = 1.9160592555999756 17100
val loss = 2.1246376037597656
training loss = 1.8934533596038818 17200
val loss = 2.3527932167053223
training loss = 1.8917629718780518 17300
val loss = 2.307924747467041
training loss = 1.889617919921875 17400
val loss = 2.327305793762207
training loss = 1.887648105621338 17500
val loss = 2.3025052547454834
training loss = 1.8873897790908813 17600
val loss = 2.3632125854492188
training loss = 1.8836790323257446 17700
val loss = 2.2699341773986816
training loss = 1.9438683986663818 17800
val loss = 2.737645387649536
training loss = 1.8796570301055908 17900
val loss = 2.237213611602783
training loss = 1.8776473999023438 18000
val loss = 2.2216672897338867
training loss = 1.8759922981262207 18100
val loss = 2.1743531227111816
training loss = 1.8736991882324219 18200
val loss = 2.1924796104431152
training loss = 1.9342942237854004 18300
val loss = 2.6622843742370605
training loss = 1.869795799255371 18400
val loss = 2.1668131351470947
training loss = 1.8678703308105469 18500
val loss = 2.1486291885375977
training loss = 1.8660168647766113 18600
val loss = 2.1488208770751953
training loss = 1.8641679286956787 18700
val loss = 2.1231448650360107
training loss = 1.9653068780899048 18800
val loss = 2.7456674575805664
training loss = 1.860580325126648 18900
val loss = 2.104785680770874
training loss = 1.858859658241272 19000
val loss = 2.087111473083496
training loss = 1.8703231811523438 19100
val loss = 2.289571762084961
training loss = 1.8554669618606567 19200
val loss = 2.063666343688965
training loss = 1.8538086414337158 19300
val loss = 2.0531911849975586
training loss = 1.8525680303573608 19400
val loss = 2.0114948749542236
training loss = 1.8506864309310913 19500
val loss = 2.0340843200683594
training loss = 1.8813090324401855 19600
val loss = 1.7257462739944458
training loss = 1.8476642370224 19700
val loss = 2.015270709991455
training loss = 1.8462281227111816 19800
val loss = 2.00667142868042
training loss = 1.8473069667816162 19900
val loss = 1.910571575164795
training loss = 1.8434392213821411 20000
val loss = 1.9921149015426636
training loss = 1.8420939445495605 20100
val loss = 1.9821443557739258
training loss = 1.8437790870666504 20200
val loss = 1.8788211345672607
training loss = 1.8394941091537476 20300
val loss = 1.9664506912231445
training loss = 2.057297706604004 20400
val loss = 1.2640312910079956
training loss = 1.8370048999786377 20500
val loss = 1.9535945653915405
training loss = 1.8358038663864136 20600
val loss = 1.945833683013916
training loss = 1.8347434997558594 20700
val loss = 1.9590187072753906
training loss = 1.833512783050537 20800
val loss = 1.9321119785308838
training loss = 1.9016196727752686 20900
val loss = 2.4371399879455566
training loss = 1.8313134908676147 21000
val loss = 1.9220261573791504
training loss = 1.8302470445632935 21100
val loss = 1.9139301776885986
training loss = 1.8306480646133423 21200
val loss = 1.8426685333251953
training loss = 1.828224778175354 21300
val loss = 1.902871012687683
training loss = 1.8272229433059692 21400
val loss = 1.898247241973877
training loss = 1.82631516456604 21500
val loss = 1.9081859588623047
training loss = 1.8253141641616821 21600
val loss = 1.8869354724884033
training loss = 1.8244092464447021 21700
val loss = 1.8959190845489502
training loss = 1.8234727382659912 21800
val loss = 1.8725547790527344
training loss = 1.8225690126419067 21900
val loss = 1.8722219467163086
training loss = 1.8220311403274536 22000
val loss = 1.9025938510894775
training loss = 1.8208097219467163 22100
val loss = 1.8631362915039062
training loss = 2.2352497577667236 22200
val loss = 0.9741597175598145
training loss = 1.819109559059143 22300
val loss = 1.857283115386963
training loss = 1.8182884454727173 22400
val loss = 1.8496342897415161
training loss = 1.9775930643081665 22500
val loss = 1.2356113195419312
training loss = 1.8166488409042358 22600
val loss = 1.839409589767456
training loss = 1.815847396850586 22700
val loss = 1.8369981050491333
training loss = 1.8231110572814941 22800
val loss = 1.9978529214859009
training loss = 1.8142261505126953 22900
val loss = 1.8291199207305908
training loss = 1.8135933876037598 23000
val loss = 1.8005099296569824
training loss = 1.8126450777053833 23100
val loss = 1.8335471153259277
training loss = 1.8117802143096924 23200
val loss = 1.8162156343460083
training loss = 1.814172387123108 23300
val loss = 1.9157416820526123
training loss = 1.8101298809051514 23400
val loss = 1.8084418773651123
training loss = 1.8092586994171143 23500
val loss = 1.8012622594833374
training loss = 1.8083772659301758 23600
val loss = 1.8032580614089966
training loss = 1.8074707984924316 23700
val loss = 1.7958714962005615
training loss = 1.8406071662902832 23800
val loss = 1.485481858253479
training loss = 1.8054869174957275 23900
val loss = 1.7850189208984375
training loss = 1.8898121118545532 24000
val loss = 2.3534183502197266
training loss = 1.8032214641571045 24100
val loss = 1.7702372074127197
training loss = 1.8018853664398193 24200
val loss = 1.7697298526763916
training loss = 1.8007829189300537 24300
val loss = 1.7329069375991821
training loss = 1.7987443208694458 24400
val loss = 1.7590162754058838
training loss = 1.796798586845398 24500
val loss = 1.7552540302276611
training loss = 1.7945904731750488 24600
val loss = 1.744178295135498
training loss = 2.0307061672210693 24700
val loss = 2.7482967376708984
training loss = 1.7889994382858276 24800
val loss = 1.7179388999938965
training loss = 1.7856435775756836 24900
val loss = 1.7101314067840576
training loss = 1.7835359573364258 25000
val loss = 1.631763219833374
training loss = 1.7786446809768677 25100
val loss = 1.6834083795547485
training loss = 1.7751436233520508 25200
val loss = 1.6646409034729004
training loss = 1.7718868255615234 25300
val loss = 1.6754990816116333
training loss = 1.7685437202453613 25400
val loss = 1.6506322622299194
training loss = 1.7695153951644897 25500
val loss = 1.7602124214172363
training loss = 1.7619413137435913 25600
val loss = 1.6329879760742188
training loss = 1.8834766149520874 25700
val loss = 2.3232979774475098
training loss = 1.7549867630004883 25800
val loss = 1.6113855838775635
training loss = 1.752029299736023 25900
val loss = 1.6561200618743896
training loss = 1.7475574016571045 26000
val loss = 1.5900352001190186
training loss = 1.7436617612838745 26100
val loss = 1.5867440700531006
training loss = 1.740590214729309 26200
val loss = 1.532274603843689
training loss = 1.7362146377563477 26300
val loss = 1.5692601203918457
training loss = 2.090010166168213 26400
val loss = 0.7399415373802185
training loss = 1.729435682296753 26500
val loss = 1.5524744987487793
training loss = 1.7263755798339844 26600
val loss = 1.5506863594055176
training loss = 1.7246179580688477 26700
val loss = 1.493391752243042
training loss = 1.7211475372314453 26800
val loss = 1.5432746410369873
training loss = 1.9704914093017578 26900
val loss = 2.5765528678894043
training loss = 1.7166473865509033 27000
val loss = 1.5372424125671387
training loss = 1.7146000862121582 27100
val loss = 1.5345001220703125
training loss = 1.7129182815551758 27200
val loss = 1.555492639541626
training loss = 1.7110581398010254 27300
val loss = 1.534401774406433
training loss = 1.7102822065353394 27400
val loss = 1.4854531288146973
training loss = 1.708011507987976 27500
val loss = 1.5347778797149658
training loss = 1.7826963663101196 27600
val loss = 2.0651016235351562
training loss = 1.7054024934768677 27700
val loss = 1.5381999015808105
training loss = 1.7041826248168945 27800
val loss = 1.5336061716079712
training loss = 1.7033498287200928 27900
val loss = 1.512548565864563
training loss = 1.702095866203308 28000
val loss = 1.5392353534698486
training loss = 1.7026199102401733 28100
val loss = 1.6089606285095215
training loss = 1.700283169746399 28200
val loss = 1.5428979396820068
training loss = 1.7072935104370117 28300
val loss = 1.7062956094741821
training loss = 1.6987000703811646 28400
val loss = 1.5439379215240479
training loss = 1.6979397535324097 28500
val loss = 1.5489089488983154
training loss = 1.7146142721176147 28600
val loss = 1.7933170795440674
training loss = 1.6966670751571655 28700
val loss = 1.5531145334243774
training loss = 1.6960285902023315 28800
val loss = 1.5553942918777466
training loss = 1.6959975957870483 28900
val loss = 1.518221139907837
training loss = 1.6949392557144165 29000
val loss = 1.5593409538269043
training loss = 1.6943817138671875 29100
val loss = 1.562356948852539
training loss = 1.6943552494049072 29200
val loss = 1.5281484127044678
training loss = 1.6934425830841064 29300
val loss = 1.5669236183166504
training loss = 1.6930097341537476 29400
val loss = 1.555450201034546
training loss = 1.6925734281539917 29500
val loss = 1.5758110284805298
training loss = 1.6921137571334839 29600
val loss = 1.573444128036499
training loss = 1.9329997301101685 29700
val loss = 2.6001501083374023
training loss = 1.6913483142852783 29800
val loss = 1.5765726566314697
training loss = 1.6909340620040894 29900
val loss = 1.5791752338409424
training loss = 1.6989548206329346 30000
val loss = 1.4254249334335327
reduced chi^2 level 2 = 1.735466718673706
Constrained alpha: 1.8704454898834229
Constrained beta: 1.645142674446106
Constrained gamma: 11.372811317443848
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 873.0429,  907.2722,  926.0436,  950.3030,  973.1216, 1041.9028,
        1088.3685, 1182.6086, 1210.1472, 1181.0775, 1194.0573, 1227.6815,
        1203.5939, 1222.2582, 1316.7035, 1456.5388, 1359.6422, 1447.2954,
        1540.0074, 1524.6750, 1631.7927, 1536.6289, 1583.8613, 1571.9263,
        1708.2010, 1816.2205, 1643.3212, 1813.3721, 1696.1602, 1715.7137,
        1619.8264, 1725.5879, 1721.7068, 1744.4054, 1663.5006, 1797.0818,
        1704.4238, 1574.0654, 1623.7662, 1625.5154, 1617.3778, 1520.0287,
        1496.0985, 1433.1355, 1335.2133, 1253.8954, 1342.6306, 1280.0527,
        1155.3372, 1195.1637, 1079.1191,  993.2733,  959.9937,  939.1194,
         867.6907,  851.6248,  828.4753,  721.1788,  591.1984,  545.7415,
         531.6008,  484.4399,  444.3660,  380.0544,  382.6248,  316.7152,
         281.8038,  258.0303,  159.2094,  189.7309,  160.7613,  135.1883,
         144.7283,  111.0353,  108.0850,   72.5665,   40.2745,   44.2453,
          31.1865,   37.8415,   21.2874,   37.0372,   37.1558])]
2720.7134620234565
3.878074573314276 19.536917760071557 99.90114680782901
val isze = 8
idinces = [57 29 39 14 23 17 55 32 25 45 51 72  1 44 10 66 30 24 19 65 28 67 34  7
 60 52 37 59 42 63 77 71 68  4 20  2  3 81  8 43 61 38 40 22 35  6 56 49
 50 41 11 82 64 62 53 47 74  0 33  5 48 79 26 69 15 27 18 36 78 16  9 58
 31 73 76 54 70 12 21 46 75 80 13]
we are doing training validation split
training loss = 482.80120849609375 100
val loss = 592.8773193359375
training loss = 30.405006408691406 200
val loss = 11.698250770568848
training loss = 15.856398582458496 300
val loss = 1.1097098588943481
training loss = 8.822253227233887 400
val loss = 0.556723952293396
training loss = 6.946671485900879 500
val loss = 0.7697650194168091
training loss = 5.888649940490723 600
val loss = 0.7905359864234924
training loss = 5.244484901428223 700
val loss = 0.720090389251709
training loss = 4.841902732849121 800
val loss = 0.8037031888961792
training loss = 4.486080646514893 900
val loss = 0.641276478767395
training loss = 4.356823921203613 1000
val loss = 0.997747004032135
training loss = 4.054113864898682 1100
val loss = 0.6349599957466125
training loss = 3.974616289138794 1200
val loss = 0.5663310885429382
training loss = 3.764631509780884 1300
val loss = 0.6321428418159485
training loss = 3.643242359161377 1400
val loss = 0.6372060775756836
training loss = 3.537804365158081 1500
val loss = 0.6503804922103882
training loss = 3.394613742828369 1600
val loss = 0.6688506603240967
training loss = 3.2702081203460693 1700
val loss = 0.7148821353912354
training loss = 3.1522557735443115 1800
val loss = 0.8145619034767151
training loss = 2.9736826419830322 1900
val loss = 0.9168305397033691
training loss = 2.883808135986328 2000
val loss = 0.9766213893890381
training loss = 2.9353346824645996 2100
val loss = 1.1862690448760986
training loss = 2.758331060409546 2200
val loss = 1.0771293640136719
training loss = 2.732186794281006 2300
val loss = 0.9699856638908386
training loss = 2.652801036834717 2400
val loss = 1.1131553649902344
training loss = 2.607320785522461 2500
val loss = 1.0759344100952148
training loss = 2.506856679916382 2600
val loss = 1.1541074514389038
training loss = 2.4814584255218506 2700
val loss = 1.0904655456542969
training loss = 2.418445110321045 2800
val loss = 1.1920688152313232
training loss = 2.4099833965301514 2900
val loss = 1.1115658283233643
training loss = 2.3636486530303955 3000
val loss = 1.224891185760498
training loss = 2.3882999420166016 3100
val loss = 1.1275608539581299
training loss = 2.325819253921509 3200
val loss = 1.2462213039398193
training loss = 2.308166742324829 3300
val loss = 1.3997576236724854
training loss = 2.29040265083313 3400
val loss = 1.264538288116455
training loss = 2.259296417236328 3500
val loss = 1.2985899448394775
training loss = 2.264028310775757 3600
val loss = 1.2643004655838013
training loss = 2.227156639099121 3700
val loss = 1.3249715566635132
training loss = 2.276230812072754 3800
val loss = 1.229634165763855
training loss = 2.2024691104888916 3900
val loss = 1.3501648902893066
training loss = 2.177245855331421 4000
val loss = 1.3720169067382812
training loss = 2.192598819732666 4100
val loss = 1.3865500688552856
training loss = 2.156447172164917 4200
val loss = 1.3910305500030518
training loss = 5.626421928405762 4300
val loss = 7.609608173370361
training loss = 2.136915445327759 4400
val loss = 1.4144350290298462
training loss = 2.117405891418457 4500
val loss = 1.4332244396209717
training loss = 2.12368106842041 4600
val loss = 1.3473647832870483
training loss = 2.1043403148651123 4700
val loss = 1.4459563493728638
training loss = 3.4969263076782227 4800
val loss = 4.394278049468994
training loss = 2.094592809677124 4900
val loss = 1.4415457248687744
training loss = 2.0803732872009277 5000
val loss = 1.472863793373108
training loss = 2.1242918968200684 5100
val loss = 1.5077913999557495
training loss = 2.074469804763794 5200
val loss = 1.4773266315460205
training loss = 2.0633199214935303 5300
val loss = 1.4878877401351929
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 900.3034,  823.3790,  957.7838,  960.6649, 1033.5657, 1059.9191,
        1072.8271, 1099.0354, 1125.8658, 1149.1063, 1205.3417, 1192.8257,
        1257.4447, 1230.4851, 1347.2251, 1430.2589, 1407.3519, 1413.7848,
        1505.7128, 1457.1271, 1595.0819, 1596.7621, 1538.9417, 1632.5508,
        1652.0876, 1720.4830, 1583.9868, 1744.5968, 1683.8704, 1696.8392,
        1645.9912, 1742.0767, 1740.8245, 1726.6912, 1754.9585, 1720.6677,
        1639.8557, 1656.2400, 1564.3973, 1622.6058, 1693.8469, 1589.3673,
        1475.0256, 1521.2767, 1335.6515, 1332.5817, 1261.3044, 1237.2017,
        1096.3114, 1190.7362, 1087.4774, 1048.1151,  966.3369,  955.0989,
         885.7143,  894.9743,  812.4215,  704.0952,  638.3978,  532.4330,
         572.8008,  509.0173,  425.9071,  410.1386,  368.1384,  364.5110,
         271.9709,  261.0541,  247.2938,  185.7464,  181.7573,  162.5108,
         127.1771,  111.0668,  104.3671,   66.0137,   54.6934,   37.5430,
          37.3676,   35.9912,   25.8149,   29.4314,   39.6882])]
2483.7736415240097
2.143510165406038 19.871578122515658 20.31426382922622
val isze = 8
idinces = [67 29 38 10 55  5 33 50 82 31 63 34 15 11 68 40 48 28 36 45 20 26  6 72
 69 57 43 74  0  7 56 17 71  4 81 62 12 80  2 78  1 58 59 75 79 60  9 51
 32 41 54 22 70  8 73 27 39 77 37 24 64 53 76 18 30 42 13 16 14 49 23  3
 46 66 52 44 25 21 19 65 35 61 47]
we are doing training validation split
training loss = 382.50323486328125 100
val loss = 496.28741455078125
training loss = 91.00797271728516 200
val loss = 101.54778289794922
training loss = 11.473271369934082 300
val loss = 5.979393482208252
training loss = 11.000059127807617 400
val loss = 5.49693489074707
training loss = 10.49073314666748 500
val loss = 4.967771530151367
training loss = 9.955913543701172 600
val loss = 4.424865245819092
training loss = 9.415900230407715 700
val loss = 3.893629789352417
training loss = 8.891672134399414 800
val loss = 3.399425983428955
training loss = 8.403640747070312 900
val loss = 2.96563458442688
training loss = 7.9694600105285645 1000
val loss = 2.6105599403381348
training loss = 7.601396083831787 1100
val loss = 2.34413480758667
training loss = 7.304114818572998 1200
val loss = 2.1655666828155518
training loss = 7.074052810668945 1300
val loss = 2.0635437965393066
training loss = 6.900942325592041 1400
val loss = 2.019320487976074
training loss = 6.770991802215576 1500
val loss = 2.0117452144622803
training loss = 6.670421600341797 1600
val loss = 2.022080421447754
training loss = 6.587921619415283 1700
val loss = 2.036850929260254
training loss = 6.515576362609863 1800
val loss = 2.0483155250549316
training loss = 6.448540210723877 1900
val loss = 2.053309917449951
training loss = 6.384192943572998 2000
val loss = 2.051509380340576
training loss = 6.321279048919678 2100
val loss = 2.043978214263916
training loss = 6.259285926818848 2200
val loss = 2.0322108268737793
training loss = 6.19807243347168 2300
val loss = 2.0176150798797607
training loss = 6.137673854827881 2400
val loss = 2.001326084136963
training loss = 6.078232765197754 2500
val loss = 1.9841612577438354
training loss = 6.019958019256592 2600
val loss = 1.966766119003296
training loss = 5.9630818367004395 2700
val loss = 1.949556589126587
training loss = 5.9078497886657715 2800
val loss = 1.932891845703125
training loss = 5.85439920425415 2900
val loss = 1.9170151948928833
training loss = 5.802588939666748 3000
val loss = 1.9021213054656982
training loss = 5.751608848571777 3100
val loss = 1.8882684707641602
training loss = 5.699042320251465 3200
val loss = 1.8752745389938354
training loss = 5.637998580932617 3300
val loss = 1.8624464273452759
training loss = 5.5462422370910645 3400
val loss = 1.8477977514266968
training loss = 5.345897674560547 3500
val loss = 1.8262994289398193
training loss = 4.9112420082092285 3600
val loss = 1.7685961723327637
training loss = 4.2346343994140625 3700
val loss = 1.5624523162841797
training loss = 3.2311997413635254 3800
val loss = 1.2865604162216187
training loss = 2.2407000064849854 3900
val loss = 1.1654505729675293
training loss = 1.8770321607589722 4000
val loss = 1.3119264841079712
training loss = 1.8076121807098389 4100
val loss = 1.4194254875183105
training loss = 1.7719950675964355 4200
val loss = 1.4627102613449097
training loss = 1.7451971769332886 4300
val loss = 1.4921833276748657
training loss = 1.7243469953536987 4400
val loss = 1.5184638500213623
training loss = 1.7079265117645264 4500
val loss = 1.5424742698669434
training loss = 1.6948524713516235 4600
val loss = 1.5640147924423218
training loss = 1.6843315362930298 4700
val loss = 1.5829741954803467
training loss = 1.6763455867767334 4800
val loss = 1.6020631790161133
training loss = 1.6689732074737549 4900
val loss = 1.612952709197998
training loss = 1.6653203964233398 5000
val loss = 1.6219398975372314
training loss = 1.6584361791610718 5100
val loss = 1.63434636592865
training loss = 1.6570014953613281 5200
val loss = 1.6510409116744995
training loss = 1.6506937742233276 5300
val loss = 1.6493279933929443
training loss = 1.6601654291152954 5400
val loss = 1.6807526350021362
training loss = 1.6446466445922852 5500
val loss = 1.6599501371383667
training loss = 1.6420750617980957 5600
val loss = 1.6635098457336426
training loss = 1.641363501548767 5700
val loss = 1.6641380786895752
training loss = 1.6374695301055908 5800
val loss = 1.6695321798324585
training loss = 1.6415501832962036 5900
val loss = 1.6703990697860718
training loss = 1.6334258317947388 6000
val loss = 1.6733096837997437
training loss = 1.6315770149230957 6100
val loss = 1.6748998165130615
training loss = 1.63009774684906 6200
val loss = 1.67448091506958
training loss = 1.628067970275879 6300
val loss = 1.676887035369873
training loss = 1.6280850172042847 6400
val loss = 1.684211254119873
training loss = 1.6248068809509277 6500
val loss = 1.678313970565796
training loss = 1.6232502460479736 6600
val loss = 1.6784359216690063
training loss = 1.6217352151870728 6700
val loss = 1.6799113750457764
training loss = 1.6202266216278076 6800
val loss = 1.6789696216583252
training loss = 1.618752121925354 6900
val loss = 1.6788387298583984
training loss = 1.6174145936965942 7000
val loss = 1.6778076887130737
training loss = 1.6158496141433716 7100
val loss = 1.6787126064300537
training loss = 1.7589421272277832 7200
val loss = 1.8092668056488037
training loss = 1.6130090951919556 7300
val loss = 1.6787179708480835
training loss = 1.6116327047348022 7400
val loss = 1.6780307292938232
training loss = 1.6302330493927002 7500
val loss = 1.6862914562225342
training loss = 1.6088415384292603 7600
val loss = 1.677574634552002
training loss = 1.6080814599990845 7700
val loss = 1.6746726036071777
training loss = 1.6061009168624878 7800
val loss = 1.6761449575424194
training loss = 1.6047130823135376 7900
val loss = 1.6762118339538574
training loss = 1.60336172580719 8000
val loss = 1.677366852760315
training loss = 1.602000117301941 8100
val loss = 1.6754827499389648
training loss = 1.6087034940719604 8200
val loss = 1.6951115131378174
training loss = 1.5992863178253174 8300
val loss = 1.675051212310791
training loss = 1.5979326963424683 8400
val loss = 1.6741420030593872
training loss = 1.5974339246749878 8500
val loss = 1.6787564754486084
training loss = 1.5952707529067993 8600
val loss = 1.673477053642273
training loss = 1.6370892524719238 8700
val loss = 1.7493172883987427
training loss = 1.59261155128479 8800
val loss = 1.6732178926467896
reduced chi^2 level 2 = 1.5914703607559204
Constrained alpha: 1.8391066789627075
Constrained beta: 3.92204213142395
Constrained gamma: 16.147939682006836
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 827.4366,  868.0879,  925.4836, 1026.3645, 1013.9327, 1075.7683,
        1070.0665, 1111.8060, 1112.3000, 1209.7512, 1237.1820, 1193.9633,
        1216.8458, 1322.8876, 1338.1564, 1436.8783, 1383.6660, 1458.6837,
        1503.3889, 1452.0085, 1596.4731, 1505.2961, 1640.4703, 1617.4484,
        1662.1676, 1690.8949, 1611.1876, 1729.0302, 1719.1296, 1737.8123,
        1688.3823, 1695.8539, 1683.6898, 1745.9000, 1783.4420, 1749.0663,
        1685.7938, 1609.0460, 1666.5197, 1595.6443, 1661.6768, 1653.3735,
        1531.8737, 1436.8550, 1367.4298, 1322.6332, 1282.8691, 1315.1993,
        1155.2333, 1176.6788, 1079.9631, 1005.5231,  896.3676,  879.7360,
         894.3238,  865.7178,  807.2136,  679.4434,  616.6274,  527.8634,
         534.2496,  448.0499,  431.2478,  401.5222,  322.3195,  329.7989,
         322.9413,  263.3351,  241.1879,  172.8016,  172.0161,  148.4288,
         136.4957,  111.7234,   94.7832,   64.4225,   50.7830,   28.0942,
          42.4929,   43.4219,   19.3745,   45.5580,   35.0507])]
2582.366117321
4.53821458638278 0.6177506498739138 59.85613723509163
val isze = 8
idinces = [24 78 16 46 63  8 29 43  4 53 73 10 51 60 31 13 23 49 39  6 30 62 19 15
 70 38 64 20 58 71  5 82 59 36 77 56 44 28 40 80 76 48 47 35 75  2 74 50
 32 22 52 17 68 25 12 21  1  9 33 81 79 11 57  3 42 55 18 65 45 27 66 26
 69 37  7 41 61  0 34 72 14 54 67]
we are doing training validation split
training loss = 50.68035125732422 100
val loss = 39.347625732421875
training loss = 11.014057159423828 200
val loss = 6.320446968078613
training loss = 6.916259765625 300
val loss = 4.30144739151001
training loss = 3.6858761310577393 400
val loss = 2.9764556884765625
training loss = 2.8446269035339355 500
val loss = 2.6967315673828125
training loss = 2.621373414993286 600
val loss = 2.6020755767822266
training loss = 2.5345659255981445 700
val loss = 2.5599114894866943
training loss = 2.50972580909729 800
val loss = 2.5983173847198486
training loss = 2.4685449600219727 900
val loss = 2.444434642791748
training loss = 2.4671263694763184 1000
val loss = 2.330368995666504
training loss = 2.438739061355591 1100
val loss = 2.3785104751586914
training loss = 2.4713735580444336 1200
val loss = 2.2294535636901855
training loss = 2.4131813049316406 1300
val loss = 2.314671277999878
training loss = 2.3996288776397705 1400
val loss = 2.2841005325317383
training loss = 2.3857579231262207 1500
val loss = 2.2475008964538574
training loss = 2.370023250579834 1600
val loss = 2.208054780960083
training loss = 2.3607821464538574 1700
val loss = 2.0954089164733887
training loss = 2.3266003131866455 1800
val loss = 2.0959420204162598
training loss = 2.299499988555908 1900
val loss = 2.0764122009277344
training loss = 2.2403695583343506 2000
val loss = 1.8706045150756836
training loss = 2.183702230453491 2100
val loss = 1.722430944442749
training loss = 2.2957470417022705 2200
val loss = 2.0084073543548584
training loss = 2.083750009536743 2300
val loss = 1.4582300186157227
training loss = 2.0902185440063477 2400
val loss = 1.288021206855774
training loss = 2.0387685298919678 2500
val loss = 1.3173834085464478
training loss = 2.042447090148926 2600
val loss = 1.1785857677459717
training loss = 2.0125293731689453 2700
val loss = 1.2249388694763184
training loss = 2.022062063217163 2800
val loss = 1.090811014175415
training loss = 1.9948703050613403 2900
val loss = 1.15669584274292
training loss = 2.204721689224243 3000
val loss = 1.7488093376159668
training loss = 1.9816398620605469 3100
val loss = 1.1070570945739746
training loss = 1.9757678508758545 3200
val loss = 1.0781989097595215
training loss = 1.9917184114456177 3300
val loss = 0.9548765420913696
training loss = 1.9653503894805908 3400
val loss = 1.0381096601486206
training loss = 1.960147738456726 3500
val loss = 1.0199166536331177
training loss = 1.9903310537338257 3600
val loss = 0.8798283934593201
training loss = 1.949467420578003 3700
val loss = 0.9863996505737305
training loss = 1.9486515522003174 3800
val loss = 0.916914165019989
training loss = 1.9374561309814453 3900
val loss = 0.9549322128295898
training loss = 1.930657982826233 4000
val loss = 0.9425548911094666
training loss = 1.9260385036468506 4100
val loss = 0.9755114316940308
training loss = 1.9152462482452393 4200
val loss = 0.9153530597686768
training loss = 2.1333768367767334 4300
val loss = 1.534902811050415
training loss = 1.897078037261963 4400
val loss = 0.8917279243469238
training loss = 1.8869116306304932 4500
val loss = 0.8753930330276489
training loss = 1.877164363861084 4600
val loss = 0.8866826295852661
training loss = 1.8655261993408203 4700
val loss = 0.8495975136756897
training loss = 1.854073166847229 4800
val loss = 0.8344895243644714
training loss = 1.8428508043289185 4900
val loss = 0.8293886184692383
training loss = 1.831383466720581 5000
val loss = 0.8077902793884277
training loss = 1.9706811904907227 5100
val loss = 0.7010510563850403
training loss = 1.8093621730804443 5200
val loss = 0.7807440161705017
training loss = 1.798842430114746 5300
val loss = 0.7693678140640259
training loss = 1.7892645597457886 5400
val loss = 0.7572973966598511
training loss = 1.7803269624710083 5500
val loss = 0.7488220930099487
training loss = 1.7718766927719116 5600
val loss = 0.7384161353111267
training loss = 1.7649160623550415 5700
val loss = 0.7441370487213135
training loss = 1.7578271627426147 5800
val loss = 0.7229766845703125
training loss = 1.8453319072723389 5900
val loss = 0.6414686441421509
training loss = 1.746338963508606 6000
val loss = 0.7061646580696106
training loss = 1.7413861751556396 6100
val loss = 0.7022595405578613
training loss = 1.737383246421814 6200
val loss = 0.7058576941490173
training loss = 1.7331770658493042 6300
val loss = 0.6891845464706421
training loss = 1.7307438850402832 6400
val loss = 0.7044060230255127
training loss = 1.726491928100586 6500
val loss = 0.6783764362335205
training loss = 1.729896903038025 6600
val loss = 0.7227962017059326
training loss = 1.720982313156128 6700
val loss = 0.6637407541275024
training loss = 1.7183736562728882 6800
val loss = 0.6621915102005005
training loss = 1.7164055109024048 6900
val loss = 0.6505911946296692
training loss = 1.7138961553573608 7000
val loss = 0.6525654792785645
training loss = 1.7603816986083984 7100
val loss = 0.584204912185669
training loss = 1.709961175918579 7200
val loss = 0.6448237299919128
training loss = 1.7080650329589844 7300
val loss = 0.640466570854187
training loss = 1.7066980600357056 7400
val loss = 0.6270022392272949
training loss = 1.704635500907898 7500
val loss = 0.632790744304657
training loss = 1.7089627981185913 7600
val loss = 0.5958832502365112
training loss = 1.7015677690505981 7700
val loss = 0.63253253698349
training loss = 1.6999143362045288 7800
val loss = 0.623679518699646
training loss = 1.7764604091644287 7900
val loss = 0.5578449368476868
training loss = 1.6972296237945557 8000
val loss = 0.621573269367218
training loss = 1.6957978010177612 8100
val loss = 0.6164813041687012
training loss = 1.6943598985671997 8200
val loss = 0.6135201454162598
training loss = 1.69381844997406 8300
val loss = 0.6277244091033936
training loss = 1.6919231414794922 8400
val loss = 0.6103958487510681
training loss = 1.6905763149261475 8500
val loss = 0.6071310639381409
training loss = 1.6900771856307983 8600
val loss = 0.5975127220153809
training loss = 1.68826162815094 8700
val loss = 0.6050846576690674
training loss = 1.7392419576644897 8800
val loss = 0.5532392263412476
training loss = 1.6861023902893066 8900
val loss = 0.6056613922119141
training loss = 1.6848652362823486 9000
val loss = 0.6007451415061951
training loss = 1.716410517692566 9100
val loss = 0.7223271727561951
training loss = 1.682779312133789 9200
val loss = 0.5982746481895447
training loss = 1.6815930604934692 9300
val loss = 0.5966933965682983
training loss = 1.6809754371643066 9400
val loss = 0.5907555818557739
training loss = 1.6795604228973389 9500
val loss = 0.5955495834350586
training loss = 1.7035988569259644 9600
val loss = 0.5485516786575317
training loss = 1.6776530742645264 9700
val loss = 0.5952543616294861
training loss = 1.6765390634536743 9800
val loss = 0.592399001121521
training loss = 1.6759099960327148 9900
val loss = 0.595433235168457
training loss = 1.6747297048568726 10000
val loss = 0.5920432806015015
training loss = 1.6736809015274048 10100
val loss = 0.5929898619651794
training loss = 1.6728956699371338 10200
val loss = 0.5930352210998535
training loss = 1.6718307733535767 10300
val loss = 0.5895114541053772
training loss = 1.673597812652588 10400
val loss = 0.5701656937599182
training loss = 1.6700628995895386 10500
val loss = 0.5885792374610901
training loss = 2.4154696464538574 10600
val loss = 1.7968989610671997
training loss = 1.6683813333511353 10700
val loss = 0.5867732763290405
training loss = 1.6673554182052612 10800
val loss = 0.5871360898017883
training loss = 1.6667042970657349 10900
val loss = 0.5880420804023743
training loss = 1.665722370147705 11000
val loss = 0.5871414542198181
training loss = 1.6647391319274902 11100
val loss = 0.5867419242858887
training loss = 1.6643165349960327 11200
val loss = 0.5946354866027832
training loss = 1.6631224155426025 11300
val loss = 0.585602879524231
training loss = 1.6748141050338745 11400
val loss = 0.6507456302642822
training loss = 1.6615084409713745 11500
val loss = 0.5856876373291016
training loss = 1.6772005558013916 11600
val loss = 0.5423967838287354
training loss = 1.6599775552749634 11700
val loss = 0.5839463472366333
training loss = 1.6590397357940674 11800
val loss = 0.5842066407203674
training loss = 1.6585922241210938 11900
val loss = 0.5802993774414062
training loss = 1.6575212478637695 12000
val loss = 0.5844710469245911
training loss = 1.663137674331665 12100
val loss = 0.6283397674560547
training loss = 1.6560300588607788 12200
val loss = 0.5847091674804688
training loss = 1.7451701164245605 12300
val loss = 0.8109249472618103
training loss = 1.6545637845993042 12400
val loss = 0.5840206742286682
training loss = 1.6545841693878174 12500
val loss = 0.5717836618423462
training loss = 1.6531404256820679 12600
val loss = 0.5871573090553284
training loss = 1.6522682905197144 12700
val loss = 0.583196222782135
training loss = 1.6521376371383667 12800
val loss = 0.5946158170700073
training loss = 1.6508923768997192 12900
val loss = 0.5842429399490356
training loss = 1.6515476703643799 13000
val loss = 0.5730836391448975
training loss = 1.6495970487594604 13100
val loss = 0.5847358703613281
training loss = 1.6493967771530151 13200
val loss = 0.595150351524353
training loss = 1.6484110355377197 13300
val loss = 0.5897600650787354
training loss = 1.6475259065628052 13400
val loss = 0.5844334363937378
training loss = 1.6514031887054443 13500
val loss = 0.561638355255127
training loss = 1.6462512016296387 13600
val loss = 0.5845516920089722
training loss = 1.646022915840149 13700
val loss = 0.5933178663253784
training loss = 1.6450361013412476 13800
val loss = 0.5848202705383301
training loss = 1.9600712060928345 13900
val loss = 1.1603679656982422
training loss = 1.6438695192337036 14000
val loss = 0.5834702253341675
training loss = 1.6431114673614502 14100
val loss = 0.5848236083984375
training loss = 1.642749309539795 14200
val loss = 0.5896002650260925
training loss = 1.6420016288757324 14300
val loss = 0.5857632160186768
training loss = 1.7336229085922241 14400
val loss = 0.8195264339447021
training loss = 1.6409298181533813 14500
val loss = 0.5871077179908752
training loss = 1.6402403116226196 14600
val loss = 0.586007833480835
training loss = 1.6401357650756836 14700
val loss = 0.5806032419204712
training loss = 1.6392523050308228 14800
val loss = 0.5869167447090149
training loss = 1.639989972114563 14900
val loss = 0.6034855842590332
training loss = 1.6383018493652344 15000
val loss = 0.5913970470428467
training loss = 1.6375794410705566 15100
val loss = 0.5872735977172852
training loss = 1.6395230293273926 15200
val loss = 0.610722541809082
training loss = 1.636566400527954 15300
val loss = 0.5875329971313477
training loss = 1.6480520963668823 15400
val loss = 0.6468050479888916
training loss = 1.635623574256897 15500
val loss = 0.5885151624679565
training loss = 1.7172839641571045 15600
val loss = 0.5585286617279053
training loss = 1.6346927881240845 15700
val loss = 0.5902687311172485
training loss = 1.6340910196304321 15800
val loss = 0.5890771150588989
training loss = 1.6339452266693115 15900
val loss = 0.5954545736312866
training loss = 1.6331892013549805 16000
val loss = 0.5891556143760681
training loss = 1.6540031433105469 16100
val loss = 0.5559584498405457
training loss = 1.6323727369308472 16200
val loss = 0.5896068811416626
training loss = 1.6318159103393555 16300
val loss = 0.5909714698791504
training loss = 1.6317981481552124 16400
val loss = 0.58382248878479
training loss = 1.6309605836868286 16500
val loss = 0.5903246402740479
training loss = 1.6312285661697388 16600
val loss = 0.581730842590332
training loss = 1.6301616430282593 16700
val loss = 0.5911273956298828
training loss = 1.657781720161438 16800
val loss = 0.5550503730773926
reduced chi^2 level 2 = 1.629830002784729
Constrained alpha: 2.0480728149414062
Constrained beta: -2.6765052384222773e-21
Constrained gamma: 16.432031631469727
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 829.8568,  917.1700,  930.9110,  958.4417, 1043.3772, 1055.0054,
        1122.9036, 1103.2196, 1166.8636, 1146.0470, 1196.7168, 1155.2930,
        1253.8356, 1221.2418, 1375.5790, 1453.5170, 1355.2375, 1496.5223,
        1569.1599, 1488.3306, 1589.8502, 1559.8279, 1613.7852, 1611.0240,
        1612.7223, 1717.9818, 1651.8059, 1734.1915, 1595.9288, 1652.4478,
        1662.7377, 1734.0563, 1651.7410, 1753.0547, 1765.7102, 1781.3815,
        1616.7551, 1587.4071, 1659.2706, 1601.3387, 1622.9830, 1585.0663,
        1539.2883, 1463.8984, 1383.9094, 1386.5099, 1238.7451, 1273.6497,
        1183.9769, 1179.2274, 1132.9312, 1014.2816,  961.6802,  934.8194,
         898.8582,  857.9517,  841.8500,  701.1822,  592.3157,  543.4704,
         547.4632,  428.8914,  478.7880,  389.1938,  373.9040,  362.7025,
         253.1205,  273.6042,  222.9125,  167.3944,  161.0638,  149.9366,
         125.9061,  116.8111,  117.4664,   72.7648,   58.4594,   54.8412,
          37.0923,   37.5924,   15.1786,   35.2718,   42.1880])]
2767.0762528202335
4.346886447048214 14.771557361757504 57.732381616312644
val isze = 8
idinces = [46 53 44  8 74 67 72 28 42 31 32 45 11 81 77 16 23 59 24 56 80 63 47 14
 17 66 69  4 13 82 39 78 68 36 48 34  3 57 71 15 40 10  5 27 58 51 41 49
  7 75 76  9 79 21 30 38 62 64 70 73  2 60 33 25 22 18 50 55 61  1 43 19
 52 65 29  0 35 20 37 12 26 54  6]
we are doing training validation split
training loss = 86.65371704101562 100
val loss = 107.64216613769531
training loss = 9.833312034606934 200
val loss = 29.603517532348633
training loss = 9.036731719970703 300
val loss = 27.121837615966797
training loss = 8.243182182312012 400
val loss = 24.361371994018555
training loss = 7.54166841506958 500
val loss = 21.535781860351562
training loss = 6.991118431091309 600
val loss = 18.870040893554688
training loss = 6.608786582946777 700
val loss = 16.55538558959961
training loss = 6.371331691741943 800
val loss = 14.703720092773438
training loss = 6.233389854431152 900
val loss = 13.326606750488281
training loss = 6.151429176330566 1000
val loss = 12.357513427734375
training loss = 6.096545219421387 1100
val loss = 11.696783065795898
training loss = 6.054061412811279 1200
val loss = 11.248541831970215
training loss = 6.017845630645752 1300
val loss = 10.938135147094727
training loss = 5.985588550567627 1400
val loss = 10.71413803100586
training loss = 5.956337928771973 1500
val loss = 10.54358196258545
training loss = 5.929555892944336 1600
val loss = 10.406715393066406
training loss = 5.904816627502441 1700
val loss = 10.29177474975586
training loss = 5.881753921508789 1800
val loss = 10.192099571228027
training loss = 5.860063552856445 1900
val loss = 10.103507995605469
training loss = 5.839484214782715 2000
val loss = 10.023398399353027
training loss = 5.819817066192627 2100
val loss = 9.950106620788574
training loss = 5.800893783569336 2200
val loss = 9.882061004638672
training loss = 5.78259801864624 2300
val loss = 9.818527221679688
training loss = 5.7650532722473145 2400
val loss = 9.777484893798828
training loss = 5.747751712799072 2500
val loss = 9.708402633666992
training loss = 5.731504917144775 2600
val loss = 9.630762100219727
training loss = 5.71529483795166 2700
val loss = 9.62088680267334
training loss = 5.700287342071533 2800
val loss = 9.565899848937988
training loss = 5.686593055725098 2900
val loss = 9.551722526550293
training loss = 5.674007415771484 3000
val loss = 9.489835739135742
training loss = 5.668694972991943 3100
val loss = 9.565720558166504
training loss = 5.653901100158691 3200
val loss = 9.426774978637695
training loss = 5.656352519989014 3300
val loss = 9.528825759887695
training loss = 5.640127658843994 3400
val loss = 9.375460624694824
training loss = 5.635221481323242 3500
val loss = 9.345693588256836
training loss = 5.631200313568115 3600
val loss = 9.331649780273438
training loss = 5.627813816070557 3700
val loss = 9.3063383102417
training loss = 5.628351211547852 3800
val loss = 9.221330642700195
training loss = 5.620350360870361 3900
val loss = 9.268009185791016
training loss = 5.613832950592041 4000
val loss = 9.232989311218262
training loss = 5.601773262023926 4100
val loss = 9.18796443939209
training loss = 5.574939727783203 4200
val loss = 9.059486389160156
training loss = 5.541951656341553 4300
val loss = 9.039484977722168
training loss = 5.281670093536377 4400
val loss = 8.028655052185059
training loss = 3.9035255908966064 4500
val loss = 4.655925273895264
training loss = 2.0664913654327393 4600
val loss = 3.658893585205078
training loss = 1.931976318359375 4700
val loss = 3.5049290657043457
training loss = 1.8800756931304932 4800
val loss = 3.4026060104370117
training loss = 1.8489079475402832 4900
val loss = 3.343742847442627
training loss = 1.8307266235351562 5000
val loss = 3.2556161880493164
training loss = 1.8158358335494995 5100
val loss = 3.29118013381958
training loss = 1.8052181005477905 5200
val loss = 3.2795777320861816
training loss = 1.7968029975891113 5300
val loss = 3.252819061279297
training loss = 1.7894067764282227 5400
val loss = 3.262617349624634
training loss = 1.7828483581542969 5500
val loss = 3.2570226192474365
training loss = 1.7773008346557617 5600
val loss = 3.2548680305480957
training loss = 1.7720935344696045 5700
val loss = 3.251805543899536
training loss = 1.7677263021469116 5800
val loss = 3.278535842895508
training loss = 1.7632375955581665 5900
val loss = 3.2465054988861084
training loss = 1.779890537261963 6000
val loss = 3.0437350273132324
training loss = 1.7564711570739746 6100
val loss = 3.2397570610046387
training loss = 1.753869652748108 6200
val loss = 3.236422061920166
training loss = 1.7520109415054321 6300
val loss = 3.205928325653076
training loss = 1.750003457069397 6400
val loss = 3.2312262058258057
training loss = 1.7494930028915405 6500
val loss = 3.2733607292175293
training loss = 1.7475481033325195 6600
val loss = 3.2269463539123535
training loss = 1.7467610836029053 6700
val loss = 3.225221633911133
training loss = 1.746633768081665 6800
val loss = 3.189026117324829
training loss = 1.7456364631652832 6900
val loss = 3.2220828533172607
training loss = 1.7479627132415771 7000
val loss = 3.30228590965271
training loss = 1.7450134754180908 7100
val loss = 3.21286940574646
training loss = 1.7448055744171143 7200
val loss = 3.2208101749420166
training loss = 1.7479500770568848 7300
val loss = 3.309190034866333
training loss = 1.7444555759429932 7400
val loss = 3.2203738689422607
training loss = 1.7449111938476562 7500
val loss = 3.257632255554199
training loss = 1.7441655397415161 7600
val loss = 3.2240724563598633
training loss = 1.744052767753601 7700
val loss = 3.2212157249450684
training loss = 1.74518883228302 7800
val loss = 3.167672634124756
training loss = 1.7436877489089966 7900
val loss = 3.221940040588379
training loss = 1.765608310699463 8000
val loss = 3.4697437286376953
training loss = 1.743209719657898 8100
val loss = 3.2250072956085205
training loss = 1.7429783344268799 8200
val loss = 3.225125551223755
training loss = 1.7428019046783447 8300
val loss = 3.2045812606811523
training loss = 1.7423067092895508 8400
val loss = 3.225980281829834
training loss = 1.7486648559570312 8500
val loss = 3.3626155853271484
training loss = 1.7414921522140503 8600
val loss = 3.2280030250549316
training loss = 1.7410770654678345 8700
val loss = 3.2270922660827637
training loss = 1.7405670881271362 8800
val loss = 3.2376909255981445
training loss = 1.7400379180908203 8900
val loss = 3.2302756309509277
training loss = 1.7453500032424927 9000
val loss = 3.117844581604004
training loss = 1.7388416528701782 9100
val loss = 3.231163501739502
training loss = 1.755531907081604 9200
val loss = 3.0482239723205566
training loss = 1.7374866008758545 9300
val loss = 3.2314157485961914
training loss = 1.7367777824401855 9400
val loss = 3.2333643436431885
training loss = 1.7363876104354858 9500
val loss = 3.265239953994751
training loss = 1.7351490259170532 9600
val loss = 3.2336578369140625
training loss = 2.16471004486084 9700
val loss = 2.6028690338134766
training loss = 1.7332823276519775 9800
val loss = 3.2341222763061523
training loss = 1.7322473526000977 9900
val loss = 3.234191417694092
training loss = 1.7845691442489624 10000
val loss = 3.630284070968628
training loss = 1.7297900915145874 10100
val loss = 3.2370057106018066
training loss = 1.7282673120498657 10200
val loss = 3.243333578109741
training loss = 1.7267853021621704 10300
val loss = 3.246225357055664
training loss = 1.7247097492218018 10400
val loss = 3.23412823677063
training loss = 1.723508358001709 10500
val loss = 3.2691049575805664
training loss = 1.720625400543213 10600
val loss = 3.234506130218506
training loss = 1.717984676361084 10700
val loss = 3.2239489555358887
training loss = 1.7159123420715332 10800
val loss = 3.2452826499938965
training loss = 1.713010311126709 10900
val loss = 3.2379140853881836
training loss = 1.7123745679855347 11000
val loss = 3.1865272521972656
training loss = 1.7081263065338135 11100
val loss = 3.2398791313171387
training loss = 1.752671718597412 11200
val loss = 3.5833969116210938
training loss = 1.7031627893447876 11300
val loss = 3.2354650497436523
training loss = 1.7003421783447266 11400
val loss = 3.2450361251831055
training loss = 1.6985567808151245 11500
val loss = 3.227964401245117
training loss = 1.6959086656570435 11600
val loss = 3.2479941844940186
training loss = 1.6963257789611816 11700
val loss = 3.201223850250244
training loss = 1.6919242143630981 11800
val loss = 3.2507567405700684
training loss = 1.6898150444030762 11900
val loss = 3.2568507194519043
training loss = 1.6893761157989502 12000
val loss = 3.299381971359253
training loss = 1.6867902278900146 12100
val loss = 3.2626819610595703
training loss = 1.7502576112747192 12200
val loss = 3.6555063724517822
training loss = 1.684279441833496 12300
val loss = 3.263577938079834
training loss = 1.682981252670288 12400
val loss = 3.274599313735962
training loss = 1.6878807544708252 12500
val loss = 3.3789069652557373
training loss = 1.681250810623169 12600
val loss = 3.281585693359375
training loss = 1.726608157157898 12700
val loss = 3.601548671722412
training loss = 1.6799404621124268 12800
val loss = 3.2843141555786133
training loss = 1.6792546510696411 12900
val loss = 3.29569411277771
training loss = 1.6790413856506348 13000
val loss = 3.303025484085083
training loss = 1.6785564422607422 13100
val loss = 3.3029744625091553
training loss = 1.8685002326965332 13200
val loss = 4.018292427062988
training loss = 1.678159475326538 13300
val loss = 3.3152542114257812
training loss = 1.6778870820999146 13400
val loss = 3.315253734588623
training loss = 1.707251787185669 13500
val loss = 3.1287097930908203
training loss = 1.6777671575546265 13600
val loss = 3.3221640586853027
training loss = 1.6776639223098755 13700
val loss = 3.3290553092956543
training loss = 1.678010106086731 13800
val loss = 3.3471555709838867
training loss = 1.6777667999267578 13900
val loss = 3.3341193199157715
training loss = 1.6807912588119507 14000
val loss = 3.4033687114715576
training loss = 1.6779958009719849 14100
val loss = 3.339911699295044
training loss = 1.678048849105835 14200
val loss = 3.3445186614990234
training loss = 1.6789398193359375 14300
val loss = 3.316096305847168
training loss = 1.6783924102783203 14400
val loss = 3.3497517108917236
training loss = 1.6784979104995728 14500
val loss = 3.354066848754883
training loss = 1.6787840127944946 14600
val loss = 3.355898857116699
training loss = 1.6789000034332275 14700
val loss = 3.3586418628692627
reduced chi^2 level 2 = 1.6789000034332275
Constrained alpha: 1.9152992963790894
Constrained beta: 3.390038251876831
Constrained gamma: 14.677396774291992
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 830.9603,  897.1716,  927.5981,  964.8245,  965.3807, 1013.7987,
        1140.2446, 1129.9050, 1123.1956, 1195.3483, 1256.4559, 1164.9753,
        1206.1949, 1255.3020, 1359.0737, 1392.1373, 1427.3485, 1415.0961,
        1510.9797, 1470.0128, 1543.7783, 1577.3588, 1628.0458, 1573.6891,
        1695.0250, 1732.2004, 1672.4550, 1709.4907, 1721.8580, 1695.0908,
        1677.8965, 1726.7881, 1661.8120, 1751.7361, 1712.0757, 1724.1119,
        1656.9706, 1679.3169, 1649.1323, 1590.6781, 1571.6212, 1529.1696,
        1543.6664, 1567.2875, 1375.6260, 1345.4517, 1235.1499, 1243.6155,
        1158.1635, 1102.9769, 1085.1814,  953.8573,  946.6837,  887.2895,
         892.5376,  885.9676,  804.6616,  685.0594,  565.6107,  531.0883,
         568.7155,  463.3199,  440.0924,  369.9697,  355.6772,  360.1148,
         288.4340,  232.5572,  210.3593,  173.3638,  155.0863,  144.0569,
         135.9758,   81.9052,  106.5837,   64.4029,   43.8891,   48.9287,
          35.1509,   43.2681,   22.0889,   41.9274,   32.6327])]
2398.964829797609
0.9358829801559349 6.6416690451039635 3.492205681370286
val isze = 8
idinces = [43 67 13 12  1 32 17 39 80 78 49 38 45 56  9 20  4  3 76 79 23 48 57 61
 51 58 70 35 74 31 77 59 64 47 28 73 30  5 55 10 21 26  6 53 54  0  7 82
 19 33 27 66 44 46 60 50 25 65  2 15 29 63 71 52 14 37 16 18 72 36 40 42
 62 75 41 22 69 68 34 24 81  8 11]
we are doing training validation split
training loss = 16.794036865234375 100
val loss = 29.593599319458008
training loss = 13.622491836547852 200
val loss = 23.041481018066406
training loss = 11.844585418701172 300
val loss = 19.58214569091797
training loss = 10.417176246643066 400
val loss = 16.5943603515625
training loss = 9.331441879272461 500
val loss = 14.136804580688477
training loss = 8.526350975036621 600
val loss = 12.156368255615234
training loss = 7.936859607696533 700
val loss = 10.57304573059082
training loss = 7.508110523223877 800
val loss = 9.30967903137207
training loss = 7.1974358558654785 900
val loss = 8.300853729248047
training loss = 6.972747802734375 1000
val loss = 7.493842124938965
training loss = 6.810296535491943 1100
val loss = 6.846831798553467
training loss = 6.69261360168457 1200
val loss = 6.327270030975342
training loss = 6.606899738311768 1300
val loss = 5.909592628479004
training loss = 6.543817043304443 1400
val loss = 5.573691368103027
training loss = 6.496581077575684 1500
val loss = 5.30385684967041
training loss = 6.46027946472168 1600
val loss = 5.087542533874512
training loss = 6.431412220001221 1700
val loss = 4.914770126342773
training loss = 6.407497406005859 1800
val loss = 4.777446746826172
training loss = 6.386811256408691 1900
val loss = 4.669032573699951
training loss = 6.368174076080322 2000
val loss = 4.584090709686279
training loss = 6.350801944732666 2100
val loss = 4.518238067626953
training loss = 6.334176540374756 2200
val loss = 4.467724323272705
training loss = 6.31797456741333 2300
val loss = 4.4295125007629395
training loss = 6.301985263824463 2400
val loss = 4.401071071624756
training loss = 6.2860493659973145 2500
val loss = 4.380341529846191
training loss = 6.270031452178955 2600
val loss = 4.365695476531982
training loss = 6.253722667694092 2700
val loss = 4.355807781219482
training loss = 6.236782073974609 2800
val loss = 4.349945545196533
training loss = 6.2185564041137695 2900
val loss = 4.347579002380371
training loss = 6.197756290435791 3000
val loss = 4.348884582519531
training loss = 6.171652793884277 3100
val loss = 4.354971885681152
training loss = 6.134210109710693 3200
val loss = 4.369401454925537
training loss = 6.071928977966309 3300
val loss = 4.401071071624756
training loss = 5.960618495941162 3400
val loss = 4.468153953552246
training loss = 5.786945343017578 3500
val loss = 4.571290016174316
training loss = 5.569576263427734 3600
val loss = 4.6252851486206055
training loss = 5.295505046844482 3700
val loss = 4.567463397979736
training loss = 4.915146827697754 3800
val loss = 4.440718650817871
training loss = 4.369074821472168 3900
val loss = 4.261117935180664
training loss = 3.6228458881378174 4000
val loss = 3.9713306427001953
training loss = 2.7875843048095703 4100
val loss = 3.493041515350342
training loss = 2.2115142345428467 4200
val loss = 2.9193854331970215
training loss = 2.031399726867676 4300
val loss = 2.5631158351898193
training loss = 1.9887789487838745 4400
val loss = 2.4504804611206055
training loss = 1.96408212184906 4500
val loss = 2.42402720451355
training loss = 1.9435092210769653 4600
val loss = 2.416860342025757
training loss = 1.9253078699111938 4700
val loss = 2.414220094680786
training loss = 1.908660888671875 4800
val loss = 2.4134392738342285
training loss = 1.8929716348648071 4900
val loss = 2.413783311843872
training loss = 1.8777962923049927 5000
val loss = 2.4150915145874023
training loss = 1.8628156185150146 5100
val loss = 2.417306900024414
training loss = 1.8478353023529053 5200
val loss = 2.4202475547790527
training loss = 1.8327457904815674 5300
val loss = 2.4241411685943604
training loss = 1.9745980501174927 5400
val loss = 2.0925002098083496
training loss = 1.80322265625 5500
val loss = 2.4347214698791504
training loss = 1.7893221378326416 5600
val loss = 2.470400810241699
training loss = 1.7753263711929321 5700
val loss = 2.4344143867492676
training loss = 1.761834740638733 5800
val loss = 2.449954032897949
training loss = 1.7494267225265503 5900
val loss = 2.4566409587860107
training loss = 1.7372490167617798 6000
val loss = 2.455334186553955
training loss = 1.7265416383743286 6100
val loss = 2.4250316619873047
training loss = 1.7152479887008667 6200
val loss = 2.463115692138672
training loss = 1.7168043851852417 6300
val loss = 2.33607816696167
training loss = 1.6960622072219849 6400
val loss = 2.471874952316284
training loss = 1.6909043788909912 6500
val loss = 2.3976643085479736
training loss = 1.6795192956924438 6600
val loss = 2.48088002204895
training loss = 1.6721138954162598 6700
val loss = 2.481734037399292
training loss = 1.6669132709503174 6800
val loss = 2.434122085571289
training loss = 1.6591800451278687 6900
val loss = 2.488107204437256
training loss = 1.809910535812378 7000
val loss = 2.162039041519165
training loss = 1.648048758506775 7100
val loss = 2.4913344383239746
training loss = 1.6429879665374756 7200
val loss = 2.4964542388916016
training loss = 1.63861083984375 7300
val loss = 2.5131659507751465
training loss = 1.6343313455581665 7400
val loss = 2.5032310485839844
training loss = 1.6512892246246338 7500
val loss = 2.7231028079986572
training loss = 1.6267846822738647 7600
val loss = 2.5093908309936523
training loss = 1.623326301574707 7700
val loss = 2.513223171234131
training loss = 1.6203408241271973 7800
val loss = 2.5276732444763184
training loss = 1.6173092126846313 7900
val loss = 2.515871286392212
training loss = 1.6210848093032837 8000
val loss = 2.6334142684936523
training loss = 1.61204195022583 8100
val loss = 2.519832134246826
training loss = 1.6948004961013794 8200
val loss = 3.016838312149048
training loss = 1.6074330806732178 8300
val loss = 2.5277209281921387
training loss = 1.6053131818771362 8400
val loss = 2.5261974334716797
training loss = 1.6046710014343262 8500
val loss = 2.482053756713867
training loss = 1.6015275716781616 8600
val loss = 2.5294790267944336
training loss = 1.942874789237976 8700
val loss = 3.740133762359619
training loss = 1.598137617111206 8800
val loss = 2.532928466796875
training loss = 1.5965678691864014 8900
val loss = 2.53474760055542
training loss = 1.6021287441253662 9000
val loss = 2.6571249961853027
training loss = 1.5936566591262817 9100
val loss = 2.5374441146850586
training loss = 1.5928410291671753 9200
val loss = 2.5728542804718018
training loss = 1.5909696817398071 9300
val loss = 2.5372114181518555
training loss = 1.5896803140640259 9400
val loss = 2.5426697731018066
training loss = 1.6035293340682983 9500
val loss = 2.722496747970581
training loss = 1.5871682167053223 9600
val loss = 2.547285795211792
training loss = 1.585893154144287 9700
val loss = 2.5513501167297363
training loss = 1.5846314430236816 9800
val loss = 2.5545849800109863
training loss = 1.5832945108413696 9900
val loss = 2.5511655807495117
training loss = 1.5839210748672485 10000
val loss = 2.6161086559295654
training loss = 1.5803585052490234 10100
val loss = 2.5563483238220215
training loss = 1.7078039646148682 10200
val loss = 2.2482049465179443
training loss = 1.5766863822937012 10300
val loss = 2.5613996982574463
training loss = 1.5743359327316284 10400
val loss = 2.565605640411377
training loss = 1.5716489553451538 10500
val loss = 2.5808968544006348
training loss = 1.5683321952819824 10600
val loss = 2.574436664581299
training loss = 1.5642175674438477 10700
val loss = 2.5793747901916504
training loss = 1.559548020362854 10800
val loss = 2.6013171672821045
reduced chi^2 level 2 = 1.5581852197647095
Constrained alpha: 1.8114460706710815
Constrained beta: 2.4277963638305664
Constrained gamma: 10.280467987060547
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 835.1861,  863.5903,  875.5784,  907.6253,  989.5067, 1035.4498,
        1106.6702, 1125.9021, 1143.7640, 1156.5854, 1202.5803, 1216.3052,
        1276.9861, 1264.9335, 1339.2515, 1378.4948, 1358.2656, 1424.3019,
        1586.2708, 1444.4669, 1680.1543, 1651.2511, 1659.3101, 1540.0767,
        1641.5394, 1718.5851, 1607.3068, 1746.8906, 1751.1407, 1757.0848,
        1697.4281, 1777.3195, 1697.4589, 1725.4188, 1699.6550, 1770.3247,
        1671.0961, 1630.3912, 1670.1157, 1548.3665, 1629.1302, 1547.3401,
        1547.6709, 1565.3478, 1366.6436, 1355.4529, 1220.2748, 1242.9973,
        1159.7542, 1211.5316, 1072.0286,  966.1271,  910.4575,  948.7567,
         864.4286,  819.4713,  797.6676,  697.4421,  615.9564,  560.1188,
         529.3660,  457.3201,  434.5495,  366.2166,  350.2992,  325.1998,
         289.6199,  256.9613,  189.2528,  172.5137,  166.4683,  144.8020,
         145.5657,  121.9312,   81.9869,   61.7162,   61.2014,   41.4272,
          43.4771,   43.1488,   21.4422,   24.8605,   39.5578])]
2757.862813208805
0.5436407304975599 3.939256605449619 80.55310894387627
val isze = 8
idinces = [19 77 37 75 18 52 11 41 31 45 39 32 51 60 40 28 65 66 78 30 43  4 48 14
 58 68 10 80 29 23 27  6 59 49 34 24 15 54 50 79 55 76 44 26 12 64 22 70
 63 21 61 20 47 57  1  8 71  0 56 42 53 17 13 33 82 46 36 62 35 69 16 81
  7 74  2 38  5 25 67  3  9 73 72]
we are doing training validation split
training loss = 25.29922866821289 100
val loss = 16.14018440246582
training loss = 19.99791717529297 200
val loss = 13.086553573608398
training loss = 16.504867553710938 300
val loss = 10.029848098754883
training loss = 13.714977264404297 400
val loss = 7.903748989105225
training loss = 11.544325828552246 500
val loss = 6.525290489196777
training loss = 9.841816902160645 600
val loss = 5.683873653411865
training loss = 8.48660945892334 700
val loss = 5.227629661560059
training loss = 7.4332098960876465 800
val loss = 5.065431594848633
training loss = 6.6820969581604 900
val loss = 5.097611427307129
training loss = 6.156137943267822 1000
val loss = 5.142858028411865
training loss = 5.685555934906006 1100
val loss = 4.982877731323242
training loss = 5.120524883270264 1200
val loss = 4.5542311668396
training loss = 4.4178972244262695 1300
val loss = 3.9745290279388428
training loss = 3.5930802822113037 1400
val loss = 3.303743839263916
training loss = 2.7915120124816895 1500
val loss = 2.7103824615478516
training loss = 2.2539751529693604 1600
val loss = 2.4224812984466553
training loss = 2.018134832382202 1700
val loss = 2.4117650985717773
training loss = 1.9392080307006836 1800
val loss = 2.4978041648864746
training loss = 1.9105503559112549 1900
val loss = 2.575169801712036
training loss = 1.9148898124694824 2000
val loss = 2.75797176361084
training loss = 1.8829858303070068 2100
val loss = 2.6832666397094727
training loss = 1.8724663257598877 2200
val loss = 2.717660427093506
training loss = 1.86260986328125 2300
val loss = 2.7619271278381348
training loss = 1.8530768156051636 2400
val loss = 2.794585943222046
training loss = 1.8431050777435303 2500
val loss = 2.8205811977386475
training loss = 1.8310900926589966 2600
val loss = 2.8426079750061035
training loss = 1.8196804523468018 2700
val loss = 2.8065943717956543
training loss = 1.794189691543579 2800
val loss = 2.842857837677002
training loss = 1.7694486379623413 2900
val loss = 2.8360586166381836
training loss = 1.7440605163574219 3000
val loss = 2.8527684211730957
training loss = 1.7720891237258911 3100
val loss = 3.0868844985961914
training loss = 1.698786973953247 3200
val loss = 2.90031361579895
training loss = 1.8659710884094238 3300
val loss = 2.8446335792541504
training loss = 1.6647027730941772 3400
val loss = 2.9438560009002686
training loss = 1.650277018547058 3500
val loss = 2.9546713829040527
training loss = 1.637625813484192 3600
val loss = 2.9601404666900635
training loss = 1.6257489919662476 3700
val loss = 2.955080986022949
training loss = 1.6177541017532349 3800
val loss = 2.989940643310547
training loss = 1.6058757305145264 3900
val loss = 2.9506711959838867
training loss = 1.5970715284347534 4000
val loss = 2.9466404914855957
training loss = 1.5912976264953613 4100
val loss = 2.914552688598633
training loss = 1.5824512243270874 4200
val loss = 2.9392001628875732
training loss = 1.5760968923568726 4300
val loss = 2.938551425933838
training loss = 1.5707297325134277 4400
val loss = 2.9258272647857666
training loss = 1.5656731128692627 4500
val loss = 2.9276418685913086
training loss = 1.562900424003601 4600
val loss = 2.95273494720459
training loss = 1.5574640035629272 4700
val loss = 2.920969009399414
training loss = 1.5889925956726074 4800
val loss = 2.829719066619873
training loss = 1.550982117652893 4900
val loss = 2.9172239303588867
training loss = 1.5481889247894287 5000
val loss = 2.9136195182800293
training loss = 1.5488216876983643 5100
val loss = 2.951408624649048
training loss = 1.5435677766799927 5200
val loss = 2.910635471343994
training loss = 1.5415199995040894 5300
val loss = 2.9081788063049316
training loss = 1.539751410484314 5400
val loss = 2.9087648391723633
training loss = 1.5380122661590576 5500
val loss = 2.9086976051330566
training loss = 1.600019097328186 5600
val loss = 3.1613609790802
training loss = 1.5350189208984375 5700
val loss = 2.9101598262786865
training loss = 1.5336018800735474 5800
val loss = 2.9103198051452637
training loss = 1.532365322113037 5900
val loss = 2.911101818084717
training loss = 1.53108549118042 6000
val loss = 2.9124975204467773
training loss = 1.5762766599655151 6100
val loss = 3.120594024658203
training loss = 1.5288218259811401 6200
val loss = 2.918633460998535
training loss = 1.5276904106140137 6300
val loss = 2.918588161468506
training loss = 1.5275901556015015 6400
val loss = 2.897291421890259
training loss = 1.525663137435913 6500
val loss = 2.9225430488586426
training loss = 1.824633002281189 6600
val loss = 3.665656089782715
training loss = 1.523800253868103 6700
val loss = 2.9252099990844727
reduced chi^2 level 2 = 1.523300051689148
Constrained alpha: 1.6928507089614868
Constrained beta: 3.8643016815185547
Constrained gamma: 29.922956466674805
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 903.3905,  854.4934,  954.8315,  919.7584,  984.5518, 1083.7786,
        1075.8619, 1129.1354, 1137.4626, 1172.6023, 1169.6273, 1238.5261,
        1216.2728, 1246.6741, 1407.1639, 1403.2343, 1414.8965, 1375.9575,
        1560.2778, 1491.6880, 1517.9890, 1627.6566, 1577.4259, 1669.9778,
        1698.2535, 1728.8285, 1602.6775, 1758.2620, 1665.4104, 1721.1062,
        1642.9663, 1771.8168, 1773.1058, 1793.5973, 1680.1073, 1740.8359,
        1725.5573, 1644.2394, 1624.6213, 1606.8433, 1671.9994, 1645.6373,
        1500.1321, 1544.2249, 1374.6868, 1360.6868, 1302.4603, 1217.1615,
        1159.4325, 1194.7805,  993.1945,  969.9534,  970.9924,  866.2750,
         909.8516,  870.5245,  885.6762,  655.5344,  600.0864,  508.0462,
         539.9026,  491.0873,  432.2677,  370.6321,  369.2675,  335.4409,
         287.6210,  271.6108,  189.8427,  179.3901,  161.8149,  129.8333,
         136.1643,  107.4137,   94.2800,   61.4273,   43.3322,   52.0302,
          38.6572,   44.6328,   16.4872,   37.7083,   38.8031])]
2865.9662961847034
3.829130363401731 13.422080967062382 10.350069673086049
val isze = 8
idinces = [65 25 33  8 48 20 71 57 15 52 53 40 27 73 34  0 80 70 74 22 59 24 38  9
 37 18 39 29 51  7 63 64 62 68 46 82 14 61 13 31 12 56  2 35 55  4 76 69
 54 42 19 17 44 45 58 60 50  3 72 66  1 36 32 26 10 30 41 47 43 67 11 23
 81  6  5 16 75 77 21 49 79 28 78]
we are doing training validation split
training loss = 470.38177490234375 100
val loss = 451.34588623046875
training loss = 12.258634567260742 200
val loss = 14.563356399536133
training loss = 10.366544723510742 300
val loss = 10.006159782409668
training loss = 9.485669136047363 400
val loss = 8.989465713500977
training loss = 8.848489761352539 500
val loss = 8.239723205566406
training loss = 8.400979995727539 600
val loss = 7.701999664306641
training loss = 8.09164047241211 700
val loss = 7.323213577270508
training loss = 7.879393100738525 800
val loss = 7.058688163757324
training loss = 7.733738899230957 900
val loss = 6.874235153198242
training loss = 7.632846832275391 1000
val loss = 6.744765281677246
training loss = 7.561463356018066 1100
val loss = 6.652363300323486
training loss = 7.509151458740234 1200
val loss = 6.584611415863037
training loss = 7.4688801765441895 1300
val loss = 6.532974720001221
training loss = 7.43603515625 1400
val loss = 6.491786003112793
training loss = 7.407646656036377 1500
val loss = 6.457320213317871
training loss = 7.381871700286865 1600
val loss = 6.427159309387207
training loss = 7.357591152191162 1700
val loss = 6.3997602462768555
training loss = 7.334153175354004 1800
val loss = 6.374258041381836
training loss = 7.311191558837891 1900
val loss = 6.350048065185547
training loss = 7.288511753082275 2000
val loss = 6.326746463775635
training loss = 7.266017436981201 2100
val loss = 6.304121017456055
training loss = 7.243655681610107 2200
val loss = 6.282055377960205
training loss = 7.221400737762451 2300
val loss = 6.2604289054870605
training loss = 7.199246406555176 2400
val loss = 6.23918342590332
training loss = 7.177186012268066 2500
val loss = 6.218252182006836
training loss = 7.155211925506592 2600
val loss = 6.197615623474121
training loss = 7.133318901062012 2700
val loss = 6.177260398864746
training loss = 7.111499309539795 2800
val loss = 6.157175064086914
training loss = 7.089750289916992 2900
val loss = 6.1373066902160645
training loss = 7.068059921264648 3000
val loss = 6.117684364318848
training loss = 7.046422481536865 3100
val loss = 6.098269939422607
training loss = 7.024821758270264 3200
val loss = 6.079096794128418
training loss = 7.003252029418945 3300
val loss = 6.060131549835205
training loss = 6.981701850891113 3400
val loss = 6.041360855102539
training loss = 6.960162162780762 3500
val loss = 6.022815704345703
training loss = 6.938636302947998 3600
val loss = 6.004511833190918
training loss = 6.9171223640441895 3700
val loss = 5.986379623413086
training loss = 6.895627021789551 3800
val loss = 5.968545913696289
training loss = 6.874171733856201 3900
val loss = 5.950938701629639
training loss = 6.852804660797119 4000
val loss = 5.933676719665527
training loss = 6.831575870513916 4100
val loss = 5.916801452636719
training loss = 6.810595512390137 4200
val loss = 5.9004058837890625
training loss = 6.7899909019470215 4300
val loss = 5.884617805480957
training loss = 6.769927978515625 4400
val loss = 5.869482517242432
training loss = 6.752376556396484 4500
val loss = 5.835674285888672
training loss = 6.733029842376709 4600
val loss = 5.840850830078125
training loss = 6.716877460479736 4700
val loss = 5.832021236419678
training loss = 6.703147888183594 4800
val loss = 5.8145012855529785
training loss = 6.6912007331848145 4900
val loss = 5.815495014190674
training loss = 6.699828624725342 5000
val loss = 5.8999152183532715
training loss = 6.674525260925293 5100
val loss = 5.806159496307373
training loss = 6.724306583404541 5200
val loss = 5.7380828857421875
training loss = 6.665347576141357 5300
val loss = 5.804782867431641
training loss = 6.6625871658325195 5400
val loss = 5.803133010864258
training loss = 6.660792827606201 5500
val loss = 5.795752048492432
training loss = 6.659148693084717 5600
val loss = 5.802229404449463
training loss = 6.658777713775635 5700
val loss = 5.818478107452393
training loss = 6.656709671020508 5800
val loss = 5.802346229553223
training loss = 6.656872749328613 5900
val loss = 5.823060035705566
training loss = 6.653840065002441 6000
val loss = 5.801638603210449
training loss = 6.6548662185668945 6100
val loss = 5.7745466232299805
training loss = 6.6484808921813965 6200
val loss = 5.79960823059082
training loss = 6.653756618499756 6300
val loss = 5.862149715423584
training loss = 6.631801128387451 6400
val loss = 5.793069362640381
training loss = 6.600177764892578 6500
val loss = 5.774737358093262
training loss = 6.446291446685791 6600
val loss = 5.686542510986328
training loss = 5.7079339027404785 6700
val loss = 5.143606185913086
training loss = 4.63395357131958 6800
val loss = 4.231660842895508
training loss = 3.4375789165496826 6900
val loss = 3.301514148712158
training loss = 2.4767415523529053 7000
val loss = 2.6467583179473877
training loss = 2.4354500770568848 7100
val loss = 2.532332420349121
training loss = 2.410311698913574 7200
val loss = 2.442030429840088
training loss = 2.394671678543091 7300
val loss = 2.3885934352874756
training loss = 2.3816280364990234 7400
val loss = 2.3290276527404785
training loss = 2.37217116355896 7500
val loss = 2.290395498275757
training loss = 2.365079402923584 7600
val loss = 2.2649331092834473
training loss = 2.3587305545806885 7700
val loss = 2.23564076423645
training loss = 2.353659152984619 7800
val loss = 2.211300849914551
training loss = 2.3492372035980225 7900
val loss = 2.1997392177581787
training loss = 2.3452422618865967 8000
val loss = 2.1857597827911377
training loss = 2.3426175117492676 8100
val loss = 2.1658573150634766
training loss = 2.338329315185547 8200
val loss = 2.163362503051758
training loss = 2.3449182510375977 8300
val loss = 2.1916685104370117
training loss = 2.332228660583496 8400
val loss = 2.1459131240844727
training loss = 2.329310655593872 8500
val loss = 2.1379337310791016
training loss = 2.3268585205078125 8600
val loss = 2.1260554790496826
training loss = 2.323932409286499 8700
val loss = 2.124060869216919
training loss = 2.321180582046509 8800
val loss = 2.1176700592041016
training loss = 2.3188886642456055 8900
val loss = 2.117276430130005
training loss = 2.3157026767730713 9000
val loss = 2.104989767074585
training loss = 2.3375658988952637 9100
val loss = 2.082530975341797
training loss = 2.3098950386047363 9200
val loss = 2.0912368297576904
training loss = 2.3068010807037354 9300
val loss = 2.08577823638916
training loss = 2.311692476272583 9400
val loss = 2.0628678798675537
training loss = 2.3001954555511475 9500
val loss = 2.072171211242676
training loss = 2.296509265899658 9600
val loss = 2.064682722091675
training loss = 2.2927072048187256 9700
val loss = 2.059624671936035
training loss = 2.2885308265686035 9800
val loss = 2.0488502979278564
training loss = 2.3336570262908936 9900
val loss = 2.1515026092529297
training loss = 2.2795724868774414 10000
val loss = 2.0308878421783447
training loss = 2.274732828140259 10100
val loss = 2.0205795764923096
training loss = 2.270137071609497 10200
val loss = 2.0143494606018066
training loss = 2.265308141708374 10300
val loss = 2.002084493637085
training loss = 2.274251699447632 10400
val loss = 2.0365357398986816
training loss = 2.25571608543396 10500
val loss = 1.9814389944076538
training loss = 2.251049757003784 10600
val loss = 1.970548391342163
training loss = 2.2466249465942383 10700
val loss = 1.956084132194519
training loss = 2.2422118186950684 10800
val loss = 1.9490773677825928
training loss = 2.250415563583374 10900
val loss = 1.9794859886169434
training loss = 2.233959197998047 11000
val loss = 1.9271506071090698
training loss = 2.23003888130188 11100
val loss = 1.9173367023468018
training loss = 2.226557493209839 11200
val loss = 1.910788655281067
training loss = 2.2228739261627197 11300
val loss = 1.8968884944915771
training loss = 2.2196004390716553 11400
val loss = 1.8906196355819702
training loss = 2.216229200363159 11500
val loss = 1.8777577877044678
training loss = 2.2130939960479736 11600
val loss = 1.867119312286377
training loss = 2.425086736679077 11700
val loss = 2.194700241088867
training loss = 2.207069158554077 11800
val loss = 1.847874641418457
training loss = 2.2040984630584717 11900
val loss = 1.8372414112091064
training loss = 2.2064459323883057 12000
val loss = 1.849074125289917
training loss = 2.198195457458496 12100
val loss = 1.8164513111114502
training loss = 2.2085697650909424 12200
val loss = 1.846326470375061
training loss = 2.1920011043548584 12300
val loss = 1.79538893699646
training loss = 2.1887032985687256 12400
val loss = 1.783649206161499
training loss = 2.192066192626953 12500
val loss = 1.760995626449585
training loss = 2.1820192337036133 12600
val loss = 1.762556552886963
training loss = 2.178673505783081 12700
val loss = 1.752833366394043
training loss = 2.1759767532348633 12800
val loss = 1.7486011981964111
training loss = 2.172569990158081 12900
val loss = 1.734316110610962
training loss = 2.1698453426361084 13000
val loss = 1.7299054861068726
training loss = 2.166929244995117 13100
val loss = 1.7151539325714111
training loss = 2.164175271987915 13200
val loss = 1.7050793170928955
training loss = 2.163048028945923 13300
val loss = 1.70530104637146
training loss = 2.1588587760925293 13400
val loss = 1.6852333545684814
training loss = 2.159749746322632 13500
val loss = 1.6658393144607544
training loss = 2.1535122394561768 13600
val loss = 1.6655030250549316
training loss = 2.1507928371429443 13700
val loss = 1.6552097797393799
training loss = 2.152669668197632 13800
val loss = 1.6352499723434448
training loss = 2.145296573638916 13900
val loss = 1.635294795036316
training loss = 2.2755935192108154 14000
val loss = 1.838599443435669
training loss = 2.139723777770996 14100
val loss = 1.6149970293045044
training loss = 2.136854648590088 14200
val loss = 1.6050575971603394
training loss = 2.1430609226226807 14300
val loss = 1.623978614807129
training loss = 2.1312708854675293 14400
val loss = 1.5855953693389893
training loss = 2.1283955574035645 14500
val loss = 1.5757787227630615
training loss = 2.1262423992156982 14600
val loss = 1.5713642835617065
training loss = 2.122783660888672 14700
val loss = 1.556884527206421
training loss = 2.119924783706665 14800
val loss = 1.5470969676971436
training loss = 2.1187796592712402 14900
val loss = 1.5475808382034302
training loss = 2.114504337310791 15000
val loss = 1.5292909145355225
training loss = 2.1187379360198975 15100
val loss = 1.5113023519515991
training loss = 2.1092607975006104 15200
val loss = 1.5120798349380493
training loss = 2.1067113876342773 15300
val loss = 1.5041873455047607
training loss = 2.1100704669952393 15400
val loss = 1.4878387451171875
training loss = 2.101860284805298 15500
val loss = 1.4891753196716309
training loss = 2.0994396209716797 15600
val loss = 1.481327772140503
training loss = 2.097348928451538 15700
val loss = 1.4739222526550293
training loss = 2.0951623916625977 15800
val loss = 1.468677282333374
training loss = 2.145653009414673 15900
val loss = 1.475131869316101
training loss = 2.091071844100952 16000
val loss = 1.4560625553131104
training loss = 2.0890707969665527 16100
val loss = 1.45095694065094
training loss = 2.090392827987671 16200
val loss = 1.439051628112793
training loss = 2.0855023860931396 16300
val loss = 1.441145896911621
training loss = 2.083705186843872 16400
val loss = 1.43612802028656
training loss = 2.083597421646118 16500
val loss = 1.4268836975097656
training loss = 2.080597162246704 16600
val loss = 1.4280544519424438
training loss = 2.0790152549743652 16700
val loss = 1.4239892959594727
training loss = 2.077921152114868 16800
val loss = 1.424379587173462
training loss = 2.076169729232788 16900
val loss = 1.4171638488769531
training loss = 2.0747220516204834 17000
val loss = 1.4135518074035645
training loss = 2.073495626449585 17100
val loss = 1.4103580713272095
training loss = 2.072214365005493 17200
val loss = 1.4080307483673096
training loss = 2.070902109146118 17300
val loss = 1.404841661453247
training loss = 2.0699939727783203 17400
val loss = 1.4004942178726196
training loss = 2.068571090698242 17500
val loss = 1.4002351760864258
training loss = 2.0694587230682373 17600
val loss = 1.3923869132995605
training loss = 2.066389322280884 17700
val loss = 1.3959825038909912
training loss = 2.065291166305542 17800
val loss = 1.3948982954025269
training loss = 2.0643258094787598 17900
val loss = 1.3928111791610718
training loss = 2.0632550716400146 18000
val loss = 1.3903403282165527
training loss = 2.0686328411102295 18100
val loss = 1.4086177349090576
training loss = 2.0613903999328613 18200
val loss = 1.3873220682144165
training loss = 2.0603792667388916 18300
val loss = 1.3853187561035156
training loss = 2.0596227645874023 18400
val loss = 1.3838026523590088
training loss = 2.0586469173431396 18500
val loss = 1.382976770401001
training loss = 2.072364568710327 18600
val loss = 1.4154858589172363
training loss = 2.0570101737976074 18700
val loss = 1.3808379173278809
training loss = 2.056257486343384 18800
val loss = 1.3812108039855957
training loss = 2.055452585220337 18900
val loss = 1.3776910305023193
training loss = 2.054549217224121 19000
val loss = 1.377061128616333
training loss = 2.059018611907959 19100
val loss = 1.3928698301315308
training loss = 2.0531914234161377 19200
val loss = 1.3755781650543213
training loss = 2.0523695945739746 19300
val loss = 1.3741391897201538
training loss = 2.053236961364746 19400
val loss = 1.3815382719039917
training loss = 2.05106520652771 19500
val loss = 1.372767448425293
training loss = 2.05027174949646 19600
val loss = 1.3703924417495728
training loss = 2.0497994422912598 19700
val loss = 1.3720589876174927
training loss = 2.0489845275878906 19800
val loss = 1.3696508407592773
training loss = 2.053378105163574 19900
val loss = 1.3854420185089111
training loss = 2.047792911529541 20000
val loss = 1.3684067726135254
training loss = 2.0471603870391846 20100
val loss = 1.3646430969238281
training loss = 2.046576738357544 20200
val loss = 1.365725040435791
training loss = 2.045689582824707 20300
val loss = 1.3645983934402466
training loss = 2.0468358993530273 20400
val loss = 1.3716604709625244
training loss = 2.0443859100341797 20500
val loss = 1.3626433610916138
training loss = 2.046870470046997 20600
val loss = 1.3723664283752441
training loss = 2.04304575920105 20700
val loss = 1.3606929779052734
training loss = 2.052011489868164 20800
val loss = 1.3567380905151367
training loss = 2.041674852371216 20900
val loss = 1.3582767248153687
training loss = 2.040820360183716 21000
val loss = 1.3564088344573975
training loss = 2.040717601776123 21100
val loss = 1.3542309999465942
training loss = 2.0395426750183105 21200
val loss = 1.354687213897705
training loss = 2.0564775466918945 21300
val loss = 1.3894683122634888
training loss = 2.038282632827759 21400
val loss = 1.3528987169265747
training loss = 2.0374667644500732 21500
val loss = 1.3516764640808105
training loss = 2.037083864212036 21600
val loss = 1.3518168926239014
training loss = 2.0362823009490967 21700
val loss = 1.3498735427856445
training loss = 2.0596370697021484 21800
val loss = 1.3559205532073975
training loss = 2.0351579189300537 21900
val loss = 1.3481881618499756
training loss = 2.0344061851501465 22000
val loss = 1.3471708297729492
training loss = 2.0376152992248535 22100
val loss = 1.3571951389312744
training loss = 2.033290147781372 22200
val loss = 1.345617651939392
training loss = 2.2033565044403076 22300
val loss = 1.569777488708496
training loss = 2.032250165939331 22400
val loss = 1.3439161777496338
training loss = 2.031566619873047 22500
val loss = 1.342919945716858
training loss = 2.031261444091797 22600
val loss = 1.34297776222229
training loss = 2.030653953552246 22700
val loss = 1.3413606882095337
training loss = 2.030024528503418 22800
val loss = 1.3405983448028564
training loss = 2.029839515686035 22900
val loss = 1.3406410217285156
training loss = 2.0291755199432373 23000
val loss = 1.338667392730713
training loss = 2.0335633754730225 23100
val loss = 1.3353626728057861
training loss = 2.028369903564453 23200
val loss = 1.3367342948913574
training loss = 2.0278115272521973 23300
val loss = 1.3357325792312622
training loss = 2.027984857559204 23400
val loss = 1.3373676538467407
training loss = 2.027053117752075 23500
val loss = 1.3339190483093262
training loss = 2.034621000289917 23600
val loss = 1.3494977951049805
training loss = 2.026376247406006 23700
val loss = 1.3320647478103638
training loss = 2.0263309478759766 23800
val loss = 1.3332054615020752
training loss = 2.0257551670074463 23900
val loss = 1.3306503295898438
training loss = 2.025256633758545 24000
val loss = 1.329064965248108
training loss = 2.025144577026367 24100
val loss = 1.3276479244232178
training loss = 2.024789571762085 24200
val loss = 1.3272714614868164
training loss = 2.024362325668335 24300
val loss = 1.326402187347412
training loss = 2.0616536140441895 24400
val loss = 1.3474061489105225
training loss = 2.0238444805145264 24500
val loss = 1.3243194818496704
training loss = 2.023916006088257 24600
val loss = 1.3256444931030273
training loss = 2.0233957767486572 24700
val loss = 1.3233568668365479
training loss = 2.0229766368865967 24800
val loss = 1.321768045425415
training loss = 2.0246362686157227 24900
val loss = 1.3256161212921143
training loss = 2.0225937366485596 25000
val loss = 1.3201550245285034
training loss = 2.0222456455230713 25100
val loss = 1.3192198276519775
training loss = 2.022339105606079 25200
val loss = 1.3200132846832275
training loss = 2.0218677520751953 25300
val loss = 1.3178974390029907
training loss = 2.0221521854400635 25400
val loss = 1.3195934295654297
training loss = 2.0215041637420654 25500
val loss = 1.3164398670196533
training loss = 2.204819440841675 25600
val loss = 1.5398435592651367
training loss = 2.021179437637329 25700
val loss = 1.3150357007980347
training loss = 2.0208992958068848 25800
val loss = 1.3143386840820312
training loss = 2.0213592052459717 25900
val loss = 1.3165918588638306
training loss = 2.0213382244110107 26000
val loss = 1.318543791770935
training loss = 2.0214738845825195 26100
val loss = 1.320550799369812
training loss = 2.0366921424865723 26200
val loss = 1.324326515197754
training loss = 2.021667242050171 26300
val loss = 1.3222301006317139
training loss = 2.0217039585113525 26400
val loss = 1.3236783742904663
training loss = 2.0217902660369873 26500
val loss = 1.3229598999023438
training loss = 2.02173113822937 26600
val loss = 1.3251502513885498
training loss = 2.0714197158813477 26700
val loss = 1.3955488204956055
training loss = 2.0216963291168213 26800
val loss = 1.3259855508804321
training loss = 2.021636486053467 26900
val loss = 1.3270546197891235
training loss = 2.0264666080474854 27000
val loss = 1.324916124343872
training loss = 2.0215396881103516 27100
val loss = 1.3271950483322144
training loss = 2.021453857421875 27200
val loss = 1.3284237384796143
training loss = 2.0333480834960938 27300
val loss = 1.3261618614196777
training loss = 2.021312713623047 27400
val loss = 1.3282874822616577
training loss = 2.0212137699127197 27500
val loss = 1.3294317722320557
training loss = 2.122892141342163 27600
val loss = 1.3974441289901733
training loss = 2.021040916442871 27700
val loss = 1.3292748928070068
training loss = 2.020925760269165 27800
val loss = 1.3305217027664185
training loss = 2.0211236476898193 27900
val loss = 1.3282666206359863
training loss = 2.02073335647583 28000
val loss = 1.3306940793991089
training loss = 2.0903687477111816 28100
val loss = 1.426017165184021
training loss = 2.0205416679382324 28200
val loss = 1.3305531740188599
training loss = 2.020418167114258 28300
val loss = 1.3316638469696045
training loss = 2.0215184688568115 28400
val loss = 1.3283026218414307
training loss = 2.0202174186706543 28500
val loss = 1.3316521644592285
training loss = 2.021352529525757 28600
val loss = 1.3300200700759888
training loss = 2.0200533866882324 28700
val loss = 1.3307554721832275
training loss = 2.019883394241333 28800
val loss = 1.3325519561767578
training loss = 2.0199122428894043 28900
val loss = 1.3331924676895142
training loss = 2.019670009613037 29000
val loss = 1.3326281309127808
training loss = 2.034644365310669 29100
val loss = 1.3331565856933594
training loss = 2.019460916519165 29200
val loss = 1.33285391330719
training loss = 2.0193285942077637 29300
val loss = 1.3335580825805664
training loss = 2.019744634628296 29400
val loss = 1.3300938606262207
training loss = 2.019131660461426 29500
val loss = 1.3333593606948853
training loss = 2.019005537033081 29600
val loss = 1.3337857723236084
training loss = 2.0190508365631104 29700
val loss = 1.3339970111846924
training loss = 2.018793821334839 29800
val loss = 1.3339420557022095
training loss = 2.0307252407073975 29900
val loss = 1.3552722930908203
training loss = 2.0185816287994385 30000
val loss = 1.3336824178695679
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 888.5618,  864.3827,  958.0820,  919.5565,  976.3217, 1067.9303,
        1089.8954, 1116.9354, 1207.6606, 1147.3186, 1165.8960, 1205.2086,
        1236.3735, 1282.0377, 1332.9303, 1397.5537, 1409.8370, 1382.0073,
        1575.4712, 1505.3984, 1605.3351, 1587.5872, 1661.4226, 1549.9795,
        1638.4253, 1727.2732, 1627.1932, 1758.8568, 1774.5432, 1662.3523,
        1762.7448, 1821.2163, 1798.7864, 1810.7454, 1687.0355, 1734.9087,
        1747.3356, 1537.9113, 1668.7505, 1643.5165, 1595.7336, 1547.7394,
        1512.8856, 1493.0054, 1377.5066, 1348.3156, 1230.9545, 1245.8569,
        1149.1766, 1146.5339, 1051.0020,  971.4935,  966.0059,  929.9126,
         878.3228,  853.4452,  812.1005,  730.4283,  599.4489,  558.7004,
         520.9738,  473.3508,  427.4526,  379.0807,  365.1953,  334.4883,
         317.3263,  251.4207,  215.5037,  165.1852,  160.8072,  137.7250,
         125.1955,  115.2719,   93.9680,   73.5979,   58.3412,   53.3083,
          25.1332,   43.0539,   12.7057,   38.3112,   31.0933])]
2744.329942078949
2.5734133193261295 3.9301112698584295 66.69159338848505
val isze = 8
idinces = [37 14 46 25  0 69 45 67 81 44 22 82 26 62 35 65 18 71 17 64  2 55 58  7
 23 59 48 19 12 10 72  8 38 75 13 52 43 47 21  3 24 51 49 68  5 63  4 70
 34 53 30 66 77 36 31 76  9 50 32 27 56 60 33 29 28 57 73 79 78 61 74 42
 15  1 11 41  6 20 16 40 80 54 39]
we are doing training validation split
training loss = 37.09221649169922 100
val loss = 45.199684143066406
training loss = 26.120317459106445 200
val loss = 31.938926696777344
training loss = 19.02783966064453 300
val loss = 22.321054458618164
training loss = 14.651046752929688 400
val loss = 15.942795753479004
training loss = 11.912534713745117 500
val loss = 11.740236282348633
training loss = 9.767675399780273 600
val loss = 8.417430877685547
training loss = 8.211128234863281 700
val loss = 5.911469459533691
training loss = 7.407182693481445 800
val loss = 4.641107559204102
training loss = 6.988007068634033 900
val loss = 4.022365570068359
training loss = 6.7617340087890625 1000
val loss = 3.73030424118042
training loss = 6.632513046264648 1100
val loss = 3.5997674465179443
training loss = 6.5501484870910645 1200
val loss = 3.546074867248535
training loss = 6.486505508422852 1300
val loss = 3.5251011848449707
training loss = 6.42343282699585 1400
val loss = 3.5137858390808105
training loss = 6.345530033111572 1500
val loss = 3.499326467514038
training loss = 6.234961986541748 1600
val loss = 3.472029685974121
training loss = 6.066931247711182 1700
val loss = 3.4231462478637695
training loss = 5.796658515930176 1800
val loss = 3.341465711593628
training loss = 5.3083977699279785 1900
val loss = 3.21008038520813
training loss = 4.378476619720459 2000
val loss = 3.022651195526123
training loss = 2.907806158065796 2100
val loss = 3.0394158363342285
training loss = 1.8446462154388428 2200
val loss = 3.5546483993530273
training loss = 1.7068160772323608 2300
val loss = 4.149406909942627
training loss = 1.6905142068862915 2400
val loss = 4.099308967590332
training loss = 1.697137475013733 2500
val loss = 4.4593071937561035
training loss = 1.6748167276382446 2600
val loss = 4.1372551918029785
training loss = 1.6718568801879883 2700
val loss = 4.293992519378662
training loss = 1.663921594619751 2800
val loss = 4.15748929977417
training loss = 1.6618832349777222 2900
val loss = 4.287408828735352
training loss = 1.6559007167816162 3000
val loss = 4.158721446990967
training loss = 1.652559518814087 3100
val loss = 4.1663079261779785
training loss = 1.6497117280960083 3200
val loss = 4.145086288452148
training loss = 1.6469510793685913 3300
val loss = 4.169166564941406
training loss = 1.6446092128753662 3400
val loss = 4.136425971984863
training loss = 1.6421853303909302 3500
val loss = 4.165835857391357
training loss = 1.6578307151794434 3600
val loss = 3.8512496948242188
training loss = 1.6379878520965576 3700
val loss = 4.15936279296875
training loss = 1.6360288858413696 3800
val loss = 4.158567428588867
training loss = 1.686886191368103 3900
val loss = 4.779468059539795
training loss = 1.6321302652359009 4000
val loss = 4.153059005737305
training loss = 1.630138635635376 4100
val loss = 4.146842956542969
training loss = 1.6282179355621338 4200
val loss = 4.170627593994141
training loss = 1.6259610652923584 4300
val loss = 4.137558937072754
training loss = 1.6461139917373657 4400
val loss = 4.519644737243652
training loss = 1.6212016344070435 4500
val loss = 4.129093647003174
training loss = 1.6184080839157104 4600
val loss = 4.119734764099121
training loss = 1.6168612241744995 4700
val loss = 4.206124305725098
training loss = 1.6119029521942139 4800
val loss = 4.102073669433594
training loss = 1.6078068017959595 4900
val loss = 4.087347984313965
training loss = 1.6034754514694214 5000
val loss = 4.103450775146484
training loss = 1.5981645584106445 5100
val loss = 4.0643720626831055
training loss = 1.625646948814392 5200
val loss = 3.6524198055267334
training loss = 1.5854883193969727 5300
val loss = 4.022496223449707
training loss = 1.577573537826538 5400
val loss = 3.983931064605713
training loss = 1.569225788116455 5500
val loss = 3.9868905544281006
training loss = 1.5593618154525757 5600
val loss = 3.9346752166748047
training loss = 1.5495225191116333 5700
val loss = 3.8488988876342773
training loss = 1.5378222465515137 5800
val loss = 3.8500962257385254
training loss = 1.9907300472259521 5900
val loss = 5.858806133270264
training loss = 1.513811469078064 6000
val loss = 3.749466896057129
training loss = 1.5013350248336792 6100
val loss = 3.685831069946289
training loss = 1.4904370307922363 6200
val loss = 3.6620779037475586
training loss = 1.4797881841659546 6300
val loss = 3.575862407684326
training loss = 1.4713894128799438 6400
val loss = 3.4739246368408203
training loss = 1.4632824659347534 6500
val loss = 3.48960542678833
training loss = 1.456713080406189 6600
val loss = 3.465768575668335
training loss = 1.4516273736953735 6700
val loss = 3.4086875915527344
training loss = 1.4469634294509888 6800
val loss = 3.396881341934204
training loss = 1.4450088739395142 6900
val loss = 3.294386148452759
training loss = 1.4399349689483643 7000
val loss = 3.359875202178955
training loss = 1.4554414749145508 7100
val loss = 3.093045949935913
training loss = 1.4346826076507568 7200
val loss = 3.3324482440948486
training loss = 1.432557225227356 7300
val loss = 3.2966222763061523
training loss = 1.4305909872055054 7400
val loss = 3.322650909423828
training loss = 1.4287437200546265 7500
val loss = 3.2979965209960938
training loss = 1.4275496006011963 7600
val loss = 3.3382506370544434
training loss = 1.4255825281143188 7700
val loss = 3.2800135612487793
training loss = 1.424100637435913 7800
val loss = 3.273542642593384
training loss = 1.4227237701416016 7900
val loss = 3.262068271636963
training loss = 1.421337366104126 8000
val loss = 3.2570486068725586
training loss = 1.4249335527420044 8100
val loss = 3.1197266578674316
training loss = 1.4186732769012451 8200
val loss = 3.2442400455474854
training loss = 1.417854905128479 8300
val loss = 3.281672477722168
training loss = 1.4160984754562378 8400
val loss = 3.246812105178833
training loss = 1.4147340059280396 8500
val loss = 3.2237131595611572
training loss = 1.5100867748260498 8600
val loss = 2.716503143310547
training loss = 1.412164330482483 8700
val loss = 3.2167296409606934
training loss = 1.4108926057815552 8800
val loss = 3.2062387466430664
training loss = 1.420173168182373 8900
val loss = 3.015373706817627
training loss = 1.4084402322769165 9000
val loss = 3.195866107940674
training loss = 1.4127259254455566 9100
val loss = 3.3424770832061768
training loss = 1.406082034111023 9200
val loss = 3.188432455062866
training loss = 1.4048997163772583 9300
val loss = 3.181730031967163
training loss = 1.4054555892944336 9400
val loss = 3.1061923503875732
training loss = 1.402640700340271 9500
val loss = 3.1753134727478027
training loss = 1.4014933109283447 9600
val loss = 3.1680543422698975
training loss = 1.4007902145385742 9700
val loss = 3.205264091491699
training loss = 1.399308681488037 9800
val loss = 3.162137031555176
training loss = 1.4182891845703125 9900
val loss = 3.455143928527832
training loss = 1.397202968597412 10000
val loss = 3.1548171043395996
training loss = 1.396111249923706 10100
val loss = 3.1521081924438477
training loss = 1.3965328931808472 10200
val loss = 3.224510908126831
training loss = 1.3941125869750977 10300
val loss = 3.138044834136963
training loss = 1.3930366039276123 10400
val loss = 3.143001079559326
training loss = 1.5903977155685425 10500
val loss = 2.5188114643096924
training loss = 1.391119360923767 10600
val loss = 3.1433072090148926
training loss = 1.3901216983795166 10700
val loss = 3.1349902153015137
training loss = 1.3905733823776245 10800
val loss = 3.220294237136841
training loss = 1.3882802724838257 10900
val loss = 3.1284422874450684
training loss = 1.387349247932434 11000
val loss = 3.1200544834136963
training loss = 1.3865946531295776 11100
val loss = 3.1102452278137207
training loss = 1.385587453842163 11200
val loss = 3.1233103275299072
training loss = 1.3882726430892944 11300
val loss = 3.2414050102233887
training loss = 1.3839223384857178 11400
val loss = 3.1203272342681885
training loss = 1.4377976655960083 11500
val loss = 3.624221086502075
training loss = 1.3823750019073486 11600
val loss = 3.111081600189209
training loss = 1.3815395832061768 11700
val loss = 3.1149330139160156
training loss = 1.406200647354126 11800
val loss = 2.8485004901885986
training loss = 1.380069375038147 11900
val loss = 3.115008592605591
training loss = 1.3793548345565796 12000
val loss = 3.097289800643921
training loss = 1.3786876201629639 12100
val loss = 3.11849045753479
training loss = 1.3779419660568237 12200
val loss = 3.1079680919647217
training loss = 1.37921142578125 12300
val loss = 3.193472385406494
training loss = 1.3766553401947021 12400
val loss = 3.104583501815796
training loss = 1.3760336637496948 12500
val loss = 3.090094804763794
training loss = 1.375493049621582 12600
val loss = 3.0940330028533936
training loss = 1.3748087882995605 12700
val loss = 3.1017231941223145
training loss = 1.4475539922714233 12800
val loss = 2.690577268600464
training loss = 1.3736934661865234 12900
val loss = 3.099463939666748
training loss = 1.3730993270874023 13000
val loss = 3.098065137863159
training loss = 1.3735668659210205 13100
val loss = 3.1579103469848633
training loss = 1.3720805644989014 13200
val loss = 3.097059726715088
training loss = 1.379675269126892 13300
val loss = 2.934861898422241
training loss = 1.3711341619491577 13400
val loss = 3.086564302444458
training loss = 1.370582103729248 13500
val loss = 3.093647003173828
training loss = 1.370294451713562 13600
val loss = 3.0785036087036133
training loss = 1.3697006702423096 13700
val loss = 3.0927109718322754
training loss = 1.369280219078064 13800
val loss = 3.0771117210388184
training loss = 1.3689004182815552 13900
val loss = 3.104461431503296
training loss = 1.3683984279632568 14000
val loss = 3.090010643005371
training loss = 1.3697227239608765 14100
val loss = 3.169912338256836
training loss = 1.3676416873931885 14200
val loss = 3.0887317657470703
training loss = 1.3672171831130981 14300
val loss = 3.0877771377563477
training loss = 1.369384765625 14400
val loss = 3.001910448074341
training loss = 1.3665250539779663 14500
val loss = 3.0871598720550537
training loss = 1.3661326169967651 14600
val loss = 3.0859317779541016
training loss = 1.366831660270691 14700
val loss = 3.031867504119873
training loss = 1.3655002117156982 14800
val loss = 3.0854828357696533
training loss = 1.3651363849639893 14900
val loss = 3.083777904510498
training loss = 1.3649343252182007 15000
val loss = 3.0795509815216064
training loss = 1.364550232887268 15100
val loss = 3.084117889404297
training loss = 1.3642983436584473 15200
val loss = 3.0659077167510986
training loss = 1.3640210628509521 15300
val loss = 3.07747745513916
training loss = 1.3636637926101685 15400
val loss = 3.0825865268707275
training loss = 1.3659014701843262 15500
val loss = 3.007596969604492
training loss = 1.3631454706192017 15600
val loss = 3.0832362174987793
training loss = 1.3628381490707397 15700
val loss = 3.0805139541625977
training loss = 1.3628841638565063 15800
val loss = 3.1100096702575684
training loss = 1.362359881401062 15900
val loss = 3.081049919128418
training loss = 1.3730932474136353 16000
val loss = 3.2870569229125977
training loss = 1.3619086742401123 16100
val loss = 3.076284170150757
training loss = 1.3616206645965576 16200
val loss = 3.0792837142944336
training loss = 1.3616893291473389 16300
val loss = 3.053128719329834
training loss = 1.3611856698989868 16400
val loss = 3.078564405441284
training loss = 1.4362355470657349 16500
val loss = 2.6596622467041016
training loss = 1.3607802391052246 16600
val loss = 3.0761890411376953
training loss = 1.3605268001556396 16700
val loss = 3.07682728767395
training loss = 1.360451340675354 16800
val loss = 3.0685362815856934
training loss = 1.3601555824279785 16900
val loss = 3.077057361602783
training loss = 1.382487177848816 17000
val loss = 2.8226194381713867
training loss = 1.359793782234192 17100
val loss = 3.0764925479888916
training loss = 1.3595635890960693 17200
val loss = 3.0739684104919434
training loss = 1.3597708940505981 17300
val loss = 3.0439672470092773
training loss = 1.3592103719711304 17400
val loss = 3.0748724937438965
training loss = 1.3595772981643677 17500
val loss = 3.03623104095459
training loss = 1.3588682413101196 17600
val loss = 3.074612617492676
training loss = 1.358780860900879 17700
val loss = 3.082127809524536
training loss = 1.358553409576416 17800
val loss = 3.075277328491211
training loss = 1.573864221572876 17900
val loss = 4.174799919128418
training loss = 1.3582711219787598 18000
val loss = 3.081608772277832
training loss = 1.3580609560012817 18100
val loss = 3.073214054107666
training loss = 1.3588424921035767 18200
val loss = 3.0217747688293457
training loss = 1.3577765226364136 18300
val loss = 3.0742006301879883
training loss = 1.3658288717269897 18400
val loss = 3.2622361183166504
training loss = 1.3575055599212646 18500
val loss = 3.0719470977783203
training loss = 1.3573299646377563 18600
val loss = 3.0770206451416016
training loss = 1.3573710918426514 18700
val loss = 3.0940446853637695
training loss = 1.3570797443389893 18800
val loss = 3.0726287364959717
training loss = 1.7138100862503052 18900
val loss = 2.3656973838806152
training loss = 1.3568475246429443 19000
val loss = 3.072446346282959
training loss = 1.356675624847412 19100
val loss = 3.071342945098877
training loss = 1.359390377998352 19200
val loss = 3.174602508544922
training loss = 1.3564566373825073 19300
val loss = 3.0721607208251953
training loss = 1.3562984466552734 19400
val loss = 3.071772575378418
training loss = 1.356484293937683 19500
val loss = 3.0467591285705566
training loss = 1.356092095375061 19600
val loss = 3.0708091259002686
training loss = 1.4460268020629883 19700
val loss = 3.7256152629852295
training loss = 1.3559058904647827 19800
val loss = 3.0772430896759033
training loss = 1.3557391166687012 19900
val loss = 3.0699849128723145
training loss = 1.3557325601577759 20000
val loss = 3.0823721885681152
training loss = 1.3555450439453125 20100
val loss = 3.069493293762207
training loss = 1.357359528541565 20200
val loss = 2.989755153656006
training loss = 1.3553965091705322 20300
val loss = 3.081646203994751
training loss = 1.3552147150039673 20400
val loss = 3.069155216217041
training loss = 1.3556060791015625 20500
val loss = 3.0333147048950195
training loss = 1.3550244569778442 20600
val loss = 3.0684397220611572
training loss = 1.3872137069702148 20700
val loss = 3.4414687156677246
training loss = 1.3548400402069092 20800
val loss = 3.0712690353393555
training loss = 1.3547223806381226 20900
val loss = 3.07651424407959
training loss = 1.3548022508621216 21000
val loss = 3.0895674228668213
training loss = 1.3545554876327515 21100
val loss = 3.068878173828125
training loss = 1.3544299602508545 21200
val loss = 3.070358991622925
training loss = 1.3544248342514038 21300
val loss = 3.0771477222442627
training loss = 1.3542691469192505 21400
val loss = 3.068073272705078
training loss = 1.3636313676834106 21500
val loss = 2.905773162841797
training loss = 1.3541176319122314 21600
val loss = 3.0690112113952637
training loss = 1.3540014028549194 21700
val loss = 3.0736868381500244
training loss = 1.354051113128662 21800
val loss = 3.087040424346924
training loss = 1.3538320064544678 21900
val loss = 3.0671706199645996
training loss = 1.35731041431427 22000
val loss = 3.1828677654266357
training loss = 1.353696584701538 22100
val loss = 3.068593978881836
training loss = 1.3535805940628052 22200
val loss = 3.062955617904663
training loss = 1.3536080121994019 22300
val loss = 3.056370258331299
training loss = 1.3534294366836548 22400
val loss = 3.066688060760498
training loss = 1.3570107221603394 22500
val loss = 3.1839804649353027
training loss = 1.353289008140564 22600
val loss = 3.067378520965576
training loss = 1.3622519969940186 22700
val loss = 3.2523701190948486
training loss = 1.3531911373138428 22800
val loss = 3.066056251525879
training loss = 1.353056788444519 22900
val loss = 3.0663986206054688
training loss = 1.3922672271728516 23000
val loss = 3.4760043621063232
training loss = 1.35294771194458 23100
val loss = 3.0676186084747314
training loss = 1.3528249263763428 23200
val loss = 3.0657308101654053
training loss = 1.358841896057129 23300
val loss = 2.931159257888794
training loss = 1.3526886701583862 23400
val loss = 3.0669288635253906
training loss = 1.37989342212677 23500
val loss = 3.407656192779541
training loss = 1.3525580167770386 23600
val loss = 3.0675923824310303
training loss = 1.3847566843032837 23700
val loss = 3.4335432052612305
training loss = 1.3524328470230103 23800
val loss = 3.0657601356506348
training loss = 1.3523156642913818 23900
val loss = 3.0637407302856445
training loss = 1.352340579032898 24000
val loss = 3.0769262313842773
training loss = 1.3521887063980103 24100
val loss = 3.0652947425842285
training loss = 1.357309103012085 24200
val loss = 2.9419212341308594
training loss = 1.3520612716674805 24300
val loss = 3.0655674934387207
training loss = 1.3617194890975952 24400
val loss = 3.2675390243530273
training loss = 1.3519394397735596 24500
val loss = 3.067674160003662
training loss = 1.4581595659255981 24600
val loss = 3.7860264778137207
training loss = 1.3518238067626953 24700
val loss = 3.0723257064819336
training loss = 1.3531676530838013 24800
val loss = 2.9950032234191895
training loss = 1.3516987562179565 24900
val loss = 3.072904348373413
training loss = 1.3515708446502686 25000
val loss = 3.0618369579315186
training loss = 1.3516740798950195 25100
val loss = 3.048938274383545
training loss = 1.3514578342437744 25200
val loss = 3.0639572143554688
training loss = 1.3682377338409424 25300
val loss = 3.326404571533203
training loss = 1.3513396978378296 25400
val loss = 3.0654711723327637
training loss = 1.3512589931488037 25500
val loss = 3.0537869930267334
training loss = 1.3513094186782837 25600
val loss = 3.0612406730651855
training loss = 1.3511630296707153 25700
val loss = 3.0644302368164062
training loss = 1.3510547876358032 25800
val loss = 3.063096046447754
training loss = 1.35430908203125 25900
val loss = 2.963658571243286
training loss = 1.3509536981582642 26000
val loss = 3.063570737838745
training loss = 1.4535995721817017 26100
val loss = 3.7704901695251465
training loss = 1.3508602380752563 26200
val loss = 3.0580880641937256
training loss = 1.3507338762283325 26300
val loss = 3.061812400817871
training loss = 1.3509725332260132 26400
val loss = 3.0364692211151123
training loss = 1.3506170511245728 26500
val loss = 3.0630619525909424
training loss = 1.354638695716858 26600
val loss = 2.9518725872039795
training loss = 1.3505061864852905 26700
val loss = 3.0623857975006104
training loss = 1.6702895164489746 26800
val loss = 2.3720011711120605
training loss = 1.3504033088684082 26900
val loss = 3.0594687461853027
training loss = 1.3502804040908813 27000
val loss = 3.0632894039154053
training loss = 1.3502862453460693 27100
val loss = 3.0611934661865234
training loss = 1.350160002708435 27200
val loss = 3.0620710849761963
training loss = 1.3522170782089233 27300
val loss = 2.983271360397339
training loss = 1.350037932395935 27400
val loss = 3.0621914863586426
training loss = 1.360463261604309 27500
val loss = 2.885863780975342
training loss = 1.349898338317871 27600
val loss = 3.063868999481201
training loss = 1.356176733970642 27700
val loss = 3.218722343444824
training loss = 1.3497778177261353 27800
val loss = 3.0623440742492676
training loss = 1.4677544832229614 27900
val loss = 3.842827320098877
training loss = 1.3496545553207397 28000
val loss = 3.058734655380249
training loss = 1.3574159145355225 28100
val loss = 3.2343389987945557
training loss = 1.3495391607284546 28200
val loss = 3.053483009338379
training loss = 1.3495128154754639 28300
val loss = 3.081526756286621
training loss = 1.3494305610656738 28400
val loss = 3.050579786300659
training loss = 1.3492552042007446 28500
val loss = 3.0605125427246094
training loss = 1.3493027687072754 28600
val loss = 3.0715746879577637
training loss = 1.3491334915161133 28700
val loss = 3.0607547760009766
training loss = 1.6947076320648193 28800
val loss = 2.3541295528411865
training loss = 1.349015712738037 28900
val loss = 3.0618269443511963
training loss = 1.348878026008606 29000
val loss = 3.059589385986328
training loss = 1.3490136861801147 29100
val loss = 3.082064628601074
training loss = 1.348746657371521 29200
val loss = 3.0599758625030518
training loss = 1.4233758449554443 29300
val loss = 2.6439428329467773
training loss = 1.3485949039459229 29400
val loss = 3.057979106903076
training loss = 1.3637757301330566 29500
val loss = 2.8459296226501465
training loss = 1.3484476804733276 29600
val loss = 3.055281162261963
training loss = 1.3482879400253296 29700
val loss = 3.0578079223632812
training loss = 1.3486528396606445 29800
val loss = 3.096935749053955
training loss = 1.3481286764144897 29900
val loss = 3.058917999267578
training loss = 1.3523448705673218 30000
val loss = 3.1880855560302734
reduced chi^2 level 2 = 1.356462836265564
Constrained alpha: 1.7180285453796387
Constrained beta: 3.6738057136535645
Constrained gamma: 14.01877498626709
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 807.3535,  838.9833,  905.5534,  987.8472, 1006.5527, 1098.3600,
        1098.4856, 1143.0873, 1101.6550, 1107.8954, 1266.8262, 1224.3784,
        1277.4572, 1318.6300, 1292.0682, 1443.6840, 1477.1683, 1488.0662,
        1584.9729, 1529.1406, 1628.0055, 1518.7113, 1609.0226, 1610.7750,
        1628.0963, 1695.7244, 1642.6462, 1716.9377, 1662.5039, 1673.7207,
        1715.9348, 1721.9398, 1778.9904, 1742.4785, 1724.0319, 1756.3022,
        1613.7382, 1590.5493, 1693.7609, 1632.8329, 1612.9233, 1567.6343,
        1453.2795, 1482.8467, 1355.9622, 1294.7855, 1235.4608, 1262.8577,
        1104.5616, 1147.8119, 1082.4452, 1034.2299,  994.2562,  866.6012,
         853.2658,  868.7729,  785.3552,  663.3501,  580.2441,  550.7053,
         551.5686,  485.9649,  470.3358,  370.9366,  369.1520,  364.5827,
         277.1118,  260.4651,  210.5499,  181.3968,  156.7980,  153.4695,
         147.9436,  128.4914,   80.9744,   79.8692,   56.4869,   36.3039,
          33.6559,   45.1386,    9.8885,   41.2801,   35.9169])]
2629.4841264824922
1.2327180117112502 19.13726115402398 65.26221050099345
val isze = 8
idinces = [36 77 41 16 75 60  7 70 39 28 65 47 74 32 33 57 53 49 13  0 37 82 52 81
 22 55 64 26 43 80 38 61 19 76 23 21 68 30 50  8  4 17 62 56 18 45 59 78
 34 46 35 42 73 67 72  1  6 14 71 63 66 44 54  9  5 51 25  2 12 31 79  3
 69 10 20 48 27 58 29 40 24 15 11]
we are doing training validation split
training loss = 233.24815368652344 100
val loss = 189.09034729003906
training loss = 33.74828338623047 200
val loss = 38.718589782714844
training loss = 14.470903396606445 300
val loss = 20.608673095703125
training loss = 13.90859603881836 400
val loss = 19.605209350585938
training loss = 13.281235694885254 500
val loss = 18.56435203552246
training loss = 12.60102367401123 600
val loss = 17.406641006469727
training loss = 11.887223243713379 700
val loss = 16.15362548828125
training loss = 11.162078857421875 800
val loss = 14.831809997558594
training loss = 10.450724601745605 900
val loss = 13.474544525146484
training loss = 9.77981185913086 1000
val loss = 12.12186050415039
training loss = 9.174726486206055 1100
val loss = 10.818740844726562
training loss = 8.655595779418945 1200
val loss = 9.610139846801758
training loss = 8.233254432678223 1300
val loss = 8.534527778625488
training loss = 7.906938552856445 1400
val loss = 7.616872787475586
training loss = 7.665071964263916 1500
val loss = 6.86478328704834
training loss = 7.489329814910889 1600
val loss = 6.269139289855957
training loss = 7.360057353973389 1700
val loss = 5.808999538421631
training loss = 7.260603904724121 1800
val loss = 5.458342552185059
training loss = 7.179219722747803 1900
val loss = 5.191399574279785
training loss = 7.108665466308594 2000
val loss = 4.985988616943359
training loss = 7.044875621795654 2100
val loss = 4.824361324310303
training loss = 6.985506534576416 2200
val loss = 4.693032264709473
training loss = 6.928977012634277 2300
val loss = 4.581820487976074
training loss = 6.873826503753662 2400
val loss = 4.482796669006348
training loss = 6.81820821762085 2500
val loss = 4.389309406280518
training loss = 6.759180545806885 2600
val loss = 4.294149875640869
training loss = 6.691129684448242 2700
val loss = 4.1865458488464355
training loss = 6.601468086242676 2800
val loss = 4.045316219329834
training loss = 6.4566426277160645 2900
val loss = 3.8163857460021973
training loss = 6.156773567199707 3000
val loss = 3.3485560417175293
training loss = 5.514293193817139 3100
val loss = 2.4764511585235596
training loss = 4.514560699462891 3200
val loss = 1.5915696620941162
training loss = 3.314650535583496 3300
val loss = 1.0512739419937134
training loss = 2.5082004070281982 3400
val loss = 1.3441516160964966
training loss = 2.3071138858795166 3500
val loss = 1.7906461954116821
training loss = 2.2396750450134277 3600
val loss = 1.937147855758667
training loss = 2.1910080909729004 3700
val loss = 1.9883699417114258
training loss = 2.15291166305542 3800
val loss = 2.020204544067383
training loss = 2.122756004333496 3900
val loss = 2.0452704429626465
training loss = 2.098768472671509 4000
val loss = 2.0661520957946777
training loss = 2.0796685218811035 4100
val loss = 2.0839009284973145
training loss = 2.0644848346710205 4200
val loss = 2.099165916442871
training loss = 2.0524635314941406 4300
val loss = 2.1123123168945312
training loss = 2.042987108230591 4400
val loss = 2.123558282852173
training loss = 2.0355639457702637 4500
val loss = 2.1330361366271973
training loss = 2.0297765731811523 4600
val loss = 2.1408910751342773
training loss = 2.025294542312622 4700
val loss = 2.1472811698913574
training loss = 2.021848678588867 4800
val loss = 2.1523067951202393
training loss = 2.019223213195801 4900
val loss = 2.156062126159668
training loss = 2.0170507431030273 5000
val loss = 2.1652088165283203
training loss = 2.014955759048462 5100
val loss = 2.162754535675049
training loss = 2.013489007949829 5200
val loss = 2.173764944076538
training loss = 2.01143741607666 5300
val loss = 2.167877435684204
training loss = 2.0098743438720703 5400
val loss = 2.16510009765625
training loss = 2.008283853530884 5500
val loss = 2.1718010902404785
training loss = 2.0784575939178467 5600
val loss = 2.0962536334991455
training loss = 2.0054261684417725 5700
val loss = 2.1758038997650146
training loss = 2.0041913986206055 5800
val loss = 2.175814628601074
training loss = 2.0030083656311035 5900
val loss = 2.1828773021698
training loss = 2.0016462802886963 6000
val loss = 2.1784324645996094
training loss = 2.061805009841919 6100
val loss = 2.346522331237793
training loss = 1.999382734298706 6200
val loss = 2.1803512573242188
training loss = 1.9984188079833984 6300
val loss = 2.180230140686035
training loss = 1.9978044033050537 6400
val loss = 2.189500331878662
training loss = 1.9964911937713623 6500
val loss = 2.1812667846679688
training loss = 2.0052764415740967 6600
val loss = 2.2340235710144043
training loss = 1.9947502613067627 6700
val loss = 2.1813392639160156
training loss = 1.9942033290863037 6800
val loss = 2.1749377250671387
training loss = 1.9932034015655518 6900
val loss = 2.18166446685791
training loss = 1.9925391674041748 7000
val loss = 2.18174409866333
training loss = 1.9921848773956299 7100
val loss = 2.1705777645111084
training loss = 1.9911985397338867 7200
val loss = 2.1814215183258057
training loss = 2.0115163326263428 7300
val loss = 2.121760845184326
training loss = 1.9899576902389526 7400
val loss = 2.181195020675659
training loss = 1.9894582033157349 7500
val loss = 2.180624008178711
training loss = 1.9889131784439087 7600
val loss = 2.175166130065918
training loss = 1.9884096384048462 7700
val loss = 2.180109739303589
training loss = 2.0135786533355713 7800
val loss = 2.282261371612549
training loss = 1.9874508380889893 7900
val loss = 2.1783525943756104
training loss = 1.987074375152588 8000
val loss = 2.1789894104003906
training loss = 1.9879733324050903 8100
val loss = 2.1963837146759033
training loss = 1.9862555265426636 8200
val loss = 2.1780776977539062
training loss = 2.189391613006592 8300
val loss = 2.1155948638916016
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 872.0165,  880.1132,  983.3633,  930.6981, 1046.4935, 1107.3560,
        1103.6014, 1159.2531, 1125.1067, 1165.2590, 1266.8348, 1216.9668,
        1233.0902, 1251.7670, 1360.5184, 1359.6281, 1330.4536, 1447.2753,
        1492.5704, 1413.4851, 1637.6497, 1634.2081, 1629.0103, 1622.9056,
        1700.3427, 1660.7585, 1590.5719, 1731.6099, 1597.2208, 1739.0948,
        1644.6522, 1755.9939, 1725.9167, 1825.7206, 1672.9873, 1763.3805,
        1668.8292, 1615.3472, 1606.3191, 1598.1400, 1662.1228, 1558.1807,
        1522.3964, 1504.1283, 1371.2141, 1298.7498, 1398.2178, 1278.4738,
        1190.0367, 1234.4084, 1102.7515,  977.6637,  955.3648,  966.6136,
         828.2018,  831.7656,  837.0508,  686.8547,  629.9424,  566.7699,
         531.9334,  486.8067,  480.3980,  410.2147,  335.2097,  367.6320,
         273.7675,  256.7349,  196.2530,  186.4687,  165.1519,  155.8986,
         145.9128,  116.8210,   94.5980,   78.0025,   54.6031,   41.3107,
          41.9753,   41.4352,   14.1498,   45.7211,   44.6107])]
2811.2055427121663
3.0394347164897666 13.602814415711048 81.58337512461112
val isze = 8
idinces = [28 53 63 49 13 15 68  3 65 42 26 23 33  5 40 37 62 51 39 79 29 27 41  9
 78 54 52 76 46 71  6 17 70 48  1 47 77 14 67 58  2 74 59 32 66 22  8  7
 30 55 60 57 10 25 69 12 34 21  4 11 75 24 36 43 82 80 18 20 61 56 38 50
 45 72 35 73 19 16 64  0 44 81 31]
we are doing training validation split
training loss = 236.5912322998047 100
val loss = 271.8888244628906
training loss = 7.672952651977539 200
val loss = 2.301137924194336
training loss = 7.597002029418945 300
val loss = 2.5108134746551514
training loss = 7.5550923347473145 400
val loss = 2.571704864501953
training loss = 7.510319232940674 500
val loss = 2.6353931427001953
training loss = 7.463802337646484 600
val loss = 2.699848175048828
training loss = 7.416269779205322 700
val loss = 2.762768507003784
training loss = 7.368175506591797 800
val loss = 2.8221821784973145
training loss = 7.319791793823242 900
val loss = 2.876777172088623
training loss = 7.271297454833984 1000
val loss = 2.9257051944732666
training loss = 7.222807884216309 1100
val loss = 2.968594551086426
training loss = 7.174410343170166 1200
val loss = 3.005634307861328
training loss = 7.126185894012451 1300
val loss = 3.037116765975952
training loss = 7.078191757202148 1400
val loss = 3.0637693405151367
training loss = 7.030481338500977 1500
val loss = 3.0860514640808105
training loss = 6.983081817626953 1600
val loss = 3.1048712730407715
training loss = 6.9360151290893555 1700
val loss = 3.120694637298584
training loss = 6.8892741203308105 1800
val loss = 3.134131908416748
training loss = 6.842851638793945 1900
val loss = 3.145575523376465
training loss = 6.796716690063477 2000
val loss = 3.155557632446289
training loss = 6.7508416175842285 2100
val loss = 3.1642119884490967
training loss = 6.705204486846924 2200
val loss = 3.171764850616455
training loss = 6.659785270690918 2300
val loss = 3.178441047668457
training loss = 6.614578723907471 2400
val loss = 3.1842432022094727
training loss = 6.569599151611328 2500
val loss = 3.189397096633911
training loss = 6.524848461151123 2600
val loss = 3.1939940452575684
training loss = 6.4802680015563965 2700
val loss = 3.198204517364502
training loss = 6.4355878829956055 2800
val loss = 3.2023143768310547
training loss = 6.38990592956543 2900
val loss = 3.2067599296569824
training loss = 6.340449810028076 3000
val loss = 3.2127199172973633
training loss = 6.277966499328613 3100
val loss = 3.2234015464782715
training loss = 6.164079666137695 3200
val loss = 3.2552711963653564
training loss = 5.835287094116211 3300
val loss = 3.4091687202453613
training loss = 5.090362071990967 3400
val loss = 3.492807626724243
training loss = 3.2864649295806885 3500
val loss = 3.2610626220703125
training loss = 1.9760096073150635 3600
val loss = 2.8430123329162598
training loss = 1.9132992029190063 3700
val loss = 2.7935287952423096
training loss = 1.8915027379989624 3800
val loss = 2.8517801761627197
training loss = 1.863956332206726 3900
val loss = 2.9077672958374023
training loss = 1.832981824874878 4000
val loss = 2.9512853622436523
training loss = 1.799224853515625 4100
val loss = 2.9595155715942383
training loss = 1.7684426307678223 4200
val loss = 2.957117795944214
training loss = 1.7424862384796143 4300
val loss = 2.935746431350708
training loss = 1.7250452041625977 4400
val loss = 2.9160780906677246
training loss = 1.712196946144104 4500
val loss = 2.9185166358947754
training loss = 1.7039660215377808 4600
val loss = 2.9299075603485107
training loss = 1.6972806453704834 4700
val loss = 2.934114933013916
training loss = 1.7061206102371216 4800
val loss = 3.0914947986602783
training loss = 1.688383936882019 4900
val loss = 2.969785690307617
training loss = 1.6849943399429321 5000
val loss = 2.9900426864624023
training loss = 1.683003544807434 5100
val loss = 2.9908878803253174
training loss = 1.6805037260055542 5200
val loss = 3.0251126289367676
reduced chi^2 level 2 = 1.6804850101470947
Constrained alpha: 1.9449775218963623
Constrained beta: 3.2688887119293213
Constrained gamma: 38.758155822753906
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 878.2192,  849.0482,  981.8552,  941.7798,  953.1102, 1096.7179,
        1094.5154, 1127.6018, 1086.6943, 1140.5555, 1236.2981, 1171.9414,
        1240.8708, 1283.6721, 1328.7355, 1400.2345, 1399.4229, 1496.7078,
        1540.5880, 1545.6238, 1549.3907, 1525.7159, 1611.6514, 1734.0336,
        1653.1365, 1722.2874, 1566.4681, 1689.8750, 1715.0015, 1718.6990,
        1644.0084, 1744.4131, 1719.8850, 1812.2377, 1709.3055, 1773.2426,
        1612.0336, 1647.1188, 1677.7704, 1610.0457, 1686.3375, 1569.5699,
        1546.7550, 1548.8079, 1367.0726, 1274.9708, 1246.6255, 1218.4762,
        1166.3499, 1203.2610, 1084.9229,  952.6522, 1026.7363,  932.2095,
         884.4095,  852.1550,  809.9949,  686.3741,  655.0051,  548.5831,
         572.3896,  477.3908,  444.2760,  378.3624,  347.1202,  345.6044,
         281.9650,  261.0125,  186.2530,  172.1290,  138.7797,  139.6163,
         135.4118,  114.5619,  118.9627,   68.5930,   54.9747,   49.3071,
          32.0386,   44.1336,   22.1750,   42.7808,   31.9212])]
2829.2888909525655
0.553631378194539 3.0220115757295707 80.78285393720395
val isze = 8
idinces = [47 48 54 20 29  7  1 74 78 64 44 75 25 34 68 52 41 82 30 22  9 50 24 40
 81 58 51 73 53 28 45  8 27 71  4 38 56 13 36 77 62 17 15 65 46 60 70 66
  0 37  3 63 14 79  6 10 59 16 21 55 12 49 31 35 39 19 61 26 32  2 43 23
 33 57 76 72 67 42 18 69 80 11  5]
we are doing training validation split
training loss = 26.70747947692871 100
val loss = 29.6423397064209
training loss = 20.306293487548828 200
val loss = 18.127361297607422
training loss = 15.68021011352539 300
val loss = 14.148521423339844
training loss = 11.79836368560791 400
val loss = 11.274343490600586
training loss = 8.810086250305176 500
val loss = 9.592552185058594
training loss = 6.9655046463012695 600
val loss = 8.984599113464355
training loss = 5.962088584899902 700
val loss = 8.922000885009766
training loss = 5.4007368087768555 800
val loss = 8.932910919189453
training loss = 4.9849162101745605 900
val loss = 8.702333450317383
training loss = 4.569995880126953 1000
val loss = 8.155338287353516
training loss = 4.141701698303223 1100
val loss = 7.485608100891113
training loss = 3.7090911865234375 1200
val loss = 6.79840087890625
training loss = 3.2838668823242188 1300
val loss = 6.079948425292969
training loss = 2.8909802436828613 1400
val loss = 5.358789443969727
training loss = 2.5648205280303955 1500
val loss = 4.694915294647217
training loss = 2.3313779830932617 1600
val loss = 4.149734973907471
training loss = 2.1900370121002197 1700
val loss = 3.749429225921631
training loss = 2.1170265674591064 1800
val loss = 3.493401050567627
training loss = 2.0832133293151855 1900
val loss = 3.3352675437927246
training loss = 2.068164825439453 2000
val loss = 3.2576980590820312
training loss = 2.0610616207122803 2100
val loss = 3.2224297523498535
training loss = 2.057170867919922 2200
val loss = 3.2010855674743652
training loss = 2.054673910140991 2300
val loss = 3.198221206665039
training loss = 2.052861213684082 2400
val loss = 3.2101480960845947
training loss = 2.0514063835144043 2500
val loss = 3.21781587600708
training loss = 2.050194263458252 2600
val loss = 3.2218496799468994
training loss = 2.049121618270874 2700
val loss = 3.235995292663574
training loss = 2.0481319427490234 2800
val loss = 3.2422051429748535
training loss = 2.047136068344116 2900
val loss = 3.2543885707855225
training loss = 2.0463802814483643 3000
val loss = 3.284712553024292
training loss = 2.0448157787323 3100
val loss = 3.270522117614746
training loss = 2.044081687927246 3200
val loss = 3.312034845352173
training loss = 2.0411195755004883 3300
val loss = 3.2800068855285645
training loss = 2.0383307933807373 3400
val loss = 3.293525218963623
training loss = 2.034147262573242 3500
val loss = 3.280771493911743
training loss = 2.0320663452148438 3600
val loss = 3.211906909942627
training loss = 2.02048397064209 3700
val loss = 3.2718520164489746
training loss = 2.2837271690368652 3800
val loss = 2.974147319793701
training loss = 1.9966394901275635 3900
val loss = 3.251002311706543
training loss = 1.9869544506072998 4000
val loss = 3.328827381134033
training loss = 1.967484951019287 4100
val loss = 3.2258481979370117
training loss = 1.9542934894561768 4200
val loss = 3.1873340606689453
training loss = 1.9427987337112427 4300
val loss = 3.1729493141174316
training loss = 1.9319132566452026 4400
val loss = 3.158114433288574
training loss = 1.9224056005477905 4500
val loss = 3.123485565185547
training loss = 1.9125103950500488 4600
val loss = 3.112987995147705
training loss = 1.9271953105926514 4700
val loss = 2.963059902191162
training loss = 1.8936352729797363 4800
val loss = 3.0689334869384766
training loss = 1.8849060535430908 4900
val loss = 3.0216166973114014
training loss = 1.8755558729171753 5000
val loss = 3.0267274379730225
training loss = 1.866713285446167 5100
val loss = 3.008035182952881
training loss = 1.8614972829818726 5200
val loss = 3.0373661518096924
training loss = 1.8507816791534424 5300
val loss = 2.972163677215576
training loss = 1.8441972732543945 5400
val loss = 2.975059747695923
training loss = 1.8369669914245605 5500
val loss = 2.9409213066101074
training loss = 1.8305847644805908 5600
val loss = 2.927488327026367
training loss = 1.8263682126998901 5700
val loss = 2.942033290863037
training loss = 1.8201154470443726 5800
val loss = 2.903768539428711
training loss = 2.233076333999634 5900
val loss = 2.876312732696533
training loss = 1.811608910560608 6000
val loss = 2.8838422298431396
training loss = 1.8078937530517578 6100
val loss = 2.876810073852539
training loss = 1.8051034212112427 6200
val loss = 2.8623969554901123
training loss = 1.8021248579025269 6300
val loss = 2.8641598224639893
training loss = 1.8038729429244995 6400
val loss = 2.818033456802368
training loss = 1.7977224588394165 6500
val loss = 2.8541646003723145
training loss = 1.795828104019165 6600
val loss = 2.852264404296875
training loss = 1.7944564819335938 6700
val loss = 2.8504440784454346
training loss = 1.7929496765136719 6800
val loss = 2.8422927856445312
training loss = 1.8313335180282593 6900
val loss = 2.7636666297912598
training loss = 1.7907581329345703 7000
val loss = 2.8360755443573
training loss = 1.7897824048995972 7100
val loss = 2.8332319259643555
training loss = 1.7895123958587646 7200
val loss = 2.819150447845459
training loss = 1.788273811340332 7300
val loss = 2.828779697418213
training loss = 1.78925621509552 7400
val loss = 2.8481078147888184
training loss = 1.7870914936065674 7500
val loss = 2.824401617050171
training loss = 1.7866106033325195 7600
val loss = 2.827662944793701
training loss = 1.78616464138031 7700
val loss = 2.823420524597168
training loss = 1.7856674194335938 7800
val loss = 2.8188695907592773
training loss = 1.819126844406128 7900
val loss = 2.949913263320923
training loss = 1.7849382162094116 8000
val loss = 2.815406084060669
training loss = 1.7845735549926758 8100
val loss = 2.814030885696411
training loss = 1.7847939729690552 8200
val loss = 2.8254151344299316
training loss = 1.7839878797531128 8300
val loss = 2.8107082843780518
training loss = 1.783689260482788 8400
val loss = 2.8092565536499023
training loss = 1.784906029701233 8500
val loss = 2.7893919944763184
training loss = 1.7831964492797852 8600
val loss = 2.8059959411621094
training loss = 1.782942295074463 8700
val loss = 2.804622173309326
training loss = 1.7827922105789185 8800
val loss = 2.8004109859466553
training loss = 1.7825112342834473 8900
val loss = 2.801931381225586
training loss = 1.8237782716751099 9000
val loss = 2.7417304515838623
training loss = 1.782128930091858 9100
val loss = 2.797693967819214
training loss = 1.7819020748138428 9200
val loss = 2.797975540161133
training loss = 1.7823727130889893 9300
val loss = 2.8109922409057617
training loss = 1.7815430164337158 9400
val loss = 2.795639753341675
training loss = 1.851930022239685 9500
val loss = 3.007800340652466
training loss = 1.7812081575393677 9600
val loss = 2.7932608127593994
training loss = 1.7810319662094116 9700
val loss = 2.790950298309326
training loss = 1.7810451984405518 9800
val loss = 2.784470796585083
training loss = 1.7807165384292603 9900
val loss = 2.7895748615264893
training loss = 1.796298623085022 10000
val loss = 2.738466501235962
training loss = 1.7804276943206787 10100
val loss = 2.787311315536499
training loss = 1.7802734375 10200
val loss = 2.7857961654663086
training loss = 1.7801693677902222 10300
val loss = 2.784341335296631
training loss = 1.7800049781799316 10400
val loss = 2.7839179039001465
training loss = 1.7830119132995605 10500
val loss = 2.7608561515808105
training loss = 1.779762625694275 10600
val loss = 2.783008337020874
training loss = 1.779613733291626 10700
val loss = 2.7810781002044678
training loss = 1.781353235244751 10800
val loss = 2.7615630626678467
training loss = 1.779379963874817 10900
val loss = 2.778885841369629
training loss = 1.7815037965774536 11000
val loss = 2.7572526931762695
training loss = 1.779198408126831 11100
val loss = 2.773775577545166
training loss = 1.7790305614471436 11200
val loss = 2.7762041091918945
training loss = 1.7862006425857544 11300
val loss = 2.823930025100708
training loss = 1.7788223028182983 11400
val loss = 2.7736995220184326
training loss = 1.778700351715088 11500
val loss = 2.773313522338867
training loss = 1.7810754776000977 11600
val loss = 2.795494794845581
training loss = 1.7785005569458008 11700
val loss = 2.7718191146850586
training loss = 1.8321592807769775 11800
val loss = 2.713685989379883
training loss = 1.778303623199463 11900
val loss = 2.7701244354248047
training loss = 1.7781982421875 12000
val loss = 2.7703282833099365
training loss = 1.7781158685684204 12100
val loss = 2.767133951187134
training loss = 1.7779922485351562 12200
val loss = 2.7669103145599365
training loss = 1.8085930347442627 12300
val loss = 2.883937358856201
training loss = 1.777801752090454 12400
val loss = 2.7652997970581055
training loss = 1.7776917219161987 12500
val loss = 2.7638235092163086
training loss = 1.77801513671875 12600
val loss = 2.77293062210083
training loss = 1.7774937152862549 12700
val loss = 2.761981964111328
training loss = 1.9462236166000366 12800
val loss = 2.7470009326934814
training loss = 1.7773079872131348 12900
val loss = 2.7587692737579346
training loss = 1.7771836519241333 13000
val loss = 2.7589211463928223
training loss = 1.783400058746338 13100
val loss = 2.725925922393799
training loss = 1.7769858837127686 13200
val loss = 2.756274700164795
training loss = 1.7768646478652954 13300
val loss = 2.756265878677368
training loss = 1.777206301689148 13400
val loss = 2.744809627532959
training loss = 1.776649832725525 13500
val loss = 2.753539562225342
training loss = 1.8244107961654663 13600
val loss = 2.9081871509552
training loss = 1.7764298915863037 13700
val loss = 2.751403331756592
training loss = 1.7763005495071411 13800
val loss = 2.7509360313415527
training loss = 1.776442050933838 13900
val loss = 2.7568817138671875
training loss = 1.7760694026947021 14000
val loss = 2.748262882232666
training loss = 1.9420082569122314 14100
val loss = 3.1285643577575684
training loss = 1.7758417129516602 14200
val loss = 2.7453060150146484
training loss = 1.7756966352462769 14300
val loss = 2.7453250885009766
training loss = 1.7771743535995483 14400
val loss = 2.766003370285034
training loss = 1.7754579782485962 14500
val loss = 2.743589162826538
training loss = 1.8121734857559204 14600
val loss = 2.8796699047088623
training loss = 1.77522873878479 14700
val loss = 2.7414355278015137
training loss = 1.7750834226608276 14800
val loss = 2.7415294647216797
training loss = 1.7774107456207275 14900
val loss = 2.7198829650878906
training loss = 1.7748620510101318 15000
val loss = 2.7404582500457764
training loss = 1.7758649587631226 15100
val loss = 2.724609136581421
training loss = 1.7746570110321045 15200
val loss = 2.7388834953308105
training loss = 1.7745243310928345 15300
val loss = 2.739727735519409
training loss = 1.7750208377838135 15400
val loss = 2.75178861618042
training loss = 1.7743494510650635 15500
val loss = 2.739623546600342
training loss = 1.7742387056350708 15600
val loss = 2.740433692932129
training loss = 1.7742204666137695 15700
val loss = 2.7380590438842773
training loss = 1.774092674255371 15800
val loss = 2.740506649017334
training loss = 1.8909283876419067 15900
val loss = 3.040102958679199
training loss = 1.7739839553833008 16000
val loss = 2.7419214248657227
training loss = 1.7738949060440063 16100
val loss = 2.74180006980896
training loss = 1.778968095779419 16200
val loss = 2.783355236053467
training loss = 1.773807406425476 16300
val loss = 2.7427754402160645
training loss = 1.7772310972213745 16400
val loss = 2.7181148529052734
training loss = 1.7737624645233154 16500
val loss = 2.7417068481445312
training loss = 1.7736809253692627 16600
val loss = 2.744859457015991
training loss = 1.7744114398956299 16700
val loss = 2.7596426010131836
training loss = 1.773634672164917 16800
val loss = 2.7460174560546875
training loss = 1.859094262123108 16900
val loss = 2.692244529724121
training loss = 1.773604393005371 17000
val loss = 2.7474422454833984
training loss = 1.7735589742660522 17100
val loss = 2.747861862182617
training loss = 1.773693323135376 17200
val loss = 2.7422730922698975
training loss = 1.773540735244751 17300
val loss = 2.74861478805542
training loss = 1.773760437965393 17400
val loss = 2.7577712535858154
training loss = 1.7736003398895264 17500
val loss = 2.7535653114318848
training loss = 1.7734944820404053 17600
val loss = 2.7501423358917236
training loss = 1.792643666267395 17700
val loss = 2.8359599113464355
training loss = 1.7734880447387695 17800
val loss = 2.7504072189331055
training loss = 1.7734568119049072 17900
val loss = 2.7512166500091553
training loss = 1.7735388278961182 18000
val loss = 2.75542950630188
training loss = 1.7734454870224 18100
val loss = 2.7516539096832275
training loss = 1.8390496969223022 18200
val loss = 2.698972702026367
training loss = 1.7734373807907104 18300
val loss = 2.7515320777893066
training loss = 1.7734098434448242 18400
val loss = 2.7537245750427246
training loss = 1.7736037969589233 18500
val loss = 2.75917387008667
training loss = 1.7733923196792603 18600
val loss = 2.7529406547546387
training loss = 1.773435115814209 18700
val loss = 2.750629186630249
training loss = 1.7733826637268066 18800
val loss = 2.753274917602539
training loss = 1.7733525037765503 18900
val loss = 2.7532880306243896
training loss = 1.7737606763839722 19000
val loss = 2.762726068496704
training loss = 1.7733443975448608 19100
val loss = 2.753138780593872
training loss = 1.7733176946640015 19200
val loss = 2.753195285797119
training loss = 1.7733997106552124 19300
val loss = 2.749281167984009
training loss = 1.7733038663864136 19400
val loss = 2.7533512115478516
training loss = 1.8694560527801514 19500
val loss = 2.70790696144104
training loss = 1.7732943296432495 19600
val loss = 2.753760576248169
training loss = 1.7732617855072021 19700
val loss = 2.753615140914917
training loss = 1.775330901145935 19800
val loss = 2.7326855659484863
training loss = 1.773250937461853 19900
val loss = 2.7531089782714844
training loss = 1.7732290029525757 20000
val loss = 2.7524068355560303
training loss = 1.7733993530273438 20100
val loss = 2.759032964706421
training loss = 1.773207664489746 20200
val loss = 2.75325345993042
training loss = 1.788713812828064 20300
val loss = 2.831979751586914
training loss = 1.773201584815979 20400
val loss = 2.75203537940979
training loss = 1.7731671333312988 20500
val loss = 2.753042697906494
training loss = 1.7745027542114258 20600
val loss = 2.771416664123535
training loss = 1.7731537818908691 20700
val loss = 2.752634048461914
training loss = 1.9183883666992188 20800
val loss = 2.7206931114196777
training loss = 1.7731599807739258 20900
val loss = 2.750572681427002
reduced chi^2 level 2 = 1.7731506824493408
Constrained alpha: 1.8380273580551147
Constrained beta: 2.944748640060425
Constrained gamma: 14.30959701538086
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 873.0124,  862.5905,  989.8484,  944.0856, 1024.9297, 1007.3146,
        1063.0895, 1171.7947, 1165.7365, 1124.3395, 1289.4137, 1118.0021,
        1115.7302, 1245.1134, 1282.2090, 1430.0104, 1403.5415, 1445.3948,
        1552.4021, 1481.2727, 1599.9078, 1605.3950, 1589.3717, 1641.6305,
        1712.5930, 1753.9524, 1566.9301, 1733.1494, 1695.8340, 1773.9592,
        1660.8243, 1707.1891, 1712.4742, 1761.7216, 1757.3589, 1802.1099,
        1677.1302, 1605.0417, 1714.0717, 1684.0460, 1592.1377, 1542.3445,
        1519.3090, 1505.3658, 1345.4812, 1355.3837, 1244.5869, 1281.5549,
        1193.1593, 1111.5787, 1071.3156,  960.8013,  990.4652,  871.9860,
         916.1412,  848.1179,  793.9009,  710.6188,  634.5671,  558.4211,
         594.2368,  458.7468,  437.5614,  420.9623,  327.4872,  350.9065,
         253.4258,  259.7904,  223.8085,  178.4358,  148.5239,  141.4114,
         126.7949,  115.5426,   95.0036,   60.7108,   50.9974,   33.4867,
          36.9444,   30.3504,   13.5440,   40.0230,   33.8263])]
3026.3204506642614
2.7855873073074493 7.149580515981642 41.64921657170218
val isze = 8
idinces = [16  2 79 32 37 41 39 68 22  0 12 56 40 66 42 24 51 43 13 61 17  6 71  1
 49 70 73  9 45 80 78 76  7 27 55 47 74 34 36 53 52 20 72 50 19 69 28 58
 29 11 75 15 65 46 60 30 25 81 64 10 63 77 54 21 31 59 62 33  4 23 18 38
  8 26 82 67 14 35 57  5 44 48  3]
we are doing training validation split
training loss = 27.24472999572754 100
val loss = 35.31590270996094
training loss = 19.355501174926758 200
val loss = 22.805416107177734
training loss = 14.80147647857666 300
val loss = 15.993021011352539
training loss = 11.808795928955078 400
val loss = 11.487188339233398
training loss = 9.880349159240723 500
val loss = 8.583162307739258
training loss = 8.639429092407227 600
val loss = 6.714613914489746
training loss = 7.837676048278809 700
val loss = 5.510647773742676
training loss = 7.320530891418457 800
val loss = 4.738612174987793
training loss = 6.988544940948486 900
val loss = 4.247826099395752
training loss = 6.7763519287109375 1000
val loss = 3.9388608932495117
training loss = 6.640798568725586 1100
val loss = 3.7461049556732178
training loss = 6.55352783203125 1200
val loss = 3.6264190673828125
training loss = 6.4961090087890625 1300
val loss = 3.551743984222412
training loss = 6.456748008728027 1400
val loss = 3.504276752471924
training loss = 6.4280476570129395 1500
val loss = 3.4728200435638428
training loss = 6.40547513961792 1600
val loss = 3.4503931999206543
training loss = 6.386338233947754 1700
val loss = 3.4330718517303467
training loss = 6.369091033935547 1800
val loss = 3.418435573577881
training loss = 6.35287618637085 1900
val loss = 3.405179023742676
training loss = 6.337241172790527 2000
val loss = 3.392643451690674
training loss = 6.32192850112915 2100
val loss = 3.3803534507751465
training loss = 6.30677604675293 2200
val loss = 3.3682446479797363
training loss = 6.291611671447754 2300
val loss = 3.356027126312256
training loss = 6.276172637939453 2400
val loss = 3.3436012268066406
training loss = 6.259974479675293 2500
val loss = 3.330535411834717
training loss = 6.242048263549805 2600
val loss = 3.316112518310547
training loss = 6.2203826904296875 2700
val loss = 3.2988414764404297
training loss = 6.190344333648682 2800
val loss = 3.2751758098602295
training loss = 6.140767574310303 2900
val loss = 3.2369272708892822
training loss = 6.046833038330078 3000
val loss = 3.1661643981933594
training loss = 5.878625392913818 3100
val loss = 3.0449249744415283
training loss = 5.635727882385254 3200
val loss = 2.8764288425445557
training loss = 5.285815238952637 3300
val loss = 2.6274032592773438
training loss = 4.728501796722412 3400
val loss = 2.2164535522460938
training loss = 3.889044761657715 3500
val loss = 1.5804104804992676
training loss = 3.0414681434631348 3600
val loss = 0.9543242454528809
training loss = 2.6779496669769287 3700
val loss = 0.7713943719863892
training loss = 2.6010754108428955 3800
val loss = 0.8058290481567383
training loss = 2.5719218254089355 3900
val loss = 0.8452699780464172
training loss = 2.550929546356201 4000
val loss = 0.876370370388031
training loss = 2.5340731143951416 4100
val loss = 0.9035360813140869
training loss = 2.5202300548553467 4200
val loss = 0.9278061985969543
training loss = 2.508702039718628 4300
val loss = 0.9492830634117126
training loss = 2.4989733695983887 4400
val loss = 0.9680110216140747
training loss = 2.4906537532806396 4500
val loss = 0.9841060042381287
training loss = 2.486219644546509 4600
val loss = 1.00364351272583
training loss = 2.4778494834899902 4700
val loss = 1.0077736377716064
training loss = 2.474806070327759 4800
val loss = 1.0166641473770142
training loss = 2.4685115814208984 4900
val loss = 1.0219063758850098
training loss = 2.464479684829712 5000
val loss = 1.0265394449234009
training loss = 2.460847854614258 5100
val loss = 1.0308328866958618
training loss = 2.4572997093200684 5200
val loss = 1.0330554246902466
training loss = 2.454092264175415 5300
val loss = 1.0350275039672852
training loss = 2.4505112171173096 5400
val loss = 1.0370818376541138
training loss = 2.4516797065734863 5500
val loss = 1.0473064184188843
training loss = 2.443760395050049 5600
val loss = 1.0399401187896729
training loss = 2.4411797523498535 5700
val loss = 1.0405242443084717
training loss = 2.4367942810058594 5800
val loss = 1.0420193672180176
training loss = 2.4331300258636475 5900
val loss = 1.0430091619491577
training loss = 2.4300382137298584 6000
val loss = 1.0439298152923584
training loss = 2.425651788711548 6100
val loss = 1.0448957681655884
training loss = 2.4396040439605713 6200
val loss = 1.0728681087493896
training loss = 2.4177937507629395 6300
val loss = 1.0468111038208008
training loss = 2.413583517074585 6400
val loss = 1.0477526187896729
training loss = 2.4094841480255127 6500
val loss = 1.0484778881072998
training loss = 2.4050261974334717 6600
val loss = 1.0495831966400146
training loss = 2.4030299186706543 6700
val loss = 1.0556633472442627
training loss = 2.3957810401916504 6800
val loss = 1.0518908500671387
training loss = 2.413210153579712 6900
val loss = 1.0873466730117798
training loss = 2.385612726211548 7000
val loss = 1.0549354553222656
training loss = 2.3798255920410156 7100
val loss = 1.0566630363464355
training loss = 2.3740599155426025 7200
val loss = 1.0606486797332764
training loss = 2.36655855178833 7300
val loss = 1.06172513961792
training loss = 2.3701515197753906 7400
val loss = 1.0761468410491943
training loss = 2.3483715057373047 7500
val loss = 1.0694586038589478
training loss = 2.335700035095215 7600
val loss = 1.0742547512054443
training loss = 2.320847272872925 7700
val loss = 1.0796070098876953
training loss = 2.302603006362915 7800
val loss = 1.0857086181640625
training loss = 2.4599592685699463 7900
val loss = 1.3158525228500366
training loss = 2.266948699951172 8000
val loss = 1.1002684831619263
training loss = 2.2516562938690186 8100
val loss = 1.1108534336090088
training loss = 2.256817579269409 8200
val loss = 1.1407814025878906
training loss = 2.2247159481048584 8300
val loss = 1.138362169265747
training loss = 2.2385592460632324 8400
val loss = 1.1938414573669434
training loss = 2.200369119644165 8500
val loss = 1.1688172817230225
training loss = 2.1887881755828857 8600
val loss = 1.1836824417114258
training loss = 2.228301525115967 8700
val loss = 1.2412177324295044
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 914.8199,  888.7534,  925.1130,  922.4756,  991.6606, 1028.3765,
        1101.3375, 1087.3934, 1160.2700, 1158.6776, 1183.4879, 1230.4073,
        1279.1406, 1227.5928, 1420.1217, 1397.1119, 1423.2097, 1399.3987,
        1424.4690, 1543.9952, 1655.6729, 1591.9279, 1559.9125, 1599.5830,
        1616.7981, 1737.1019, 1627.8907, 1812.2487, 1723.0134, 1724.0210,
        1689.2094, 1764.9847, 1652.3108, 1779.5511, 1720.1660, 1718.1014,
        1656.7089, 1531.7311, 1613.0272, 1645.9191, 1539.8499, 1507.9933,
        1493.9816, 1524.1541, 1378.6445, 1293.7604, 1355.3340, 1217.7478,
        1203.5103, 1186.8241, 1053.5891,  960.2184,  983.4697,  937.5696,
         832.0109,  847.1659,  838.7661,  661.6997,  582.7207,  541.3337,
         495.4077,  508.1307,  434.8353,  366.5920,  336.0409,  351.2501,
         288.1674,  268.1818,  191.7146,  171.4958,  184.9037,  151.6930,
         153.4642,   96.2900,   84.2653,   71.7826,   45.3082,   45.0341,
          41.4113,   41.6057,   19.4337,   39.6513,   29.8890])]
2677.7673107643395
4.993649511449264 8.881473619999603 4.758872381451706
val isze = 8
idinces = [61 45 23 70 18  6 56 65 25  5 26 36 74 63 28 20 13  7  2 29 73 43 38 66
 41 79 69 32 77 50 51 34 19 11 54 68 42 27  1 80 53 31 78 37 15  0 47 12
 60  3 52  8 82 24 39 14 58 57 71 40 33 49 62 16 55 81 30 17 76 72 64 44
 46 10 35  4  9 75 21 59 22 48 67]
we are doing training validation split
training loss = 11.308866500854492 100
val loss = 3.0993504524230957
training loss = 9.284392356872559 200
val loss = 2.020606279373169
training loss = 8.190910339355469 300
val loss = 1.781428575515747
training loss = 7.542900562286377 400
val loss = 1.8692286014556885
training loss = 7.17464542388916 500
val loss = 2.1167006492614746
training loss = 6.969197750091553 600
val loss = 2.4128472805023193
training loss = 6.8553786277771 700
val loss = 2.6960906982421875
training loss = 6.791860580444336 800
val loss = 2.937828302383423
training loss = 6.755225658416748 900
val loss = 3.12886381149292
training loss = 6.732492446899414 1000
val loss = 3.270648956298828
training loss = 6.716651439666748 1100
val loss = 3.369818925857544
training loss = 6.70404577255249 1200
val loss = 3.4348108768463135
training loss = 6.692831993103027 1300
val loss = 3.473965644836426
training loss = 6.6821208000183105 1400
val loss = 3.4946494102478027
training loss = 6.671499729156494 1500
val loss = 3.502694606781006
training loss = 6.660778999328613 1600
val loss = 3.502558946609497
training loss = 6.6498703956604 1700
val loss = 3.4973344802856445
training loss = 6.638740062713623 1800
val loss = 3.4890716075897217
training loss = 6.6273698806762695 1900
val loss = 3.4791243076324463
training loss = 6.61574649810791 2000
val loss = 3.4682528972625732
training loss = 6.603874683380127 2100
val loss = 3.4569060802459717
training loss = 6.591745853424072 2200
val loss = 3.4453442096710205
training loss = 6.579368591308594 2300
val loss = 3.433659315109253
training loss = 6.566748142242432 2400
val loss = 3.4219114780426025
training loss = 6.55389928817749 2500
val loss = 3.4101624488830566
training loss = 6.540839672088623 2600
val loss = 3.3984012603759766
training loss = 6.5276103019714355 2700
val loss = 3.3866796493530273
training loss = 6.5142645835876465 2800
val loss = 3.3750224113464355
training loss = 6.500889301300049 2900
val loss = 3.363449811935425
training loss = 6.487664222717285 3000
val loss = 3.3556041717529297
training loss = 6.47516393661499 3100
val loss = 3.3446710109710693
training loss = 6.4632110595703125 3200
val loss = 3.3336753845214844
training loss = 6.452695846557617 3300
val loss = 3.3326492309570312
training loss = 6.4423394203186035 3400
val loss = 3.316167116165161
training loss = 6.444209575653076 3500
val loss = 3.2965307235717773
training loss = 6.4264726638793945 3600
val loss = 3.3009724617004395
training loss = 6.420468807220459 3700
val loss = 3.2952308654785156
training loss = 6.415425777435303 3800
val loss = 3.2870962619781494
training loss = 6.4112868309021 3900
val loss = 3.2801785469055176
training loss = 6.4095377922058105 4000
val loss = 3.2730443477630615
training loss = 6.40460205078125 4100
val loss = 3.270581007003784
training loss = 6.40121603012085 4200
val loss = 3.2644922733306885
training loss = 6.398187160491943 4300
val loss = 3.2585582733154297
training loss = 6.3914337158203125 4400
val loss = 3.2548906803131104
training loss = 6.560854911804199 4500
val loss = 3.317317008972168
training loss = 6.360330581665039 4600
val loss = 3.2367818355560303
training loss = 6.282154083251953 4700
val loss = 3.1921732425689697
training loss = 5.744867324829102 4800
val loss = 2.7703909873962402
training loss = 4.434735298156738 4900
val loss = 2.2704687118530273
training loss = 2.6315057277679443 5000
val loss = 2.926621913909912
training loss = 1.8580623865127563 5100
val loss = 4.871819496154785
training loss = 1.773848533630371 5200
val loss = 5.205610275268555
training loss = 1.7285391092300415 5300
val loss = 5.216349124908447
reduced chi^2 level 2 = 1.8767998218536377
Constrained alpha: 1.9712698459625244
Constrained beta: 3.863978385925293
Constrained gamma: 9.031121253967285
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 836.5215,  890.5250,  963.5036,  951.6077,  998.9959, 1040.7610,
        1097.6611, 1127.9688, 1175.3398, 1229.5432, 1202.5039, 1216.2244,
        1271.9857, 1219.0526, 1310.4082, 1420.4454, 1410.3068, 1424.9840,
        1506.6052, 1431.9216, 1674.4990, 1563.1725, 1609.7074, 1628.2653,
        1669.2930, 1752.7273, 1561.8475, 1663.8079, 1818.1523, 1740.4773,
        1717.4425, 1722.9479, 1716.4618, 1754.0625, 1716.5641, 1694.8096,
        1648.9843, 1724.2767, 1682.2200, 1648.9305, 1609.5227, 1610.5214,
        1443.6292, 1485.2297, 1355.9417, 1275.9104, 1285.7463, 1267.3721,
        1110.5227, 1148.1082, 1104.4700,  949.7297,  944.1381,  907.5896,
         886.6854,  857.9857,  858.3669,  735.2424,  586.4065,  537.0275,
         550.4641,  481.3169,  431.5147,  383.8258,  357.1343,  350.6020,
         321.9468,  266.6782,  193.2937,  191.2549,  150.0127,  132.5634,
         158.2315,   98.1915,   98.9837,   60.8661,   57.9218,   44.4243,
          38.0745,   39.1421,   25.5954,   43.7465,   25.6532])]
2666.1609905537744
2.978241250324329 5.010987249811452 65.18161700609359
val isze = 8
idinces = [14  9 40 27 74 65 51 30 25 11 32 64 58 62 68  2 50 17 33 20  8 29 47 77
 23 39 31 48 56 53 46 21 24 45 41 78 13 60 63 59 69 73 57 15 80  6 70 54
 71 72 81 43 28 76 36 18 10 26  7 22 66 34 55 37 61  5 19 75 42 52  3 82
 44  1  4 79 35 16 49 12 67 38  0]
we are doing training validation split
training loss = 35.411869049072266 100
val loss = 25.24368667602539
training loss = 23.973995208740234 200
val loss = 16.722087860107422
training loss = 17.136018753051758 300
val loss = 11.56699275970459
training loss = 13.123774528503418 400
val loss = 8.732843399047852
training loss = 10.704583168029785 500
val loss = 7.219936370849609
training loss = 9.199886322021484 600
val loss = 6.446394920349121
training loss = 8.238761901855469 700
val loss = 6.088243007659912
training loss = 7.609507083892822 800
val loss = 5.963916778564453
training loss = 7.188530445098877 900
val loss = 5.972082138061523
training loss = 6.905086517333984 1000
val loss = 6.054062843322754
training loss = 6.715373516082764 1100
val loss = 6.17163610458374
training loss = 6.589046478271484 1200
val loss = 6.298443794250488
training loss = 6.5042195320129395 1300
val loss = 6.41685676574707
training loss = 6.4451375007629395 1400
val loss = 6.516242980957031
training loss = 6.400641441345215 1500
val loss = 6.5913872718811035
training loss = 6.3628716468811035 1600
val loss = 6.640552997589111
training loss = 6.325861930847168 1700
val loss = 6.6634674072265625
training loss = 6.283862590789795 1800
val loss = 6.658899307250977
training loss = 6.22891902923584 1900
val loss = 6.621043682098389
training loss = 6.1464104652404785 2000
val loss = 6.532641410827637
training loss = 6.007608413696289 2100
val loss = 6.352741718292236
training loss = 5.766045570373535 2200
val loss = 6.01577615737915
training loss = 5.355615615844727 2300
val loss = 5.469908714294434
training loss = 4.219637870788574 2400
val loss = 3.7355189323425293
training loss = 2.9650957584381104 2500
val loss = 2.7195379734039307
training loss = 2.546522855758667 2600
val loss = 2.3065431118011475
training loss = 2.476209878921509 2700
val loss = 2.260941505432129
training loss = 2.460848331451416 2800
val loss = 2.291977882385254
training loss = 2.4502110481262207 2900
val loss = 2.3362247943878174
training loss = 2.438020944595337 3000
val loss = 2.381737470626831
training loss = 2.422360420227051 3100
val loss = 2.422288417816162
training loss = 2.4019954204559326 3200
val loss = 2.4495480060577393
training loss = 2.3769443035125732 3300
val loss = 2.456271171569824
training loss = 2.348996162414551 3400
val loss = 2.4431228637695312
training loss = 2.3210136890411377 3500
val loss = 2.4237709045410156
training loss = 2.2955639362335205 3600
val loss = 2.385685443878174
training loss = 2.2738239765167236 3700
val loss = 2.349311590194702
training loss = 2.2563703060150146 3800
val loss = 2.32356595993042
training loss = 2.2510552406311035 3900
val loss = 2.272646188735962
training loss = 2.2328312397003174 4000
val loss = 2.2788045406341553
training loss = 2.2254955768585205 4100
val loss = 2.262399196624756
training loss = 2.2203171253204346 4200
val loss = 2.2523136138916016
training loss = 2.227497100830078 4300
val loss = 2.2968008518218994
training loss = 2.2140250205993652 4400
val loss = 2.238494634628296
training loss = 2.2560901641845703 4500
val loss = 2.2039568424224854
training loss = 2.2110135555267334 4600
val loss = 2.233368396759033
training loss = 2.2102179527282715 4700
val loss = 2.2333266735076904
training loss = 2.2097623348236084 4800
val loss = 2.2305469512939453
training loss = 2.2094194889068604 4900
val loss = 2.234724998474121
training loss = 2.21506667137146 5000
val loss = 2.214440107345581
training loss = 2.2092013359069824 5100
val loss = 2.237476348876953
training loss = 2.2100744247436523 5200
val loss = 2.2502760887145996
training loss = 2.209139108657837 5300
val loss = 2.2404541969299316
training loss = 2.2593865394592285 5400
val loss = 2.3800525665283203
training loss = 2.2089571952819824 5500
val loss = 2.242570638656616
training loss = 2.2088968753814697 5600
val loss = 2.241368293762207
training loss = 2.2084999084472656 5700
val loss = 2.243295192718506
training loss = 2.2081871032714844 5800
val loss = 2.24330472946167
training loss = 2.213085889816284 5900
val loss = 2.2732999324798584
training loss = 2.207125186920166 6000
val loss = 2.2423343658447266
training loss = 2.206589698791504 6100
val loss = 2.2417376041412354
training loss = 2.2058746814727783 6200
val loss = 2.2408158779144287
training loss = 2.205152988433838 6300
val loss = 2.2393856048583984
training loss = 2.206770658493042 6400
val loss = 2.223310708999634
training loss = 2.203472137451172 6500
val loss = 2.2366204261779785
training loss = 2.2473299503326416 6600
val loss = 2.3584418296813965
training loss = 2.201608180999756 6700
val loss = 2.2339320182800293
training loss = 2.20065975189209 6800
val loss = 2.231658458709717
training loss = 2.199665069580078 6900
val loss = 2.2271132469177246
training loss = 2.198535680770874 7000
val loss = 2.2282772064208984
training loss = 2.486246109008789 7100
val loss = 2.3693127632141113
training loss = 2.1962826251983643 7200
val loss = 2.223442316055298
training loss = 2.195122718811035 7300
val loss = 2.2228665351867676
training loss = 2.1939778327941895 7400
val loss = 2.2213776111602783
training loss = 2.1945853233337402 7500
val loss = 2.236207962036133
training loss = 2.1914865970611572 7600
val loss = 2.217775344848633
training loss = 2.1902430057525635 7700
val loss = 2.21616530418396
training loss = 2.198554754257202 7800
val loss = 2.2588391304016113
training loss = 2.1876120567321777 7900
val loss = 2.2128207683563232
training loss = 2.1863486766815186 8000
val loss = 2.2085139751434326
training loss = 2.1848652362823486 8100
val loss = 2.2109124660491943
training loss = 2.183439016342163 8200
val loss = 2.2080483436584473
training loss = 2.1856157779693604 8300
val loss = 2.230536460876465
training loss = 2.1804847717285156 8400
val loss = 2.204659938812256
training loss = 2.2424683570861816 8500
val loss = 2.36354660987854
training loss = 2.177405595779419 8600
val loss = 2.2008609771728516
training loss = 2.1758170127868652 8700
val loss = 2.200822353363037
training loss = 2.1741960048675537 8800
val loss = 2.199207305908203
training loss = 2.1725497245788574 8900
val loss = 2.1965208053588867
training loss = 2.1723713874816895 9000
val loss = 2.209853410720825
training loss = 2.169170379638672 9100
val loss = 2.192831516265869
training loss = 2.167417287826538 9200
val loss = 2.191751480102539
training loss = 2.1656694412231445 9300
val loss = 2.1908254623413086
training loss = 2.1638851165771484 9400
val loss = 2.188014507293701
training loss = 2.327988862991333 9500
val loss = 2.513753890991211
training loss = 2.1602060794830322 9600
val loss = 2.183957576751709
training loss = 2.15830397605896 9700
val loss = 2.1828274726867676
training loss = 2.159306526184082 9800
val loss = 2.165376663208008
training loss = 2.154472589492798 9900
val loss = 2.179108142852783
training loss = 2.1524581909179688 10000
val loss = 2.1773123741149902
training loss = 2.150545597076416 10100
val loss = 2.1792261600494385
training loss = 2.1484029293060303 10200
val loss = 2.1739492416381836
training loss = 2.146334171295166 10300
val loss = 2.1701855659484863
training loss = 2.1441755294799805 10400
val loss = 2.170408248901367
training loss = 2.14194655418396 10500
val loss = 2.168346881866455
training loss = 2.1398017406463623 10600
val loss = 2.1651077270507812
training loss = 2.137561798095703 10700
val loss = 2.1647660732269287
training loss = 2.1352014541625977 10800
val loss = 2.164212226867676
training loss = 2.132885694503784 10900
val loss = 2.1594223976135254
training loss = 2.130429744720459 11000
val loss = 2.1590003967285156
training loss = 2.132023572921753 11100
val loss = 2.183488607406616
training loss = 2.125415802001953 11200
val loss = 2.1550772190093994
training loss = 2.3354854583740234 11300
val loss = 2.552825927734375
training loss = 2.120135545730591 11400
val loss = 2.151078701019287
training loss = 2.117333173751831 11500
val loss = 2.148794412612915
training loss = 2.1147327423095703 11600
val loss = 2.141507387161255
training loss = 2.1116268634796143 11700
val loss = 2.1435019969940186
training loss = 2.1120903491973877 11800
val loss = 2.1651175022125244
training loss = 2.1055514812469482 11900
val loss = 2.1380863189697266
training loss = 2.168261766433716 12000
val loss = 2.3018529415130615
training loss = 2.0990233421325684 12100
val loss = 2.1312808990478516
training loss = 2.095665216445923 12200
val loss = 2.133939266204834
training loss = 2.0919947624206543 12300
val loss = 2.1230239868164062
training loss = 2.088127613067627 12400
val loss = 2.1215715408325195
training loss = 2.084235906600952 12500
val loss = 2.1192665100097656
training loss = 2.080090045928955 12600
val loss = 2.1142306327819824
training loss = 2.0777699947357178 12700
val loss = 2.096579074859619
training loss = 2.0713560581207275 12800
val loss = 2.1059441566467285
training loss = 2.0665271282196045 12900
val loss = 2.101804733276367
training loss = 2.061594009399414 13000
val loss = 2.096487522125244
training loss = 2.056442975997925 13100
val loss = 2.0930910110473633
training loss = 2.0517520904541016 13200
val loss = 2.100973129272461
training loss = 2.045363426208496 13300
val loss = 2.083110809326172
training loss = 2.039443016052246 13400
val loss = 2.079695224761963
training loss = 2.037965774536133 13500
val loss = 2.1063966751098633
training loss = 2.0272326469421387 13600
val loss = 2.0711612701416016
training loss = 2.0206849575042725 13700
val loss = 2.0657620429992676
training loss = 2.0145175457000732 13800
val loss = 2.0667693614959717
training loss = 2.0079522132873535 13900
val loss = 2.0593578815460205
training loss = 2.007192373275757 14000
val loss = 2.0340824127197266
training loss = 1.9950944185256958 14100
val loss = 2.052497148513794
training loss = 2.0087804794311523 14200
val loss = 2.0186800956726074
training loss = 1.9826518297195435 14300
val loss = 2.0456302165985107
training loss = 1.9765572547912598 14400
val loss = 2.0444576740264893
training loss = 1.9713855981826782 14500
val loss = 2.0334768295288086
training loss = 1.9651856422424316 14600
val loss = 2.0404622554779053
training loss = 2.0573644638061523 14700
val loss = 2.0319528579711914
training loss = 1.9548083543777466 14800
val loss = 2.037501573562622
training loss = 1.949867844581604 14900
val loss = 2.035230875015259
training loss = 1.9457296133041382 15000
val loss = 2.0414867401123047
training loss = 1.941138505935669 15100
val loss = 2.032567024230957
training loss = 1.9967771768569946 15200
val loss = 2.0071263313293457
training loss = 1.933332920074463 15300
val loss = 2.030390739440918
training loss = 1.9296704530715942 15400
val loss = 2.0310020446777344
training loss = 1.92654550075531 15500
val loss = 2.0249617099761963
training loss = 1.9232983589172363 15600
val loss = 2.027237892150879
training loss = 1.927147626876831 15700
val loss = 2.0654263496398926
training loss = 1.9177337884902954 15800
val loss = 2.0255401134490967
training loss = 1.9150646924972534 15900
val loss = 2.0246376991271973
training loss = 1.912894368171692 16000
val loss = 2.0286543369293213
training loss = 1.910468339920044 16100
val loss = 2.0230553150177
training loss = 2.0596747398376465 16200
val loss = 2.339684009552002
training loss = 1.906243920326233 16300
val loss = 2.0212018489837646
training loss = 1.9042057991027832 16400
val loss = 2.02053165435791
training loss = 1.902548909187317 16500
val loss = 2.0250887870788574
training loss = 1.9005494117736816 16600
val loss = 2.019568681716919
training loss = 1.9001457691192627 16700
val loss = 2.0078771114349365
training loss = 1.8971482515335083 16800
val loss = 2.018458843231201
training loss = 1.8997217416763306 16900
val loss = 2.047390937805176
training loss = 1.8939412832260132 17000
val loss = 2.0185062885284424
training loss = 1.892307162284851 17100
val loss = 2.016432046890259
training loss = 1.8909456729888916 17200
val loss = 2.020630359649658
training loss = 1.8893221616744995 17300
val loss = 2.0171377658843994
training loss = 1.8932814598083496 17400
val loss = 1.995858907699585
training loss = 1.8864331245422363 17500
val loss = 2.016871452331543
training loss = 1.8848873376846313 17600
val loss = 2.016854763031006
training loss = 1.884055256843567 17700
val loss = 2.0098624229431152
training loss = 1.8820106983184814 17800
val loss = 2.017101764678955
training loss = 1.9129958152770996 17900
val loss = 2.116814374923706
training loss = 1.8792294263839722 18000
val loss = 2.0173370838165283
training loss = 1.8776772022247314 18100
val loss = 2.0177385807037354
training loss = 1.8784985542297363 18200
val loss = 2.0073044300079346
training loss = 1.8748430013656616 18300
val loss = 2.01863956451416
training loss = 1.8796873092651367 18400
val loss = 2.0524165630340576
training loss = 1.8719786405563354 18500
val loss = 2.0209033489227295
training loss = 1.8702372312545776 18600
val loss = 2.018871307373047
training loss = 1.8695157766342163 18700
val loss = 2.030986785888672
training loss = 1.8669462203979492 18800
val loss = 2.018449068069458
training loss = 1.883396863937378 18900
val loss = 2.085979700088501
training loss = 1.862776517868042 19000
val loss = 2.0175814628601074
training loss = 1.8598240613937378 19100
val loss = 2.018570899963379
training loss = 1.8579308986663818 19200
val loss = 2.0210952758789062
training loss = 1.8549294471740723 19300
val loss = 2.0185694694519043
training loss = 1.8543381690979004 19400
val loss = 2.0397348403930664
training loss = 1.8498079776763916 19500
val loss = 2.02108097076416
training loss = 1.8707786798477173 19600
val loss = 2.0909852981567383
training loss = 1.8443868160247803 19700
val loss = 2.023470401763916
training loss = 1.841031551361084 19800
val loss = 2.025674343109131
training loss = 1.8392529487609863 19900
val loss = 2.029745578765869
training loss = 1.8362354040145874 20000
val loss = 2.029301643371582
training loss = 1.8330175876617432 20100
val loss = 2.0337917804718018
training loss = 1.831482172012329 20200
val loss = 2.0325732231140137
training loss = 1.8287179470062256 20300
val loss = 2.0365638732910156
training loss = 1.8531197309494019 20400
val loss = 2.057123899459839
training loss = 1.824979543685913 20500
val loss = 2.0397493839263916
training loss = 1.8228462934494019 20600
val loss = 2.0433709621429443
training loss = 1.822300910949707 20700
val loss = 2.040614128112793
training loss = 1.820693016052246 20800
val loss = 2.044227361679077
training loss = 1.820597767829895 20900
val loss = 2.0594112873077393
training loss = 1.8194682598114014 21000
val loss = 2.0432257652282715
training loss = 1.8186438083648682 21100
val loss = 2.0449960231781006
training loss = 1.8197749853134155 21200
val loss = 2.0468740463256836
training loss = 1.8184796571731567 21300
val loss = 2.041217803955078
training loss = 1.8181169033050537 21400
val loss = 2.0416030883789062
training loss = 1.8186523914337158 21500
val loss = 2.0366594791412354
training loss = 1.8184162378311157 21600
val loss = 2.036867141723633
training loss = 1.9261784553527832 21700
val loss = 2.156886100769043
training loss = 1.81910240650177 21800
val loss = 2.0303616523742676
training loss = 1.8190556764602661 21900
val loss = 2.030380964279175
training loss = 2.028334856033325 22000
val loss = 2.2570981979370117
training loss = 1.819767951965332 22100
val loss = 2.0244061946868896
reduced chi^2 level 2 = 1.8197849988937378
Constrained alpha: 1.8409119844436646
Constrained beta: 1.0063579082489014
Constrained gamma: 13.564945220947266
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 871.1038,  901.8364,  956.2802,  976.4059, 1003.1249, 1069.3171,
        1102.5662, 1123.7560, 1112.5189, 1150.9554, 1201.2808, 1193.4193,
        1259.1173, 1232.3126, 1329.0560, 1393.2351, 1404.7604, 1448.9075,
        1534.7164, 1534.1849, 1560.9619, 1553.9437, 1617.5903, 1627.9579,
        1694.3574, 1723.0502, 1644.3204, 1784.3446, 1716.7119, 1661.7235,
        1685.8743, 1732.6942, 1747.6257, 1733.1041, 1694.0011, 1689.3783,
        1708.1735, 1628.4530, 1660.9016, 1617.6786, 1601.8409, 1577.1729,
        1517.4269, 1435.9723, 1341.7394, 1329.1815, 1268.8750, 1265.4480,
        1110.4545, 1124.6838, 1080.2623,  938.9300,  940.3506,  875.8946,
         868.9786,  905.8534,  815.7353,  677.7742,  575.3163,  541.9755,
         527.8840,  444.8597,  432.4001,  376.9265,  320.5489,  333.1201,
         284.0325,  221.2262,  175.4753,  171.6942,  160.7632,  143.4106,
         130.2669,  101.2944,  103.7390,   61.8361,   58.3379,   36.8800,
          30.2943,   46.9916,   21.4761,   36.5905,   39.5802])]
2206.751391832254
2.0776615274007115 15.008887223100436 40.89995861528261
val isze = 8
idinces = [66 56 61 68  2 14  0  3 80 71 33 15 23 29 10 58  4  7 51 25  1 32 39 18
 57 49 37 36 38 75 52  6 41  8 78 40 26 24 82 76 60 59 72 79 12 63 81 70
 54 16 30 53 19 13 21 35 17 27 31 11 64 55 48 45 67 65 69 62 77 34 46 28
  5 20 50  9 44 22 43 42 73 74 47]
we are doing training validation split
training loss = 207.79307556152344 100
val loss = 321.44610595703125
training loss = 8.653252601623535 200
val loss = 7.759138107299805
training loss = 7.464617729187012 300
val loss = 2.8425076007843018
training loss = 7.400825500488281 400
val loss = 2.918386459350586
training loss = 7.3338446617126465 500
val loss = 3.009164810180664
training loss = 7.266304016113281 600
val loss = 3.1189146041870117
training loss = 7.200192928314209 700
val loss = 3.246148109436035
training loss = 7.136908531188965 800
val loss = 3.38785457611084
training loss = 7.077273845672607 900
val loss = 3.5397698879241943
training loss = 7.0215888023376465 1000
val loss = 3.6968588829040527
training loss = 6.969715118408203 1100
val loss = 3.8535444736480713
training loss = 6.921203136444092 1200
val loss = 4.004570484161377
training loss = 6.875418663024902 1300
val loss = 4.14523458480835
training loss = 6.831669330596924 1400
val loss = 4.271799564361572
training loss = 6.789296627044678 1500
val loss = 4.382006645202637
training loss = 6.747764587402344 1600
val loss = 4.47474479675293
training loss = 6.706668376922607 1700
val loss = 4.550041198730469
training loss = 6.6657609939575195 1800
val loss = 4.608901023864746
training loss = 6.624927043914795 1900
val loss = 4.652998924255371
training loss = 6.584157466888428 2000
val loss = 4.684207439422607
training loss = 6.543533802032471 2100
val loss = 4.70475959777832
training loss = 6.503190040588379 2200
val loss = 4.716777324676514
training loss = 6.463286876678467 2300
val loss = 4.7220916748046875
training loss = 6.423999786376953 2400
val loss = 4.72260046005249
training loss = 6.385481357574463 2500
val loss = 4.719616889953613
training loss = 6.3478617668151855 2600
val loss = 4.714578151702881
training loss = 6.311196327209473 2700
val loss = 4.708552360534668
training loss = 6.275439262390137 2800
val loss = 4.7022905349731445
training loss = 6.2403974533081055 2900
val loss = 4.696757793426514
training loss = 6.205653667449951 3000
val loss = 4.6925458908081055
training loss = 6.170437812805176 3100
val loss = 4.690212726593018
training loss = 6.133330821990967 3200
val loss = 4.690425395965576
training loss = 6.091546535491943 3300
val loss = 4.694068908691406
training loss = 6.038829803466797 3400
val loss = 4.703242778778076
training loss = 5.958187103271484 3500
val loss = 4.725176811218262
training loss = 5.795413017272949 3600
val loss = 4.792194366455078
training loss = 5.415457725524902 3700
val loss = 4.9886908531188965
training loss = 4.681174278259277 3800
val loss = 5.0844831466674805
training loss = 3.591268301010132 3900
val loss = 4.4746623039245605
training loss = 2.4314091205596924 4000
val loss = 3.503392457962036
training loss = 1.7719199657440186 4100
val loss = 2.898432731628418
training loss = 1.630118489265442 4200
val loss = 2.803839683532715
training loss = 1.597056269645691 4300
val loss = 2.813899278640747
training loss = 1.5750806331634521 4400
val loss = 2.825078248977661
training loss = 1.5569279193878174 4500
val loss = 2.8345887660980225
training loss = 1.5414528846740723 4600
val loss = 2.8430590629577637
training loss = 1.5280182361602783 4700
val loss = 2.8504762649536133
training loss = 1.5161677598953247 4800
val loss = 2.8567750453948975
training loss = 1.5055603981018066 4900
val loss = 2.8619725704193115
training loss = 1.4964241981506348 5000
val loss = 2.868426561355591
training loss = 1.4876798391342163 5100
val loss = 2.8698911666870117
training loss = 1.480203628540039 5200
val loss = 2.874781608581543
reduced chi^2 level 2 = 1.4762020111083984
Constrained alpha: 1.8956751823425293
Constrained beta: 3.5887537002563477
Constrained gamma: 28.426912307739258
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 815.7943,  870.7163,  914.6171,  943.3464,  953.1658, 1027.3507,
        1121.2098, 1167.6819, 1163.9277, 1162.9698, 1256.0098, 1205.3834,
        1218.6338, 1287.8643, 1312.2798, 1384.1858, 1414.5719, 1462.0648,
        1600.1727, 1465.1306, 1601.9329, 1570.7782, 1553.7303, 1640.7308,
        1712.6993, 1676.0145, 1714.4679, 1750.7222, 1760.9258, 1679.1431,
        1639.2292, 1698.1228, 1647.1249, 1777.0876, 1683.1772, 1791.4004,
        1642.6108, 1693.7805, 1695.7690, 1596.9734, 1653.6371, 1507.1161,
        1489.9110, 1560.8176, 1375.2152, 1316.6167, 1232.1926, 1191.2351,
        1101.3713, 1182.6396, 1092.3125,  976.7621,  997.6075,  914.8334,
         842.6809,  814.4448,  812.4946,  683.3450,  595.2278,  521.2750,
         557.5040,  493.3252,  432.1112,  360.1448,  356.3157,  328.9862,
         279.3137,  254.1324,  195.1019,  170.1398,  142.2442,  154.2777,
         159.3518,  105.7152,  110.1393,   74.0444,   53.2771,   47.8473,
          43.5786,   34.8917,   17.3492,   34.6665,   31.9124])]
2806.993948347699
0.3858695157537073 7.461348032577273 22.748856727447087
val isze = 8
idinces = [39 32 30 20 79 15 21 70 43 67 78 16 69 71 63 75 38 47 18 54  8 64  9 82
  7 33 72 23 44 27 56 13 50 73 68 34 51  5 14 66  4 29 22  0 10 55 24 62
 37 80 58 57 42 25 65 36 59 35 77 28  6 60  2 48 19 17 61 74 26 53 76 31
 49 46 81  1  3 11 12 41 45 40 52]
we are doing training validation split
training loss = 10.948391914367676 100
val loss = 3.0466670989990234
training loss = 8.101251602172852 200
val loss = 3.935772657394409
training loss = 7.991958141326904 300
val loss = 3.864241361618042
training loss = 7.882571220397949 400
val loss = 3.8143210411071777
training loss = 7.780390739440918 500
val loss = 3.772930145263672
training loss = 7.689266204833984 600
val loss = 3.741816997528076
training loss = 7.610647201538086 700
val loss = 3.720775604248047
training loss = 7.544440269470215 800
val loss = 3.7086896896362305
training loss = 7.489645481109619 900
val loss = 3.70395565032959
training loss = 7.444782257080078 1000
val loss = 3.704709768295288
training loss = 7.408232688903809 1100
val loss = 3.7092158794403076
training loss = 7.37838888168335 1200
val loss = 3.7159337997436523
training loss = 7.353790283203125 1300
val loss = 3.723536491394043
training loss = 7.333174228668213 1400
val loss = 3.731163740158081
training loss = 7.315479278564453 1500
val loss = 3.7379112243652344
training loss = 7.299853801727295 1600
val loss = 3.743314504623413
training loss = 7.2856245040893555 1700
val loss = 3.747009515762329
training loss = 7.272290229797363 1800
val loss = 3.748969554901123
training loss = 7.259462356567383 1900
val loss = 3.749035596847534
training loss = 7.246852397918701 2000
val loss = 3.7473080158233643
training loss = 7.234258651733398 2100
val loss = 3.743943691253662
training loss = 7.221504211425781 2200
val loss = 3.7390222549438477
training loss = 7.20844841003418 2300
val loss = 3.7326557636260986
training loss = 7.194926738739014 2400
val loss = 3.724992513656616
training loss = 7.18074893951416 2500
val loss = 3.716001033782959
training loss = 7.165622711181641 2600
val loss = 3.705685615539551
training loss = 7.149106025695801 2700
val loss = 3.693761110305786
training loss = 7.130443096160889 2800
val loss = 3.6797544956207275
training loss = 7.108345031738281 2900
val loss = 3.662804126739502
training loss = 7.080478668212891 3000
val loss = 3.6409764289855957
training loss = 7.04261589050293 3100
val loss = 3.611135721206665
training loss = 6.9873433113098145 3200
val loss = 3.5674915313720703
training loss = 6.903491497039795 3300
val loss = 3.5022029876708984
training loss = 6.7796549797058105 3400
val loss = 3.4095396995544434
training loss = 6.608413219451904 3500
val loss = 3.2913079261779785
training loss = 6.3741278648376465 3600
val loss = 3.145376443862915
training loss = 6.032510757446289 3700
val loss = 2.9486727714538574
training loss = 5.50227165222168 3800
val loss = 2.658190965652466
training loss = 4.673521518707275 3900
val loss = 2.23490309715271
training loss = 3.5328691005706787 4000
val loss = 1.7401070594787598
training loss = 2.5538523197174072 4100
val loss = 1.5037755966186523
training loss = 2.237067937850952 4200
val loss = 1.6369545459747314
training loss = 2.1647448539733887 4300
val loss = 1.7798047065734863
training loss = 2.120997428894043 4400
val loss = 1.8812386989593506
training loss = 2.0876190662384033 4500
val loss = 1.9661540985107422
training loss = 2.0616533756256104 4600
val loss = 2.040656089782715
training loss = 2.04127836227417 4700
val loss = 2.105991840362549
training loss = 2.0251572132110596 4800
val loss = 2.162816047668457
training loss = 2.012284517288208 4900
val loss = 2.211920976638794
training loss = 2.001898765563965 5000
val loss = 2.253880023956299
training loss = 1.9934159517288208 5100
val loss = 2.289443016052246
training loss = 2.014265775680542 5200
val loss = 2.154892921447754
training loss = 1.9806275367736816 5300
val loss = 2.3446996212005615
training loss = 1.9756710529327393 5400
val loss = 2.3647329807281494
training loss = 1.9721771478652954 5500
val loss = 2.3460516929626465
training loss = 1.9673254489898682 5600
val loss = 2.397258758544922
training loss = 1.9872052669525146 5700
val loss = 2.635643243789673
training loss = 1.9597501754760742 5800
val loss = 2.4218597412109375
training loss = 1.955580711364746 5900
val loss = 2.429199695587158
training loss = 1.9505176544189453 6000
val loss = 2.4376206398010254
training loss = 1.9438717365264893 6100
val loss = 2.4372034072875977
training loss = 1.94210946559906 6200
val loss = 2.3263297080993652
training loss = 1.9231846332550049 6300
val loss = 2.4159021377563477
training loss = 1.9107388257980347 6400
val loss = 2.3968570232391357
training loss = 1.9073140621185303 6500
val loss = 2.509756326675415
training loss = 1.890633225440979 6600
val loss = 2.375906467437744
training loss = 1.882341980934143 6700
val loss = 2.3783957958221436
training loss = 1.8750108480453491 6800
val loss = 2.416710376739502
training loss = 1.8670506477355957 6900
val loss = 2.3944308757781982
training loss = 1.9020891189575195 7000
val loss = 2.161238193511963
training loss = 1.8521134853363037 7100
val loss = 2.413638114929199
training loss = 1.8446588516235352 7200
val loss = 2.416024923324585
training loss = 1.8432193994522095 7300
val loss = 2.5454201698303223
training loss = 1.8297604322433472 7400
val loss = 2.424856662750244
training loss = 1.822167992591858 7500
val loss = 2.430399179458618
training loss = 1.814803123474121 7600
val loss = 2.419483184814453
training loss = 1.807267665863037 7700
val loss = 2.4349558353424072
training loss = 1.8003259897232056 7800
val loss = 2.4733846187591553
training loss = 1.7927519083023071 7900
val loss = 2.4397685527801514
training loss = 1.7855607271194458 8000
val loss = 2.4390904903411865
training loss = 1.7791630029678345 8100
val loss = 2.470582962036133
training loss = 1.7722982168197632 8200
val loss = 2.4398550987243652
training loss = 2.033665895462036 8300
val loss = 3.605630397796631
training loss = 1.7600523233413696 8400
val loss = 2.4361588954925537
training loss = 1.7543622255325317 8500
val loss = 2.438588857650757
training loss = 1.7519574165344238 8600
val loss = 2.531558036804199
training loss = 1.7442655563354492 8700
val loss = 2.4370408058166504
training loss = 1.7396020889282227 8800
val loss = 2.437197208404541
training loss = 1.7359811067581177 8900
val loss = 2.400930166244507
training loss = 1.7316089868545532 9000
val loss = 2.435864210128784
reduced chi^2 level 2 = 1.8036682605743408
Constrained alpha: 1.6974462270736694
Constrained beta: 3.876293182373047
Constrained gamma: 18.101116180419922
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 912.4742,  832.5432,  959.6479,  889.9787,  994.1317, 1056.8815,
        1093.1942, 1168.2968, 1170.1909, 1135.7482, 1215.0898, 1251.9308,
        1214.5153, 1201.9440, 1334.3560, 1383.7482, 1394.9187, 1457.5691,
        1540.8390, 1532.2191, 1557.3257, 1546.2992, 1640.5089, 1608.9863,
        1737.4274, 1730.9524, 1573.0526, 1712.8864, 1727.3915, 1693.6041,
        1746.8199, 1727.2560, 1750.2312, 1723.7458, 1677.3875, 1680.7166,
        1672.4706, 1576.1519, 1641.8674, 1574.2987, 1554.0232, 1570.1541,
        1545.0820, 1534.0193, 1323.4152, 1330.3964, 1280.6035, 1280.7662,
        1184.0173, 1142.8459, 1048.0076,  970.8641,  920.9985,  917.2245,
         917.1035,  839.0660,  849.3932,  667.9573,  613.7454,  485.2475,
         537.8628,  456.7801,  488.5981,  393.2000,  342.5202,  343.0038,
         288.3197,  259.8991,  175.5379,  168.9688,  141.1175,  128.5843,
         153.8156,  105.2629,   99.4369,   52.5705,   39.9910,   45.6376,
          32.6479,   37.8285,   21.2100,   37.6576,   38.5213])]
2594.331235904973
4.642857106557991 9.346134424167653 28.67938001918865
val isze = 8
idinces = [ 3 68 29 32 33  0 25 66 62 81 39 10  1 63 17 76 60 42 65 59 36 69 30 80
 82 64 16 40 13 58 49 31 48 73 44 37  2 53 45 54 38 11  5 20 71 61 67 52
  4 14 75 41 43 51 15 26 22 19 79 35 74 28  8 18 12 77 57 34  7  6 24 78
 56 46 50 47 70 21  9 27 23 72 55]
we are doing training validation split
training loss = 7.879323959350586 100
val loss = 14.662830352783203
training loss = 6.875870704650879 200
val loss = 12.58831787109375
training loss = 6.601651191711426 300
val loss = 11.068075180053711
training loss = 6.426438331604004 400
val loss = 9.906534194946289
training loss = 6.322919845581055 500
val loss = 9.066344261169434
training loss = 6.2634382247924805 600
val loss = 8.479000091552734
training loss = 6.228409767150879 700
val loss = 8.078600883483887
training loss = 6.2059221267700195 800
val loss = 7.812203407287598
training loss = 6.18941068649292 900
val loss = 7.639777183532715
training loss = 6.175537109375 1000
val loss = 7.531712532043457
training loss = 6.1627349853515625 1100
val loss = 7.466447353363037
training loss = 6.1503095626831055 1200
val loss = 7.428744316101074
training loss = 6.137980937957764 1300
val loss = 7.407848358154297
training loss = 6.125638008117676 1400
val loss = 7.396745204925537
training loss = 6.113229274749756 1500
val loss = 7.390957355499268
training loss = 6.100736618041992 1600
val loss = 7.387693881988525
training loss = 6.088149547576904 1700
val loss = 7.385488033294678
training loss = 6.075454235076904 1800
val loss = 7.38364315032959
training loss = 6.062643051147461 1900
val loss = 7.381659030914307
training loss = 6.0497145652771 2000
val loss = 7.379471778869629
training loss = 6.036670207977295 2100
val loss = 7.3770294189453125
training loss = 6.02352237701416 2200
val loss = 7.374392509460449
training loss = 6.0102925300598145 2300
val loss = 7.371571063995361
training loss = 5.998053073883057 2400
val loss = 7.327993392944336
training loss = 5.983969688415527 2500
val loss = 7.36338996887207
training loss = 5.971556186676025 2600
val loss = 7.3819074630737305
training loss = 5.958861827850342 2700
val loss = 7.352114677429199
training loss = 5.947293758392334 2800
val loss = 7.344597339630127
training loss = 5.946173667907715 2900
val loss = 7.481057167053223
training loss = 5.926977634429932 3000
val loss = 7.347968101501465
training loss = 5.929790019989014 3100
val loss = 7.242670059204102
training loss = 5.9112372398376465 3200
val loss = 7.3565778732299805
training loss = 5.905113220214844 3300
val loss = 7.37825870513916
training loss = 5.899474620819092 3400
val loss = 7.369421005249023
training loss = 5.894264221191406 3500
val loss = 7.372518539428711
training loss = 5.889076232910156 3600
val loss = 7.353245258331299
training loss = 5.881918430328369 3700
val loss = 7.381583213806152
training loss = 5.9062066078186035 3800
val loss = 7.639267921447754
training loss = 5.843281269073486 3900
val loss = 7.388485908508301
training loss = 5.745769023895264 4000
val loss = 7.408093452453613
training loss = 5.359414577484131 4100
val loss = 7.536373138427734
training loss = 4.692755699157715 4200
val loss = 7.021085739135742
training loss = 3.616866111755371 4300
val loss = 5.887948989868164
training loss = 2.6069490909576416 4400
val loss = 4.287405014038086
training loss = 2.4602279663085938 4500
val loss = 3.6628594398498535
training loss = 2.4269187450408936 4600
val loss = 3.548844337463379
training loss = 2.421752691268921 4700
val loss = 3.582998752593994
training loss = 2.3581478595733643 4800
val loss = 3.374149799346924
training loss = 2.3340964317321777 4900
val loss = 3.290292501449585
training loss = 2.3160688877105713 5000
val loss = 3.2086527347564697
training loss = 2.3013501167297363 5100
val loss = 3.1552789211273193
training loss = 2.288818120956421 5200
val loss = 3.1048731803894043
training loss = 2.2767550945281982 5300
val loss = 3.070570945739746
training loss = 2.2688512802124023 5400
val loss = 3.0603413581848145
training loss = 2.2551496028900146 5500
val loss = 3.0016512870788574
training loss = 2.245685577392578 5600
val loss = 2.980224132537842
training loss = 2.235227108001709 5700
val loss = 2.9398207664489746
training loss = 2.225677490234375 5800
val loss = 2.910181999206543
training loss = 2.2169349193573 5900
val loss = 2.8884360790252686
training loss = 2.2078776359558105 6000
val loss = 2.855287551879883
training loss = 2.208287000656128 6100
val loss = 2.8692030906677246
training loss = 2.191335916519165 6200
val loss = 2.8031749725341797
training loss = 2.276879072189331 6300
val loss = 2.9746127128601074
training loss = 2.1762824058532715 6400
val loss = 2.753955125808716
training loss = 2.1692070960998535 6500
val loss = 2.731111764907837
training loss = 2.1674578189849854 6600
val loss = 2.6982855796813965
training loss = 2.156766414642334 6700
val loss = 2.688755512237549
training loss = 2.1509506702423096 6800
val loss = 2.6676599979400635
training loss = 2.1469218730926514 6900
val loss = 2.6436305046081543
training loss = 2.1410365104675293 7000
val loss = 2.631027936935425
training loss = 2.2611892223358154 7100
val loss = 2.670321464538574
training loss = 2.1329140663146973 7200
val loss = 2.5999841690063477
training loss = 2.1292874813079834 7300
val loss = 2.5835094451904297
training loss = 2.1382205486297607 7400
val loss = 2.6112098693847656
training loss = 2.123392105102539 7500
val loss = 2.557579517364502
training loss = 2.3487489223480225 7600
val loss = 2.9184367656707764
training loss = 2.1188406944274902 7700
val loss = 2.5352625846862793
training loss = 2.11684513092041 7800
val loss = 2.5250868797302246
training loss = 2.115363836288452 7900
val loss = 2.5173981189727783
training loss = 2.113858938217163 8000
val loss = 2.508284568786621
training loss = 2.112966537475586 8100
val loss = 2.506263017654419
training loss = 2.111677408218384 8200
val loss = 2.49513578414917
training loss = 2.110713243484497 8300
val loss = 2.4886727333068848
training loss = 2.1101629734039307 8400
val loss = 2.4825592041015625
training loss = 2.109313726425171 8500
val loss = 2.477966785430908
training loss = 2.1137516498565674 8600
val loss = 2.4694316387176514
training loss = 2.1083037853240967 8700
val loss = 2.469989776611328
training loss = 2.3738412857055664 8800
val loss = 2.693742513656616
training loss = 2.10756254196167 8900
val loss = 2.46311616897583
training loss = 2.10719895362854 9000
val loss = 2.4591407775878906
training loss = 2.1072604656219482 9100
val loss = 2.4558093547821045
training loss = 2.106729030609131 9200
val loss = 2.4539217948913574
training loss = 2.1106555461883545 9300
val loss = 2.467770576477051
training loss = 2.106393575668335 9400
val loss = 2.450247287750244
training loss = 2.106196641921997 9500
val loss = 2.44720721244812
training loss = 2.106414556503296 9600
val loss = 2.4501709938049316
training loss = 2.1059679985046387 9700
val loss = 2.444169044494629
training loss = 2.1097395420074463 9800
val loss = 2.4404714107513428
training loss = 2.10579514503479 9900
val loss = 2.442136287689209
training loss = 2.105724811553955 10000
val loss = 2.4407143592834473
training loss = 2.1057381629943848 10100
val loss = 2.439326286315918
training loss = 2.1055290699005127 10200
val loss = 2.4378504753112793
training loss = 2.1087191104888916 10300
val loss = 2.4342474937438965
training loss = 2.105417490005493 10400
val loss = 2.436596393585205
training loss = 2.3849947452545166 10500
val loss = 2.69103741645813
training loss = 2.105321168899536 10600
val loss = 2.4357075691223145
training loss = 2.105212688446045 10700
val loss = 2.433847188949585
training loss = 2.1052405834198 10800
val loss = 2.435971736907959
training loss = 2.1051173210144043 10900
val loss = 2.433384895324707
training loss = 2.488710880279541 11000
val loss = 2.799018144607544
training loss = 2.1050362586975098 11100
val loss = 2.433793306350708
training loss = 2.104909896850586 11200
val loss = 2.431591272354126
training loss = 2.106793165206909 11300
val loss = 2.4411871433258057
training loss = 2.1047840118408203 11400
val loss = 2.4312477111816406
training loss = 2.151578903198242 11500
val loss = 2.459444522857666
training loss = 2.1046528816223145 11600
val loss = 2.431309223175049
training loss = 2.1045167446136475 11700
val loss = 2.429734945297241
training loss = 2.1049716472625732 11800
val loss = 2.4296932220458984
training loss = 2.1043307781219482 11900
val loss = 2.4298274517059326
training loss = 2.212549924850464 12000
val loss = 2.511500120162964
training loss = 2.1040964126586914 12100
val loss = 2.430305242538452
training loss = 2.1038806438446045 12200
val loss = 2.42887282371521
training loss = 2.1068506240844727 12300
val loss = 2.4267265796661377
training loss = 2.1034514904022217 12400
val loss = 2.429299831390381
training loss = 2.3201651573181152 12500
val loss = 2.6209211349487305
training loss = 2.1027698516845703 12600
val loss = 2.4308295249938965
training loss = 2.1021852493286133 12700
val loss = 2.42999267578125
training loss = 2.1023333072662354 12800
val loss = 2.430964708328247
training loss = 2.100661277770996 12900
val loss = 2.433292865753174
training loss = 2.0999624729156494 13000
val loss = 2.431879997253418
training loss = 2.0984272956848145 13100
val loss = 2.4380717277526855
training loss = 2.097140312194824 13200
val loss = 2.437096357345581
training loss = 2.1028683185577393 13300
val loss = 2.4368443489074707
training loss = 2.095125675201416 13400
val loss = 2.436378240585327
training loss = 2.504587411880493 13500
val loss = 2.843249797821045
training loss = 2.0935914516448975 13600
val loss = 2.4351868629455566
training loss = 2.0927789211273193 13700
val loss = 2.4326491355895996
training loss = 2.0986809730529785 13800
val loss = 2.433361768722534
training loss = 2.091352701187134 13900
val loss = 2.432621479034424
training loss = 2.090510368347168 14000
val loss = 2.4315450191497803
training loss = 2.0900533199310303 14100
val loss = 2.4359374046325684
training loss = 2.0889298915863037 14200
val loss = 2.432868719100952
training loss = 2.1098601818084717 14300
val loss = 2.4439239501953125
training loss = 2.0871999263763428 14400
val loss = 2.4343998432159424
training loss = 2.0861799716949463 14500
val loss = 2.434323310852051
training loss = 2.0854313373565674 14600
val loss = 2.4354910850524902
training loss = 2.0842444896698 14700
val loss = 2.4361250400543213
training loss = 2.083710193634033 14800
val loss = 2.441478729248047
training loss = 2.0822179317474365 14900
val loss = 2.4385249614715576
training loss = 2.0810704231262207 15000
val loss = 2.4382057189941406
training loss = 2.0802760124206543 15100
val loss = 2.442140579223633
training loss = 2.07907772064209 15200
val loss = 2.4423422813415527
training loss = 2.0778698921203613 15300
val loss = 2.4428658485412598
training loss = 2.0771303176879883 15400
val loss = 2.445183753967285
training loss = 2.0756306648254395 15500
val loss = 2.446666717529297
training loss = 2.1337082386016846 15600
val loss = 2.4993338584899902
training loss = 2.073385715484619 15700
val loss = 2.4511451721191406
training loss = 2.072111129760742 15800
val loss = 2.4522218704223633
training loss = 2.0715105533599854 15900
val loss = 2.45519757270813
training loss = 2.069845676422119 16000
val loss = 2.4571971893310547
training loss = 2.0695126056671143 16100
val loss = 2.4609196186065674
training loss = 2.0675618648529053 16200
val loss = 2.4627256393432617
training loss = 2.066298484802246 16300
val loss = 2.4649906158447266
training loss = 2.065394639968872 16400
val loss = 2.469661235809326
training loss = 2.063976526260376 16500
val loss = 2.470731496810913
training loss = 2.0629560947418213 16600
val loss = 2.4752039909362793
training loss = 2.0617213249206543 16700
val loss = 2.477383613586426
training loss = 2.0905582904815674 16800
val loss = 2.524049758911133
training loss = 2.0595200061798096 16900
val loss = 2.4846978187561035
training loss = 2.058288097381592 17000
val loss = 2.4876279830932617
training loss = 2.057389974594116 17100
val loss = 2.492784023284912
training loss = 2.0561752319335938 17200
val loss = 2.4952805042266846
training loss = 2.0659570693969727 17300
val loss = 2.514280319213867
training loss = 2.0540637969970703 17400
val loss = 2.503455400466919
training loss = 2.084559202194214 17500
val loss = 2.538817882537842
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 854.3409,  832.0383,  942.9816,  993.3420,  984.9927, 1065.1539,
        1091.7052, 1150.7000, 1194.8838, 1077.4601, 1245.3790, 1204.2185,
        1269.8907, 1305.6788, 1339.2903, 1421.4076, 1379.0829, 1405.5521,
        1530.9193, 1502.4966, 1629.7778, 1527.2928, 1620.6154, 1699.4884,
        1634.0565, 1651.6184, 1606.3751, 1725.6469, 1759.3851, 1745.2054,
        1662.1710, 1711.1293, 1760.9403, 1721.4153, 1715.8274, 1707.2709,
        1705.6775, 1690.8033, 1628.2766, 1662.8165, 1615.6555, 1504.1130,
        1498.2863, 1492.8453, 1333.8853, 1313.8824, 1273.3909, 1278.6658,
        1160.1270, 1203.6176, 1112.4374, 1045.1339,  996.6221,  919.6644,
         856.9718,  853.6744,  809.5609,  674.4264,  627.5128,  539.6349,
         544.7041,  490.3632,  436.9116,  390.9124,  360.5075,  325.9973,
         279.1914,  265.9582,  199.9743,  169.4880,  187.1242,  152.6700,
         147.9640,  114.7494,  110.7261,   64.0478,   47.3453,   50.4770,
          30.4203,   45.2285,   14.5977,   31.4459,   36.1942])]
2274.718303255249
4.50054421828661 9.577042821654674 86.05453177644658
val isze = 8
idinces = [43 44 76 82 62 16  6 23 57  8 60 15 64 65  0 45 50 58 70 38  4 29 47 69
 53 35 24 54 67 26 33 55 13 36 25 21 12 73 75 20 71 22  7 72  9 59  1 19
 79 42 51 39 37 56 48 78 11 40 46 52  2 10 49 18 14 27 41 34 80  3 28 30
 17 81 31 74 61 63  5 77 66 68 32]
we are doing training validation split
training loss = 267.229736328125 100
val loss = 211.82159423828125
training loss = 13.221806526184082 200
val loss = 11.01203727722168
training loss = 9.917190551757812 300
val loss = 8.834694862365723
training loss = 8.062352180480957 400
val loss = 7.688318252563477
training loss = 7.074340343475342 500
val loss = 7.127657890319824
training loss = 6.563543319702148 600
val loss = 6.869734764099121
training loss = 6.300482273101807 700
val loss = 6.757241249084473
training loss = 6.1631293296813965 800
val loss = 6.711096286773682
training loss = 6.088845252990723 900
val loss = 6.69309139251709
training loss = 6.045804023742676 1000
val loss = 6.685293197631836
training loss = 6.01801061630249 1100
val loss = 6.679744720458984
training loss = 5.997594833374023 1200
val loss = 6.673361778259277
training loss = 5.9807915687561035 1300
val loss = 6.665290832519531
training loss = 5.965871334075928 1400
val loss = 6.655710697174072
training loss = 5.952053070068359 1500
val loss = 6.645049571990967
training loss = 5.9389967918396 1600
val loss = 6.6337738037109375
training loss = 5.926535129547119 1700
val loss = 6.622252464294434
training loss = 5.914580821990967 1800
val loss = 6.610745429992676
training loss = 5.903071403503418 1900
val loss = 6.599391937255859
training loss = 5.891958236694336 2000
val loss = 6.588303565979004
training loss = 5.881191730499268 2100
val loss = 6.577485084533691
training loss = 5.870729446411133 2200
val loss = 6.566924571990967
training loss = 5.860524654388428 2300
val loss = 6.556606769561768
training loss = 5.85053014755249 2400
val loss = 6.546490669250488
training loss = 5.840707778930664 2500
val loss = 6.536532402038574
training loss = 5.831008434295654 2600
val loss = 6.5266947746276855
training loss = 5.821380615234375 2700
val loss = 6.516891956329346
training loss = 5.811779022216797 2800
val loss = 6.507098197937012
training loss = 5.802150726318359 2900
val loss = 6.497249603271484
training loss = 5.792443752288818 3000
val loss = 6.4872846603393555
training loss = 5.782598495483398 3100
val loss = 6.477114200592041
training loss = 5.772550106048584 3200
val loss = 6.4667229652404785
training loss = 5.762236595153809 3300
val loss = 6.455976486206055
training loss = 5.763175010681152 3400
val loss = 6.4297637939453125
training loss = 5.740464687347412 3500
val loss = 6.433095932006836
training loss = 5.7723870277404785 3600
val loss = 6.411587715148926
training loss = 5.716998100280762 3700
val loss = 6.408687114715576
training loss = 5.704679012298584 3800
val loss = 6.394136905670166
training loss = 5.692044258117676 3900
val loss = 6.380559921264648
training loss = 5.679164409637451 4000
val loss = 6.367374420166016
training loss = 5.66693639755249 4100
val loss = 6.347598075866699
training loss = 5.653271198272705 4200
val loss = 6.338479995727539
training loss = 5.646882057189941 4300
val loss = 6.3127641677856445
training loss = 5.628572463989258 4400
val loss = 6.310443878173828
training loss = 5.617087364196777 4500
val loss = 6.296343803405762
training loss = 5.6064839363098145 4600
val loss = 6.283375263214111
training loss = 5.596827983856201 4700
val loss = 6.271380424499512
training loss = 5.598960876464844 4800
val loss = 6.295029640197754
training loss = 5.5797200202941895 4900
val loss = 6.249053001403809
training loss = 5.571087837219238 5000
val loss = 6.236379623413086
training loss = 5.5611677169799805 5100
val loss = 6.227235794067383
training loss = 5.544849395751953 5200
val loss = 6.198632717132568
training loss = 5.5015997886657715 5300
val loss = 6.13648796081543
training loss = 5.28679895401001 5400
val loss = 5.86380672454834
training loss = 4.735408782958984 5500
val loss = 5.340314865112305
training loss = 3.506025552749634 5600
val loss = 4.164107799530029
training loss = 1.5749353170394897 5700
val loss = 2.215778112411499
training loss = 1.4889445304870605 5800
val loss = 2.2298531532287598
training loss = 1.5132757425308228 5900
val loss = 2.430281162261963
training loss = 1.4485465288162231 6000
val loss = 2.2279787063598633
training loss = 1.4399757385253906 6100
val loss = 2.193946123123169
training loss = 1.4293406009674072 6200
val loss = 2.2071328163146973
training loss = 1.4382573366165161 6300
val loss = 2.163132667541504
training loss = 1.4180324077606201 6400
val loss = 2.1938161849975586
training loss = 1.4137860536575317 6500
val loss = 2.188783884048462
training loss = 1.410879135131836 6600
val loss = 2.1789815425872803
training loss = 1.4077637195587158 6700
val loss = 2.180391311645508
training loss = 1.4834092855453491 6800
val loss = 2.123189926147461
training loss = 1.4034382104873657 6900
val loss = 2.1754202842712402
training loss = 1.4015427827835083 7000
val loss = 2.171760082244873
training loss = 1.4008691310882568 7100
val loss = 2.1577987670898438
training loss = 1.3986735343933105 7200
val loss = 2.167130470275879
training loss = 1.4275691509246826 7300
val loss = 2.11405873298645
training loss = 1.3963981866836548 7400
val loss = 2.1632258892059326
training loss = 1.401816487312317 7500
val loss = 2.1287758350372314
training loss = 1.394576907157898 7600
val loss = 2.162142753601074
training loss = 1.3935927152633667 7700
val loss = 2.1583504676818848
training loss = 1.3933579921722412 7800
val loss = 2.1640450954437256
training loss = 1.392151951789856 7900
val loss = 2.1551895141601562
training loss = 1.40557861328125 8000
val loss = 2.119072437286377
training loss = 1.3909564018249512 8100
val loss = 2.1534345149993896
training loss = 1.3902688026428223 8200
val loss = 2.1514508724212646
training loss = 1.3910512924194336 8300
val loss = 2.165653705596924
training loss = 1.3892635107040405 8400
val loss = 2.149148941040039
training loss = 1.6466224193572998 8500
val loss = 2.1579983234405518
training loss = 1.388383388519287 8600
val loss = 2.1458935737609863
training loss = 1.38784658908844 8700
val loss = 2.1436471939086914
training loss = 1.3875889778137207 8800
val loss = 2.145526170730591
training loss = 1.3870267868041992 8900
val loss = 2.1440718173980713
training loss = 1.3875895738601685 9000
val loss = 2.155970335006714
training loss = 1.386364221572876 9100
val loss = 2.1422290802001953
training loss = 1.3859241008758545 9200
val loss = 2.1427035331726074
training loss = 1.385975956916809 9300
val loss = 2.147193670272827
training loss = 1.3852850198745728 9400
val loss = 2.139768600463867
training loss = 1.3895907402038574 9500
val loss = 2.1737704277038574
training loss = 1.3847386837005615 9600
val loss = 2.138204574584961
training loss = 1.3845049142837524 9700
val loss = 2.1318135261535645
training loss = 1.384332537651062 9800
val loss = 2.138516902923584
training loss = 1.3838666677474976 9900
val loss = 2.13602876663208
training loss = 1.3878029584884644 10000
val loss = 2.173325777053833
training loss = 1.3834229707717896 10100
val loss = 2.134422779083252
training loss = 1.383072853088379 10200
val loss = 2.131801128387451
training loss = 1.3830621242523193 10300
val loss = 2.1363065242767334
training loss = 1.3826125860214233 10400
val loss = 2.1327133178710938
training loss = 1.3850209712982178 10500
val loss = 2.1555778980255127
training loss = 1.3821923732757568 10600
val loss = 2.1315529346466064
training loss = 1.3824418783187866 10700
val loss = 2.143308401107788
training loss = 1.3818199634552002 10800
val loss = 2.13010835647583
training loss = 1.3821271657943726 10900
val loss = 2.1186716556549072
training loss = 1.381628394126892 11000
val loss = 2.126023054122925
training loss = 1.3811792135238647 11100
val loss = 2.1284852027893066
training loss = 1.4031952619552612 11200
val loss = 2.0902562141418457
training loss = 1.3808917999267578 11300
val loss = 2.1269021034240723
training loss = 1.3805910348892212 11400
val loss = 2.1250743865966797
training loss = 1.3807988166809082 11500
val loss = 2.1206016540527344
training loss = 1.3802552223205566 11600
val loss = 2.1256825923919678
training loss = 1.3818494081497192 11700
val loss = 2.1440377235412598
training loss = 1.379957675933838 11800
val loss = 2.124959945678711
training loss = 1.43690824508667 11900
val loss = 2.293870449066162
training loss = 1.379758596420288 12000
val loss = 2.1246063709259033
training loss = 1.3794575929641724 12100
val loss = 2.1231513023376465
training loss = 1.382486343383789 12200
val loss = 2.10191011428833
training loss = 1.3792067766189575 12300
val loss = 2.1227059364318848
training loss = 1.4015161991119385 12400
val loss = 2.0812923908233643
training loss = 1.378944993019104 12500
val loss = 2.121091365814209
training loss = 1.5963839292526245 12600
val loss = 2.114211082458496
training loss = 1.3787528276443481 12700
val loss = 2.1220908164978027
training loss = 1.3784631490707397 12800
val loss = 2.1202962398529053
training loss = 1.380183458328247 12900
val loss = 2.1385223865509033
training loss = 1.3782777786254883 13000
val loss = 2.1196320056915283
training loss = 1.5022839307785034 13100
val loss = 2.073401689529419
training loss = 1.3781726360321045 13200
val loss = 2.1205952167510986
training loss = 1.3778657913208008 13300
val loss = 2.1181960105895996
training loss = 1.383801817893982 13400
val loss = 2.159945249557495
training loss = 1.377663493156433 13500
val loss = 2.1173038482666016
training loss = 1.3808740377426147 13600
val loss = 2.149643898010254
training loss = 1.3774640560150146 13700
val loss = 2.1166672706604004
training loss = 1.38710355758667 13800
val loss = 2.1769309043884277
training loss = 1.3773038387298584 13900
val loss = 2.115701198577881
training loss = 1.3770431280136108 14000
val loss = 2.1160905361175537
training loss = 1.377441644668579 14100
val loss = 2.1229000091552734
training loss = 1.3768820762634277 14200
val loss = 2.1149158477783203
training loss = 1.3789467811584473 14300
val loss = 2.103475570678711
training loss = 1.376776933670044 14400
val loss = 2.114835739135742
training loss = 1.3765361309051514 14500
val loss = 2.1141600608825684
training loss = 1.3767282962799072 14600
val loss = 2.11421275138855
training loss = 1.3763983249664307 14700
val loss = 2.113342761993408
training loss = 1.444499135017395 14800
val loss = 2.071044921875
training loss = 1.3763359785079956 14900
val loss = 2.112168788909912
training loss = 1.376086950302124 15000
val loss = 2.11208438873291
training loss = 1.3793498277664185 15100
val loss = 2.0916428565979004
training loss = 1.3759833574295044 15200
val loss = 2.111940860748291
training loss = 1.3758131265640259 15300
val loss = 2.114201068878174
training loss = 1.3761985301971436 15400
val loss = 2.1072921752929688
training loss = 1.3757390975952148 15500
val loss = 2.110873222351074
training loss = 1.3755385875701904 15600
val loss = 2.1098716259002686
training loss = 1.3757903575897217 15700
val loss = 2.113050699234009
training loss = 1.3754496574401855 15800
val loss = 2.109886646270752
training loss = 1.385921835899353 15900
val loss = 2.080331802368164
training loss = 1.375353455543518 16000
val loss = 2.1087160110473633
training loss = 1.3811861276626587 16100
val loss = 2.149320125579834
training loss = 1.3752620220184326 16200
val loss = 2.1116585731506348
training loss = 1.374997854232788 16300
val loss = 2.107313632965088
training loss = 1.3752834796905518 16400
val loss = 2.104050874710083
training loss = 1.3748798370361328 16500
val loss = 2.108034133911133
training loss = 1.3805938959121704 16600
val loss = 2.1453638076782227
training loss = 1.3747819662094116 16700
val loss = 2.107698917388916
training loss = 1.3758876323699951 16800
val loss = 2.1351969242095947
training loss = 1.3746910095214844 16900
val loss = 2.1081721782684326
training loss = 1.37462317943573 17000
val loss = 2.1011104583740234
training loss = 1.374668836593628 17100
val loss = 2.10829496383667
training loss = 1.3743914365768433 17200
val loss = 2.10617733001709
training loss = 1.4030200242996216 17300
val loss = 2.0636563301086426
training loss = 1.374403476715088 17400
val loss = 2.106511354446411
training loss = 1.3741744756698608 17500
val loss = 2.1053850650787354
training loss = 1.3799256086349487 17600
val loss = 2.0782103538513184
training loss = 1.3741099834442139 17700
val loss = 2.1049022674560547
training loss = 1.7913625240325928 17800
val loss = 2.7745532989501953
training loss = 1.3740978240966797 17900
val loss = 2.1031699180603027
training loss = 1.373844861984253 18000
val loss = 2.104013442993164
training loss = 1.3756591081619263 18100
val loss = 2.1230947971343994
training loss = 1.3737694025039673 18200
val loss = 2.1036806106567383
training loss = 1.3931553363800049 18300
val loss = 2.0640554428100586
training loss = 1.3736735582351685 18400
val loss = 2.103192090988159
training loss = 1.5574631690979004 18500
val loss = 2.092909812927246
training loss = 1.3737058639526367 18600
val loss = 2.1049249172210693
training loss = 1.3734503984451294 18700
val loss = 2.1026859283447266
training loss = 1.392226219177246 18800
val loss = 2.1833739280700684
training loss = 1.373453974723816 18900
val loss = 2.1028075218200684
training loss = 1.3732420206069946 19000
val loss = 2.1020007133483887
training loss = 1.3746063709259033 19100
val loss = 2.0891666412353516
training loss = 1.3732256889343262 19200
val loss = 2.1017727851867676
training loss = 1.374787449836731 19300
val loss = 2.1215262413024902
training loss = 1.3732645511627197 19400
val loss = 2.1036930084228516
training loss = 1.3730028867721558 19500
val loss = 2.1009864807128906
training loss = 1.4380484819412231 19600
val loss = 2.0588419437408447
training loss = 1.3730807304382324 19700
val loss = 2.1006293296813965
training loss = 1.3728519678115845 19800
val loss = 2.1004137992858887
training loss = 1.396939992904663 19900
val loss = 2.058422088623047
training loss = 1.3727936744689941 20000
val loss = 2.0996460914611816
training loss = 1.976237177848816 20100
val loss = 2.3099751472473145
training loss = 1.3728541135787964 20200
val loss = 2.1018853187561035
training loss = 1.372598648071289 20300
val loss = 2.0995898246765137
training loss = 1.3729859590530396 20400
val loss = 2.0976192951202393
training loss = 1.3725253343582153 20500
val loss = 2.0993826389312744
training loss = 1.431665301322937 20600
val loss = 2.056807518005371
training loss = 1.3724967241287231 20700
val loss = 2.0984914302825928
training loss = 1.3725203275680542 20800
val loss = 2.092097759246826
training loss = 1.3724361658096313 20900
val loss = 2.099872350692749
training loss = 1.3722118139266968 21000
val loss = 2.099109649658203
training loss = 1.3726345300674438 21100
val loss = 2.1056697368621826
training loss = 1.3721479177474976 21200
val loss = 2.0979042053222656
training loss = 1.3812090158462524 21300
val loss = 2.148512125015259
training loss = 1.3721590042114258 21400
val loss = 2.0978660583496094
training loss = 1.3720144033432007 21500
val loss = 2.1005020141601562
training loss = 1.3721866607666016 21600
val loss = 2.101574659347534
training loss = 1.3718985319137573 21700
val loss = 2.0978596210479736
training loss = 1.3722574710845947 21800
val loss = 2.0917367935180664
training loss = 1.3718148469924927 21900
val loss = 2.09688401222229
training loss = 1.3727055788040161 22000
val loss = 2.1089415550231934
training loss = 1.3717584609985352 22100
val loss = 2.0965771675109863
training loss = 1.3737819194793701 22200
val loss = 2.080233573913574
training loss = 1.371696949005127 22300
val loss = 2.09661865234375
training loss = 1.4309759140014648 22400
val loss = 2.265902042388916
training loss = 1.3716927766799927 22500
val loss = 2.0957870483398438
training loss = 1.3714982271194458 22600
val loss = 2.0970005989074707
training loss = 1.3718209266662598 22700
val loss = 2.101508855819702
training loss = 1.371437668800354 22800
val loss = 2.095675468444824
training loss = 1.3728652000427246 22900
val loss = 2.081315517425537
training loss = 1.3713741302490234 23000
val loss = 2.09543514251709
training loss = 1.376298427581787 23100
val loss = 2.0698440074920654
training loss = 1.3713210821151733 23200
val loss = 2.095278263092041
training loss = 1.3727431297302246 23300
val loss = 2.0881001949310303
training loss = 1.3713027238845825 23400
val loss = 2.0943496227264404
training loss = 1.376187801361084 23500
val loss = 2.0677895545959473
training loss = 1.3712835311889648 23600
val loss = 2.0927865505218506
training loss = 1.3711034059524536 23700
val loss = 2.0909295082092285
training loss = 1.3712687492370605 23800
val loss = 2.0915164947509766
training loss = 1.3709924221038818 23900
val loss = 2.0924081802368164
training loss = 1.3712084293365479 24000
val loss = 2.0985872745513916
training loss = 1.3709264993667603 24100
val loss = 2.096188545227051
training loss = 1.3711578845977783 24200
val loss = 2.098726272583008
training loss = 1.3708325624465942 24300
val loss = 2.094151020050049
training loss = 1.3709979057312012 24400
val loss = 2.0931262969970703
training loss = 1.370756983757019 24500
val loss = 2.0933878421783447
training loss = 1.371278166770935 24600
val loss = 2.0868945121765137
training loss = 1.3707245588302612 24700
val loss = 2.0930073261260986
training loss = 1.371114730834961 24800
val loss = 2.0894110202789307
training loss = 1.3706700801849365 24900
val loss = 2.0930542945861816
training loss = 1.396732211112976 25000
val loss = 2.189702033996582
training loss = 1.3706220388412476 25100
val loss = 2.0931315422058105
training loss = 1.4223518371582031 25200
val loss = 2.0533597469329834
training loss = 1.3705999851226807 25300
val loss = 2.091494083404541
training loss = 1.370949387550354 25400
val loss = 2.0823376178741455
training loss = 1.370656132698059 25500
val loss = 2.0891828536987305
training loss = 1.370361566543579 25600
val loss = 2.092198371887207
training loss = 1.3705480098724365 25700
val loss = 2.092564344406128
training loss = 1.3702985048294067 25800
val loss = 2.091374397277832
training loss = 1.37075936794281 25900
val loss = 2.0987939834594727
training loss = 1.370236873626709 26000
val loss = 2.0914225578308105
training loss = 1.371500015258789 26100
val loss = 2.0781757831573486
training loss = 1.370192289352417 26200
val loss = 2.0912556648254395
training loss = 1.3708395957946777 26300
val loss = 2.102954387664795
training loss = 1.37013840675354 26400
val loss = 2.0914242267608643
training loss = 1.371969223022461 26500
val loss = 2.1174254417419434
training loss = 1.370157241821289 26600
val loss = 2.0914859771728516
training loss = 1.3699862957000732 26700
val loss = 2.088531970977783
training loss = 1.3701364994049072 26800
val loss = 2.0934395790100098
training loss = 1.369928002357483 26900
val loss = 2.088197946548462
training loss = 1.3702107667922974 27000
val loss = 2.086179256439209
training loss = 1.369841456413269 27100
val loss = 2.090364694595337
training loss = 1.3703266382217407 27200
val loss = 2.0912463665008545
training loss = 1.3699264526367188 27300
val loss = 2.0906083583831787
training loss = 1.369742512702942 27400
val loss = 2.0900607109069824
training loss = 1.3739603757858276 27500
val loss = 2.120461940765381
training loss = 1.3698076009750366 27600
val loss = 2.090151309967041
training loss = 1.3696296215057373 27700
val loss = 2.089303731918335
training loss = 1.3702031373977661 27800
val loss = 2.0831704139709473
training loss = 1.369621992111206 27900
val loss = 2.089647054672241
training loss = 1.3916229009628296 28000
val loss = 2.0514073371887207
training loss = 1.3697036504745483 28100
val loss = 2.090097188949585
training loss = 1.3695015907287598 28200
val loss = 2.0893003940582275
training loss = 1.370695948600769 28300
val loss = 2.0767509937286377
training loss = 1.3694740533828735 28400
val loss = 2.089165210723877
training loss = 1.3827868700027466 28500
val loss = 2.053502082824707
training loss = 1.3694449663162231 28600
val loss = 2.0889101028442383
training loss = 1.4523557424545288 28700
val loss = 2.292574167251587
training loss = 1.3694112300872803 28800
val loss = 2.0887231826782227
training loss = 1.5273470878601074 28900
val loss = 2.4049112796783447
training loss = 1.3694398403167725 29000
val loss = 2.0908730030059814
training loss = 1.369213342666626 29100
val loss = 2.0889596939086914
training loss = 1.3696080446243286 29200
val loss = 2.0955166816711426
training loss = 1.3691654205322266 29300
val loss = 2.088547706604004
training loss = 1.3717846870422363 29400
val loss = 2.069718360900879
training loss = 1.3691706657409668 29500
val loss = 2.088440179824829
training loss = 1.50941002368927 29600
val loss = 2.3788986206054688
training loss = 1.369165062904358 29700
val loss = 2.088390827178955
training loss = 1.3692877292633057 29800
val loss = 2.080704689025879
training loss = 1.369200348854065 29900
val loss = 2.0863475799560547
training loss = 1.368947982788086 30000
val loss = 2.0878162384033203
reduced chi^2 level 2 = 1.3689463138580322
Constrained alpha: 2.0687294006347656
Constrained beta: 4.19114351272583
Constrained gamma: 12.223965644836426
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 888.3898,  903.6779,  962.6405,  945.9717,  984.7241, 1058.8867,
        1120.9976, 1156.1748, 1207.2885, 1127.2831, 1161.4392, 1150.6066,
        1266.6016, 1255.5220, 1389.0516, 1439.0431, 1420.2001, 1410.1332,
        1561.6478, 1518.9988, 1583.2891, 1564.4032, 1556.6241, 1622.9144,
        1712.5865, 1674.0914, 1620.2871, 1723.1193, 1619.0806, 1648.7150,
        1662.0857, 1770.0527, 1701.4856, 1800.9167, 1726.6837, 1671.5328,
        1588.0718, 1643.1193, 1605.8154, 1665.5997, 1609.0563, 1477.8235,
        1519.2554, 1419.6295, 1391.1111, 1358.2482, 1306.3397, 1248.8934,
        1137.4441, 1171.6884, 1045.2194,  928.5067,  931.3079,  936.7918,
         866.5237,  926.3780,  829.1436,  710.5652,  610.9266,  554.0252,
         543.5808,  472.7376,  447.4145,  375.1751,  388.8712,  350.1288,
         288.2492,  247.1678,  220.0202,  169.7081,  163.1108,  159.8551,
         141.2350,   90.0961,  106.9240,   67.0126,   63.7229,   39.8569,
          42.5435,   39.3829,   25.0297,   34.2579,   32.8246])]
2644.4732100297097
2.314413809313497 0.9992578980968436 79.73692736590785
val isze = 8
idinces = [12 13 37 72 24 28  8 17 70 52 60 76  4 58 56 34 20 16 10 21 51 22 33 43
 11 79 14 68 32 48 41 53  6 82 59 35 55 62 44 36  7 45  1  5 47 30 26 65
 38 74 29 39 15 66 61 54 69 49 31 25 64 40 57  0 67  2 75 50 19 63 77 42
 73 23  3 27 71 78 18 80 81 46  9]
we are doing training validation split
training loss = 58.36198043823242 100
val loss = 36.752296447753906
training loss = 27.956928253173828 200
val loss = 18.762657165527344
training loss = 12.5155668258667 300
val loss = 8.420201301574707
training loss = 6.795802593231201 400
val loss = 4.706142425537109
training loss = 5.016617774963379 500
val loss = 3.8212249279022217
training loss = 4.045609474182129 600
val loss = 3.259016513824463
training loss = 3.3739168643951416 700
val loss = 2.779813766479492
training loss = 2.9410698413848877 800
val loss = 2.4337568283081055
training loss = 2.683894395828247 900
val loss = 2.1915230751037598
training loss = 2.539179801940918 1000
val loss = 2.02817964553833
training loss = 2.4551327228546143 1100
val loss = 1.9190542697906494
training loss = 2.3988046646118164 1200
val loss = 1.8440444469451904
training loss = 2.3533034324645996 1300
val loss = 1.789268970489502
training loss = 2.3112456798553467 1400
val loss = 1.7460848093032837
training loss = 2.272789478302002 1500
val loss = 1.6936237812042236
training loss = 2.2278659343719482 1600
val loss = 1.6777606010437012
training loss = 2.187188148498535 1700
val loss = 1.6638309955596924
training loss = 2.1435282230377197 1800
val loss = 1.6252093315124512
training loss = 2.1027472019195557 1900
val loss = 1.607409119606018
training loss = 2.06394100189209 2000
val loss = 1.5869029760360718
training loss = 2.0283005237579346 2100
val loss = 1.5696771144866943
training loss = 1.9957249164581299 2200
val loss = 1.5602715015411377
training loss = 1.9665031433105469 2300
val loss = 1.550632357597351
training loss = 1.9404712915420532 2400
val loss = 1.543764591217041
training loss = 1.9171024560928345 2500
val loss = 1.537773609161377
training loss = 1.8963078260421753 2600
val loss = 1.5351507663726807
training loss = 1.877445936203003 2700
val loss = 1.5333788394927979
training loss = 1.860419511795044 2800
val loss = 1.533257246017456
training loss = 1.84473717212677 2900
val loss = 1.5357861518859863
training loss = 1.8310140371322632 3000
val loss = 1.540771484375
training loss = 1.8167695999145508 3100
val loss = 1.5429885387420654
training loss = 1.8127466440200806 3200
val loss = 1.5600477457046509
training loss = 1.792281150817871 3300
val loss = 1.5529296398162842
training loss = 1.9091931581497192 3400
val loss = 1.7285678386688232
training loss = 1.7706762552261353 3500
val loss = 1.564062476158142
training loss = 1.7608048915863037 3600
val loss = 1.5700455904006958
training loss = 1.751736044883728 3700
val loss = 1.5757631063461304
training loss = 1.7430428266525269 3800
val loss = 1.5812792778015137
training loss = 1.7351099252700806 3900
val loss = 1.5857051610946655
training loss = 1.727530598640442 4000
val loss = 1.5923724174499512
training loss = 1.7640464305877686 4100
val loss = 1.642398476600647
training loss = 1.7140922546386719 4200
val loss = 1.6028110980987549
training loss = 1.708009123802185 4300
val loss = 1.6085221767425537
training loss = 1.7026236057281494 4400
val loss = 1.6120797395706177
training loss = 1.6972850561141968 4500
val loss = 1.6179370880126953
training loss = 1.6927480697631836 4600
val loss = 1.6234641075134277
training loss = 1.6880611181259155 4700
val loss = 1.6272199153900146
training loss = 1.6938159465789795 4800
val loss = 1.6576145887374878
training loss = 1.6801996231079102 4900
val loss = 1.6358954906463623
training loss = 1.676653265953064 5000
val loss = 1.6399033069610596
training loss = 1.684822916984558 5100
val loss = 1.6727639436721802
training loss = 1.6704963445663452 5200
val loss = 1.6470119953155518
training loss = 1.6677029132843018 5300
val loss = 1.6509315967559814
training loss = 1.66553795337677 5400
val loss = 1.6514430046081543
training loss = 1.662871241569519 5500
val loss = 1.657012701034546
training loss = 1.663959264755249 5600
val loss = 1.675541639328003
training loss = 1.6587638854980469 5700
val loss = 1.6625478267669678
training loss = 1.6568853855133057 5800
val loss = 1.6650300025939941
training loss = 1.6561226844787598 5900
val loss = 1.6626474857330322
training loss = 1.6536401510238647 6000
val loss = 1.6690174341201782
training loss = 1.6654311418533325 6100
val loss = 1.7083674669265747
training loss = 1.6508691310882568 6200
val loss = 1.6721246242523193
training loss = 1.6497315168380737 6300
val loss = 1.67680823802948
training loss = 1.648547649383545 6400
val loss = 1.67335844039917
training loss = 1.6473543643951416 6500
val loss = 1.6766636371612549
training loss = 1.6476882696151733 6600
val loss = 1.6860530376434326
training loss = 1.6454144716262817 6700
val loss = 1.6785545349121094
training loss = 1.686050534248352 6800
val loss = 1.7740967273712158
training loss = 1.6437277793884277 6900
val loss = 1.6796685457229614
training loss = 1.6429048776626587 7000
val loss = 1.6810510158538818
training loss = 1.6426191329956055 7100
val loss = 1.684969186782837
training loss = 1.6414780616760254 7200
val loss = 1.681788682937622
training loss = 1.6420388221740723 7300
val loss = 1.675492763519287
training loss = 1.6401927471160889 7400
val loss = 1.682349443435669
training loss = 1.784682035446167 7500
val loss = 1.9596813917160034
training loss = 1.639052152633667 7600
val loss = 1.6819547414779663
training loss = 1.6384408473968506 7700
val loss = 1.6837316751480103
reduced chi^2 level 2 = 1.6380243301391602
Constrained alpha: 1.858185887336731
Constrained beta: 3.23382306098938
Constrained gamma: 23.36028289794922
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 862.4456,  800.4010,  948.2694, 1028.5125, 1002.2623, 1070.3110,
        1154.2289, 1089.5085, 1171.9414, 1153.5009, 1197.7467, 1201.9377,
        1215.1129, 1249.8557, 1309.7386, 1524.1086, 1447.8643, 1478.5701,
        1535.9272, 1524.8945, 1611.6805, 1600.9547, 1620.9470, 1699.8987,
        1758.9799, 1761.6434, 1580.8658, 1725.8624, 1733.0393, 1696.8077,
        1636.8408, 1760.7239, 1673.5560, 1757.4325, 1693.3175, 1748.3657,
        1669.6096, 1615.0608, 1647.2054, 1701.2056, 1610.9167, 1483.5469,
        1571.6233, 1503.1770, 1354.8121, 1334.1868, 1288.5720, 1293.0657,
        1166.5361, 1195.4031, 1106.1786, 1003.1263,  904.7392,  977.3169,
         928.2233,  858.6572,  835.7374,  747.8820,  595.8776,  531.8275,
         537.2288,  459.8354,  410.3376,  409.2477,  369.3621,  363.5334,
         321.6608,  244.7639,  202.4018,  179.1743,  171.9525,  134.4396,
         146.8449,  108.2690,   95.9530,   77.3825,   61.9319,   42.0662,
          35.5432,   48.1052,   13.2521,   36.0500,   37.6038])]
3038.6005781879444
2.2442258213363924 13.223856695327882 18.171417387861954
val isze = 8
idinces = [ 7 53 80 45 20 64 77 74  1  3 62 59 68 41 51 19 61 75 79 57 55 37 40 82
 71 50 33 42 25  0 36 47  2 30 46 44 66 58 34 48 35 81 14 43 54 12 52 22
 28  4 70 49 32 21 31 39 78 11 17 16  8 29 26 18 24 13 10 65 63 38  5 76
  9 56 67 72  6 73 60 27 15 23 69]
we are doing training validation split
training loss = 230.9443359375 100
val loss = 197.50381469726562
training loss = 8.026870727539062 200
val loss = 4.737005710601807
training loss = 7.917754173278809 300
val loss = 4.921844482421875
training loss = 7.827505111694336 400
val loss = 5.032961368560791
training loss = 7.7447896003723145 500
val loss = 5.150173187255859
training loss = 7.6721014976501465 600
val loss = 5.267834186553955
training loss = 7.609833240509033 700
val loss = 5.381101608276367
training loss = 7.557158470153809 800
val loss = 5.486114501953125
training loss = 7.512645721435547 900
val loss = 5.580047130584717
training loss = 7.474667549133301 1000
val loss = 5.661020278930664
training loss = 7.441657066345215 1100
val loss = 5.728035926818848
training loss = 7.41223669052124 1200
val loss = 5.780821800231934
training loss = 7.385258197784424 1300
val loss = 5.819662094116211
training loss = 7.359824180603027 1400
val loss = 5.845300197601318
training loss = 7.335252285003662 1500
val loss = 5.8587775230407715
training loss = 7.311056613922119 1600
val loss = 5.861356735229492
training loss = 7.286900997161865 1700
val loss = 5.854369640350342
training loss = 7.262561798095703 1800
val loss = 5.839175224304199
training loss = 7.2379045486450195 1900
val loss = 5.81709098815918
training loss = 7.21285343170166 2000
val loss = 5.789340972900391
training loss = 7.187382221221924 2100
val loss = 5.757020950317383
training loss = 7.161494255065918 2200
val loss = 5.721097946166992
training loss = 7.13521671295166 2300
val loss = 5.682392120361328
training loss = 7.108606338500977 2400
val loss = 5.641608715057373
training loss = 7.081728458404541 2500
val loss = 5.599323272705078
training loss = 7.054681301116943 2600
val loss = 5.556062698364258
training loss = 7.02759313583374 2700
val loss = 5.512238502502441
training loss = 7.000601768493652 2800
val loss = 5.4682488441467285
training loss = 6.973891735076904 2900
val loss = 5.424465179443359
training loss = 6.947665214538574 3000
val loss = 5.381231784820557
training loss = 6.922141075134277 3100
val loss = 5.338869571685791
training loss = 6.897551536560059 3200
val loss = 5.297673225402832
training loss = 6.874117374420166 3300
val loss = 5.257902145385742
training loss = 6.852027416229248 3400
val loss = 5.219700813293457
training loss = 6.831399917602539 3500
val loss = 5.183079719543457
training loss = 6.812255382537842 3600
val loss = 5.147871017456055
training loss = 6.7944560050964355 3700
val loss = 5.113686561584473
training loss = 6.7776079177856445 3800
val loss = 5.079730987548828
training loss = 6.760807514190674 3900
val loss = 5.0443010330200195
training loss = 6.741800785064697 4000
val loss = 5.00305700302124
training loss = 6.713323593139648 4100
val loss = 4.941370964050293
training loss = 6.642894268035889 4200
val loss = 4.790807723999023
training loss = 6.386898517608643 4300
val loss = 4.24330997467041
training loss = 5.886907577514648 4400
val loss = 3.470781087875366
training loss = 5.0611467361450195 4500
val loss = 2.5638785362243652
training loss = 3.690058708190918 4600
val loss = 1.4607152938842773
training loss = 2.71795916557312 4700
val loss = 2.074145555496216
training loss = 2.630218744277954 4800
val loss = 2.6062726974487305
training loss = 2.612067937850952 4900
val loss = 2.5922582149505615
training loss = 2.5988810062408447 5000
val loss = 2.550687789916992
training loss = 2.5898308753967285 5100
val loss = 2.525956630706787
training loss = 2.581259250640869 5200
val loss = 2.4988718032836914
training loss = 2.612046003341675 5300
val loss = 2.4624087810516357
training loss = 2.5695879459381104 5400
val loss = 2.4701437950134277
training loss = 2.5649383068084717 5500
val loss = 2.4603137969970703
training loss = 2.563465118408203 5600
val loss = 2.465179204940796
training loss = 2.557107925415039 5700
val loss = 2.4471275806427
training loss = 2.6026222705841064 5800
val loss = 2.4219164848327637
training loss = 2.5505499839782715 5900
val loss = 2.438943862915039
training loss = 2.5478203296661377 6000
val loss = 2.4389564990997314
training loss = 2.5448527336120605 6100
val loss = 2.434025287628174
training loss = 2.542182445526123 6200
val loss = 2.4301223754882812
training loss = 2.5425379276275635 6300
val loss = 2.417273998260498
training loss = 2.537245035171509 6400
val loss = 2.4261398315429688
training loss = 2.552704334259033 6500
val loss = 2.4643478393554688
training loss = 2.5326671600341797 6600
val loss = 2.4222564697265625
training loss = 2.5304782390594482 6700
val loss = 2.4210963249206543
training loss = 2.5287024974823 6800
val loss = 2.415376901626587
training loss = 2.5263140201568604 6900
val loss = 2.418323278427124
training loss = 2.537313461303711 7000
val loss = 2.398437976837158
training loss = 2.522343397140503 7100
val loss = 2.415815591812134
training loss = 2.520530939102173 7200
val loss = 2.4174106121063232
training loss = 2.518589735031128 7300
val loss = 2.411816120147705
training loss = 2.5167510509490967 7400
val loss = 2.412200927734375
training loss = 2.546546459197998 7500
val loss = 2.3880703449249268
training loss = 2.5131986141204834 7600
val loss = 2.410041332244873
training loss = 2.5173447132110596 7700
val loss = 2.431333065032959
training loss = 2.5097925662994385 7800
val loss = 2.4085681438446045
training loss = 2.508122682571411 7900
val loss = 2.4067087173461914
training loss = 2.5065529346466064 8000
val loss = 2.403806209564209
training loss = 2.5049338340759277 8100
val loss = 2.4046616554260254
training loss = 2.6477227210998535 8200
val loss = 2.5894992351531982
training loss = 2.5018486976623535 8300
val loss = 2.4029860496520996
training loss = 2.500337600708008 8400
val loss = 2.4017069339752197
training loss = 2.49888277053833 8500
val loss = 2.399514675140381
training loss = 2.497473955154419 8600
val loss = 2.399679183959961
training loss = 2.4960646629333496 8700
val loss = 2.3998830318450928
training loss = 2.4946646690368652 8800
val loss = 2.3981425762176514
training loss = 2.4933135509490967 8900
val loss = 2.3969242572784424
training loss = 2.4921302795410156 9000
val loss = 2.4011952877044678
training loss = 2.4906954765319824 9100
val loss = 2.3948919773101807
training loss = 2.4894044399261475 9200
val loss = 2.3944883346557617
training loss = 2.488311529159546 9300
val loss = 2.390092372894287
training loss = 2.4869604110717773 9400
val loss = 2.3924121856689453
training loss = 2.828494071960449 9500
val loss = 2.7636096477508545
training loss = 2.4845850467681885 9600
val loss = 2.3898229598999023
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 851.1653,  822.0869,  951.8318, 1003.4972,  974.0930, 1108.0118,
        1127.5195, 1205.9121, 1126.4716, 1136.8740, 1214.0774, 1135.1851,
        1250.9116, 1244.7904, 1317.3789, 1407.8630, 1411.3030, 1483.4335,
        1547.2961, 1555.3529, 1518.5055, 1566.4084, 1603.0352, 1658.3137,
        1633.7510, 1716.7124, 1514.9768, 1701.0341, 1728.8951, 1639.6146,
        1676.6793, 1768.3584, 1679.1730, 1720.9562, 1596.9546, 1704.4929,
        1691.2964, 1590.4188, 1627.8928, 1614.6720, 1558.6066, 1590.1617,
        1571.2815, 1505.6329, 1368.8271, 1306.5360, 1270.1422, 1270.1483,
        1161.7173, 1189.1844, 1073.9304,  957.2615,  976.6640,  885.9677,
         811.4565,  838.9084,  829.6560,  695.8183,  661.4342,  548.0476,
         572.7068,  522.4975,  415.2769,  411.6481,  335.3278,  352.2983,
         291.9492,  253.3971,  198.1734,  185.9596,  162.9683,  129.6569,
         135.1045,  115.3436,   98.5316,   70.2639,   48.0857,   49.9316,
          41.3125,   43.4918,   19.2390,   41.2225,   45.7978])]
2813.0658388478864
3.8095212225522177 13.983244033170067 19.93044867858208
val isze = 8
idinces = [68 18 19 43 24 69 73 21 77  7 64 58 38 23 37  1 79 33 82 39 67 15 74 55
 13 31 20 44 25 80 11 34 48 35 10  6 26 45 56 50 22 61 28 72 62 63 60 12
 65  9  4  2 16 30 70 46 47 78  3 76 27 54 51 40 42 59 52 53 81 36 75 32
  5 17  0 49 57 29 14 41 66 71  8]
we are doing training validation split
training loss = 433.49603271484375 100
val loss = 279.08392333984375
training loss = 8.00271224975586 200
val loss = 6.455411911010742
training loss = 7.500978469848633 300
val loss = 5.682194709777832
training loss = 7.3854146003723145 400
val loss = 5.673712253570557
training loss = 7.2871599197387695 500
val loss = 5.68251895904541
training loss = 7.206055641174316 600
val loss = 5.700507164001465
training loss = 7.139549255371094 700
val loss = 5.721667289733887
training loss = 7.084405899047852 800
val loss = 5.740579128265381
training loss = 7.037573337554932 900
val loss = 5.753696441650391
training loss = 6.996527671813965 1000
val loss = 5.758864402770996
training loss = 6.959380149841309 1100
val loss = 5.755454063415527
training loss = 6.924798488616943 1200
val loss = 5.743753910064697
training loss = 6.89190673828125 1300
val loss = 5.724592208862305
training loss = 6.860162258148193 1400
val loss = 5.699368953704834
training loss = 6.829245090484619 1500
val loss = 5.6696248054504395
training loss = 6.7989821434021 1600
val loss = 5.636483192443848
training loss = 6.769275665283203 1700
val loss = 5.601359844207764
training loss = 6.740077495574951 1800
val loss = 5.565121173858643
training loss = 6.7113471031188965 1900
val loss = 5.528497219085693
training loss = 6.6830620765686035 2000
val loss = 5.492117881774902
training loss = 6.655191421508789 2100
val loss = 5.456194877624512
training loss = 6.627704620361328 2200
val loss = 5.421145439147949
training loss = 6.600571155548096 2300
val loss = 5.386826992034912
training loss = 6.5737504959106445 2400
val loss = 5.353427410125732
training loss = 6.547199249267578 2500
val loss = 5.320939064025879
training loss = 6.520879745483398 2600
val loss = 5.289182662963867
training loss = 6.494748592376709 2700
val loss = 5.258207321166992
training loss = 6.468760967254639 2800
val loss = 5.227842807769775
training loss = 6.442872047424316 2900
val loss = 5.198184013366699
training loss = 6.417043685913086 3000
val loss = 5.168910980224609
training loss = 6.391242027282715 3100
val loss = 5.140427589416504
training loss = 6.365443229675293 3200
val loss = 5.112309455871582
training loss = 6.339644432067871 3300
val loss = 5.08458948135376
training loss = 6.313862323760986 3400
val loss = 5.057309150695801
training loss = 6.288156032562256 3500
val loss = 5.030648708343506
training loss = 6.26263427734375 3600
val loss = 5.004410743713379
training loss = 6.237475872039795 3700
val loss = 4.979325294494629
training loss = 6.2129340171813965 3800
val loss = 4.954895496368408
training loss = 6.189333915710449 3900
val loss = 4.931971549987793
training loss = 6.166999340057373 4000
val loss = 4.910171031951904
training loss = 6.146080017089844 4100
val loss = 4.890287399291992
training loss = 6.127689361572266 4200
val loss = 4.769450664520264
training loss = 6.107873916625977 4300
val loss = 4.853732109069824
training loss = 6.0917744636535645 4400
val loss = 4.557960510253906
training loss = 6.042135715484619 4500
val loss = 4.800049304962158
training loss = 5.8620991706848145 4600
val loss = 5.362611770629883
training loss = 4.753262042999268 4700
val loss = 3.816973924636841
training loss = 3.127277135848999 4800
val loss = 2.5985770225524902
training loss = 2.313183546066284 4900
val loss = 2.1673271656036377
training loss = 2.369262456893921 5000
val loss = 2.864569902420044
training loss = 2.2385895252227783 5100
val loss = 2.084411144256592
training loss = 2.2392497062683105 5200
val loss = 1.8379087448120117
training loss = 2.207923173904419 5300
val loss = 2.0896615982055664
training loss = 2.200383424758911 5400
val loss = 2.1880266666412354
training loss = 2.1904489994049072 5500
val loss = 2.1001224517822266
training loss = 2.184109687805176 5600
val loss = 2.094876289367676
training loss = 2.1795499324798584 5700
val loss = 2.0421621799468994
training loss = 2.1742138862609863 5800
val loss = 2.095767021179199
training loss = 2.2614176273345947 5900
val loss = 2.7539122104644775
training loss = 2.1664910316467285 6000
val loss = 2.0956833362579346
training loss = 2.1631596088409424 6100
val loss = 2.0951743125915527
training loss = 2.1601548194885254 6200
val loss = 2.111927032470703
training loss = 2.157224416732788 6300
val loss = 2.093008041381836
training loss = 2.155475616455078 6400
val loss = 2.0343403816223145
training loss = 2.1519274711608887 6500
val loss = 2.092170238494873
training loss = 2.149468183517456 6600
val loss = 2.076416492462158
training loss = 2.1470541954040527 6700
val loss = 2.0864133834838867
training loss = 2.1447856426239014 6800
val loss = 2.087691307067871
training loss = 2.144270658493042 6900
val loss = 2.0088248252868652
training loss = 2.140221357345581 7000
val loss = 2.091978073120117
training loss = 2.1379592418670654 7100
val loss = 2.0848026275634766
training loss = 2.135727882385254 7200
val loss = 2.1010921001434326
training loss = 2.1333577632904053 7300
val loss = 2.0819995403289795
training loss = 2.1561477184295654 7400
val loss = 1.8111274242401123
training loss = 2.12853741645813 7500
val loss = 2.082981586456299
training loss = 2.1259541511535645 7600
val loss = 2.0752620697021484
training loss = 2.1233468055725098 7700
val loss = 2.0716216564178467
training loss = 2.120615243911743 7800
val loss = 2.0770840644836426
training loss = 2.12138295173645 7900
val loss = 2.193100929260254
training loss = 2.1145212650299072 8000
val loss = 2.0700674057006836
training loss = 2.1111934185028076 8100
val loss = 2.072511672973633
training loss = 2.1109628677368164 8200
val loss = 1.9668315649032593
training loss = 2.104067325592041 8300
val loss = 2.068389415740967
training loss = 2.1001358032226562 8400
val loss = 2.060927391052246
training loss = 2.096184730529785 8500
val loss = 2.0519728660583496
training loss = 2.0919408798217773 8600
val loss = 2.063537120819092
training loss = 2.0882620811462402 8700
val loss = 2.016007900238037
training loss = 2.083354949951172 8800
val loss = 2.0594260692596436
training loss = 2.0912373065948486 8900
val loss = 1.8630175590515137
training loss = 2.074439764022827 9000
val loss = 2.063563108444214
training loss = 2.0698649883270264 9100
val loss = 2.0535008907318115
training loss = 2.0925166606903076 9200
val loss = 2.3819785118103027
training loss = 2.0606162548065186 9300
val loss = 2.049741268157959
training loss = 2.0558133125305176 9400
val loss = 2.0467450618743896
training loss = 2.0515995025634766 9500
val loss = 2.0837626457214355
training loss = 2.0464417934417725 9600
val loss = 2.0430850982666016
training loss = 2.1068642139434814 9700
val loss = 2.5750973224639893
training loss = 2.036888360977173 9800
val loss = 2.040736198425293
training loss = 2.0319266319274902 9900
val loss = 2.0375046730041504
training loss = 2.027350425720215 10000
val loss = 2.048891305923462
training loss = 2.022571325302124 10100
val loss = 2.0336496829986572
training loss = 2.017526626586914 10200
val loss = 2.032215118408203
training loss = 2.012904644012451 10300
val loss = 2.0059776306152344
training loss = 2.007761240005493 10400
val loss = 2.0292398929595947
training loss = 2.0035176277160645 10500
val loss = 2.0894532203674316
training loss = 1.9972217082977295 10600
val loss = 2.0159292221069336
training loss = 1.9914809465408325 10700
val loss = 2.0260207653045654
training loss = 1.992445468902588 10800
val loss = 1.8827182054519653
training loss = 1.9792563915252686 10900
val loss = 2.024944305419922
training loss = 1.9721826314926147 11000
val loss = 2.023897647857666
training loss = 1.965522050857544 11100
val loss = 1.97395920753479
training loss = 1.9568002223968506 11200
val loss = 2.0209298133850098
training loss = 1.9620720148086548 11300
val loss = 2.241964340209961
training loss = 1.9399573802947998 11400
val loss = 2.020345449447632
training loss = 1.9317586421966553 11500
val loss = 2.0113255977630615
training loss = 2.283649444580078 11600
val loss = 3.431640148162842
training loss = 1.9168648719787598 11700
val loss = 2.0083861351013184
training loss = 1.910069465637207 11800
val loss = 2.0023064613342285
training loss = 1.90435791015625 11900
val loss = 2.046574115753174
training loss = 1.8977344036102295 12000
val loss = 2.0001108646392822
training loss = 1.8934175968170166 12100
val loss = 1.932949185371399
training loss = 1.8866853713989258 12200
val loss = 2.007982015609741
training loss = 1.8815600872039795 12300
val loss = 2.002148151397705
training loss = 1.957527756690979 12400
val loss = 1.593641757965088
training loss = 1.872186303138733 12500
val loss = 2.002084255218506
training loss = 1.86781907081604 12600
val loss = 2.004782199859619
training loss = 1.864268183708191 12700
val loss = 1.9780900478363037
training loss = 1.8603471517562866 12800
val loss = 2.00813364982605
training loss = 2.057295083999634 12900
val loss = 2.9796907901763916
training loss = 1.8539339303970337 13000
val loss = 2.007286310195923
training loss = 1.851065754890442 13100
val loss = 2.0135951042175293
training loss = 1.8499505519866943 13200
val loss = 2.079169750213623
training loss = 1.8462975025177002 13300
val loss = 2.0174291133880615
training loss = 2.0091373920440674 13400
val loss = 2.871568202972412
training loss = 1.8424650430679321 13500
val loss = 2.018068790435791
training loss = 1.8407928943634033 13600
val loss = 2.0238735675811768
training loss = 1.8395373821258545 13700
val loss = 2.0046377182006836
training loss = 1.8380967378616333 13800
val loss = 2.0269432067871094
training loss = 1.837389349937439 13900
val loss = 1.9920437335968018
training loss = 1.8360077142715454 14000
val loss = 2.029426097869873
training loss = 1.8350915908813477 14100
val loss = 2.0327529907226562
training loss = 1.8358006477355957 14200
val loss = 1.9706329107284546
training loss = 1.8336505889892578 14300
val loss = 2.0349292755126953
training loss = 2.0996692180633545 14400
val loss = 3.187955379486084
training loss = 1.832516074180603 14500
val loss = 2.033566951751709
training loss = 1.8319917917251587 14600
val loss = 2.039146900177002
training loss = 1.8326116800308228 14700
val loss = 2.095141887664795
training loss = 1.831181526184082 14800
val loss = 2.0414247512817383
training loss = 1.8441210985183716 14900
val loss = 1.8570998907089233
reduced chi^2 level 2 = 1.8329455852508545
Constrained alpha: 1.9300881624221802
Constrained beta: 1.6393399238586426
Constrained gamma: 13.24317741394043
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 869.0557,  854.0952,  947.9566,  962.5134,  981.9785, 1098.5762,
        1024.6840, 1097.3062, 1164.4783, 1149.1237, 1203.6936, 1185.8285,
        1289.9888, 1283.5601, 1390.9094, 1399.2975, 1394.1158, 1436.8226,
        1555.5238, 1515.8988, 1572.5527, 1587.4728, 1641.3777, 1623.1160,
        1633.4794, 1707.5477, 1607.9449, 1720.3853, 1732.6987, 1626.9928,
        1658.2688, 1771.6876, 1756.0989, 1757.1727, 1710.1024, 1796.7012,
        1679.5112, 1531.5499, 1595.7985, 1691.1649, 1626.9493, 1629.7075,
        1446.4653, 1432.4153, 1355.7789, 1290.8268, 1297.9603, 1222.3861,
        1248.2552, 1216.2349, 1066.2323, 1005.3282,  930.4077,  899.1671,
         908.7062,  886.8614,  815.7966,  665.1847,  583.6890,  537.2341,
         593.0414,  497.4949,  409.0260,  403.9488,  350.7576,  330.1912,
         289.6253,  264.3071,  205.9106,  179.2238,  158.4972,  142.1334,
         129.1788,  123.8491,  102.6842,   82.7365,   46.5645,   40.3505,
          37.0428,   39.5407,   14.7779,   48.7724,   34.4512])]
2597.159439629072
1.005202128499349 10.551972432660232 85.30155012826054
val isze = 8
idinces = [30  6 36 49 40  1  4 22 57 20 25 80 69 74 44 17 31 78 81 61 43 14 75 82
 24 19 66 71 13 56 76 34  8 38 26 29  9 45 50 63 11 33 41 28 39 18 35  7
 59 15 53 65 10  5 27 68 72 55 79 42 62 52 64 73 77 37 21 47  2 60 12 23
 67 16  3 46 58  0 54 32 51 70 48]
we are doing training validation split
training loss = 14.690654754638672 100
val loss = 19.976070404052734
training loss = 6.718137264251709 200
val loss = 6.413818359375
training loss = 6.68227481842041 300
val loss = 6.592281818389893
training loss = 6.64406681060791 400
val loss = 6.787138938903809
training loss = 6.605796813964844 500
val loss = 6.993946552276611
training loss = 6.568969249725342 600
val loss = 7.203656196594238
training loss = 6.534403324127197 700
val loss = 7.408360958099365
training loss = 6.502407550811768 800
val loss = 7.601222038269043
training loss = 6.4729228019714355 900
val loss = 7.777021408081055
training loss = 6.445653438568115 1000
val loss = 7.932117462158203
training loss = 6.420181751251221 1100
val loss = 8.064441680908203
training loss = 6.396048545837402 1200
val loss = 8.173598289489746
training loss = 6.372796535491943 1300
val loss = 8.260384559631348
training loss = 6.349967002868652 1400
val loss = 8.326607704162598
training loss = 6.327075004577637 1500
val loss = 8.374617576599121
training loss = 6.303571701049805 1600
val loss = 8.407145500183105
training loss = 6.278758525848389 1700
val loss = 8.426939010620117
training loss = 6.251700401306152 1800
val loss = 8.43637466430664
training loss = 6.22108268737793 1900
val loss = 8.437677383422852
training loss = 6.18502950668335 2000
val loss = 8.432655334472656
training loss = 6.140833854675293 2100
val loss = 8.42243766784668
training loss = 6.084649562835693 2200
val loss = 8.406998634338379
training loss = 6.011035919189453 2300
val loss = 8.383613586425781
training loss = 5.912075996398926 2400
val loss = 8.343950271606445
training loss = 5.775147914886475 2500
val loss = 8.269493103027344
training loss = 5.578007221221924 2600
val loss = 8.125697135925293
training loss = 5.281139373779297 2700
val loss = 7.857535362243652
training loss = 4.8229193687438965 2800
val loss = 7.39262580871582
training loss = 4.141511917114258 2900
val loss = 6.655588626861572
training loss = 3.2771174907684326 3000
val loss = 5.611583709716797
training loss = 2.5058956146240234 3100
val loss = 4.434330940246582
training loss = 2.1167187690734863 3200
val loss = 3.5288968086242676
training loss = 2.0157852172851562 3300
val loss = 3.071812629699707
training loss = 1.9989778995513916 3400
val loss = 2.901172161102295
training loss = 1.995235800743103 3500
val loss = 2.8449175357818604
training loss = 1.9931999444961548 3600
val loss = 2.8248677253723145
training loss = 1.9915045499801636 3700
val loss = 2.8152518272399902
training loss = 1.9899553060531616 3800
val loss = 2.8087215423583984
training loss = 1.988509178161621 3900
val loss = 2.8034470081329346
training loss = 1.9871549606323242 4000
val loss = 2.798903465270996
training loss = 1.9858829975128174 4100
val loss = 2.795055627822876
training loss = 1.9846891164779663 4200
val loss = 2.7917449474334717
training loss = 1.9835679531097412 4300
val loss = 2.7889866828918457
training loss = 1.9825154542922974 4400
val loss = 2.7866873741149902
training loss = 1.9815269708633423 4500
val loss = 2.7848381996154785
training loss = 1.9806039333343506 4600
val loss = 2.7833807468414307
training loss = 1.9797391891479492 4700
val loss = 2.7822859287261963
training loss = 1.9789347648620605 4800
val loss = 2.781374931335449
training loss = 1.9783862829208374 4900
val loss = 2.7646002769470215
training loss = 1.9772685766220093 5000
val loss = 2.7771687507629395
training loss = 1.9764404296875 5100
val loss = 2.779404401779175
training loss = 1.975619912147522 5200
val loss = 2.7723426818847656
training loss = 1.9748502969741821 5300
val loss = 2.7795543670654297
training loss = 1.9739062786102295 5400
val loss = 2.767066478729248
training loss = 2.025146007537842 5500
val loss = 3.050783634185791
training loss = 1.9721347093582153 5600
val loss = 2.7615723609924316
training loss = 1.9713548421859741 5700
val loss = 2.7578201293945312
training loss = 1.9717496633529663 5800
val loss = 2.791419506072998
training loss = 1.9695738554000854 5900
val loss = 2.7505247592926025
training loss = 1.9689624309539795 6000
val loss = 2.7594308853149414
training loss = 1.9680054187774658 6100
val loss = 2.7516984939575195
training loss = 1.9672433137893677 6200
val loss = 2.7433369159698486
training loss = 1.9668819904327393 6300
val loss = 2.720682144165039
training loss = 1.9657410383224487 6400
val loss = 2.737224578857422
training loss = 1.9652917385101318 6500
val loss = 2.7530460357666016
training loss = 1.9643237590789795 6600
val loss = 2.732935905456543
training loss = 1.9819096326828003 6700
val loss = 2.8883047103881836
training loss = 1.962996244430542 6800
val loss = 2.7297556400299072
training loss = 1.9659098386764526 6900
val loss = 2.7895216941833496
training loss = 1.9617302417755127 7000
val loss = 2.7234363555908203
training loss = 1.9611749649047852 7100
val loss = 2.723822832107544
training loss = 2.0086417198181152 7200
val loss = 2.997793197631836
training loss = 1.9600040912628174 7300
val loss = 2.7199811935424805
training loss = 1.9594991207122803 7400
val loss = 2.719710350036621
training loss = 1.9591535329818726 7500
val loss = 2.7336907386779785
training loss = 1.9584274291992188 7600
val loss = 2.7169313430786133
training loss = 1.9585355520248413 7700
val loss = 2.695188522338867
training loss = 1.9574310779571533 7800
val loss = 2.7214713096618652
training loss = 1.9569344520568848 7900
val loss = 2.7147464752197266
training loss = 2.0495190620422363 8000
val loss = 2.551712989807129
training loss = 1.955950140953064 8100
val loss = 2.711366891860962
training loss = 1.9555050134658813 8200
val loss = 2.7124342918395996
training loss = 1.969361424446106 8300
val loss = 2.6187920570373535
training loss = 1.9545546770095825 8400
val loss = 2.710174083709717
training loss = 1.9541175365447998 8500
val loss = 2.71073055267334
training loss = 1.9537994861602783 8600
val loss = 2.696239471435547
training loss = 1.9531770944595337 8700
val loss = 2.7089920043945312
training loss = 1.953284502029419 8800
val loss = 2.6881275177001953
training loss = 1.952267050743103 8900
val loss = 2.7029387950897217
training loss = 1.9517902135849 9000
val loss = 2.707860231399536
training loss = 1.967339277267456 9100
val loss = 2.609933614730835
training loss = 1.9508159160614014 9200
val loss = 2.7062578201293945
training loss = 1.9503446817398071 9300
val loss = 2.7062106132507324
training loss = 1.9498088359832764 9400
val loss = 2.707571506500244
training loss = 1.9493123292922974 9500
val loss = 2.70519757270813
training loss = 1.9494129419326782 9600
val loss = 2.676480293273926
training loss = 1.9482060670852661 9700
val loss = 2.705690860748291
training loss = 1.947632074356079 9800
val loss = 2.7037854194641113
training loss = 1.947635531425476 9900
val loss = 2.679701328277588
training loss = 1.9463529586791992 10000
val loss = 2.7030043601989746
training loss = 2.0918872356414795 10100
val loss = 3.2642860412597656
training loss = 1.9448857307434082 10200
val loss = 2.699502468109131
training loss = 1.9440582990646362 10300
val loss = 2.701303482055664
training loss = 1.9487195014953613 10400
val loss = 2.777707815170288
training loss = 1.942121982574463 10500
val loss = 2.700104236602783
training loss = 1.940955400466919 10600
val loss = 2.6986818313598633
training loss = 1.9397786855697632 10700
val loss = 2.710156202316284
training loss = 1.9381200075149536 10800
val loss = 2.6980881690979004
training loss = 1.936752200126648 10900
val loss = 2.675743818283081
training loss = 1.934364914894104 11000
val loss = 2.6969857215881348
training loss = 1.9320363998413086 11100
val loss = 2.69621205329895
training loss = 1.9330025911331177 11200
val loss = 2.645750045776367
training loss = 1.9264817237854004 11300
val loss = 2.693852186203003
training loss = 2.1993467807769775 11400
val loss = 3.5557124614715576
training loss = 1.9200012683868408 11500
val loss = 2.6876163482666016
training loss = 1.9165406227111816 11600
val loss = 2.687966823577881
training loss = 1.913188099861145 11700
val loss = 2.69362735748291
training loss = 1.9095919132232666 11800
val loss = 2.6789746284484863
training loss = 1.999038577079773 11900
val loss = 2.5264711380004883
training loss = 1.9025365114212036 12000
val loss = 2.668386936187744
training loss = 1.8988773822784424 12100
val loss = 2.66245174407959
training loss = 1.9005489349365234 12200
val loss = 2.7293624877929688
training loss = 1.8914262056350708 12300
val loss = 2.6499006748199463
training loss = 1.887442708015442 12400
val loss = 2.6447396278381348
training loss = 1.8834320306777954 12500
val loss = 2.638878345489502
training loss = 1.8792885541915894 12600
val loss = 2.6329078674316406
training loss = 1.898715615272522 12700
val loss = 2.790498971939087
training loss = 1.8706393241882324 12800
val loss = 2.6216583251953125
training loss = 1.8660465478897095 12900
val loss = 2.620741128921509
training loss = 1.8615902662277222 13000
val loss = 2.6153883934020996
training loss = 1.8569494485855103 13100
val loss = 2.608375072479248
training loss = 1.860674262046814 13200
val loss = 2.6885628700256348
training loss = 1.8476331233978271 13300
val loss = 2.5994436740875244
training loss = 1.8428308963775635 13400
val loss = 2.5965933799743652
training loss = 1.8396286964416504 13500
val loss = 2.5636725425720215
training loss = 1.8337228298187256 13600
val loss = 2.58998441696167
training loss = 1.8290328979492188 13700
val loss = 2.5886261463165283
training loss = 1.8246911764144897 13800
val loss = 2.5861825942993164
training loss = 1.8203320503234863 13900
val loss = 2.58268666267395
training loss = 1.8179378509521484 14000
val loss = 2.550189733505249
training loss = 1.8120750188827515 14100
val loss = 2.578554391860962
training loss = 1.8086228370666504 14200
val loss = 2.5985584259033203
training loss = 1.8045638799667358 14300
val loss = 2.5748348236083984
training loss = 1.8010714054107666 14400
val loss = 2.5760278701782227
training loss = 1.7976404428482056 14500
val loss = 2.569399356842041
training loss = 1.794541835784912 14600
val loss = 2.574594497680664
training loss = 1.7915197610855103 14700
val loss = 2.5750656127929688
training loss = 1.8052159547805786 14800
val loss = 2.503330707550049
training loss = 1.7860702276229858 14900
val loss = 2.575779676437378
training loss = 1.7835628986358643 15000
val loss = 2.5691146850585938
training loss = 1.7813012599945068 15100
val loss = 2.5765669345855713
training loss = 1.7791177034378052 15200
val loss = 2.5763254165649414
training loss = 1.8134422302246094 15300
val loss = 2.4823145866394043
training loss = 1.775285243988037 15400
val loss = 2.578282356262207
training loss = 1.773489236831665 15500
val loss = 2.578078031539917
training loss = 1.772038221359253 15600
val loss = 2.586658477783203
training loss = 1.770386815071106 15700
val loss = 2.579707145690918
training loss = 1.8398315906524658 15800
val loss = 2.4839494228363037
training loss = 1.7677119970321655 15900
val loss = 2.5811827182769775
training loss = 1.7664530277252197 16000
val loss = 2.5817136764526367
training loss = 1.7656795978546143 16100
val loss = 2.596219778060913
training loss = 1.7643078565597534 16200
val loss = 2.5840117931365967
training loss = 1.768341064453125 16300
val loss = 2.545407772064209
training loss = 1.7624430656433105 16400
val loss = 2.5857129096984863
training loss = 1.7615782022476196 16500
val loss = 2.582637310028076
training loss = 1.7608956098556519 16600
val loss = 2.5928049087524414
training loss = 1.760071873664856 16700
val loss = 2.587902545928955
training loss = 1.7707421779632568 16800
val loss = 2.5319056510925293
training loss = 1.7587668895721436 16900
val loss = 2.5858333110809326
training loss = 1.7580925226211548 17000
val loss = 2.5901007652282715
training loss = 1.7837187051773071 17100
val loss = 2.518423080444336
training loss = 1.7569780349731445 17200
val loss = 2.591390371322632
training loss = 1.7564221620559692 17300
val loss = 2.591963291168213
training loss = 1.7566585540771484 17400
val loss = 2.5759544372558594
training loss = 1.7554951906204224 17500
val loss = 2.593019485473633
training loss = 1.7550163269042969 17600
val loss = 2.5934977531433105
training loss = 1.7546112537384033 17700
val loss = 2.5952813625335693
training loss = 1.7541780471801758 17800
val loss = 2.5943665504455566
training loss = 1.868720293045044 17900
val loss = 2.964029312133789
training loss = 1.7534046173095703 18000
val loss = 2.5967888832092285
training loss = 1.7530053853988647 18100
val loss = 2.596795082092285
training loss = 1.7527177333831787 18200
val loss = 2.591752052307129
training loss = 1.7523075342178345 18300
val loss = 2.5964298248291016
training loss = 1.7569336891174316 18400
val loss = 2.556222915649414
training loss = 1.751654863357544 18500
val loss = 2.596670627593994
training loss = 1.751353144645691 18600
val loss = 2.585920810699463
training loss = 1.7510358095169067 18700
val loss = 2.5988850593566895
training loss = 1.750736951828003 18800
val loss = 2.6014926433563232
training loss = 1.7505437135696411 18900
val loss = 2.6047682762145996
training loss = 1.7501373291015625 19000
val loss = 2.5980777740478516
training loss = 1.7528492212295532 19100
val loss = 2.566437244415283
training loss = 1.7496063709259033 19200
val loss = 2.598649263381958
reduced chi^2 level 2 = 1.749539852142334
Constrained alpha: 1.8602375984191895
Constrained beta: 2.8299331665039062
Constrained gamma: 14.862361907958984
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 839.8582,  875.8209,  917.2339,  982.1137, 1008.8352, 1116.3047,
        1045.4520, 1125.6853, 1125.4081, 1171.0809, 1155.4865, 1171.6527,
        1328.6655, 1214.2461, 1378.8256, 1435.4409, 1398.9493, 1474.2809,
        1560.6847, 1474.9467, 1538.3309, 1514.6138, 1573.6158, 1584.9484,
        1624.9597, 1673.8209, 1564.8784, 1685.7305, 1776.2249, 1699.6554,
        1694.8033, 1802.2958, 1747.4199, 1764.7018, 1651.6305, 1704.3323,
        1614.6370, 1723.9062, 1677.0615, 1625.0288, 1604.6715, 1558.1495,
        1517.8223, 1487.4260, 1349.9512, 1269.9166, 1270.7615, 1319.5883,
        1117.6012, 1154.6078, 1072.0411, 1031.0117, 1021.6613,  944.1668,
         880.9370,  842.8898,  776.5310,  699.5716,  667.7740,  519.6713,
         528.0001,  436.6146,  445.9135,  413.8174,  374.0556,  353.1354,
         294.5936,  267.6179,  208.3092,  170.6764,  187.9602,  145.9420,
         139.2574,  101.8193,   83.5266,   59.8469,   47.7170,   36.3645,
          26.7206,   40.7846,   17.5809,   47.8181,   30.0976])]
2684.4617316366375
0.03667861065793865 18.791519325639854 69.65114380718451
val isze = 8
idinces = [81 74 40 63  8 22 16 49 48  2 75 13 68 37 53 65 56 78 60  7 57 77 10 44
  4 39 19 30 28 47 38  5 82 29 52 46 72 45 73 43 17  3 55 33 51 35 71 18
 36  9 69 26 41 20 62 11 61 79 21 31 23 32  0  1 76 67 54 27 24 15 70 25
 66 42 34 50 59 14 64 80 58  6 12]
we are doing training validation split
training loss = 164.2165985107422 100
val loss = 150.44052124023438
training loss = 22.429689407348633 200
val loss = 28.567039489746094
training loss = 13.668251991271973 300
val loss = 19.595579147338867
training loss = 13.024243354797363 400
val loss = 18.93682098388672
training loss = 12.29811954498291 500
val loss = 18.21031379699707
training loss = 11.506949424743652 600
val loss = 17.406766891479492
training loss = 10.672622680664062 700
val loss = 16.542701721191406
training loss = 9.82130241394043 800
val loss = 15.638177871704102
training loss = 8.98356819152832 900
val loss = 14.717547416687012
training loss = 8.192997932434082 1000
val loss = 13.809165954589844
training loss = 7.482754230499268 1100
val loss = 12.943594932556152
training loss = 6.880252361297607 1200
val loss = 12.150238037109375
training loss = 6.401310920715332 1300
val loss = 11.452919006347656
training loss = 6.046164512634277 1400
val loss = 10.865681648254395
training loss = 5.799881458282471 1500
val loss = 10.390714645385742
training loss = 5.63761568069458 1600
val loss = 10.01933479309082
training loss = 5.532371997833252 1700
val loss = 9.735767364501953
training loss = 5.461390495300293 1800
val loss = 9.521659851074219
training loss = 5.408933639526367 1900
val loss = 9.359622955322266
training loss = 5.365808486938477 2000
val loss = 9.235088348388672
training loss = 5.327411651611328 2100
val loss = 9.136800765991211
training loss = 5.291713714599609 2200
val loss = 9.056471824645996
training loss = 5.257913112640381 2300
val loss = 8.988231658935547
training loss = 5.225637435913086 2400
val loss = 8.927978515625
training loss = 5.194607257843018 2500
val loss = 8.87283992767334
training loss = 5.164464473724365 2600
val loss = 8.820697784423828
training loss = 5.134698867797852 2700
val loss = 8.769792556762695
training loss = 5.10457706451416 2800
val loss = 8.718339920043945
training loss = 5.072978973388672 2900
val loss = 8.664094924926758
training loss = 5.038056373596191 3000
val loss = 8.603463172912598
training loss = 4.996299743652344 3100
val loss = 8.529600143432617
training loss = 4.940091133117676 3200
val loss = 8.427422523498535
training loss = 4.851415157318115 3300
val loss = 8.260735511779785
training loss = 4.692477226257324 3400
val loss = 7.95627498626709
training loss = 4.420762062072754 3500
val loss = 7.455474853515625
training loss = 4.002162933349609 3600
val loss = 6.7429938316345215
training loss = 3.3659229278564453 3700
val loss = 5.623018264770508
training loss = 2.5813655853271484 3800
val loss = 4.010531902313232
training loss = 2.0716757774353027 3900
val loss = 2.6067864894866943
training loss = 1.942447304725647 4000
val loss = 2.0861761569976807
training loss = 1.9064710140228271 4100
val loss = 2.0292305946350098
training loss = 1.8825798034667969 4200
val loss = 2.073746919631958
training loss = 1.8638769388198853 4300
val loss = 2.12843918800354
training loss = 1.848976731300354 4400
val loss = 2.178257465362549
training loss = 1.8370403051376343 4500
val loss = 2.221769094467163
training loss = 1.8274497985839844 4600
val loss = 2.259449005126953
training loss = 1.819732666015625 4700
val loss = 2.291951894760132
training loss = 1.8135286569595337 4800
val loss = 2.3198792934417725
training loss = 1.8306734561920166 4900
val loss = 2.375462055206299
training loss = 1.8046454191207886 5000
val loss = 2.364095687866211
training loss = 1.8160371780395508 5100
val loss = 2.384948968887329
training loss = 1.7988686561584473 5200
val loss = 2.3941264152526855
training loss = 1.7973673343658447 5300
val loss = 2.406867504119873
training loss = 1.7948849201202393 5400
val loss = 2.415789842605591
training loss = 1.7933716773986816 5500
val loss = 2.422858238220215
training loss = 1.792057991027832 5600
val loss = 2.4322659969329834
training loss = 1.7908096313476562 5700
val loss = 2.4366631507873535
training loss = 1.7906818389892578 5800
val loss = 2.4459452629089355
training loss = 1.7888542413711548 5900
val loss = 2.447093963623047
training loss = 1.8805818557739258 6000
val loss = 2.515627384185791
training loss = 1.7873437404632568 6100
val loss = 2.4552054405212402
training loss = 1.7867777347564697 6200
val loss = 2.457828998565674
training loss = 1.786167025566101 6300
val loss = 2.461941719055176
training loss = 1.7856706380844116 6400
val loss = 2.463437795639038
training loss = 1.8655625581741333 6500
val loss = 2.5177910327911377
training loss = 1.784830927848816 6600
val loss = 2.4672510623931885
training loss = 1.7845017910003662 6700
val loss = 2.4688305854797363
training loss = 1.7857072353363037 6800
val loss = 2.474404811859131
training loss = 1.7838765382766724 6900
val loss = 2.4714460372924805
training loss = 1.7837793827056885 7000
val loss = 2.471473217010498
training loss = 1.7834094762802124 7100
val loss = 2.4740827083587646
training loss = 1.7831779718399048 7200
val loss = 2.4739038944244385
training loss = 2.067173957824707 7300
val loss = 2.772019624710083
training loss = 1.7827939987182617 7400
val loss = 2.4749596118927
training loss = 1.782677412033081 7500
val loss = 2.475034236907959
training loss = 1.7840497493743896 7600
val loss = 2.473719596862793
training loss = 1.7823669910430908 7700
val loss = 2.475656032562256
training loss = 1.7822909355163574 7800
val loss = 2.4754695892333984
training loss = 1.7823221683502197 7900
val loss = 2.4771595001220703
training loss = 1.7820465564727783 8000
val loss = 2.4755585193634033
training loss = 1.8333874940872192 8100
val loss = 2.5063140392303467
training loss = 1.7818514108657837 8200
val loss = 2.475451707839966
training loss = 1.7821530103683472 8300
val loss = 2.473869800567627
training loss = 1.7816838026046753 8400
val loss = 2.4750168323516846
training loss = 1.781653642654419 8500
val loss = 2.4744157791137695
training loss = 1.7815263271331787 8600
val loss = 2.474611282348633
training loss = 1.781474232673645 8700
val loss = 2.4741549491882324
training loss = 1.7814555168151855 8800
val loss = 2.4734489917755127
training loss = 1.7814013957977295 8900
val loss = 2.473029136657715
training loss = 1.7813265323638916 9000
val loss = 2.472788095474243
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 847.6569,  897.6727,  959.6196,  919.4467, 1017.0970, 1084.2357,
        1049.9723, 1134.6991, 1138.0890, 1188.4294, 1205.1738, 1155.2769,
        1203.7212, 1232.4651, 1337.6348, 1475.2441, 1414.3357, 1379.0535,
        1464.7258, 1493.8379, 1566.0494, 1592.1047, 1539.0704, 1596.0320,
        1649.8785, 1688.4023, 1613.7125, 1718.1543, 1722.4890, 1659.2268,
        1654.9418, 1819.0515, 1660.4703, 1747.9521, 1668.5883, 1723.0731,
        1700.1387, 1705.4990, 1606.3707, 1588.7510, 1594.5192, 1572.7374,
        1512.6027, 1557.9055, 1344.3834, 1371.6187, 1322.7987, 1246.7054,
        1184.2085, 1158.7432, 1054.8782, 1043.2649,  964.3561,  944.4275,
         866.2498,  854.5660,  794.2203,  708.1260,  650.2304,  584.0986,
         569.3375,  458.7509,  452.4717,  374.2303,  356.7020,  324.3822,
         305.3833,  238.4872,  210.3336,  185.8439,  148.5731,  137.5918,
         151.7431,  113.3035,   92.7664,   48.0793,   55.6585,   24.3717,
          29.6155,   40.0467,   18.0070,   40.9114,   32.7080])]
2449.359874526396
2.055803030417136 4.633245384362765 59.466200328262055
val isze = 8
idinces = [64 68  5 24 80 32 41 40 74 35 77 55 29 47 30 39 51  9 81 58  6  7 27 34
 19 45 18 69 82 66 37 38 56 33 25 31 43 62 17 12 46 20 59 14 52 16 50 70
 36  0 48  1 10 42 15 53 57  3  8 54 21 75 67  4 63 23 22 78 76 72 60 44
 26 28  2 49 61 13 65 71 11 79 73]
we are doing training validation split
training loss = 17.418506622314453 100
val loss = 21.743961334228516
training loss = 14.155010223388672 200
val loss = 16.907033920288086
training loss = 11.476335525512695 300
val loss = 13.09067153930664
training loss = 9.497218132019043 400
val loss = 10.111772537231445
training loss = 8.096001625061035 500
val loss = 7.8786845207214355
training loss = 7.120704174041748 600
val loss = 6.231510162353516
training loss = 6.44730806350708 700
val loss = 5.024562835693359
training loss = 5.984737873077393 800
val loss = 4.142602920532227
training loss = 5.668394565582275 900
val loss = 3.498631238937378
training loss = 5.452951908111572 1000
val loss = 3.0283219814300537
training loss = 5.3067522048950195 1100
val loss = 2.6845502853393555
training loss = 5.2077131271362305 1200
val loss = 2.4328174591064453
training loss = 5.140443801879883 1300
val loss = 2.248081684112549
training loss = 5.0942487716674805 1400
val loss = 2.112091064453125
training loss = 5.0617170333862305 1500
val loss = 2.0115811824798584
training loss = 5.037735939025879 1600
val loss = 1.9369502067565918
training loss = 5.018758773803711 1700
val loss = 1.8810497522354126
training loss = 5.002303123474121 1800
val loss = 1.8388400077819824
training loss = 4.986540794372559 1900
val loss = 1.8064181804656982
training loss = 4.969935417175293 2000
val loss = 1.7807375192642212
training loss = 4.950929641723633 2100
val loss = 1.7595510482788086
training loss = 4.92748498916626 2200
val loss = 1.740555763244629
training loss = 4.896311283111572 2300
val loss = 1.721684455871582
training loss = 4.8513922691345215 2400
val loss = 1.700182557106018
training loss = 4.781047344207764 2500
val loss = 1.672457218170166
training loss = 4.6641845703125 2600
val loss = 1.6344144344329834
training loss = 4.475639820098877 2700
val loss = 1.5839221477508545
training loss = 4.213194370269775 2800
val loss = 1.5144703388214111
training loss = 3.888895034790039 2900
val loss = 1.3915024995803833
training loss = 3.4959452152252197 3000
val loss = 1.1941423416137695
training loss = 3.045159101486206 3100
val loss = 0.9454845190048218
training loss = 2.5855867862701416 3200
val loss = 0.6902346611022949
training loss = 2.1980538368225098 3300
val loss = 0.49455446004867554
training loss = 1.9532634019851685 3400
val loss = 0.40957480669021606
training loss = 1.8436483144760132 3500
val loss = 0.41360682249069214
training loss = 1.8060606718063354 3600
val loss = 0.44422024488449097
training loss = 1.7932606935501099 3700
val loss = 0.46732890605926514
training loss = 1.7870358228683472 3800
val loss = 0.4791697859764099
training loss = 1.7824732065200806 3900
val loss = 0.484393447637558
training loss = 1.778425693511963 4000
val loss = 0.4867197871208191
training loss = 1.774613857269287 4100
val loss = 0.4880203306674957
training loss = 1.7709579467773438 4200
val loss = 0.4890662133693695
training loss = 1.7674343585968018 4300
val loss = 0.4901190400123596
training loss = 1.7640351057052612 4400
val loss = 0.49127596616744995
training loss = 1.7607609033584595 4500
val loss = 0.4925457537174225
training loss = 1.7576099634170532 4600
val loss = 0.49391722679138184
training loss = 1.754582166671753 4700
val loss = 0.49536529183387756
training loss = 1.7516778707504272 4800
val loss = 0.4968603253364563
training loss = 1.7488857507705688 4900
val loss = 0.498401015996933
training loss = 1.7462036609649658 5000
val loss = 0.4999277591705322
training loss = 1.7436171770095825 5100
val loss = 0.5014607906341553
training loss = 1.7411110401153564 5200
val loss = 0.5028359889984131
training loss = 1.7387850284576416 5300
val loss = 0.5073456168174744
training loss = 1.7362604141235352 5400
val loss = 0.5061076283454895
training loss = 1.734198808670044 5500
val loss = 0.5124168992042542
training loss = 1.7314783334732056 5600
val loss = 0.5092659592628479
training loss = 1.7291077375411987 5700
val loss = 0.5097944736480713
training loss = 1.7267323732376099 5800
val loss = 0.5108597278594971
training loss = 1.7242252826690674 5900
val loss = 0.5141502618789673
training loss = 1.722222924232483 6000
val loss = 0.5212962031364441
training loss = 1.7192859649658203 6100
val loss = 0.517909586429596
training loss = 1.716763973236084 6200
val loss = 0.5192018747329712
training loss = 1.7154020071029663 6300
val loss = 0.5153117179870605
training loss = 1.711684226989746 6400
val loss = 0.5227130055427551
training loss = 1.7090930938720703 6500
val loss = 0.5245675444602966
training loss = 1.7712528705596924 6600
val loss = 0.5378355979919434
training loss = 1.7038315534591675 6700
val loss = 0.528063178062439
training loss = 1.7011202573776245 6800
val loss = 0.5298000574111938
training loss = 1.6984601020812988 6900
val loss = 0.5308598279953003
training loss = 1.6956273317337036 7000
val loss = 0.5341479778289795
training loss = 1.6989986896514893 7100
val loss = 0.5276345014572144
training loss = 1.6898900270462036 7200
val loss = 0.5383549928665161
training loss = 1.972647786140442 7300
val loss = 0.7258853912353516
training loss = 1.6838215589523315 7400
val loss = 0.5432096719741821
training loss = 1.6805720329284668 7500
val loss = 0.5455911755561829
training loss = 1.6772502660751343 7600
val loss = 0.5497763752937317
training loss = 1.6736056804656982 7700
val loss = 0.5510396957397461
training loss = 1.6950503587722778 7800
val loss = 0.5536591410636902
training loss = 1.6656603813171387 7900
val loss = 0.557080090045929
training loss = 1.6610291004180908 8000
val loss = 0.5604421496391296
training loss = 1.6561356782913208 8100
val loss = 0.5641899108886719
training loss = 1.650602102279663 8200
val loss = 0.568385124206543
training loss = 1.6892551183700562 8300
val loss = 0.6456688642501831
training loss = 1.6380597352981567 8400
val loss = 0.577254593372345
reduced chi^2 level 2 = 1.6352919340133667
Constrained alpha: 1.8830718994140625
Constrained beta: 3.303173303604126
Constrained gamma: 24.841081619262695
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 888.1819,  875.7864,  898.4924,  946.7980, 1017.0683, 1108.9192,
        1102.8918, 1162.8331, 1132.0986, 1165.8092, 1192.7622, 1216.6250,
        1181.7385, 1235.1589, 1297.4794, 1425.0161, 1391.0132, 1425.8019,
        1459.3898, 1503.6260, 1527.3009, 1644.6934, 1683.9436, 1571.3752,
        1579.9980, 1663.1949, 1628.1886, 1702.1219, 1770.5819, 1738.3839,
        1649.9359, 1717.5768, 1774.8142, 1710.7212, 1726.8488, 1787.8224,
        1574.0486, 1632.8048, 1678.9915, 1634.1478, 1631.6322, 1543.0029,
        1447.8385, 1467.6560, 1312.0283, 1280.8188, 1287.0049, 1165.0596,
        1189.5698, 1122.3649, 1087.4016,  957.8395, 1015.4323,  907.9957,
         887.5991,  821.3832,  853.6304,  669.7905,  635.2485,  507.7946,
         570.4467,  488.6185,  499.0738,  380.8347,  358.1877,  324.0303,
         286.4325,  255.7180,  220.8479,  187.1286,  157.4366,  144.6249,
         144.3849,  104.0901,  114.0691,   73.1179,   45.8578,   42.2720,
          36.6808,   37.8209,   20.3256,   33.7277,   38.4083])]
2722.481962851852
1.6537884327842967 2.089813468371562 62.07207652120839
val isze = 8
idinces = [74 78 47 75 65 62  4 60 56 67 38 27  7 20 34 82 79 28 43 29 61  2 53 16
 49 81 14 10 32 72 31 50 80 25 35  0 36 69 39 37 51 64 58 42  1 13 45  3
 40 48 18 26 59 73 57 12 52 54 19 70  9 24 21  6 77 17 15 71 22 23  8 44
 55  5 46 11 63 76 33 30 68 66 41]
we are doing training validation split
training loss = 63.383113861083984 100
val loss = 56.911033630371094
training loss = 38.47915267944336 200
val loss = 30.675689697265625
training loss = 23.755189895629883 300
val loss = 16.00189208984375
training loss = 15.162132263183594 400
val loss = 7.911856651306152
training loss = 10.126222610473633 500
val loss = 4.455749034881592
training loss = 7.325183391571045 600
val loss = 3.9154255390167236
training loss = 5.886412143707275 700
val loss = 4.746148586273193
training loss = 5.161529064178467 800
val loss = 5.641338348388672
training loss = 4.677445411682129 900
val loss = 5.8679609298706055
training loss = 4.2262444496154785 1000
val loss = 5.477396011352539
training loss = 3.7789063453674316 1100
val loss = 4.913026332855225
training loss = 3.349940299987793 1200
val loss = 4.436765670776367
training loss = 2.9613547325134277 1300
val loss = 4.092499256134033
training loss = 2.636836290359497 1400
val loss = 3.89443039894104
training loss = 2.391448736190796 1500
val loss = 3.838041305541992
training loss = 2.2247462272644043 1600
val loss = 3.8884427547454834
training loss = 2.1218299865722656 1700
val loss = 3.991773843765259
training loss = 2.0616276264190674 1800
val loss = 4.097672462463379
training loss = 2.0255632400512695 1900
val loss = 4.17572021484375
training loss = 2.001417398452759 2000
val loss = 4.216755390167236
training loss = 1.982666015625 2100
val loss = 4.224830627441406
training loss = 1.9663509130477905 2200
val loss = 4.208749771118164
training loss = 1.9518154859542847 2300
val loss = 4.1778340339660645
training loss = 1.937195897102356 2400
val loss = 4.136073112487793
training loss = 1.926629900932312 2500
val loss = 4.09328031539917
training loss = 1.9109174013137817 2600
val loss = 4.042486667633057
training loss = 1.9219979047775269 2700
val loss = 4.002870082855225
training loss = 1.8873474597930908 2800
val loss = 3.9461116790771484
training loss = 1.8817241191864014 2900
val loss = 3.9004435539245605
training loss = 1.8665416240692139 3000
val loss = 3.85386323928833
training loss = 1.8614907264709473 3100
val loss = 3.812770366668701
training loss = 1.8484615087509155 3200
val loss = 3.7684810161590576
training loss = 1.8554211854934692 3300
val loss = 3.7305569648742676
training loss = 1.8328003883361816 3400
val loss = 3.690990924835205
training loss = 1.86940336227417 3500
val loss = 3.662829875946045
training loss = 1.8191931247711182 3600
val loss = 3.621710777282715
training loss = 1.8129793405532837 3700
val loss = 3.590291738510132
training loss = 1.8097935914993286 3800
val loss = 3.5578458309173584
training loss = 1.8016163110733032 3900
val loss = 3.5323290824890137
training loss = 1.8199312686920166 4000
val loss = 3.5254158973693848
training loss = 1.7913132905960083 4100
val loss = 3.4813599586486816
training loss = 1.7864567041397095 4200
val loss = 3.4578189849853516
training loss = 1.8088171482086182 4300
val loss = 3.427804946899414
training loss = 1.7773181200027466 4400
val loss = 3.4148457050323486
training loss = 1.7729549407958984 4500
val loss = 3.3952674865722656
training loss = 1.7693819999694824 4600
val loss = 3.3746275901794434
training loss = 1.7647980451583862 4700
val loss = 3.359203338623047
training loss = 1.76087486743927 4800
val loss = 3.341827154159546
training loss = 1.7571439743041992 4900
val loss = 3.325796604156494
training loss = 1.7535197734832764 5000
val loss = 3.311018943786621
training loss = 1.8059107065200806 5100
val loss = 3.3579514026641846
training loss = 1.7466402053833008 5200
val loss = 3.282092571258545
training loss = 1.743350625038147 5300
val loss = 3.267951488494873
training loss = 1.7405935525894165 5400
val loss = 3.2526111602783203
training loss = 1.7371983528137207 5500
val loss = 3.2417774200439453
training loss = 1.7353805303573608 5600
val loss = 3.223588228225708
training loss = 1.7314693927764893 5700
val loss = 3.215728282928467
training loss = 1.7286853790283203 5800
val loss = 3.2048001289367676
training loss = 1.7295560836791992 5900
val loss = 3.205676555633545
training loss = 1.7235227823257446 6000
val loss = 3.182140350341797
training loss = 1.7249691486358643 6100
val loss = 3.1594691276550293
training loss = 1.7187199592590332 6200
val loss = 3.1594042778015137
training loss = 1.7163749933242798 6300
val loss = 3.1499528884887695
training loss = 1.7148051261901855 6400
val loss = 3.1345129013061523
training loss = 1.7120414972305298 6500
val loss = 3.130411148071289
training loss = 1.7161424160003662 6600
val loss = 3.110442638397217
training loss = 1.7079936265945435 6700
val loss = 3.11175537109375
training loss = 1.7060189247131348 6800
val loss = 3.102905511856079
training loss = 1.7044639587402344 6900
val loss = 3.0902836322784424
training loss = 1.7023627758026123 7000
val loss = 3.0858848094940186
training loss = 1.7060433626174927 7100
val loss = 3.0579960346221924
training loss = 1.6989384889602661 7200
val loss = 3.069943904876709
training loss = 1.697724461555481 7300
val loss = 3.0650055408477783
training loss = 1.6957319974899292 7400
val loss = 3.0560662746429443
training loss = 1.6941243410110474 7500
val loss = 3.0483789443969727
training loss = 1.692794919013977 7600
val loss = 3.0379979610443115
training loss = 1.6912126541137695 7700
val loss = 3.034144878387451
training loss = 1.690985918045044 7800
val loss = 3.0427684783935547
training loss = 1.6884952783584595 7900
val loss = 3.021571636199951
training loss = 1.6870882511138916 8000
val loss = 3.014920234680176
training loss = 1.6866700649261475 8100
val loss = 2.9990367889404297
training loss = 1.6845976114273071 8200
val loss = 3.003763198852539
training loss = 1.683268427848816 8300
val loss = 2.997765064239502
training loss = 1.6822773218154907 8400
val loss = 2.9959349632263184
training loss = 1.6809879541397095 8500
val loss = 2.9873745441436768
training loss = 1.6798042058944702 8600
val loss = 2.9850258827209473
training loss = 1.6787383556365967 8700
val loss = 2.976776599884033
training loss = 1.6775411367416382 8800
val loss = 2.971916913986206
training loss = 1.6899445056915283 8900
val loss = 2.92756986618042
training loss = 1.6753792762756348 9000
val loss = 2.9633138179779053
training loss = 1.6759133338928223 9100
val loss = 2.9383842945098877
training loss = 1.6733314990997314 9200
val loss = 2.9576549530029297
training loss = 1.6721481084823608 9300
val loss = 2.94905424118042
training loss = 1.6713504791259766 9400
val loss = 2.9388175010681152
training loss = 1.670116662979126 9500
val loss = 2.9411418437957764
training loss = 1.768843173980713 9600
val loss = 3.1575310230255127
training loss = 1.6681368350982666 9700
val loss = 2.933661937713623
training loss = 1.6670328378677368 9800
val loss = 2.9286696910858154
training loss = 1.6665064096450806 9900
val loss = 2.917771339416504
training loss = 1.6651617288589478 10000
val loss = 2.922217845916748
training loss = 1.7802443504333496 10100
val loss = 3.1727170944213867
training loss = 1.6632882356643677 10200
val loss = 2.918663263320923
training loss = 1.6622058153152466 10300
val loss = 2.9115676879882812
training loss = 1.662087082862854 10400
val loss = 2.9249720573425293
training loss = 1.660408616065979 10500
val loss = 2.9056825637817383
training loss = 1.6736570596694946 10600
val loss = 2.984591007232666
training loss = 1.6585934162139893 10700
val loss = 2.898732900619507
training loss = 1.6575744152069092 10800
val loss = 2.8974084854125977
training loss = 1.6571234464645386 10900
val loss = 2.8837952613830566
training loss = 1.6558188199996948 11000
val loss = 2.891540050506592
training loss = 1.6555566787719727 11100
val loss = 2.8786182403564453
training loss = 1.65419602394104 11200
val loss = 2.8870978355407715
training loss = 1.65324068069458 11300
val loss = 2.884060859680176
training loss = 1.6539533138275146 11400
val loss = 2.9092631340026855
training loss = 1.6516362428665161 11500
val loss = 2.8799197673797607
training loss = 1.650822401046753 11600
val loss = 2.885378122329712
training loss = 1.650214672088623 11700
val loss = 2.870278835296631
training loss = 1.649234652519226 11800
val loss = 2.873994827270508
training loss = 1.6483275890350342 11900
val loss = 2.8696088790893555
training loss = 1.6477916240692139 12000
val loss = 2.8771064281463623
training loss = 1.6468669176101685 12100
val loss = 2.8685240745544434
training loss = 1.8424609899520874 12200
val loss = 2.635669469833374
training loss = 1.6454083919525146 12300
val loss = 2.8692431449890137
training loss = 1.6445202827453613 12400
val loss = 2.8630480766296387
training loss = 1.6450247764587402 12500
val loss = 2.8358922004699707
training loss = 1.6430505514144897 12600
val loss = 2.860666275024414
training loss = 1.6483482122421265 12700
val loss = 2.8082504272460938
training loss = 1.6416277885437012 12800
val loss = 2.8562607765197754
training loss = 1.6407902240753174 12900
val loss = 2.8535304069519043
training loss = 1.6402205228805542 13000
val loss = 2.8535051345825195
training loss = 1.6393979787826538 13100
val loss = 2.8528668880462646
training loss = 1.6523516178131104 13200
val loss = 2.764465093612671
training loss = 1.6380687952041626 13300
val loss = 2.8507957458496094
training loss = 1.6372610330581665 13400
val loss = 2.849804401397705
training loss = 1.6368346214294434 13500
val loss = 2.840862274169922
training loss = 1.6359647512435913 13600
val loss = 2.846865177154541
training loss = 1.6778593063354492 13700
val loss = 3.068295955657959
training loss = 1.6346747875213623 13800
val loss = 2.8462305068969727
training loss = 1.633893609046936 13900
val loss = 2.8416709899902344
training loss = 1.6337058544158936 14000
val loss = 2.8589465618133545
training loss = 1.632672905921936 14100
val loss = 2.8411288261413574
training loss = 2.0349907875061035 14200
val loss = 3.7266180515289307
training loss = 1.6315609216690063 14300
val loss = 2.843278408050537
training loss = 1.630819320678711 14400
val loss = 2.8379149436950684
training loss = 1.6304055452346802 14500
val loss = 2.8180224895477295
training loss = 1.6296497583389282 14600
val loss = 2.8310601711273193
training loss = 1.6288920640945435 14700
val loss = 2.8343586921691895
training loss = 1.632686734199524 14800
val loss = 2.899195671081543
training loss = 1.6277437210083008 14900
val loss = 2.8329057693481445
training loss = 1.6330344676971436 15000
val loss = 2.9143199920654297
training loss = 1.6266305446624756 15100
val loss = 2.8246052265167236
training loss = 1.6258822679519653 15200
val loss = 2.8290066719055176
training loss = 1.6262205839157104 15300
val loss = 2.8571982383728027
training loss = 1.6247713565826416 15400
val loss = 2.827965259552002
training loss = 1.6337424516677856 15500
val loss = 2.7387313842773438
training loss = 1.6236742734909058 15600
val loss = 2.827670097351074
training loss = 1.622973918914795 15700
val loss = 2.8241477012634277
training loss = 1.6231272220611572 15800
val loss = 2.7990097999572754
training loss = 1.6219305992126465 15900
val loss = 2.8226823806762695
training loss = 1.6747394800186157 16000
val loss = 3.115036964416504
training loss = 1.620910406112671 16100
val loss = 2.8160037994384766
training loss = 1.6202099323272705 16200
val loss = 2.8191380500793457
training loss = 1.6345313787460327 16300
val loss = 2.9560508728027344
training loss = 1.6191773414611816 16400
val loss = 2.8186588287353516
training loss = 1.6185595989227295 16500
val loss = 2.825230598449707
training loss = 1.618222951889038 16600
val loss = 2.8261518478393555
training loss = 1.6174685955047607 16700
val loss = 2.8143327236175537
training loss = 1.6172150373458862 16800
val loss = 2.8440582752227783
training loss = 1.616480827331543 16900
val loss = 2.8119168281555176
training loss = 1.6157963275909424 17000
val loss = 2.811098098754883
training loss = 1.616999864578247 17100
val loss = 2.8595049381256104
training loss = 1.614790439605713 17200
val loss = 2.809415340423584
training loss = 1.690399169921875 17300
val loss = 3.197373628616333
training loss = 1.6137548685073853 17400
val loss = 2.809852361679077
training loss = 1.6130495071411133 17500
val loss = 2.8063926696777344
training loss = 1.6166272163391113 17600
val loss = 2.734327793121338
training loss = 1.6119734048843384 17700
val loss = 2.8051085472106934
training loss = 1.7951639890670776 17800
val loss = 3.4802308082580566
training loss = 1.6109356880187988 17900
val loss = 2.8107151985168457
training loss = 1.6101454496383667 18000
val loss = 2.802504301071167
training loss = 1.6096962690353394 18100
val loss = 2.811643123626709
training loss = 1.60890531539917 18200
val loss = 2.800513505935669
training loss = 1.6080197095870972 18300
val loss = 2.800297260284424
training loss = 1.6079238653182983 18400
val loss = 2.826765298843384
training loss = 1.6065387725830078 18500
val loss = 2.804058313369751
training loss = 1.6712870597839355 18600
val loss = 2.5417380332946777
training loss = 1.6047797203063965 18700
val loss = 2.811429977416992
training loss = 1.6036232709884644 18800
val loss = 2.811404228210449
training loss = 1.6031477451324463 18900
val loss = 2.798673152923584
training loss = 1.6018668413162231 19000
val loss = 2.8192620277404785
training loss = 1.7189595699310303 19100
val loss = 3.3776092529296875
training loss = 1.6003438234329224 19200
val loss = 2.8239850997924805
training loss = 1.5994840860366821 19300
val loss = 2.8265180587768555
training loss = 1.5990947484970093 19400
val loss = 2.823359251022339
training loss = 1.5983225107192993 19500
val loss = 2.8272125720977783
training loss = 1.6352918148040771 19600
val loss = 3.140291213989258
training loss = 1.5972514152526855 19700
val loss = 2.8254661560058594
training loss = 1.5965217351913452 19800
val loss = 2.825730562210083
training loss = 1.5967334508895874 19900
val loss = 2.800889492034912
training loss = 1.5955156087875366 20000
val loss = 2.8262977600097656
training loss = 1.6201934814453125 20100
val loss = 3.033923625946045
training loss = 1.5945518016815186 20200
val loss = 2.825103282928467
training loss = 1.5938490629196167 20300
val loss = 2.824584722518921
training loss = 1.5935404300689697 20400
val loss = 2.830371618270874
training loss = 1.5928349494934082 20500
val loss = 2.8240580558776855
training loss = 1.6551780700683594 20600
val loss = 3.2389397621154785
training loss = 1.5918956995010376 20700
val loss = 2.821467399597168
training loss = 1.5918203592300415 20800
val loss = 2.8254456520080566
training loss = 1.5922139883041382 20900
val loss = 2.8576152324676514
training loss = 1.5916497707366943 21000
val loss = 2.8367466926574707
training loss = 1.591413140296936 21100
val loss = 2.8293111324310303
training loss = 1.5955058336257935 21200
val loss = 2.7376415729522705
training loss = 1.5909544229507446 21300
val loss = 2.832780599594116
training loss = 1.5906245708465576 21400
val loss = 2.836184501647949
training loss = 1.590990662574768 21500
val loss = 2.8767237663269043
training loss = 1.589902400970459 21600
val loss = 2.8443167209625244
training loss = 1.7112760543823242 21700
val loss = 3.454953193664551
training loss = 1.589067816734314 21800
val loss = 2.85212779045105
training loss = 1.588605284690857 21900
val loss = 2.858322858810425
training loss = 1.5883991718292236 22000
val loss = 2.8470916748046875
training loss = 1.5878231525421143 22100
val loss = 2.8670477867126465
training loss = 1.6152191162109375 22200
val loss = 2.6752872467041016
training loss = 1.5871503353118896 22300
val loss = 2.869422435760498
training loss = 1.5867946147918701 22400
val loss = 2.8750598430633545
training loss = 1.587891936302185 22500
val loss = 2.8285927772521973
training loss = 1.5862183570861816 22600
val loss = 2.8767266273498535
training loss = 1.597181797027588 22700
val loss = 3.026249408721924
training loss = 1.5856884717941284 22800
val loss = 2.879033327102661
training loss = 1.5854496955871582 22900
val loss = 2.890934944152832
training loss = 1.5851908922195435 23000
val loss = 2.883650302886963
training loss = 1.5848805904388428 23100
val loss = 2.8801562786102295
training loss = 1.599560022354126 23200
val loss = 2.7236475944519043
training loss = 1.5843865871429443 23300
val loss = 2.8810784816741943
training loss = 1.5858570337295532 23400
val loss = 2.8252882957458496
training loss = 1.5839474201202393 23500
val loss = 2.877453327178955
training loss = 1.5836358070373535 23600
val loss = 2.8829593658447266
training loss = 1.6639211177825928 23700
val loss = 3.363039016723633
training loss = 1.5831667184829712 23800
val loss = 2.8832550048828125
training loss = 1.5828683376312256 23900
val loss = 2.885314702987671
training loss = 1.584688425064087 24000
val loss = 2.950636863708496
training loss = 1.5823842287063599 24100
val loss = 2.8869171142578125
training loss = 1.621718168258667 24200
val loss = 3.206477642059326
training loss = 1.5819332599639893 24300
val loss = 2.886725425720215
training loss = 1.5816341638565063 24400
val loss = 2.889348268508911
training loss = 1.5836327075958252 24500
val loss = 2.966015338897705
training loss = 1.5812097787857056 24600
val loss = 2.890911102294922
training loss = 1.580909013748169 24700
val loss = 2.8930931091308594
training loss = 1.581268072128296 24800
val loss = 2.8608388900756836
training loss = 1.58045494556427 24900
val loss = 2.894298553466797
training loss = 1.703498125076294 25000
val loss = 3.546142101287842
training loss = 1.5800387859344482 25100
val loss = 2.8934855461120605
training loss = 1.5797442197799683 25200
val loss = 2.897611141204834
training loss = 1.5893871784210205 25300
val loss = 2.759087324142456
training loss = 1.5793038606643677 25400
val loss = 2.8979883193969727
training loss = 1.5790047645568848 25500
val loss = 2.899325370788574
reduced chi^2 level 2 = 1.5808992385864258
Constrained alpha: 1.901016354560852
Constrained beta: -0.0021545637864619493
Constrained gamma: 13.47055721282959
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 906.7990,  861.3951,  911.6826,  952.1586,  966.8516, 1061.3633,
        1146.8615, 1165.4467, 1181.5002, 1129.1552, 1230.9325, 1233.1571,
        1233.7164, 1252.7980, 1327.0211, 1488.8093, 1381.2678, 1475.0811,
        1601.6393, 1485.4961, 1578.0811, 1501.1803, 1614.0703, 1546.0264,
        1715.3058, 1792.6732, 1542.7961, 1675.2539, 1727.2408, 1621.9021,
        1600.6893, 1706.9517, 1765.2653, 1733.1498, 1704.3309, 1728.9230,
        1727.3077, 1613.5707, 1659.1238, 1664.4583, 1682.6982, 1530.6870,
        1433.5277, 1505.4167, 1400.5913, 1346.2087, 1256.2437, 1192.2896,
        1158.4672, 1151.0387, 1095.4507, 1012.2272,  911.2784,  892.8703,
         924.2380,  834.3119,  831.3704,  698.5312,  592.2855,  526.4453,
         546.3585,  522.4645,  438.9650,  394.2580,  353.6476,  340.0825,
         308.9106,  254.0606,  211.6704,  169.6518,  163.0850,  169.9605,
         173.7479,  109.4424,   99.2086,   75.9311,   53.7707,   42.4821,
          31.1890,   42.7119,   24.9889,   35.2889,   44.9614])]
3042.4348449534455
0.02012501623593732 1.684690870374792 39.27799606582779
val isze = 8
idinces = [19 30 14 26 40 17  5 43 55 22 34 57 54 46  1 41 47 62 50  0  9 64 73 36
 52 80 61 35 21 65 79 49 32 71 27 13 77 29 16  2 42 33 63 23 56  6 72 51
 69  8 68 10 24 39 44 12 74 82 70 60 37 81 59 75 15 48 78 20 18  3 31 67
 38 66 28  4 53 45 25 11 76 58  7]
we are doing training validation split
training loss = 21.684734344482422 100
val loss = 26.075834274291992
training loss = 13.098820686340332 200
val loss = 13.781821250915527
training loss = 8.853137969970703 300
val loss = 8.804424285888672
training loss = 6.920036792755127 400
val loss = 5.948914527893066
training loss = 6.008003234863281 500
val loss = 4.7014055252075195
training loss = 5.223547458648682 600
val loss = 4.184072017669678
training loss = 4.418886184692383 700
val loss = 3.754774332046509
training loss = 3.6511881351470947 800
val loss = 3.382251262664795
training loss = 3.0599656105041504 900
val loss = 3.141481876373291
training loss = 2.696744203567505 1000
val loss = 2.922579765319824
training loss = 2.5468485355377197 1100
val loss = 2.8410794734954834
training loss = 2.4746134281158447 1200
val loss = 2.837042808532715
training loss = 2.429765224456787 1300
val loss = 2.8544092178344727
training loss = 2.3949739933013916 1400
val loss = 2.8789327144622803
training loss = 2.3685007095336914 1500
val loss = 2.8892765045166016
training loss = 2.3415026664733887 1600
val loss = 2.937803268432617
training loss = 2.3231723308563232 1700
val loss = 2.998776912689209
training loss = 2.302198648452759 1800
val loss = 2.994396448135376
training loss = 2.2928309440612793 1900
val loss = 3.0805037021636963
training loss = 2.271789789199829 2000
val loss = 3.0491936206817627
training loss = 2.258817672729492 2100
val loss = 3.073188543319702
training loss = 2.247148036956787 2200
val loss = 3.1022427082061768
training loss = 2.2363317012786865 2300
val loss = 3.124476194381714
training loss = 2.22837233543396 2400
val loss = 3.122987985610962
training loss = 2.217196464538574 2500
val loss = 3.1718382835388184
training loss = 2.2159605026245117 2600
val loss = 3.1488306522369385
training loss = 2.2006797790527344 2700
val loss = 3.217716693878174
training loss = 2.2018582820892334 2800
val loss = 3.184969186782837
training loss = 2.186373233795166 2900
val loss = 3.260312557220459
training loss = 2.1798489093780518 3000
val loss = 3.278963088989258
training loss = 2.1740760803222656 3100
val loss = 3.2972185611724854
training loss = 2.168431520462036 3200
val loss = 3.3195106983184814
training loss = 2.4867749214172363 3300
val loss = 3.4187793731689453
training loss = 2.158550500869751 3400
val loss = 3.358065128326416
training loss = 2.153980016708374 3500
val loss = 3.3737123012542725
training loss = 2.1553218364715576 3600
val loss = 3.454009532928467
training loss = 2.146212100982666 3700
val loss = 3.406080722808838
training loss = 2.1425411701202393 3800
val loss = 3.4215774536132812
training loss = 2.141387701034546 3900
val loss = 3.4720263481140137
training loss = 2.1363108158111572 4000
val loss = 3.450016975402832
training loss = 2.133739948272705 4100
val loss = 3.4489219188690186
training loss = 2.130953073501587 4200
val loss = 3.4702742099761963
training loss = 2.128340721130371 4300
val loss = 3.4884111881256104
training loss = 2.1289994716644287 4400
val loss = 3.4588918685913086
training loss = 2.124049186706543 4500
val loss = 3.5100016593933105
training loss = 2.1219489574432373 4600
val loss = 3.5231056213378906
training loss = 2.1203649044036865 4700
val loss = 3.522982597351074
training loss = 2.1184470653533936 4800
val loss = 3.5403029918670654
training loss = 2.1171517372131348 4900
val loss = 3.5449185371398926
training loss = 2.115334987640381 5000
val loss = 3.558600664138794
training loss = 2.113757848739624 5100
val loss = 3.5659337043762207
training loss = 2.1131672859191895 5200
val loss = 3.5518951416015625
training loss = 2.111112117767334 5300
val loss = 3.580627202987671
training loss = 2.109724283218384 5400
val loss = 3.587867021560669
training loss = 2.1106081008911133 5500
val loss = 3.5603089332580566
training loss = 2.1072938442230225 5600
val loss = 3.601194381713867
training loss = 2.108083963394165 5700
val loss = 3.5582923889160156
training loss = 2.1050126552581787 5800
val loss = 3.613107204437256
training loss = 2.1037955284118652 5900
val loss = 3.618997097015381
training loss = 2.104193687438965 6000
val loss = 3.657172679901123
training loss = 2.1016407012939453 6100
val loss = 3.630021095275879
training loss = 2.4998271465301514 6200
val loss = 3.7371363639831543
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 784.4202,  930.2686,  962.1797,  940.6941, 1051.9998, 1064.9675,
        1080.8712, 1140.1498, 1104.5580, 1152.4364, 1185.9698, 1245.5999,
        1209.8767, 1290.7374, 1390.2262, 1432.6200, 1322.3605, 1438.6083,
        1571.8834, 1453.1727, 1571.1321, 1537.6893, 1591.4375, 1599.2156,
        1636.3745, 1728.3455, 1589.1836, 1720.3198, 1741.8285, 1773.8014,
        1674.1603, 1759.8887, 1725.0079, 1741.6492, 1770.2566, 1733.4030,
        1684.4362, 1644.5258, 1695.3312, 1630.9219, 1636.5804, 1661.4292,
        1514.9443, 1534.0997, 1367.0529, 1298.4803, 1258.0652, 1284.1211,
        1207.2246, 1145.0238, 1027.5693,  962.6381,  916.1821,  892.8140,
         869.9091,  873.4076,  790.3749,  663.4609,  658.7823,  574.9038,
         592.3854,  490.7936,  454.9952,  365.7482,  373.6493,  368.0826,
         307.5150,  276.8638,  212.9604,  167.2784,  175.6162,  145.3610,
         149.0754,   93.4417,   91.3126,   66.9821,   46.4487,   36.9489,
          32.2130,   42.2098,   15.1538,   30.9929,   46.5115])]
2821.643006416566
3.9147413902433223 6.090674413867823 39.73562310618999
val isze = 8
idinces = [34 31  1  5 15 61 50 49 52 44 14 29 12 27 40 75 72 26 30 37  9 23 24 19
 64 39 47 42 46 70  3 73 54 17 63 56 77 53 69 57 16 82  4 45 38 33  2 21
 43 18 76 25 20  8 59 79 28 48 60 41 22 36 74 51 58 32 62 81 13 11 35 10
 67 71 55 78 80 66 65  7 68  6  0]
we are doing training validation split
training loss = 57.622989654541016 100
val loss = 89.38927459716797
training loss = 23.62135887145996 200
val loss = 30.891353607177734
training loss = 10.338244438171387 300
val loss = 14.438426971435547
training loss = 4.9426679611206055 400
val loss = 5.9973039627075195
training loss = 3.981025457382202 500
val loss = 3.9339122772216797
training loss = 3.5413177013397217 600
val loss = 3.3289904594421387
training loss = 3.2187511920928955 700
val loss = 2.940243721008301
training loss = 2.970071792602539 800
val loss = 2.6380362510681152
training loss = 2.7749922275543213 900
val loss = 2.3664727210998535
training loss = 2.6182658672332764 1000
val loss = 2.201150417327881
training loss = 2.4934353828430176 1100
val loss = 2.028651475906372
training loss = 2.393002986907959 1200
val loss = 1.906687617301941
training loss = 2.3123278617858887 1300
val loss = 1.7961781024932861
training loss = 2.267669439315796 1400
val loss = 1.8994452953338623
training loss = 2.1951847076416016 1500
val loss = 1.618601679801941
training loss = 2.1531450748443604 1600
val loss = 1.546436071395874
training loss = 2.1359262466430664 1700
val loss = 1.6704845428466797
training loss = 2.0919747352600098 1800
val loss = 1.4425334930419922
training loss = 2.07150936126709 1900
val loss = 1.3594456911087036
training loss = 2.0520482063293457 2000
val loss = 1.3703290224075317
training loss = 2.037478446960449 2100
val loss = 1.343576431274414
training loss = 2.025418996810913 2200
val loss = 1.3203845024108887
training loss = 2.015698194503784 2300
val loss = 1.322094440460205
training loss = 2.0068230628967285 2400
val loss = 1.2879647016525269
training loss = 2.001288890838623 2500
val loss = 1.2311006784439087
training loss = 1.9930036067962646 2600
val loss = 1.2675708532333374
training loss = 1.996659755706787 2700
val loss = 1.164481520652771
training loss = 1.98213529586792 2800
val loss = 1.2548998594284058
training loss = 1.9829002618789673 2900
val loss = 1.1764676570892334
training loss = 1.9732048511505127 3000
val loss = 1.2476192712783813
training loss = 1.99452805519104 3100
val loss = 1.4532928466796875
training loss = 1.9657220840454102 3200
val loss = 1.2424187660217285
training loss = 1.9718296527862549 3300
val loss = 1.3628443479537964
training loss = 1.95944344997406 3400
val loss = 1.2419164180755615
training loss = 1.9777885675430298 3500
val loss = 1.110522747039795
training loss = 1.954258680343628 3600
val loss = 1.2400718927383423
training loss = 1.9517427682876587 3700
val loss = 1.2467756271362305
training loss = 1.9505212306976318 3800
val loss = 1.2692537307739258
training loss = 1.948011875152588 3900
val loss = 1.249237060546875
training loss = 2.066568613052368 4000
val loss = 1.8018896579742432
training loss = 1.9450124502182007 4100
val loss = 1.2520793676376343
training loss = 1.9433988332748413 4200
val loss = 1.2706915140151978
training loss = 1.9427043199539185 4300
val loss = 1.2737234830856323
training loss = 1.9410454034805298 4400
val loss = 1.2620115280151367
training loss = 1.9411715269088745 4500
val loss = 1.2800825834274292
training loss = 1.9393746852874756 4600
val loss = 1.2689653635025024
training loss = 1.9381096363067627 4700
val loss = 1.2694274187088013
training loss = 1.9390848875045776 4800
val loss = 1.2353405952453613
training loss = 1.9366750717163086 4900
val loss = 1.2765347957611084
training loss = 1.938206434249878 5000
val loss = 1.2403043508529663
training loss = 1.9354325532913208 5100
val loss = 1.2833731174468994
training loss = 1.93470299243927 5200
val loss = 1.2638099193572998
training loss = 1.9345300197601318 5300
val loss = 1.298540472984314
training loss = 1.9335590600967407 5400
val loss = 1.2900161743164062
training loss = 1.9388242959976196 5500
val loss = 1.3679254055023193
training loss = 1.9329313039779663 5600
val loss = 1.296437382698059
training loss = 1.932129144668579 5700
val loss = 1.2977300882339478
training loss = 1.9386436939239502 5800
val loss = 1.2152602672576904
training loss = 1.9315564632415771 5900
val loss = 1.3068784475326538
training loss = 1.930838704109192 6000
val loss = 1.305442214012146
training loss = 1.9327409267425537 6100
val loss = 1.260913610458374
training loss = 1.9303207397460938 6200
val loss = 1.3110982179641724
training loss = 2.3162825107574463 6300
val loss = 2.5689685344696045
training loss = 1.9299894571304321 6400
val loss = 1.320330023765564
training loss = 1.929383635520935 6500
val loss = 1.3176848888397217
training loss = 1.929783582687378 6600
val loss = 1.3195902109146118
training loss = 1.9290555715560913 6700
val loss = 1.3222495317459106
training loss = 2.0556116104125977 6800
val loss = 1.0207602977752686
training loss = 1.9289913177490234 6900
val loss = 1.3198250532150269
training loss = 1.928420066833496 7000
val loss = 1.3290295600891113
training loss = 1.954329252243042 7100
val loss = 1.1714797019958496
training loss = 1.9283794164657593 7200
val loss = 1.340579867362976
training loss = 1.9279004335403442 7300
val loss = 1.3345993757247925
training loss = 1.9294084310531616 7400
val loss = 1.3109939098358154
training loss = 1.9279102087020874 7500
val loss = 1.3419933319091797
training loss = 1.927497148513794 7600
val loss = 1.340079665184021
training loss = 1.928192377090454 7700
val loss = 1.32887864112854
training loss = 1.9274662733078003 7800
val loss = 1.3445812463760376
training loss = 2.3695926666259766 7900
val loss = 1.0254948139190674
training loss = 1.9275082349777222 8000
val loss = 1.3421674966812134
training loss = 1.9271153211593628 8100
val loss = 1.3497302532196045
training loss = 1.9368668794631958 8200
val loss = 1.2407352924346924
training loss = 1.9271610975265503 8300
val loss = 1.353661298751831
training loss = 1.928268313407898 8400
val loss = 1.4060347080230713
training loss = 1.927527904510498 8500
val loss = 1.3767600059509277
training loss = 1.9270154237747192 8600
val loss = 1.35814368724823
training loss = 3.3466711044311523 8700
val loss = 1.580199956893921
training loss = 1.9271901845932007 8800
val loss = 1.3585865497589111
training loss = 1.926884412765503 8900
val loss = 1.3620262145996094
reduced chi^2 level 2 = 1.9812592267990112
Constrained alpha: 3.9391820430755615
Constrained beta: 2.566746711730957
Constrained gamma: 21.618215560913086
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 785.9180,  889.6260,  978.6546,  939.8541,  942.2209, 1012.2451,
        1098.9646, 1162.9591, 1189.8401, 1163.0125, 1180.3378, 1182.2822,
        1199.2842, 1201.7395, 1318.4135, 1467.7341, 1343.1897, 1389.7523,
        1602.1292, 1541.4165, 1572.1625, 1505.9247, 1574.5758, 1573.2516,
        1617.9806, 1714.8684, 1580.5234, 1724.9999, 1697.3760, 1714.4971,
        1605.3251, 1817.0370, 1763.1040, 1676.7228, 1706.6577, 1754.0229,
        1631.7998, 1661.4088, 1635.1962, 1542.1792, 1613.8624, 1509.9017,
        1530.3600, 1572.2408, 1402.6829, 1294.0463, 1301.7495, 1291.3245,
        1125.5817, 1170.2297, 1135.8794,  982.4125,  966.3873,  925.4803,
         903.1700,  862.8145,  830.3785,  694.8893,  601.8721,  556.5275,
         562.6348,  465.0776,  441.3675,  373.2484,  366.1589,  367.7729,
         303.7777,  245.1959,  215.7845,  161.6754,  142.7691,  146.9159,
         157.3629,  103.9375,  107.1123,   67.3909,   45.4742,   32.7037,
          27.4466,   42.3868,    9.5964,   39.9902,   34.7506])]
3245.4041015929884
3.88474470645646 6.357510885262427 1.5086660619236247
val isze = 8
idinces = [46 68 13 44 72 23 31 80 58 15 24  1  8 73 12  7 32 81 35 61 57 64 49 82
 53 71 55 19  9 29 47 43 52 16 17 39 42  3 37 10 26  2 36 50 45 33  6 21
  5 28 70 22 79 78 67 62  4 63 27 69 60 14 11  0 30 40 74 25 75 51 48 76
 38 41 56 65 77 34 59 20 18 66 54]
we are doing training validation split
training loss = 161.84408569335938 100
val loss = 136.09115600585938
training loss = 45.158302307128906 200
val loss = 45.887115478515625
training loss = 26.233139038085938 300
val loss = 25.274246215820312
training loss = 17.831113815307617 400
val loss = 16.20169448852539
training loss = 13.408123016357422 500
val loss = 11.7808837890625
training loss = 10.841596603393555 600
val loss = 9.520727157592773
training loss = 9.260951042175293 700
val loss = 8.363786697387695
training loss = 8.249615669250488 800
val loss = 7.803182125091553
training loss = 7.586119174957275 900
val loss = 7.574533462524414
training loss = 7.143569469451904 1000
val loss = 7.531507968902588
training loss = 6.84518575668335 1100
val loss = 7.589712142944336
training loss = 6.642542362213135 1200
val loss = 7.699103355407715
training loss = 6.504167556762695 1300
val loss = 7.829637050628662
training loss = 6.409146308898926 1400
val loss = 7.963322162628174
training loss = 6.343361854553223 1500
val loss = 8.089643478393555
training loss = 6.297205448150635 1600
val loss = 8.202800750732422
training loss = 6.264096736907959 1700
val loss = 8.299976348876953
training loss = 6.239542007446289 1800
val loss = 8.380374908447266
training loss = 6.220483779907227 1900
val loss = 8.444343566894531
training loss = 6.204855918884277 2000
val loss = 8.493070602416992
training loss = 6.191295623779297 2100
val loss = 8.52808666229248
training loss = 6.178894519805908 2200
val loss = 8.551203727722168
training loss = 6.167069911956787 2300
val loss = 8.564151763916016
training loss = 6.155455112457275 2400
val loss = 8.569507598876953
training loss = 6.143834114074707 2500
val loss = 8.568260192871094
training loss = 6.132084369659424 2600
val loss = 8.560697555541992
training loss = 6.120108127593994 2700
val loss = 8.548476219177246
training loss = 6.107849597930908 2800
val loss = 8.532822608947754
training loss = 6.095287322998047 2900
val loss = 8.514644622802734
training loss = 6.08241081237793 3000
val loss = 8.49460506439209
training loss = 6.069219589233398 3100
val loss = 8.473172187805176
training loss = 6.055725574493408 3200
val loss = 8.450718879699707
training loss = 6.04194450378418 3300
val loss = 8.427486419677734
training loss = 6.027910232543945 3400
val loss = 8.403656005859375
training loss = 6.013652801513672 3500
val loss = 8.379403114318848
training loss = 5.999222755432129 3600
val loss = 8.35487174987793
training loss = 5.984683990478516 3700
val loss = 8.33022689819336
training loss = 5.9701080322265625 3800
val loss = 8.305593490600586
training loss = 5.955586910247803 3900
val loss = 8.281124114990234
training loss = 5.941235542297363 4000
val loss = 8.257072448730469
training loss = 5.927179336547852 4100
val loss = 8.233633041381836
training loss = 5.913559436798096 4200
val loss = 8.210976600646973
training loss = 5.900522708892822 4300
val loss = 8.189358711242676
training loss = 5.888208866119385 4400
val loss = 8.168936729431152
training loss = 5.876731872558594 4500
val loss = 8.149848937988281
training loss = 5.8661417961120605 4600
val loss = 8.13214111328125
training loss = 5.857288360595703 4700
val loss = 8.13393497467041
training loss = 5.84739351272583 4800
val loss = 8.104907989501953
training loss = 5.838372707366943 4900
val loss = 8.081792831420898
training loss = 5.82844352722168 5000
val loss = 8.056398391723633
training loss = 5.814244747161865 5100
val loss = 8.029826164245605
training loss = 5.782567024230957 5200
val loss = 7.992183685302734
training loss = 5.620817184448242 5300
val loss = 7.784276485443115
training loss = 4.7563629150390625 5400
val loss = 6.230607032775879
training loss = 3.7052507400512695 5500
val loss = 4.949439525604248
training loss = 2.8886637687683105 5600
val loss = 4.117588996887207
training loss = 2.667630910873413 5700
val loss = 3.982478380203247
training loss = 2.6446259021759033 5800
val loss = 3.9554543495178223
training loss = 2.6346590518951416 5900
val loss = 3.9294815063476562
training loss = 2.6270792484283447 6000
val loss = 3.9046638011932373
training loss = 2.6209559440612793 6100
val loss = 3.8853492736816406
training loss = 2.615447759628296 6200
val loss = 3.8712213039398193
training loss = 2.610837459564209 6300
val loss = 3.867060661315918
training loss = 2.6063072681427 6400
val loss = 3.854433059692383
training loss = 2.614802598953247 6500
val loss = 3.9047908782958984
training loss = 2.598768711090088 6600
val loss = 3.8456358909606934
training loss = 2.5953962802886963 6700
val loss = 3.844407796859741
training loss = 2.6052422523498535 6800
val loss = 3.8110342025756836
training loss = 2.5891401767730713 6900
val loss = 3.8431243896484375
training loss = 2.5862064361572266 7000
val loss = 3.8432817459106445
training loss = 2.583472490310669 7100
val loss = 3.841705083847046
training loss = 2.5806915760040283 7200
val loss = 3.846991539001465
training loss = 2.5827879905700684 7300
val loss = 3.825076103210449
training loss = 2.575550079345703 7400
val loss = 3.8502066135406494
training loss = 2.5730435848236084 7500
val loss = 3.8530325889587402
training loss = 2.5712947845458984 7600
val loss = 3.866215705871582
training loss = 2.568121910095215 7700
val loss = 3.85774564743042
training loss = 2.5659162998199463 7800
val loss = 3.855151891708374
training loss = 2.5632224082946777 7900
val loss = 3.86488938331604
training loss = 2.56076979637146 8000
val loss = 3.8669562339782715
training loss = 2.559659481048584 8100
val loss = 3.8570940494537354
training loss = 2.555784225463867 8200
val loss = 3.873048782348633
training loss = 2.5532515048980713 8300
val loss = 3.877368211746216
training loss = 2.8302321434020996 8400
val loss = 4.346074104309082
training loss = 2.5477771759033203 8500
val loss = 3.8867948055267334
training loss = 2.5448532104492188 8600
val loss = 3.8914968967437744
training loss = 2.5420570373535156 8700
val loss = 3.890225410461426
training loss = 2.5386691093444824 8800
val loss = 3.9027159214019775
training loss = 2.5352768898010254 8900
val loss = 3.90743350982666
training loss = 2.531738042831421 9000
val loss = 3.915050506591797
training loss = 2.5280237197875977 9100
val loss = 3.925400495529175
training loss = 2.5387353897094727 9200
val loss = 3.9943809509277344
training loss = 2.520047187805176 9300
val loss = 3.943408727645874
training loss = 2.515759229660034 9400
val loss = 3.954005718231201
training loss = 2.5119242668151855 9500
val loss = 3.9562528133392334
training loss = 2.5074496269226074 9600
val loss = 3.972771167755127
training loss = 2.5030508041381836 9700
val loss = 3.983473300933838
training loss = 2.501415252685547 9800
val loss = 3.9760830402374268
training loss = 2.4944040775299072 9900
val loss = 4.002765655517578
training loss = 2.489966630935669 10000
val loss = 4.0134992599487305
training loss = 2.4859726428985596 10100
val loss = 4.026582717895508
training loss = 2.481713056564331 10200
val loss = 4.030857563018799
training loss = 2.4775781631469727 10300
val loss = 4.044826984405518
training loss = 2.4735355377197266 10400
val loss = 4.050090312957764
training loss = 2.469576835632324 10500
val loss = 4.057764530181885
training loss = 2.466738224029541 10600
val loss = 4.055197715759277
training loss = 2.4620447158813477 10700
val loss = 4.073858737945557
training loss = 2.458353042602539 10800
val loss = 4.082552909851074
training loss = 2.455106258392334 10900
val loss = 4.084290504455566
training loss = 2.4515156745910645 11000
val loss = 4.09726619720459
training loss = 2.4481582641601562 11100
val loss = 4.103218078613281
training loss = 2.4450528621673584 11200
val loss = 4.1093034744262695
training loss = 2.4419760704040527 11300
val loss = 4.1180830001831055
training loss = 2.4438774585723877 11400
val loss = 4.105790138244629
training loss = 2.4361977577209473 11500
val loss = 4.1298065185546875
training loss = 2.433417320251465 11600
val loss = 4.136145114898682
training loss = 2.431654453277588 11700
val loss = 4.131565093994141
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 805.7124,  881.5390,  978.0604,  972.3578, 1061.3961, 1091.4247,
        1073.9741, 1087.0820, 1126.1775, 1145.0231, 1138.7297, 1137.7269,
        1195.2178, 1239.2402, 1314.0591, 1357.9021, 1384.2081, 1411.3468,
        1612.4631, 1489.2838, 1675.9635, 1611.1975, 1643.7887, 1615.2957,
        1682.4696, 1725.0923, 1623.8334, 1760.9713, 1725.6624, 1708.0034,
        1715.3647, 1771.1490, 1724.2365, 1793.6880, 1692.3005, 1739.7242,
        1699.5870, 1652.9312, 1677.6598, 1656.1433, 1600.7513, 1520.4319,
        1502.4460, 1476.2838, 1321.7115, 1309.7770, 1286.5250, 1242.1484,
        1164.7032, 1207.1569, 1100.6885,  942.5431,  914.6379,  922.4026,
         825.3267,  850.3082,  860.8868,  679.9789,  629.1500,  531.1422,
         523.5480,  440.1278,  433.3120,  383.5660,  289.3912,  330.3961,
         277.7633,  252.0070,  191.9118,  149.0333,  154.5527,  163.1901,
         146.1693,  114.0097,  102.3739,   81.0017,   48.7018,   40.2621,
          33.9707,   46.7839,   20.9869,   42.0615,   34.0603])]
2968.7387251031037
2.1392509067227383 18.491688998027588 93.72398147085163
val isze = 8
idinces = [74  7 72 37 60 34 17 24 59 42  3 49 41 40 23 61 79  9 46 15  0 13 52 32
 35 19 22 57 71 14 78  8 55 20 51 73 30 64 76 58 70  1 77 28  6 10 68 82
 12 62 56 11 47 18  4 39 21 69 80 43 44 75 25 26 50 63 54 67 27 33 81 16
 36 45 53 29  5 65 66 38 48  2 31]
we are doing training validation split
training loss = 282.57598876953125 100
val loss = 214.63751220703125
training loss = 26.766651153564453 200
val loss = 38.35796356201172
training loss = 11.5697021484375 300
val loss = 21.594608306884766
training loss = 11.2296142578125 400
val loss = 20.99749755859375
training loss = 10.86025619506836 500
val loss = 20.303709030151367
training loss = 10.476168632507324 600
val loss = 19.54884910583496
training loss = 10.091561317443848 700
val loss = 18.750314712524414
training loss = 9.72019100189209 800
val loss = 17.927473068237305
training loss = 9.37459945678711 900
val loss = 17.100975036621094
training loss = 9.065085411071777 1000
val loss = 16.29218864440918
training loss = 8.798590660095215 1100
val loss = 15.521543502807617
training loss = 8.577905654907227 1200
val loss = 14.80683708190918
training loss = 8.401528358459473 1300
val loss = 14.161321640014648
training loss = 8.26432991027832 1400
val loss = 13.592613220214844
training loss = 8.158927917480469 1500
val loss = 13.102381706237793
training loss = 8.07725715637207 1600
val loss = 12.687257766723633
training loss = 8.01192855834961 1700
val loss = 12.339900016784668
training loss = 7.957042217254639 1800
val loss = 12.051034927368164
training loss = 7.90841007232666 1900
val loss = 11.810789108276367
training loss = 7.863359451293945 2000
val loss = 11.60951042175293
training loss = 7.8203535079956055 2100
val loss = 11.438804626464844
training loss = 7.778618335723877 2200
val loss = 11.29153823852539
training loss = 7.737822532653809 2300
val loss = 11.16209602355957
training loss = 7.697897434234619 2400
val loss = 11.04610538482666
training loss = 7.658854961395264 2500
val loss = 10.940293312072754
training loss = 7.620789527893066 2600
val loss = 10.842466354370117
training loss = 7.5837883949279785 2700
val loss = 10.750850677490234
training loss = 7.547931671142578 2800
val loss = 10.664468765258789
training loss = 7.513258457183838 2900
val loss = 10.582328796386719
training loss = 7.479756832122803 3000
val loss = 10.503815650939941
training loss = 7.447319507598877 3100
val loss = 10.428064346313477
training loss = 7.415685176849365 3200
val loss = 10.354135513305664
training loss = 7.384339332580566 3300
val loss = 10.280494689941406
training loss = 7.352262496948242 3400
val loss = 10.204585075378418
training loss = 7.317360877990723 3500
val loss = 10.12123966217041
training loss = 7.2746992111206055 3600
val loss = 10.01803970336914
training loss = 7.211053371429443 3700
val loss = 9.860127449035645
training loss = 7.0911736488342285 3800
val loss = 9.550941467285156
training loss = 6.851410388946533 3900
val loss = 8.953303337097168
training loss = 6.335586071014404 4000
val loss = 7.846408843994141
training loss = 5.039462089538574 4100
val loss = 5.157867431640625
training loss = 3.3274483680725098 4200
val loss = 2.178697109222412
training loss = 2.6773762702941895 4300
val loss = 1.1144828796386719
training loss = 2.636096954345703 4400
val loss = 1.030003309249878
training loss = 2.624857187271118 4500
val loss = 1.0282659530639648
training loss = 2.610532522201538 4600
val loss = 1.0115034580230713
training loss = 2.633323907852173 4700
val loss = 1.0217012166976929
training loss = 2.5917062759399414 4800
val loss = 1.0086501836776733
training loss = 2.644148349761963 4900
val loss = 1.047013759613037
training loss = 2.5758750438690186 5000
val loss = 1.0125229358673096
training loss = 2.5693447589874268 5100
val loss = 1.013916015625
training loss = 2.561896324157715 5200
val loss = 1.0199949741363525
training loss = 2.555389642715454 5300
val loss = 1.0239686965942383
training loss = 2.549363374710083 5400
val loss = 1.0287420749664307
training loss = 2.543424606323242 5500
val loss = 1.0326926708221436
training loss = 2.5392537117004395 5600
val loss = 1.0362269878387451
training loss = 2.5325183868408203 5700
val loss = 1.0425368547439575
training loss = 2.533964157104492 5800
val loss = 1.0632859468460083
training loss = 2.522519588470459 5900
val loss = 1.053241491317749
training loss = 2.5177760124206543 6000
val loss = 1.0590167045593262
training loss = 2.5132594108581543 6100
val loss = 1.062887191772461
training loss = 2.508880376815796 6200
val loss = 1.0700923204421997
training loss = 2.5053226947784424 6300
val loss = 1.0799884796142578
training loss = 2.5005853176116943 6400
val loss = 1.0824466943740845
training loss = 2.496532678604126 6500
val loss = 1.0883002281188965
training loss = 2.492739677429199 6600
val loss = 1.0942686796188354
training loss = 2.4889659881591797 6700
val loss = 1.1001482009887695
training loss = 2.5574190616607666 6800
val loss = 1.1603116989135742
training loss = 2.4817824363708496 6900
val loss = 1.1123048067092896
training loss = 2.4782490730285645 7000
val loss = 1.1190458536148071
training loss = 2.4749369621276855 7100
val loss = 1.1255419254302979
training loss = 2.471529722213745 7200
val loss = 1.131027102470398
training loss = 2.468111515045166 7300
val loss = 1.1388335227966309
training loss = 2.464799165725708 7400
val loss = 1.143417239189148
training loss = 2.461198329925537 7500
val loss = 1.1517454385757446
training loss = 2.459130048751831 7600
val loss = 1.1631325483322144
training loss = 2.4537200927734375 7700
val loss = 1.1668457984924316
training loss = 2.6365702152252197 7800
val loss = 1.4023938179016113
training loss = 2.4447953701019287 7900
val loss = 1.1850072145462036
training loss = 2.4393372535705566 8000
val loss = 1.19657564163208
training loss = 2.434790849685669 8100
val loss = 1.2127795219421387
training loss = 2.4261083602905273 8200
val loss = 1.2219423055648804
training loss = 2.4178850650787354 8300
val loss = 1.2381788492202759
training loss = 2.4101545810699463 8400
val loss = 1.2513281106948853
training loss = 2.4008631706237793 8500
val loss = 1.2668203115463257
training loss = 2.392486333847046 8600
val loss = 1.2794862985610962
training loss = 2.384582281112671 8700
val loss = 1.2878016233444214
training loss = 2.3770294189453125 8800
val loss = 1.295042634010315
training loss = 2.3862249851226807 8900
val loss = 1.3120983839035034
training loss = 2.36244797706604 9000
val loss = 1.305427074432373
training loss = 2.355208158493042 9100
val loss = 1.3115473985671997
training loss = 2.3481621742248535 9200
val loss = 1.3164303302764893
training loss = 2.3410656452178955 9300
val loss = 1.3235963582992554
training loss = 2.338189125061035 9400
val loss = 1.3342366218566895
training loss = 2.3272244930267334 9500
val loss = 1.3377413749694824
training loss = 2.490882158279419 9600
val loss = 1.532781720161438
training loss = 2.31375789642334 9700
val loss = 1.3538212776184082
(1, 75)
(1, 8)
84022.69550317441
sum total data
84022.69550317441
data before
tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 8.5375e+02, 8.8451e+02,
        9.1303e+02, 9.4615e+02, 9.8197e+02, 1.0303e+03, 1.0622e+03, 1.0799e+03,
        1.1273e+03, 1.1582e+03, 1.1837e+03, 1.2262e+03, 1.2711e+03, 1.2834e+03,
        1.3417e+03, 1.3721e+03, 1.4066e+03, 1.4300e+03, 1.4696e+03, 1.4924e+03,
        1.5356e+03, 1.5693e+03, 1.5970e+03, 1.6294e+03, 1.6491e+03, 1.6628e+03,
        1.6731e+03, 1.6938e+03, 1.7012e+03, 1.7208e+03, 1.7317e+03, 1.7366e+03,
        1.7462e+03, 1.7429e+03, 1.7231e+03, 1.7133e+03, 1.7049e+03, 1.6919e+03,
        1.6757e+03, 1.6553e+03, 1.6217e+03, 1.5844e+03, 1.5365e+03, 1.5060e+03,
        1.4707e+03, 1.4209e+03, 1.3790e+03, 1.3293e+03, 1.2799e+03, 1.2264e+03,
        1.1679e+03, 1.1137e+03, 1.0638e+03, 9.9498e+02, 9.3998e+02, 8.8709e+02,
        8.2562e+02, 7.7088e+02, 7.1967e+02, 6.7052e+02, 6.1710e+02, 5.6989e+02,
        5.2468e+02, 4.7788e+02, 4.3278e+02, 3.8999e+02, 3.5104e+02, 3.1683e+02,
        2.8258e+02, 2.4760e+02, 2.1879e+02, 1.9069e+02, 1.6458e+02, 1.4113e+02,
        1.1964e+02, 1.0045e+02, 8.3539e+01, 6.8478e+01, 5.6437e+01, 4.5088e+01,
        3.5741e+01, 2.8309e+01, 2.1787e+01, 1.7284e+01, 1.3543e+01, 1.0692e+01,
        8.4232e+00, 6.1906e+00, 4.1710e+00, 1.4399e+00, 3.6971e-02, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,
        0.0000e+00, 0.0000e+00, 0.0000e+00], dtype=torch.float64)
16
data after
tensor([ 853.7512,  884.5090,  913.0284,  946.1503,  981.9678, 1030.2844,
        1062.1692, 1079.9126, 1127.3398, 1158.2001, 1183.6812, 1226.2260,
        1271.0793, 1283.4240, 1341.6590, 1372.0759, 1406.6224, 1429.9867,
        1469.5890, 1492.4009, 1535.5549, 1569.3282, 1597.0355, 1629.4014,
        1649.0963, 1662.7874, 1673.1361, 1693.8126, 1701.2031, 1720.7915,
        1731.6857, 1736.6157, 1746.2047, 1742.9122, 1723.1027, 1713.3046,
        1704.9344, 1691.8707, 1675.7460, 1655.2995, 1621.6752, 1584.3581,
        1536.5423, 1505.9962, 1470.6603, 1420.9208, 1379.0289, 1329.2753,
        1279.8788, 1226.3549, 1167.8590, 1113.6545, 1063.7798,  994.9806,
         939.9850,  887.0900,  825.6216,  770.8766,  719.6721,  670.5180,
         617.1005,  569.8945,  524.6844,  477.8828,  432.7815,  389.9932,
         351.0400,  316.8330,  282.5817,  247.6020,  218.7909,  190.6924,
         164.5846,  141.1295,  119.6378,  100.4470,   83.5386,   68.4779,
          56.4372,   45.0879,   35.7414,   50.0856,   61.7220],
       dtype=torch.float64)
data,sig_tot
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[ 871.35970406  903.16087215  932.40426315  966.50615987 1002.94539965
 1051.50320921 1084.65874023 1103.89131247 1151.60225457 1182.27298564
 1208.95637403 1251.77662867 1297.31685553 1309.21985873 1368.80806365
 1398.94302026 1432.61598115 1456.04549493 1495.22795884 1517.09737032
 1559.24913163 1591.43387114 1617.66397579 1648.27391593 1665.86634194
 1676.69377911 1683.55614848 1702.07057217 1705.14919332 1720.73494945
 1727.78123543 1728.2496327  1733.96130688 1725.62939127 1701.55470284
 1687.22401593 1673.88814011 1656.5843491  1636.17061516 1611.0367513
 1573.5097949  1532.20861111 1480.54018939 1446.32023024 1407.36964724
 1354.55106197 1309.62164202 1257.54669372 1205.2847248  1150.14745923
 1090.15058243 1035.22635803  984.05102144  914.74570675  859.18360818
  806.60070357  745.21049896  689.89219295  639.50953995  591.16211876
  538.19826233  492.75322024  448.35634861  404.19631267  361.29670913
  321.70451494  285.87156061  255.10566889  225.06799471  194.56070326
  169.62672364  146.31756582  124.41655585  105.64719191   87.88712034
   72.85283872   60.04517074   48.33290974   39.50421338   31.10270448
   24.24315155   33.53761341   40.15353577]
level2
[871.3597040603263, 903.1608721468876, 932.404263151764, 966.506159867994, 1002.9453996493544, 1051.5032092148374, 1084.65874023448, 1103.8913124666544, 1151.6022545702037, 1182.2729856356452, 1208.9563740340463, 1251.7766286723136, 1297.3168555282546, 1309.2198587261257, 1368.808063653265, 1398.9430202625895, 1432.6159811540335, 1456.045494932031, 1495.2279588358665, 1517.0973703206523, 1559.249131629821, 1591.4338711357295, 1617.6639757900155, 1648.2739159268272, 1665.8663419363363, 1676.6937791113762, 1683.556148479128, 1702.0705721664922, 1705.1491933171924, 1720.7349494515756, 1727.7812354292168, 1728.2496326997832, 1733.9613068828469, 1725.6293912689878, 1701.5547028415704, 1687.2240159300318, 1673.8881401060553, 1656.5843491023936, 1636.1706151600968, 1611.036751301851, 1573.5097949028705, 1532.2086111084782, 1480.5401893922194, 1446.3202302430836, 1407.369647243475, 1354.5510619716829, 1309.6216420226815, 1257.5466937242893, 1205.284724797863, 1150.1474592310656, 1090.1505824333326, 1035.2263580309018, 984.0510214421192, 914.7457067537166, 859.1836081818228, 806.6007035728305, 745.2104989631758, 689.8921929469808, 639.5095399457425, 591.1621187555841, 538.1982623267514, 492.7532202410572, 448.35634861031394, 404.1963126695521, 361.29670912797627, 321.70451494019744, 285.8715606092776, 255.10566889459182, 225.06799470657648, 194.56070326283472, 169.6267236363617, 146.31756581598557, 124.41655584707156, 105.64719191294353, 87.88712033591032, 72.85283871530216, 60.04517074303556, 48.33290973909333, 39.50421337837852, 31.102704483517126, 24.243151548541785, 33.537613413790346, 40.153535766777814]
[tensor([ 834.6794,  888.4021,  941.5417,  986.2445,  982.8270, 1006.3077,
        1126.2825, 1091.8026, 1104.1353, 1165.0027, 1202.5829, 1215.1204,
        1192.6644, 1305.0425, 1321.8082, 1369.3818, 1441.3779, 1434.7849,
        1516.4819, 1481.5594, 1562.1499, 1562.3016, 1613.2357, 1636.1094,
        1649.0157, 1728.4948, 1556.8125, 1676.4775, 1685.5973, 1678.0345,
        1604.9059, 1769.0797, 1715.6086, 1747.5892, 1672.7572, 1786.4424,
        1697.3489, 1641.1337, 1654.3445, 1641.4767, 1612.6938, 1497.6692,
        1424.7887, 1497.4818, 1348.7086, 1318.1140, 1300.4829, 1248.3458,
        1088.6360, 1183.6833, 1074.6964, 1039.1885,  935.7048,  973.3298,
         882.2411,  874.2452,  798.5287,  700.4024,  640.2394,  586.3448,
         571.3906,  517.2668,  456.8168,  380.2693,  374.9056,  363.0504,
         278.9969,  234.0312,  224.5033,  178.5443,  144.7367,  148.8561,
         124.7987,  101.7567,   93.1392,   58.7894,   49.8399,   24.5574,
          33.3437,   51.1315,   29.4741,   28.7745,   24.8752])]
2387.459537342118
3.3991589559062763 7.6069175583277815 26.943115958724896
val isze = 8
idinces = [56 37 36  1 16 66 64 82 75 58  5 14 42 78 39 54 45 57 10  2 29  8 25 63
 31 52 21 19 13 61 38 62  3 34  9 60 30 27 81 67 20 24 71 48 18 70 11 46
 12 53 80 50 77 33 55 32 35 69 40 65 72  0 15 74 23 28 22  4 76 43 59 79
 73 44 47 49 68  7 17  6 26 51 41]
we are doing training validation split
training loss = 75.4169921875 100
val loss = 91.59011840820312
training loss = 21.445993423461914 200
val loss = 27.345611572265625
training loss = 14.480364799499512 300
val loss = 18.718135833740234
training loss = 10.660599708557129 400
val loss = 13.618941307067871
training loss = 8.49466323852539 500
val loss = 10.416508674621582
training loss = 7.224887847900391 600
val loss = 8.316702842712402
training loss = 6.4617695808410645 700
val loss = 6.898085594177246
training loss = 5.995596885681152 800
val loss = 5.9193525314331055
training loss = 5.707779884338379 900
val loss = 5.233463287353516
training loss = 5.528616905212402 1000
val loss = 4.7467451095581055
training loss = 5.416027069091797 1100
val loss = 4.39774751663208
training loss = 5.344156265258789 1200
val loss = 4.1453094482421875
training loss = 5.29697847366333 1300
val loss = 3.9613895416259766
training loss = 5.26453971862793 1400
val loss = 3.8266069889068604
training loss = 5.24069356918335 1500
val loss = 3.7273247241973877
training loss = 5.221689701080322 1600
val loss = 3.6538925170898438
training loss = 5.205286979675293 1700
val loss = 3.5993032455444336
training loss = 5.1901679039001465 1800
val loss = 3.558441162109375
training loss = 5.175586223602295 1900
val loss = 3.527463436126709
training loss = 5.161140441894531 2000
val loss = 3.5036070346832275
training loss = 5.146609783172607 2100
val loss = 3.484670400619507
training loss = 5.131893634796143 2200
val loss = 3.4691295623779297
training loss = 5.116948127746582 2300
val loss = 3.4558005332946777
training loss = 5.101761817932129 2400
val loss = 3.443876266479492
training loss = 5.086347579956055 2500
val loss = 3.432750940322876
training loss = 5.070733547210693 2600
val loss = 3.42208194732666
training loss = 5.054953098297119 2700
val loss = 3.411578893661499
training loss = 5.039056777954102 2800
val loss = 3.401120185852051
training loss = 5.023105621337891 2900
val loss = 3.3906407356262207
training loss = 5.007173538208008 3000
val loss = 3.380140542984009
training loss = 4.991353988647461 3100
val loss = 3.3696389198303223
training loss = 4.975749492645264 3200
val loss = 3.3591930866241455
training loss = 4.9604902267456055 3300
val loss = 3.348860025405884
training loss = 4.945708751678467 3400
val loss = 3.3387701511383057
training loss = 4.9315714836120605 3500
val loss = 3.3290607929229736
training loss = 4.918237209320068 3600
val loss = 3.319883108139038
training loss = 4.905863285064697 3700
val loss = 3.311373472213745
training loss = 4.89459228515625 3800
val loss = 3.3037054538726807
training loss = 4.884522914886475 3900
val loss = 3.2970385551452637
training loss = 4.875690460205078 4000
val loss = 3.2914230823516846
training loss = 4.868052005767822 4100
val loss = 3.286898612976074
training loss = 4.861450672149658 4200
val loss = 3.2833523750305176
training loss = 4.855593681335449 4300
val loss = 3.2805800437927246
training loss = 4.849975109100342 4400
val loss = 3.2782716751098633
training loss = 4.843669891357422 4500
val loss = 3.275738000869751
training loss = 4.83467960357666 4600
val loss = 3.271904468536377
training loss = 4.817718029022217 4700
val loss = 3.264270067214966
training loss = 4.775840759277344 4800
val loss = 3.2464301586151123
training loss = 4.664239406585693 4900
val loss = 3.20580792427063
training loss = 4.466522693634033 5000
val loss = 3.1264805793762207
training loss = 4.149510860443115 5100
val loss = 2.8497085571289062
training loss = 3.5536580085754395 5200
val loss = 2.3273777961730957
training loss = 2.780885696411133 5300
val loss = 1.3276100158691406
training loss = 2.1797847747802734 5400
val loss = 1.0839173793792725
training loss = 2.1122608184814453 5500
val loss = 1.1416124105453491
training loss = 2.0408098697662354 5600
val loss = 1.018507719039917
training loss = 2.0147922039031982 5700
val loss = 1.1367032527923584
training loss = 1.9210869073867798 5800
val loss = 0.9099552035331726
training loss = 1.8819116353988647 5900
val loss = 0.8875981569290161
training loss = 1.8523274660110474 6000
val loss = 0.864112138748169
training loss = 1.8256086111068726 6100
val loss = 0.8588553667068481
training loss = 1.8023951053619385 6200
val loss = 0.8200182318687439
training loss = 1.7784512042999268 6300
val loss = 0.8320821523666382
training loss = 1.8788799047470093 6400
val loss = 0.6858479976654053
training loss = 1.735399842262268 6500
val loss = 0.814478874206543
training loss = 1.714674711227417 6600
val loss = 0.8072801232337952
training loss = 1.6964292526245117 6700
val loss = 0.8107901811599731
training loss = 1.677491545677185 6800
val loss = 0.8033592700958252
training loss = 1.6606322526931763 6900
val loss = 0.8079196214675903
training loss = 1.6441408395767212 7000
val loss = 0.8117443919181824
training loss = 1.636086344718933 7100
val loss = 0.8694663047790527
training loss = 1.6177068948745728 7200
val loss = 0.8324400186538696
training loss = 1.6064683198928833 7300
val loss = 0.842612624168396
training loss = 1.5985013246536255 7400
val loss = 0.8603052496910095
training loss = 1.5913314819335938 7500
val loss = 0.8828126788139343
training loss = 1.5906158685684204 7600
val loss = 0.9365666508674622
training loss = 1.5820780992507935 7700
val loss = 0.919844388961792
training loss = 1.7657389640808105 7800
val loss = 0.896457314491272
training loss = 1.5770304203033447 7900
val loss = 0.9536454677581787
training loss = 1.57528817653656 8000
val loss = 0.9708513617515564
training loss = 1.581331491470337 8100
val loss = 0.9512852430343628
training loss = 1.5734435319900513 8200
val loss = 0.9962748289108276
training loss = 1.5727860927581787 8300
val loss = 1.010204792022705
training loss = 1.572554111480713 8400
val loss = 1.0189504623413086
training loss = 1.5721940994262695 8500
val loss = 1.0291388034820557
training loss = 1.610640048980713 8600
val loss = 0.9908514022827148
training loss = 1.5719621181488037 8700
val loss = 1.0446159839630127
training loss = 1.6467797756195068 8800
val loss = 1.0106892585754395
training loss = 1.5719499588012695 8900
val loss = 1.054764986038208
training loss = 1.5718727111816406 9000
val loss = 1.0651841163635254
training loss = 1.57208251953125 9100
val loss = 1.0625240802764893
training loss = 1.5719155073165894 9200
val loss = 1.0751711130142212
training loss = 1.572418451309204 9300
val loss = 1.0702970027923584
training loss = 1.571967601776123 9400
val loss = 1.0823311805725098
training loss = 1.5719330310821533 9500
val loss = 1.0887601375579834
training loss = 1.5728304386138916 9600
val loss = 1.074880599975586
training loss = 1.5719445943832397 9700
val loss = 1.0925836563110352
training loss = 1.5718716382980347 9800
val loss = 1.098851203918457
training loss = 1.5729061365127563 9900
val loss = 1.0867085456848145
training loss = 1.5717840194702148 10000
val loss = 1.1045259237289429
training loss = 1.5826410055160522 10100
val loss = 1.1601641178131104
training loss = 1.5716043710708618 10200
val loss = 1.1079683303833008
training loss = 1.5713765621185303 10300
val loss = 1.115935206413269
training loss = 1.5718098878860474 10400
val loss = 1.1244475841522217
training loss = 1.5708738565444946 10500
val loss = 1.1216439008712769
training loss = 1.6832195520401 10600
val loss = 1.3551344871520996
training loss = 1.5700603723526 10700
val loss = 1.1269652843475342
training loss = 1.5694571733474731 10800
val loss = 1.1331894397735596
training loss = 1.5750069618225098 10900
val loss = 1.16591215133667
training loss = 1.5682957172393799 11000
val loss = 1.1398056745529175
training loss = 1.5717867612838745 11100
val loss = 1.127913475036621
training loss = 1.5672736167907715 11200
val loss = 1.1484817266464233
training loss = 1.5667585134506226 11300
val loss = 1.1589128971099854
training loss = 1.5670636892318726 11400
val loss = 1.1517674922943115
training loss = 1.5659514665603638 11500
val loss = 1.1689562797546387
training loss = 1.5824716091156006 11600
val loss = 1.1397075653076172
reduced chi^2 level 2 = 1.5656064748764038
Constrained alpha: 1.8354597091674805
Constrained beta: 2.966193675994873
Constrained gamma: 16.67242431640625
(1, 75)
(1, 8)
