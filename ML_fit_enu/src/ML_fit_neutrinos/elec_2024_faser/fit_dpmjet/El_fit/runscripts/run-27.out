data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         18.54770328 22.06836739 25.69733589 29.7692914
 33.69446817 37.71868728 41.2839357  44.8570441  47.41772947 49.00613891
 49.06113251 46.95973269 41.63525185 33.75256836 24.88206784 17.33100296
 14.35249396 12.97307195  1.25570095  0.          0.        ]
4
data after
[40.61607067368269, 25.69733588950753, 29.769291403943303, 33.69446816906512, 37.71868727979064, 41.283935695894364, 44.85704410096645, 47.41772946900249, 49.00613890574604, 49.06113250940442, 46.95973269024491, 41.63525185120851, 33.752568362621965, 24.88206784006767, 45.912269821535446]
6
data,sig_tot
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
4.105547195438589 13.774367031189922 47.773055814362664
val isze = 0
idinces = [ 7 10  5  6  3 18 13  2 14  8 17 16 19 12 11  1  0 15  4  9]
training loss = 30.415542602539062 500
training loss = 30.106403350830078 1000
training loss = 29.77188491821289 1500
training loss = 28.19038200378418 2000
training loss = 21.781339645385742 2500
training loss = 18.048845291137695 3000
training loss = 14.690634727478027 3500
training loss = 11.099017143249512 4000
training loss = 7.415721416473389 4500
training loss = 4.65897798538208 5000
(1, 20)
(1, 0)
