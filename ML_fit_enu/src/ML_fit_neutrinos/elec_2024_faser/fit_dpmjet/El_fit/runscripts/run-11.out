data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         18.54770328 22.06836739 25.69733589 29.7692914
 33.69446817 37.71868728 41.2839357  44.8570441  47.41772947 49.00613891
 49.06113251 46.95973269 41.63525185 33.75256836 24.88206784 17.33100296
 14.35249396 12.97307195  1.25570095  0.          0.        ]
4
data after
[40.61607067368269, 25.69733588950753, 29.769291403943303, 33.69446816906512, 37.71868727979064, 41.283935695894364, 44.85704410096645, 47.41772946900249, 49.00613890574604, 49.06113250940442, 46.95973269024491, 41.63525185120851, 33.752568362621965, 24.88206784006767, 45.912269821535446]
6
data,sig_tot
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
4.8318231204354385 14.481852340617035 85.76465975227349
val isze = 0
idinces = [ 7 10  5  6  3 18 13  2 14  8 17 16 19 12 11  1  0 15  4  9]
training loss = 53.179046630859375 500
training loss = 36.60673141479492 1000
training loss = 34.4207649230957 1500
training loss = 33.97918701171875 2000
training loss = 33.8669548034668 2500
training loss = 33.81195068359375 3000
training loss = 33.75326156616211 3500
training loss = 33.6747932434082 4000
training loss = 33.56734085083008 4500
training loss = 33.421443939208984 5000
(1, 20)
(1, 0)
