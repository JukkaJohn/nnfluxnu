data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         18.54770328 22.06836739 25.69733589 29.7692914
 33.69446817 37.71868728 41.2839357  44.8570441  47.41772947 49.00613891
 49.06113251 46.95973269 41.63525185 33.75256836 24.88206784 17.33100296
 14.35249396 12.97307195  1.25570095  0.          0.        ]
4
data after
[40.61607067368269, 25.69733588950753, 29.769291403943303, 33.69446816906512, 37.71868727979064, 41.283935695894364, 44.85704410096645, 47.41772946900249, 49.00613890574604, 49.06113250940442, 46.95973269024491, 41.63525185120851, 33.752568362621965, 24.88206784006767, 45.912269821535446]
6
data,sig_tot
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
4.531062391821139 17.605933746000915 84.76943445316991
val isze = 0
idinces = [ 7 10  5  6  3 18 13  2 14  8 17 16 19 12 11  1  0 15  4  9]
training loss = 28.609235763549805 500
training loss = 28.5736026763916 1000
training loss = 28.540878295898438 1500
training loss = 28.498987197875977 2000
training loss = 28.44758415222168 2500
training loss = 28.385732650756836 3000
training loss = 28.31086540222168 3500
training loss = 28.216983795166016 4000
training loss = 28.09012222290039 4500
training loss = 27.89780616760254 5000
(1, 20)
(1, 0)
