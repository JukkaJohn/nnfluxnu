data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         18.54770328 22.06836739 25.69733589 29.7692914
 33.69446817 37.71868728 41.2839357  44.8570441  47.41772947 49.00613891
 49.06113251 46.95973269 41.63525185 33.75256836 24.88206784 17.33100296
 14.35249396 12.97307195  1.25570095  0.          0.        ]
4
data after
[40.61607067368269, 25.69733588950753, 29.769291403943303, 33.69446816906512, 37.71868727979064, 41.283935695894364, 44.85704410096645, 47.41772946900249, 49.00613890574604, 49.06113250940442, 46.95973269024491, 41.63525185120851, 33.752568362621965, 24.88206784006767, 45.912269821535446]
6
data,sig_tot
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
[40.61607067 25.69733589 29.7692914  33.69446817 37.71868728 41.2839357
 44.8570441  47.41772947 49.00613891 49.06113251 46.95973269 41.63525185
 33.75256836 24.88206784 45.91226982 22.10442776 25.39306926 21.5471506
 21.60587836 30.00005238]
4.937158106912285 14.88004277541184 71.46564875136703
val isze = 0
idinces = [ 7 10  5  6  3 18 13  2 14  8 17 16 19 12 11  1  0 15  4  9]
training loss = 30.823450088500977 500
training loss = 28.22372817993164 1000
training loss = 28.13772964477539 1500
training loss = 28.09272575378418 2000
training loss = 28.0334415435791 2500
training loss = 27.955331802368164 3000
training loss = 27.853498458862305 3500
training loss = 27.722257614135742 4000
training loss = 27.553693771362305 4500
training loss = 27.337665557861328 5000
(1, 20)
(1, 0)
