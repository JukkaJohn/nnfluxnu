data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 6.59355677e-04
 1.89627358e-02 1.65692183e-01 4.50647634e-01 8.39034098e-01
 1.42486610e+00 2.27300246e+00 3.41022410e+00 5.38954298e+00
 8.36257391e+00 1.17464560e+01 1.76025910e+01 2.78847724e+01
 3.36743950e+01 3.73539853e+01 4.16859133e+01 4.61028041e+01
 5.06516709e+01 5.33865406e+01 5.39087564e+01 5.16248135e+01
 4.50371262e+01 3.54741274e+01 2.47531601e+01 1.64649407e+01
 1.15840972e+01 9.85306536e+00 1.13932246e+00 0.00000000e+00
 0.00000000e+00]
4
data after
[22.335205557327946, 29.349047054649297, 27.88477244557023, 33.67439495696962, 37.35398530520797, 41.68591326270461, 46.10280410109222, 50.651670933413804, 53.38654059837818, 53.90875641222001, 51.624813539335136, 45.03712618718818, 35.474127418469635, 24.753160059997068, 39.041425702489946]
9
data,sig_tot
[22.33520556 29.34904705 27.88477245 33.67439496 37.35398531 41.68591326
 46.1028041  50.65167093 53.3865406  53.90875641 51.62481354 45.03712619
 35.47412742 24.75316006 39.0414257  26.94656936 28.92535225 21.54075446
 43.23790002]
[22.33520556 29.34904705 27.88477245 33.67439496 37.35398531 41.68591326
 46.1028041  50.65167093 53.3865406  53.90875641 51.62481354 45.03712619
 35.47412742 24.75316006 39.0414257  26.94656936 28.92535225 21.54075446
 43.23790002]
4.932899805436685 14.649915223388966 33.712739545825634
val isze = 0
idinces = [ 3  7 17  5  6  2 10 13  8 14 16 18 12 11  1  0 15  4  9]
training loss = 45.208457946777344 500
training loss = 36.035640716552734 1000
training loss = 35.24146270751953 1500
training loss = 35.12161636352539 2000
training loss = 35.066368103027344 2500
training loss = 35.00767135620117 3000
training loss = 34.94035339355469 3500
training loss = 34.864288330078125 4000
training loss = 34.77975845336914 4500
training loss = 34.68332290649414 5000
(1, 19)
(1, 0)
