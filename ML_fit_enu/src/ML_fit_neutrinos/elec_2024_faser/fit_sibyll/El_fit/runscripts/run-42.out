data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         16.01057878 18.80903445 21.60008665 24.47482322
 26.96743781 29.16325909 30.50389214 31.26188957 30.43797267 28.10180099
 24.41295403 19.91822533 15.16629036 10.96496343  7.5227208   5.03922493
  4.09402748  3.70270844  0.35892254  0.          0.        ]
5
data after
[34.81961323485136, 21.60008664993882, 24.474823224906327, 26.967437811353804, 29.16325908910036, 30.50389213723719, 31.261889567720143, 30.437972668584734, 28.101800988161415, 24.41295402737856, 35.08451569841653, 31.682567610504265]
11
data,sig_tot
[34.81961323 21.60008665 24.47482322 26.96743781 29.16325909 30.50389214
 31.26188957 30.43797267 28.10180099 24.41295403 35.0845157  31.68256761
 21.80479976 34.5319641 ]
[34.81961323 21.60008665 24.47482322 26.96743781 29.16325909 30.50389214
 31.26188957 30.43797267 28.10180099 24.41295403 35.0845157  31.68256761
 21.80479976 34.5319641 ]
4.801600961855094 4.735560322460881 17.425687258424503
val isze = 0
idinces = [ 3  7 12  6  8  2 10  5 11 13  1  0  4  9]
training loss = 26.484224319458008 500
training loss = 25.66283416748047 1000
training loss = 23.75264549255371 1500
training loss = 17.1248836517334 2000
training loss = 13.685970306396484 2500
training loss = 12.690275192260742 3000
training loss = 11.322665214538574 3500
training loss = 9.033514022827148 4000
training loss = 5.7061238288879395 4500
training loss = 3.2201828956604004 5000
(1, 14)
(1, 0)
