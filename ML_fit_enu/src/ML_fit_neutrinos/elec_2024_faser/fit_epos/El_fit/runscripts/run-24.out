data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         17.62549039 20.43675217 23.10966879 25.59818981
 27.43395391 28.56876383 28.4199347  27.46584946 24.97353653 21.36008436
 17.39216035 13.67658068 10.44624044  7.32048235  4.56410717  2.64958647
  1.96686448  1.76807184  0.17125836  0.          0.        ]
6
data after
[38.062242560656074, 23.109668787825107, 25.598189806829094, 27.43395391413063, 28.56876383123398, 28.419934704074265, 27.465849455823598, 24.97353652573183, 21.36008436308578, 31.068741029431674, 28.886611107920448]
5
data,sig_tot
[38.06224256 23.10966879 25.59818981 27.43395391 28.56876383 28.4199347
 27.46584946 24.97353653 21.36008436 31.06874103 28.88661111 20.96774152
 25.38611024 21.65535778]
[38.06224256 23.10966879 25.59818981 27.43395391 28.56876383 28.4199347
 27.46584946 24.97353653 21.36008436 31.06874103 28.88661111 20.96774152
 25.38611024 21.65535778]
4.803844012446134 1.2068982428709751 64.66465229308193
val isze = 0
idinces = [ 3  7 12  6  8  2 10  5 11 13  1  0  4  9]
training loss = 239.01116943359375 500
training loss = 57.7006950378418 1000
training loss = 28.559829711914062 1500
training loss = 20.296165466308594 2000
training loss = 17.283296585083008 2500
training loss = 15.97113037109375 3000
training loss = 15.256906509399414 3500
training loss = 14.730061531066895 4000
training loss = 14.233235359191895 4500
training loss = 13.73100757598877 5000
(1, 14)
(1, 0)
