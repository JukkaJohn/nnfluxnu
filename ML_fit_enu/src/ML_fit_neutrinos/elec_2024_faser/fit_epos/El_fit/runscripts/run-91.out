data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         17.62549039 20.43675217 23.10966879 25.59818981
 27.43395391 28.56876383 28.4199347  27.46584946 24.97353653 21.36008436
 17.39216035 13.67658068 10.44624044  7.32048235  4.56410717  2.64958647
  1.96686448  1.76807184  0.17125836  0.          0.        ]
6
data after
[38.062242560656074, 23.109668787825107, 25.598189806829094, 27.43395391413063, 28.56876383123398, 28.419934704074265, 27.465849455823598, 24.97353652573183, 21.36008436308578, 31.068741029431674, 28.886611107920448]
5
data,sig_tot
[38.06224256 23.10966879 25.59818981 27.43395391 28.56876383 28.4199347
 27.46584946 24.97353653 21.36008436 31.06874103 28.88661111 20.96774152
 25.38611024 21.65535778]
[38.06224256 23.10966879 25.59818981 27.43395391 28.56876383 28.4199347
 27.46584946 24.97353653 21.36008436 31.06874103 28.88661111 20.96774152
 25.38611024 21.65535778]
4.623998773841101 3.449093913463348 85.56180553049894
val isze = 0
idinces = [ 3  7 12  6  8  2 10  5 11 13  1  0  4  9]
training loss = 30.934864044189453 500
training loss = 17.713632583618164 1000
training loss = 16.17905044555664 1500
training loss = 15.918560028076172 2000
training loss = 15.779131889343262 2500
training loss = 15.613277435302734 3000
training loss = 15.397165298461914 3500
training loss = 15.116962432861328 4000
training loss = 14.748968124389648 4500
training loss = 14.258520126342773 5000
(1, 14)
(1, 0)
