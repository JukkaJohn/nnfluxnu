data before
[ 0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.          0.          0.          0.          0.
  0.          0.         17.62549039 20.43675217 23.10966879 25.59818981
 27.43395391 28.56876383 28.4199347  27.46584946 24.97353653 21.36008436
 17.39216035 13.67658068 10.44624044  7.32048235  4.56410717  2.64958647
  1.96686448  1.76807184  0.17125836  0.          0.        ]
6
data after
[38.062242560656074, 23.109668787825107, 25.598189806829094, 27.43395391413063, 28.56876383123398, 28.419934704074265, 27.465849455823598, 24.97353652573183, 21.36008436308578, 31.068741029431674, 28.886611107920448]
5
data,sig_tot
[38.06224256 23.10966879 25.59818981 27.43395391 28.56876383 28.4199347
 27.46584946 24.97353653 21.36008436 31.06874103 28.88661111 20.96774152
 25.38611024 21.65535778]
[38.06224256 23.10966879 25.59818981 27.43395391 28.56876383 28.4199347
 27.46584946 24.97353653 21.36008436 31.06874103 28.88661111 20.96774152
 25.38611024 21.65535778]
4.551314178727954 17.549802544937016 55.341662120430335
val isze = 0
idinces = [ 3  7 12  6  8  2 10  5 11 13  1  0  4  9]
training loss = 20.764135360717773 500
training loss = 19.668149948120117 1000
training loss = 19.03036880493164 1500
training loss = 18.741796493530273 2000
training loss = 18.596168518066406 2500
training loss = 18.49408531188965 3000
training loss = 18.390522003173828 3500
training loss = 18.255455017089844 4000
training loss = 18.0477352142334 4500
training loss = 17.670034408569336 5000
(1, 14)
(1, 0)
