data before
[ 0.58736053  3.48994531  6.28174463  8.1707002   9.3623418  10.18766211
 10.63869531 10.70842383 10.82166992 10.68842187 10.54506836 10.21558398
  9.92804004  9.61217676  9.28226563  8.85658887  8.52410059  8.15602686
  7.77344727  7.49398975  7.13096045  6.75358887  6.41394043  6.18030469
  5.87239648  5.61477148  5.27291846  5.01169141  4.78122266  4.54209619
  4.308604    4.12110693  3.91629126  3.73967822  3.57371973  3.34977197
  3.14987158  3.05588867  2.88484131  2.74073633  2.57762231  2.44327832
  2.31567529  2.20909302  2.08969604  1.96149072  1.84261206  1.752271
  1.65112219  1.57000977  1.45894067  1.38685718  1.32668591  1.22374719
  1.15921497  1.10158728  1.02660315  0.94006787  0.893521    0.85035962
  0.77210437  0.72341583  0.69646997  0.59635864  0.57597272  0.53788922
  0.4785433   0.45648047  0.41985831  0.39739334  0.38332248  0.33981812
  0.30887094  0.27267654  0.25045419  0.23358508  0.20864397  0.20082184
  0.1692247   0.15640422  0.13910237  0.13014287  0.10755015  0.1107094
  0.09425523  0.08763971  0.07934152  0.06753456  0.05820401  0.04998888
  0.04593969  0.04365994  0.03422952  0.02907339  0.02700648  0.0226212
  0.0180115   0.01751235  0.01375868  0.01141645]
40
data after
[27.892092468261723, 20.826357421875002, 21.53009375, 21.233490234374997, 20.1436240234375, 27.75103125, 24.45357470703125, 21.3785390625, 24.0814130859375, 23.91653271484375, 21.850439697265625, 20.316831298828124, 29.511096727371214]
64
data,sig_tot
[27.89209247 20.82635742 21.53009375 21.23349023 20.14362402 27.75103125
 24.45357471 21.37853906 24.08141309 23.91653271 21.8504397  20.3168313
 29.51109673 20.9206669  22.02273779 25.06395703]
[27.89209247 20.82635742 21.53009375 21.23349023 20.14362402 27.75103125
 24.45357471 21.37853906 24.08141309 23.91653271 21.8504397  20.3168313
 29.51109673 20.9206669  22.02273779 25.06395703]
0.308560325549887 3.3953204205701515 37.306139614595715
val isze = 0
idinces = [ 5  3 14  7  6  8 10  2 11 12 15  1  0  4 13  9]
training loss = 2.028810739517212 500
training loss = 2.011096715927124 1000
training loss = 2.004920244216919 1500
(1, 16)
(1, 0)
