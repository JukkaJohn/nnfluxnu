data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.23578827e-02
 1.89425105e-01 1.71908614e+00 4.45237138e+00 8.20367407e+00
 1.32591981e+01 2.13233740e+01 3.03467935e+01 4.57105019e+01
 6.83330915e+01 9.05550353e+01 1.26229980e+02 1.83446760e+02
 2.04611612e+02 2.03450712e+02 1.99434047e+02 1.89164801e+02
 1.71797200e+02 1.44275206e+02 1.10994699e+02 7.83505169e+01
 4.80897271e+01 2.57742304e+01 1.13306993e+01 3.64090142e+00
 7.15239870e-01 7.02579579e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
7
data after
[27.836112690401155, 21.323373965162677, 30.346793533035402, 45.71050188729408, 68.33309151559975, 90.5550352934397, 126.22997966321785, 183.4467600757188, 204.6116121848042, 203.45071155726998, 199.43404664388152, 189.16480084039867, 171.79719967606204, 144.27520611403403, 110.99469914520319, 78.35051685726901, 48.089727106653825, 41.5313289178017]
9
data,sig_tot
[ 27.83611269  21.32337397  30.34679353  45.71050189  68.33309152
  90.55503529 126.22997966 183.44676008 204.61161218 203.45071156
 199.43404664 189.16480084 171.79719968 144.27520611 110.99469915
  78.35051686  48.08972711  41.53132892  21.01226301  31.39028186
  23.55166811  30.32544787  39.67504367  51.93160841  46.20837186
  45.40148373  42.62265521  36.14736333  29.08260661  21.3715337
  33.12897278]
[ 27.83611269  21.32337397  30.34679353  45.71050189  68.33309152
  90.55503529 126.22997966 183.44676008 204.61161218 203.45071156
 199.43404664 189.16480084 171.79719968 144.27520611 110.99469915
  78.35051686  48.08972711  41.53132892  21.01226301  31.39028186
  23.55166811  30.32544787  39.67504367  51.93160841  46.20837186
  45.40148373  42.62265521  36.14736333  29.08260661  21.3715337
  33.12897278]
4.460143859728162 11.465858652335381 24.585202242241632
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 36.99848556518555 500
training loss = 4.208632946014404 1000
training loss = 3.273585081100464 1500
training loss = 2.491832971572876 2000
training loss = 1.9438982009887695 2500
training loss = 1.7510664463043213 3000
training loss = 1.6902785301208496 3500
training loss = 1.6700439453125 4000
training loss = 1.6605236530303955 4500
training loss = 1.6483540534973145 5000
reduced chi^2 level 2 = 1.6483064889907837
Constrained alpha: 2.681056261062622
Constrained beta: 4.317440509796143
Constrained gamma: 7.30136775970459
(1, 31)
(1, 0)
