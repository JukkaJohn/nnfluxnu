data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.23578827e-02
 1.89425105e-01 1.71908614e+00 4.45237138e+00 8.20367407e+00
 1.32591981e+01 2.13233740e+01 3.03467935e+01 4.57105019e+01
 6.83330915e+01 9.05550353e+01 1.26229980e+02 1.83446760e+02
 2.04611612e+02 2.03450712e+02 1.99434047e+02 1.89164801e+02
 1.71797200e+02 1.44275206e+02 1.10994699e+02 7.83505169e+01
 4.80897271e+01 2.57742304e+01 1.13306993e+01 3.64090142e+00
 7.15239870e-01 7.02579579e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
7
data after
[27.836112690401155, 21.323373965162677, 30.346793533035402, 45.71050188729408, 68.33309151559975, 90.5550352934397, 126.22997966321785, 183.4467600757188, 204.6116121848042, 203.45071155726998, 199.43404664388152, 189.16480084039867, 171.79719967606204, 144.27520611403403, 110.99469914520319, 78.35051685726901, 48.089727106653825, 41.5313289178017]
9
data,sig_tot
[ 27.83611269  21.32337397  30.34679353  45.71050189  68.33309152
  90.55503529 126.22997966 183.44676008 204.61161218 203.45071156
 199.43404664 189.16480084 171.79719968 144.27520611 110.99469915
  78.35051686  48.08972711  41.53132892  21.01226301  31.39028186
  23.55166811  30.32544787  39.67504367  51.93160841  46.20837186
  45.40148373  42.62265521  36.14736333  29.08260661  21.3715337
  33.12897278]
[ 27.83611269  21.32337397  30.34679353  45.71050189  68.33309152
  90.55503529 126.22997966 183.44676008 204.61161218 203.45071156
 199.43404664 189.16480084 171.79719968 144.27520611 110.99469915
  78.35051686  48.08972711  41.53132892  21.01226301  31.39028186
  23.55166811  30.32544787  39.67504367  51.93160841  46.20837186
  45.40148373  42.62265521  36.14736333  29.08260661  21.3715337
  33.12897278]
4.405853855269232 5.240246743186258 38.5281713532281
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 28.660463333129883 500
training loss = 23.449369430541992 1000
training loss = 2.214712142944336 1500
training loss = 2.000711441040039 2000
training loss = 1.9432514905929565 2500
training loss = 1.8543928861618042 3000
training loss = 1.7370795011520386 3500
training loss = 1.6710541248321533 4000
training loss = 1.6400761604309082 4500
training loss = 1.6230889558792114 5000
reduced chi^2 level 2 = 1.6230679750442505
Constrained alpha: 3.835599660873413
Constrained beta: 3.9989116191864014
Constrained gamma: 33.19615173339844
(1, 31)
(1, 0)
