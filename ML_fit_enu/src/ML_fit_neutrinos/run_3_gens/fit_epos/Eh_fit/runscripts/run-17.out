data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 6.66504634e-04 1.53757010e-02
 1.86590361e-01 1.75755461e+00 4.48697112e+00 8.05560689e+00
 1.33473727e+01 2.12851205e+01 3.06141378e+01 4.63080121e+01
 6.97512243e+01 9.28790753e+01 1.30610027e+02 1.92297352e+02
 2.14174085e+02 2.15187220e+02 2.12834838e+02 2.02477458e+02
 1.86225784e+02 1.60054195e+02 1.26668245e+02 9.29966552e+01
 6.30570170e+01 3.73905832e+01 1.80035923e+01 6.19114324e+00
 1.31728862e+00 1.14058502e-01 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
5
data after
[27.850137940785466, 21.28512053373237, 30.614137806953632, 46.30801212482875, 69.75122433080725, 92.87907530936201, 130.61002654275055, 192.29735215396678, 214.1740851122954, 215.18722024316628, 212.8348381085473, 202.47745754174113, 186.22578388062132, 160.05419542613342, 126.66824493926612, 92.99665516746248, 63.05701704670814, 37.39058319213591, 25.626082643208598]
9
data,sig_tot
[ 27.85013794  21.28512053  30.61413781  46.30801212  69.75122433
  92.87907531 130.61002654 192.29735215 214.17408511 215.18722024
 212.83483811 202.47745754 186.22578388 160.05419543 126.66824494
  92.99665517  63.05701705  37.39058319  25.62608264  32.04792297
  38.66904638  28.29923073  37.87329877  49.78399599  45.02585241
  44.52792172  42.51664031  36.73201773  29.99038634  22.59757681
  40.03094479]
[ 27.85013794  21.28512053  30.61413781  46.30801212  69.75122433
  92.87907531 130.61002654 192.29735215 214.17408511 215.18722024
 212.83483811 202.47745754 186.22578388 160.05419543 126.66824494
  92.99665517  63.05701705  37.39058319  25.62608264  32.04792297
  38.66904638  28.29923073  37.87329877  49.78399599  45.02585241
  44.52792172  42.51664031  36.73201773  29.99038634  22.59757681
  40.03094479]
3.680006901017574 5.260371392124563 81.8071794934205
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 2.176938533782959 500
training loss = 1.3386001586914062 1000
training loss = 1.2997678518295288 1500
training loss = 1.2485787868499756 2000
training loss = 1.1773386001586914 2500
training loss = 1.1032661199569702 3000
training loss = 1.027546763420105 3500
training loss = 0.8739979267120361 4000
training loss = 0.8568918704986572 4500
training loss = 0.8527159094810486 5000
reduced chi^2 level 2 = 0.852703332901001
Constrained alpha: 2.8643369674682617
Constrained beta: 4.538059711456299
Constrained gamma: 12.295132637023926
(1, 31)
(1, 0)
