data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.47565511e+02 1.72230840e+02 1.97499956e+02 2.21795457e+02
 2.46654492e+02 2.68575559e+02 2.86674697e+02 2.98739968e+02
 3.00813673e+02 2.91920419e+02 2.71025396e+02 2.37799213e+02
 1.92828217e+02 1.37303026e+02 8.29962610e+01 3.71485721e+01
 1.14049175e+01 1.27155245e+00 8.70179549e-03 0.00000000e+00
 0.00000000e+00]
5
data after
[147.56551125643458, 172.23084018395252, 197.49995616115712, 221.79545670522518, 246.65449165227122, 268.5755594392394, 286.67469713071057, 298.7399678966218, 300.81367283944854, 291.9204192868681, 271.02539648442945, 237.79921263735164, 192.82821747421158, 137.3030264200614, 82.99626099298044, 49.83374380206182]
6
data,sig_tot
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
1.6794696896754835 12.114537906003719 99.35320782585936
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 6.154898166656494 500
training loss = 4.309981346130371 1000
training loss = 1.4216458797454834 1500
training loss = 1.093241810798645 2000
training loss = 1.0614209175109863 2500
training loss = 1.0472239255905151 3000
training loss = 1.0367748737335205 3500
training loss = 1.0279719829559326 4000
training loss = 1.0210422277450562 4500
training loss = 1.0142184495925903 5000
reduced chi^2 level 2 = 1.0142086744308472
Constrained alpha: 2.402310848236084
Constrained beta: 1.8732846975326538
Constrained gamma: 53.314659118652344
(1, 31)
(1, 0)
