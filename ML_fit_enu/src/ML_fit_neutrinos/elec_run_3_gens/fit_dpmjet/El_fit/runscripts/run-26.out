data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.47565511e+02 1.72230840e+02 1.97499956e+02 2.21795457e+02
 2.46654492e+02 2.68575559e+02 2.86674697e+02 2.98739968e+02
 3.00813673e+02 2.91920419e+02 2.71025396e+02 2.37799213e+02
 1.92828217e+02 1.37303026e+02 8.29962610e+01 3.71485721e+01
 1.14049175e+01 1.27155245e+00 8.70179549e-03 0.00000000e+00
 0.00000000e+00]
5
data after
[147.56551125643458, 172.23084018395252, 197.49995616115712, 221.79545670522518, 246.65449165227122, 268.5755594392394, 286.67469713071057, 298.7399678966218, 300.81367283944854, 291.9204192868681, 271.02539648442945, 237.79921263735164, 192.82821747421158, 137.3030264200614, 82.99626099298044, 49.83374380206182]
6
data,sig_tot
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
0.7355186646189033 5.613301525597716 46.942081859446304
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 2.972297430038452 500
training loss = 2.601962089538574 1000
training loss = 2.377706527709961 1500
training loss = 1.7632516622543335 2000
training loss = 1.6554073095321655 2500
training loss = 1.5814203023910522 3000
training loss = 1.5252197980880737 3500
training loss = 1.4931384325027466 4000
training loss = 1.4748343229293823 4500
training loss = 1.4666595458984375 5000
reduced chi^2 level 2 = 1.4666473865509033
Constrained alpha: 3.3671042919158936
Constrained beta: 2.5277953147888184
Constrained gamma: 10.234108924865723
(1, 31)
(1, 0)
