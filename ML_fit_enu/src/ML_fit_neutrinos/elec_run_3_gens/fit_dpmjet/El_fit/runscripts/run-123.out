data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.47565511e+02 1.72230840e+02 1.97499956e+02 2.21795457e+02
 2.46654492e+02 2.68575559e+02 2.86674697e+02 2.98739968e+02
 3.00813673e+02 2.91920419e+02 2.71025396e+02 2.37799213e+02
 1.92828217e+02 1.37303026e+02 8.29962610e+01 3.71485721e+01
 1.14049175e+01 1.27155245e+00 8.70179549e-03 0.00000000e+00
 0.00000000e+00]
5
data after
[147.56551125643458, 172.23084018395252, 197.49995616115712, 221.79545670522518, 246.65449165227122, 268.5755594392394, 286.67469713071057, 298.7399678966218, 300.81367283944854, 291.9204192868681, 271.02539648442945, 237.79921263735164, 192.82821747421158, 137.3030264200614, 82.99626099298044, 49.83374380206182]
6
data,sig_tot
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
[147.56551126 172.23084018 197.49995616 221.79545671 246.65449165
 268.57555944 286.67469713 298.7399679  300.81367284 291.92041929
 271.02539648 237.79921264 192.82821747 137.30302642  82.99626099
  49.8337438   24.42778292  29.93803556  36.25756474  43.04022468
  50.46464033  58.01947009  65.03812866  70.59556837  74.36903186
  74.56794914  70.69458045  62.32359483  50.07121433  35.41821426
  34.11287203]
3.3599580235626414 13.378413901135904 30.221349158638965
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 3.5780551433563232 500
training loss = 1.751914381980896 1000
training loss = 1.3315407037734985 1500
training loss = 1.237825632095337 2000
training loss = 1.217644453048706 2500
reduced chi^2 level 2 = 1.2129706144332886
Constrained alpha: 2.3707926273345947
Constrained beta: 3.8992395401000977
Constrained gamma: 8.833314895629883
(1, 31)
(1, 0)
