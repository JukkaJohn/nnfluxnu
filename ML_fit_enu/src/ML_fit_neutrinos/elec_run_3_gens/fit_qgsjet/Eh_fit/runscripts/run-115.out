data before
[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 0.00000000e+00 0.00000000e+00 0.00000000e+00 1.23578827e-02
 1.89425105e-01 1.71908614e+00 4.45237138e+00 8.20367407e+00
 1.32591981e+01 2.13233740e+01 3.03467935e+01 4.57105019e+01
 6.83330915e+01 9.05550353e+01 1.26229980e+02 1.83446760e+02
 2.04611612e+02 2.03450712e+02 1.99434047e+02 1.89164801e+02
 1.71797200e+02 1.44275206e+02 1.10994699e+02 7.83505169e+01
 4.80897271e+01 2.57742304e+01 1.13306993e+01 3.64090142e+00
 7.15239870e-01 7.02579579e-02 0.00000000e+00 0.00000000e+00
 0.00000000e+00]
7
data after
[27.836112690401155, 21.323373965162677, 30.346793533035402, 45.71050188729408, 68.33309151559975, 90.5550352934397, 126.22997966321785, 183.4467600757188, 204.6116121848042, 203.45071155726998, 199.43404664388152, 189.16480084039867, 171.79719967606204, 144.27520611403403, 110.99469914520319, 78.35051685726901, 48.089727106653825, 41.5313289178017]
9
data,sig_tot
[ 27.83611269  21.32337397  30.34679353  45.71050189  68.33309152
  90.55503529 126.22997966 183.44676008 204.61161218 203.45071156
 199.43404664 189.16480084 171.79719968 144.27520611 110.99469915
  78.35051686  48.08972711  41.53132892  21.01226301  31.39028186
  23.55166811  30.32544787  39.67504367  51.93160841  46.20837186
  45.40148373  42.62265521  36.14736333  29.08260661  21.3715337
  33.12897278]
[ 27.83611269  21.32337397  30.34679353  45.71050189  68.33309152
  90.55503529 126.22997966 183.44676008 204.61161218 203.45071156
 199.43404664 189.16480084 171.79719968 144.27520611 110.99469915
  78.35051686  48.08972711  41.53132892  21.01226301  31.39028186
  23.55166811  30.32544787  39.67504367  51.93160841  46.20837186
  45.40148373  42.62265521  36.14736333  29.08260661  21.3715337
  33.12897278]
4.366710823631204 7.84722898767787 85.66110884826404
val isze = 0
idinces = [20  7  5  2  3 21 13 27 12  1 19 14 18  6 11 23 24 28 22 10 26 30  8 25
 16 17  0 15  4 29  9]
training loss = 38.89445877075195 500
training loss = 34.4859619140625 1000
training loss = 8.815836906433105 1500
training loss = 4.7536540031433105 2000
training loss = 3.4397144317626953 2500
training loss = 2.7031211853027344 3000
training loss = 2.2913992404937744 3500
training loss = 1.9835540056228638 4000
training loss = 1.3644777536392212 4500
training loss = 1.0141911506652832 5000
reduced chi^2 level 2 = 1.0139497518539429
Constrained alpha: 3.944323778152466
Constrained beta: 4.066714286804199
Constrained gamma: 71.412353515625
(1, 31)
(1, 0)
