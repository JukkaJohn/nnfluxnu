data before
[  0.           0.           0.           0.           0.
   0.           0.           0.           0.           0.
   0.           0.           0.           0.           0.
   0.           0.           0.           0.           0.
 128.70353686 148.78283719 168.15354617 187.22370306 202.75516579
 214.93512338 219.19366606 217.96930231 205.64917786 184.28630033
 155.71763212 123.41973493  90.76418067  62.18380302  39.84537931
  25.29659514  19.9822009   18.03774272   1.74792816   0.
   0.        ]
3
data after
[128.70353685897828, 148.78283719064714, 168.15354617245674, 187.2237030576873, 202.75516579301834, 214.93512337660792, 219.19366606149674, 217.96930231076, 205.64917785764695, 184.2863003275919, 155.71763212081194, 123.41973492524028, 90.76418066874594, 62.183803022389114, 39.845379313286394, 25.296595141325145, 39.76787178337821]
8
data,sig_tot
[128.70353686 148.78283719 168.15354617 187.22370306 202.75516579
 214.93512338 219.19366606 217.96930231 205.64917786 184.28630033
 155.71763212 123.41973493  90.76418067  62.18380302  39.84537931
  25.29659514  39.76787178  41.12103908  26.48390527  30.94695871
  34.91469695  37.99947578  39.94603801  40.14426542  37.5465598
  32.90800041  26.5003386   43.19923573]
[128.70353686 148.78283719 168.15354617 187.22370306 202.75516579
 214.93512338 219.19366606 217.96930231 205.64917786 184.28630033
 155.71763212 123.41973493  90.76418067  62.18380302  39.84537931
  25.29659514  39.76787178  41.12103908  26.48390527  30.94695871
  34.91469695  37.99947578  39.94603801  40.14426542  37.5465598
  32.90800041  26.5003386   43.19923573]
4.6181061215566785 9.760126232302888 78.00708377944254
val isze = 0
idinces = [ 7 21  5  2 13 19 11 12  1 22 25 14 18  3 26  6 20 10 24 27  8 23 16 17
  0 15  4  9]
training loss = 63.71638107299805 500
training loss = 62.295860290527344 1000
training loss = 61.751380920410156 1500
training loss = 61.106475830078125 2000
training loss = 60.235076904296875 2500
training loss = 59.02361297607422 3000
training loss = 57.21971130371094 3500
training loss = 54.20969009399414 4000
training loss = 48.529151916503906 4500
training loss = 37.795005798339844 5000
(1, 28)
(1, 0)
