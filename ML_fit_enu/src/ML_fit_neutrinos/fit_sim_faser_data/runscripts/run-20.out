LHAPDF 6.5.4 loading /data/theorie/jjohn/miniconda3/envs/nnpdf_dev/share/LHAPDF/FASERv_EPOS+POWHEG_7TeV/FASERv_EPOS+POWHEG_7TeV_0000.dat
FASERv_EPOS+POWHEG_7TeV PDF set, member #0, version 1; LHAPDF ID = 70001000
LHAPDF 6.5.4 loading /data/theorie/jjohn/miniconda3/envs/nnpdf_dev/share/LHAPDF/FASERv_EPOS+POWHEG_7TeV/FASERv_EPOS+POWHEG_7TeV_0000.dat
FASERv_EPOS+POWHEG_7TeV PDF set, member #0, version 1; LHAPDF ID = 70001000
1.0141292155472226 8.688131840478041 91.30055829556812
val isze = 0
idinces = [2 5 0 3 4 1]
training loss = 6.105921745300293 100
training loss = 1.30788254737854 200
training loss = 1.2908194065093994 300
training loss = 1.2878059148788452 400
training loss = 1.284471869468689 500
training loss = 1.2809041738510132 600
training loss = 1.2771739959716797 700
training loss = 1.2733447551727295 800
training loss = 1.269478678703308 900
training loss = 1.265629529953003 1000
training loss = 1.2618389129638672 1100
training loss = 1.2581549882888794 1200
training loss = 1.25460684299469 1300
training loss = 1.2512226104736328 1400
training loss = 1.248023509979248 1500
training loss = 1.24589204788208 1600
training loss = 1.242347002029419 1700
training loss = 1.2403755187988281 1800
training loss = 1.2376002073287964 1900
training loss = 1.2354414463043213 2000
training loss = 1.239274501800537 2100
training loss = 1.2318181991577148 2200
training loss = 1.2300416231155396 2300
training loss = 1.2372143268585205 2400
training loss = 1.2266120910644531 2500
training loss = 1.222973108291626 2600
training loss = 1.218493938446045 2700
training loss = 3.116520404815674 2800
training loss = 1.2152302265167236 2900
rep 1 done out of 1
reduced chi^2 level 2 = 1.2063297033309937
Constrained alpha: 1.8775476217269897
Constrained beta: 10.489548683166504
Constrained gamma: 67.67212677001953
(1, 6)
(1, 0)
Thanks for using LHAPDF 6.5.4. Please make sure to cite the paper:
  Eur.Phys.J. C75 (2015) 3, 132  (http://arxiv.org/abs/1412.7420)
